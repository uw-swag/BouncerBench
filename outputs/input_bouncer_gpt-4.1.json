{
    "astropy__astropy-11693": {
        "explanation": "The issue is well-specified. It describes a concrete bug (NoConvergence error when plotting WCS with non-linear distortions), provides a minimal reproducible example, details the expected behavior (plot should be produced, as happens when 'quiet=True' is set), and gives the actual error output. The request is clear: make it possible to get the plot without hacking the library code, likely by allowing the 'quiet' parameter to be set or by handling NoConvergence more gracefully in the plotting context. All necessary information for a meaningful attempt at a solution is present.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12544": {
        "explanation": "The issue provides a clear description of the current behavior (automatic masking when reading tables with NaN values in Astropy 5), the problem this causes for certain workflows, and a concrete suggestion for a solution (adding a keyword argument like mask=False to Table.read to disable masking). While it does not specify every detail of the implementation, it is clear what the desired outcome is: a way to opt out of automatic masking when reading tables. This is sufficient for an experienced developer to attempt a solution, possibly by adding the keyword and ensuring the behavior is as described.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12825": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the observed error, and the expected behavior. It specifies that aggregation should work on Table columns, and only fail for columns where the operation does not make sense (e.g., SkyCoord with np.mean). The stack trace and system details are included. The only minor ambiguity is exactly how to handle columns that cannot be aggregated (should they be dropped, set to None, etc.), but the main requirement is clear: aggregation should not fail for the whole operation due to a single incompatible column. This is a sensible and actionable interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-12842": {
        "explanation": "The issue provides a clear description of the problem: after upgrading Astropy from 4.2.1 to 5.0+, ECSV files with a 'datetime64' column written by the BinnedTimeSeries class can no longer be read due to stricter type checking. The error message, relevant code references, and a detailed reproduction path (including code snippets and a sample header) are all provided. The user also suggests a possible solution direction (relaxing type checking or allowing 'datetime64'). The expected and actual behaviors are well described. This is sufficient for an experienced developer to attempt a solution, such as modifying the allowed types or providing a migration path.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12880": {
        "explanation": "The issue is well-specified: it describes a clear regression (reading ECSV files with a datetime64 column written by Astropy 4.2.1 fails in 5.0+), provides the relevant error message, points to the commit/PR that introduced the breaking change, and gives a concrete example of the file header and the code used to read/write. The user also suggests a possible solution (relaxing type checking or allowing datetime64), and provides system details. There is enough information to reproduce the problem and to attempt a fix (e.g., by updating the allowed types in ECSV parsing).",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12891": {
        "explanation": "The issue describes a missing feature: the 'where' keyword argument of np.mean is not supported for astropy.units.Quantity instances. It provides context about when 'where' was added to numpy and asks if there is a limitation or if it could be implemented. However, it does not specify exactly what the expected behavior should be, how errors manifest, or what tests would constitute success. There is a sensible interpretation (add support for 'where' in Quantity.mean), but some details would need to be filled in by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-12907": {
        "explanation": "The issue provides a clear description of the problem: the separability_matrix function does not compute separability correctly for nested CompoundModels. It includes concrete code examples showing the expected and actual outputs for both flat and nested compound models. The expected behavior is implied by the outputs of the non-nested case, and the unexpected behavior is clearly demonstrated in the nested case. While the issue author expresses some uncertainty ('I might be missing something'), the bug is well-demonstrated and the desired outcome (consistency with the non-nested case) is clear. Therefore, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12962": {
        "explanation": "The issue is well-described, with a clear explanation of the current behavior, the problem (lack of a direct way to convert CCDData to ImageHDU), and concrete proposed solutions (three interface options). The user provides code examples, context, and analogies to similar functionality (Table/BinTableHDU), making it clear what is expected. There is some flexibility in implementation (which interface to choose), but the core requirement\u2014enabling direct conversion from CCDData to ImageHDU\u2014is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13032": {
        "explanation": "The issue provides a clear description of the bug, including the expected and actual behavior, as well as code snippets to reproduce the problem. It specifies which input to the ModelBoundingBox is being ignored incorrectly, and contrasts the behavior with another input that works as expected. The context is sufficient to understand what needs to be fixed: the handling of the 'ignored' parameter for certain inputs. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13033": {
        "explanation": "The issue provides a clear description of the problem: when a required column is removed from a TimeSeries object, the resulting exception is misleading. The expected and actual behaviors are both described, and a minimal reproducible code example is provided. The desired outcome (an informative exception about missing required columns) is specified. This is sufficient information to attempt a solution, such as improving the exception message when required columns are missing.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13068": {
        "explanation": "The issue provides a clear description of the problem: converting a timestamp from ISO format to Julian Date and back using astropy.time.Time does not yield the original value, and the user is concerned about the loss of precision. The user provides a concrete code example, the expected behavior (round-tripping should preserve the original value), and the actual result. However, the issue does not specify whether the solution should be a documentation update, a bug fix in astropy, or a workaround in user code. There is also some ambiguity about whether the user expects bit-for-bit reversibility (which may not be possible due to floating point limitations) or just higher precision. Still, a reasonable interpretation is to investigate and improve the round-trip precision or document the limitations. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13075": {
        "explanation": "The issue provides a clear objective: register an HTML format for Cosmology.write that outputs a table with MathJax formatting. It references existing IO patterns and documentation, and suggests a concrete approach (using cosmology.io.table.to_table, formatting with MathJax, and writing with QTable.write). The edit about storing MathJax representations on Parameter objects is a specific implementation detail. While there is some room for interpretation in 'nice MathJax', the overall requirements and workflow are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13132": {
        "explanation": "The issue provides a clear objective: to add __array_func__ support for astropy.time.Time, referencing a specific GitHub issue for context. It gives a concrete example of desired behavior (using np.linspace with Time objects), and mentions that tests, documentation, and changelog entries are needed. While it does not specify every detail of the implementation, the intent and expected outcome are clear, and the referenced issue likely provides further context. This is sufficient for an experienced developer familiar with Astropy and NumPy protocols to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13158": {
        "explanation": "The issue provides a clear description of the bug: model evaluation fails when a model parameter is a MagUnit type value. It includes a minimal reproducible example, the exact error traceback, and points to the likely problematic code section. The expected behavior is also specified (the model should evaluate cleanly to a MagUnit quantity). While the issue author requests input from those familiar with astropy.units for the best approach, the problem and its context are well-defined, and a solution attempt (e.g., improving logic in _param_sets to handle MagUnit) is possible without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13234": {
        "explanation": "The issue provides a clear description of the bug: structured columns with a field named 'name' cannot be round-tripped through ECSV serialization in astropy. It includes a minimal reproducible code example, the actual and expected behavior, and the full traceback of the error. The context and the area of code affected are clear, and there is even a suggestion for a possible direction to fix the serialization format. This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13236": {
        "explanation": "The issue provides a clear description of the current behavior (auto-transforming structured np.array into NdarrayMixin when added to a Table), the historical rationale, and a concrete proposal for change. The proposal includes two actionable steps: (1) add a FutureWarning to inform users of the upcoming change, and (2) remove the auto-transform behavior in a future version (5.2). The code snippet to be modified is provided, and the desired user-facing behavior is specified. While some implementation details (e.g., exact wording of the warning) are left to the implementer, the intent and scope are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13306": {
        "explanation": "The issue provides a clear description of the bug (vstack fails with structured array columns), a minimal reproducible code example, and the exact error message. The expected behavior is implied (vstack should work with structured array columns as it does with regular columns). The system details are also provided. While the issue does not specify the exact desired fix, it is clear what the problem is and what a successful solution would look like: making vstack work for structured array columns without the casting error. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13390": {
        "explanation": "The issue provides detailed test failures, including stack traces, error messages, and the context of the failing tests. It is clear that the failures are related to changes in numpy 1.23.0rc3, specifically around comparison operations between unicode and bytes columns in astropy.table. However, the issue does not specify what the expected behavior should be in these cases, nor does it state whether the goal is to restore previous behavior, update the tests, or change the implementation to match new numpy behavior. While a developer could make a reasonable attempt to fix the compatibility issues based on the errors and the test intent, some interpretation is required regarding the desired outcome. Thus, the issue is not perfectly specified, but it is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13398": {
        "explanation": "The issue provides a clear description of the problem (the need for a direct ITRS <-> Observed transformation that stays within ITRS and avoids unintuitive behavior), the motivation (recurring user confusion and technical pitfalls with current transforms), and a concrete, working code example that implements the proposed solution. The expected behavior is well-described, and the code is ready for review and integration, with only minor optional enhancements (like refraction) mentioned. The issue is actionable and specific enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13404": {
        "explanation": "The issue provides a clear description of the problem: the _array2string dispatch function in masked.function_helpers fails on masked arrays with structured dtypes, while it works for non-masked arrays. It includes a code example to reproduce the error, specifies the expected behavior, and provides system details. The only minor ambiguity is the exact error message, but the reproduction steps are sufficient for an experienced engineer to investigate and attempt a fix. Therefore, the issue is well-specified for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13453": {
        "explanation": "The issue is well-specified. It clearly describes the problem: the 'formats' argument to astropy.table.Table.write() is ignored when outputting to HTML, but works for other formats. The expected and actual behaviors are described, and reproducible code examples are provided. The desired outcome (HTML output should respect the 'formats' argument) is explicit. There is no ambiguity about what needs to be fixed or how to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13462": {
        "explanation": "The issue provides a failing test case, a link to the relevant code, and a description of the problem (the test fails under pyinstaller because the computed error is different depending on the order of the arguments). However, it does not specify what the expected behavior should be, nor does it clarify whether the test or the implementation is at fault, or what the correct fix would be. There is ambiguity about whether the test should be changed, the implementation fixed, or if this is an acceptable edge case. Thus, while there is enough information to investigate, there is significant ambiguity about what constitutes a successful solution.",
        "label": "VAGUE"
    },
    "astropy__astropy-13469": {
        "explanation": "The issue describes a specific problem: converting a list of Astropy Table objects into a NumPy array with dtype=object, and encountering a ValueError due to the way Table.__array__() handles dtype arguments. The issue also links to a StackOverflow post for context and suggests a possible change: allowing dtype=object to pass through in __array__(). However, it does not specify exactly what the desired behavior should be (e.g., should __array__() return self, or something else, when dtype=object?), nor does it specify what tests or use cases should be supported. There is a sensible interpretation (allow dtype=object in __array__() and allow conversion to work), but some implementation details are left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13477": {
        "explanation": "The issue provides a clear description of the problem: comparing a Frame with data to a SkyCoord with the same data raises an exception, but the user expects a boolean result (True/False) based on data equality. The issue includes a reproducible code example, the actual and expected behavior, and the traceback. While it does not specify the exact implementation details of the fix, it is clear what the desired outcome is: 'frame == coord' should return True or False, not raise. This is sufficient for a meaningful attempt at a solution, as the engineer can interpret that the equality operator should be made to handle this case gracefully.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13579": {
        "explanation": "The issue provides a clear description of the observed bug, including the context (3D WCS with a non-trivial PCij matrix), the expected and actual behaviors, and a minimal reproducible code example. It also details the system environment and references a related issue. The steps to reproduce are explicit, and the difference between expected and actual results is well-documented. This is sufficient for an experienced developer to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13668": {
        "explanation": "The issue provides a clear description of the problem: 'wcslint' crashes on valid WCS files. It includes specific references to the codebase (with links to relevant lines), multiple concrete example files that trigger the bug, and the exact error messages and stack traces observed. The expected behavior is implied: 'wcslint' should not crash on these files, and should instead validate them gracefully, reporting warnings or errors as appropriate. While the root cause is not fully diagnosed, the issue is well-specified in terms of reproducibility and scope, making it possible to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13745": {
        "explanation": "The issue provides a clear description of the problem: the float32 representation of pi/2 is slightly greater than pi/2, causing Astropy's Latitude class to reject it as out of bounds. The issue includes code snippets to reproduce the problem, the error message, and a suggestion for a possible solution (be lenient, e.g., compare up to float32 precision). While the exact implementation details (e.g., how lenient to be, or whether to round or clip) are not specified, the intent is clear and there is a sensible interpretation for a solution: adjust the validation logic to account for float32 precision limitations. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13803": {
        "explanation": "The issue provides a clear description of the problem: the float32 representation of pi/2 is slightly greater than pi/2, causing Astropy's Latitude class to reject it due to strict float64-based bounds checking. The issue includes a reproducible code snippet, the observed error, and a suggestion for a possible solution (be more lenient, e.g., compare up to float32 precision). While the exact implementation of 'leniency' is not strictly specified, the intent is clear and there is enough information to make a reasonable attempt at a solution (e.g., allow a small epsilon tolerance or match the input's precision).",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13838": {
        "explanation": "The issue provides a clear description of the bug: printing an Astropy Table with 0-length array cells causes an IndexError. The user supplies a minimal reproducible example, the expected and actual behavior, and a full traceback. The problem is well-scoped (printing tables with 0-length array cells), and the error is reproducible with the provided code. The expected outcome (table should print without error) is also specified. This is sufficient for a developer to attempt a fix and write a test for the scenario.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13933": {
        "explanation": "The issue provides a clear description of the bug: after pickling and unpickling an Angle object with hourangle units, calling to_string() fails with a specific ValueError. The issue includes a minimal reproducible code example, expected and actual behavior, and relevant system details. The problem is well-scoped and actionable for a developer familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13977": {
        "explanation": "The issue is a well-articulated feature request or behavioral change proposal for astropy.units.Quantity.__array_ufunc__. The user provides a clear description of the current behavior, a minimal reproducible example, the error encountered, and a rationale for why returning NotImplemented would be preferable (to allow fallback to __radd__). The expected change is unambiguous: modify __array_ufunc__ to return NotImplemented instead of raising ValueError when inputs are incompatible. This is a clear, actionable request for a code change.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14096": {
        "explanation": "The issue is clearly described: when subclassing SkyCoord and adding a property that accesses a non-existent attribute, the AttributeError message incorrectly states that the property itself does not exist, rather than the missing attribute. The code sample and the actual error message are provided, and the expected behavior is implied (the error should mention 'random_attr', not 'prop'). This is enough information to investigate and attempt a fix in the SkyCoord class's __getattr__ implementation.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14182": {
        "explanation": "The issue clearly describes the desired feature: supporting the 'header_rows' argument in the 'ascii.rst' (RestructuredText) output format for Astropy tables, similar to how it works for 'ascii.fixed_width'. It provides example code, expected output, and the current error. The goal is unambiguous: make 'header_rows' work for 'ascii.rst' output. The only minor ambiguity is the exact formatting of multiple header rows in RST, but the examples and context make a sensible implementation possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14253": {
        "explanation": "The issue is primarily a discussion prompt about the correct semantics for propagating the `info` attribute in the context of the `Quantity` class, especially regarding the use of `__array_finalize__`. It lists some cases and asks for opinions on which behaviors are reasonable, but it does not specify a concrete change to make, nor does it define acceptance criteria for a solution. The issue is open-ended and lacks a clear, actionable request.",
        "label": "VAGUE"
    },
    "astropy__astropy-14295": {
        "explanation": "The issue provides a clear description of the problem: when both SIP and PV keywords are present in a FITS header, astropy.wcs ignores the PV keywords, treating them as redundant, which is not the desired behavior. The issue includes a concrete example header, code to reproduce the problem, expected and actual behavior (with images), and a discussion of the relevant code (the _fix_scamp function). The desired outcome is clear: astropy should not ignore PV keywords when SIP is present, and the suggestion is to remove the heuristic code that causes this. There is enough information to attempt a meaningful solution, such as modifying or removing the _fix_scamp function and ensuring that both SIP and PV keywords are handled appropriately.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14309": {
        "explanation": "The issue provides a clear error traceback, a minimal reproducible example, and context about a recent code change that likely introduced the bug. It specifies the function and the scenario in which the error occurs, and even includes a code snippet that triggers the problem. The expected behavior (no IndexError) is implied, and the root cause is suggested. This is sufficient information for an experienced developer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14365": {
        "explanation": "The issue clearly describes the problem: the ascii.qdp reader in astropy expects QDP commands to be uppercase, but QDP itself is case-insensitive. The expected behavior is that commands in any case (e.g., 'read serr 1 2') should be accepted. The issue provides a concrete example, steps to reproduce, the observed error, and the expected outcome. The required change is to make the command parsing case-insensitive. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14369": {
        "explanation": "The issue provides a clear description of the problem: astropy.table's ascii.cds reader is incorrectly parsing composite units from MRT (CDS format) files. It specifies the expected behavior (units in the resulting Table should match the input), provides a concrete example input file, shows the incorrect output, and explains exactly what is wrong (the order of units in the parsed output is incorrect). The issue also references the relevant standard and provides version information. This is sufficient for a developer to reproduce the problem and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14371": {
        "explanation": "The issue clearly requests adding an 'atol' (absolute tolerance) argument to the 'is_O3' function, or alternatively, to use the maximum precision of the matrix's dtype instead of the hardcoded 1e-15. The relevant code location is provided. While it leaves some discretion (add argument or change default), the intent and the required change are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14379": {
        "explanation": "The issue clearly describes the current behavior of Angle.to_string (no space between value and unit), provides a code example, and requests a feature to optionally include a space via a boolean keyword argument. It also raises the question of whether the default should be changed to include a space, as Quantity does. While there is a minor ambiguity about whether to add an option or change the default, the main request (add a boolean keyword argument to control spacing) is clear and actionable. A sensible solution would be to add such an argument, defaulting to the current behavior, and document it. The issue is thus reasonably specified, though not perfectly so due to the open question about changing the default.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14413": {
        "explanation": "The issue provides a clear description of the problem (extraneous space in unit string representations), specifies the affected formats ('unicode' and 'console'), and gives concrete examples of both the current and expected behavior. The expected behavior is explicitly stated: 'unit.to_string(format) should never start with a space, independent of format.' The reproduction steps are clear and actionable. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14439": {
        "explanation": "The issue clearly describes the current behavior (Astropy outputs 'beam-1 Jy' for Jy/beam in FITS string format), the desired behavior (output should be 'Jy/beam' as per common radio astronomy convention), and why this is a problem (parsing and convention). It also provides context and references to related issues. The required change is to adjust the FITS string representation of the unit. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14508": {
        "explanation": "The issue provides a clear description of the problem (float string representation in io.fits.Card is unnecessarily long, causing comment truncation), a concrete example with code and a test file, the root cause in the codebase, and a suggested direction for a fix. The expected behavior is also well-defined: being able to create any valid FITS Card via io.fits.Card. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14528": {
        "explanation": "The issue clearly describes the problem: creating an ImageHDU with zero-dimensional data leads to a corrupt FITS file. It provides a minimal reproducible example, specifies the expected behavior (should not silently create a corrupt file), and gives environment details. The required solution is clear: prevent or handle zero-dimensional data in ImageHDU to avoid corrupt files. There is little ambiguity about what needs to be fixed or how to verify the fix.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14539": {
        "explanation": "The issue provides a clear description of the bug, including the context (comparing FITS files with VLAs using FITSDiff), the expected behavior, and a minimal reproducible example. The suspected cause (VLA handling) is mentioned, but even if that is not the root cause, the bug is well-demonstrated. The output is shown, and the environment is specified. This is sufficient for a developer to attempt a fix and to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14566": {
        "explanation": "The issue provides a clear description of the problem: the 'decimalyear' format in astropy.time.Time produces errors when applied to a MaskedColumn, while 'jyear' does not. The expected behavior is stated (they should behave similarly), and a minimal reproducible example is provided, including code and environment details. While the exact error message is not included, the code is sufficient to reproduce the problem and investigate the root cause. The issue is well-specified for a developer familiar with astropy and MaskedColumn to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14578": {
        "explanation": "The issue provides a clear description of the problem: writing a Table containing Python objects (dtype 'O') to a FITS file fails, while writing to ASCII works. It includes a minimal reproducible example, the error message, and points out the root cause (FITS writer not handling dtype 'O'). The issue also suggests two possible solutions: either serialize objects as strings or provide a better error message. This gives enough direction for a meaningful attempt at a solution, though the exact choice between the two options may require a design decision. However, implementing either is a reasonable interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14590": {
        "explanation": "The issue provides a detailed traceback of a test failure involving np.fix with numpy-dev, including the exact error message, the context in which it occurs, and the relevant code paths. However, the issue does not specify what the expected behavior should be, what the correct fix is, or what the intended interaction between astropy's masked arrays and numpy's fix function should be. It also mentions that someone is investigating, but does not provide a clear action item or acceptance criteria for resolving the issue. While a developer could investigate and potentially propose a fix based on the traceback, there is ambiguity about the desired outcome and the correct approach, making the issue only reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14598": {
        "explanation": "The issue provides a clear description of the problem (inconsistent handling of double single-quotes in FITS Card values), includes reproducible code examples, expected behavior, and observed incorrect behavior. The context (astropy.io.fits, Card, fromstring, etc.) is well-specified, and the bug is demonstrated with concrete input/output. The goal is to ensure that card values with double single-quotes are handled consistently, so that serialization and deserialization are lossless. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14701": {
        "explanation": "The issue provides a clear objective: register a 'latex' writer for Cosmology.write that exports a Cosmology object as a LaTeX table. It references existing IO patterns and documentation, and suggests a concrete approach (using cosmology.io.table.to_table, formatting column names, and using QTable.write with format='latex'). While some implementation details (e.g., exact formatting of all columns) are left to the implementer, the requirements and context are clear enough for a meaningful solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14702": {
        "explanation": "The issue clearly describes the problem: the string representation of a VOTable object is identical to that of a Table object, which is confusing because they are different types with different behaviors. The reporter suggests a concrete solution: change the representation of the VOTable object so that it displays as <VOTable length=4> instead of <Table length=4>. The code examples provided illustrate the current behavior and the confusion it causes. The desired outcome is unambiguous, and the scope of the change is clear (alter the __repr__ or similar method for the VOTable class).",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14938": {
        "explanation": "The issue clearly states that the Cosmology write methods should support the 'ascii.latex' format key, matching the existing support in Table. The desired outcome is to register the method under 'ascii.latex' in addition to any existing registration. The steps to achieve this are clear: identify the relevant write method(s) and register them with the new label. No significant ambiguities are present.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14966": {
        "explanation": "The issue clearly describes the current behavior (group keys lose their units and are just floats) and the expected behavior (group keys should retain the full quantity, including units). It provides a reproducible code example and specifies the environment. The required change is unambiguous: ensure that group keys for quantity columns in QTable retain their units. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14991": {
        "explanation": "The issue provides a clear description of the suspected bug (incorrect sign in the exponent of an equation in a specific file and line), the expected correction, and the reasoning behind it. The file and line number are specified, and the expected code change is given. While the reproduction steps are placeholders and not detailed, the core of the issue (the code fix) is unambiguous and actionable. Therefore, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14995": {
        "explanation": "The issue provides a clear description of the bug, including the version where it was introduced, the expected behavior, and a detailed reproduction script. It specifies that mask propagation fails when one operand lacks a mask, and that the expected behavior is to copy over the existing mask or behave as in v5.2. The error message is included, and the difference in behavior between versions is highlighted. This is sufficient information to attempt a fix and to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-6938": {
        "explanation": "The issue points out a possible bug in the use of the 'replace' method, suggesting that it may not be operating in-place as intended. It also notes that removing the code does not cause any tests to fail, implying a lack of test coverage. However, the issue does not specify what the correct behavior should be, what the intended effect of the code is, or what the expected output is. There is ambiguity about whether the code should be fixed, removed, or tested, and no clear acceptance criteria are provided. Therefore, while the issue highlights a potential problem, it is too vague to attempt a meaningful solution without further clarification.",
        "label": "VAGUE"
    },
    "astropy__astropy-7008": {
        "explanation": "The issue provides a clear motivation (the need to temporarily set a version of constants), a concrete example of the desired API (a context manager named 'constants_set' that takes a version like 'astropyconst13'), and an expected usage pattern. While it does not specify every implementation detail, it is clear what is required: implement a context manager that sets the constants version within its scope. The codebase and existing constants infrastructure would provide the necessary context for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7166": {
        "explanation": "The issue clearly describes a specific problem: the InheritDocstrings metaclass does not work for properties because it uses `inspect.isfunction`, which does not return True for properties. The expected behavior is implied: the metaclass should also handle properties so that their docstrings are inherited. While it does not specify the exact desired implementation, it is clear what needs to be fixed and why. Therefore, a meaningful solution can be attempted.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7218": {
        "explanation": "The issue clearly describes the current behavior of the HDUList.copy() method, including code examples and the observed output. It is implied that the expected behavior is for HDUList.copy() to return an HDUList instance (not a plain list). The context and the desired change are both clear, making it straightforward to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7336": {
        "explanation": "The issue provides a clear description of the problem, a minimal reproducible example, the observed error, the environment, a workaround, and even a suggestion for a possible fix. It is clear what is required: the units.quantity_input decorator should not fail when used on constructors with a return type hint of None. The expected behavior is well-defined, and the solution direction is suggested.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7441": {
        "explanation": "The issue clearly states that astropy.time.TimeDelta should have a method to convert to Python's built-in datetime.timedelta, and suggests the method name 'to_timedelta'. It also mentions that conversion to np.timedelta64 would be desirable. The main requirement (to_timedelta) is clear and actionable, and the secondary suggestion (np.timedelta64) is also understandable. The expected output (a datetime.timedelta object or objects) is specified. There is little ambiguity about what is being requested, though some details (e.g., handling arrays) may need to be inferred from context or existing code patterns.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7606": {
        "explanation": "The issue provides a clear description of the problem: comparing an UnrecognizedUnit instance to None using '==' raises a TypeError, but the expected behavior is that it should simply return False. The code snippet, stack trace, and expected outcome are all provided. The location in the codebase (astropy/units/core.py, __eq__ method) is also indicated. This is sufficient information to attempt a fix: modify the equality method to handle None gracefully and return False instead of raising an error.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7671": {
        "explanation": "The issue provides a clear description of the problem: after a change in PR #7647, the minversion utility fails with a TypeError when comparing certain version strings, due to a bug in LooseVersion. The issue includes a reproducible example, the traceback, and a comparison with the previous behavior using pkg_resources.parse_version. The expected behavior is implied: minversion should not fail and should correctly compare versions like '1.14.3' and '1.14dev'. The solution direction is also suggested (possibly reverting to or using parse_version instead of LooseVersion). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7737": {
        "explanation": "The issue clearly describes the current behavior (biweight_location returns nan for constant arrays), the cause (0/0 division due to zero MAD), and proposes a concrete solution (return the median in this special case). The expected behavior is well-defined, and the code example makes the problem and the desired fix explicit. This is sufficient information to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7973": {
        "explanation": "The issue is not a request for a code change or a specific technical solution, but rather a call for votes and opinions on two proposed approaches to a design decision. It does not specify which approach to implement, nor does it provide concrete requirements for a code change. Therefore, it is not possible to attempt a meaningful solution based on this issue alone.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "astropy__astropy-8005": {
        "explanation": "The issue clearly describes an inconsistency in how cosmological parameters are sourced in two different equivalencies: one hardcodes Planck15, the other uses the default cosmology (currently WMAP9). The desired outcome is to make this consistent, ideally by using the default cosmology in both places, especially if the default is changed to Planck15. The issue references related work and provides enough context to attempt a solution (refactor the code to use the default cosmology in both cases, ensuring backward compatibility and test coverage).",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8263": {
        "explanation": "The issue provides a detailed description of the error encountered, including the specific exception, the context in which it occurs, and steps to reproduce it. It also traces the problem to a specific commit in the astropy library. However, the issue does not specify what the desired solution is: should the code be changed to work around the astropy bug, should the dependency be pinned, or is a patch to astropy expected? There is enough information to investigate and potentially fix the problem in the codebase, but some interpretation is needed regarding the expected resolution path.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-8339": {
        "explanation": "The issue provides a clear description of the bug (UnboundLocalError for 'ncp_prior' in the bayesian_blocks algorithm), the context in which it occurs (when calling bayesian_blocks with ncp_prior set), and a concrete code fix (add an else clause to assign ncp_prior from self.ncp_prior). The expected behavior and the location of the bug are both specified, making it straightforward to implement and test the solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8519": {
        "explanation": "The issue provides a clear code example that demonstrates the problem: subtracting or adding two ABmag quantities results in a loss of the 'type' of magnitude, leading to a UnitConversionError. The expected behavior is also stated: the result of the operation should retain the ABmag type. The error messages and the context (astropy.units, ABmag, nJy) are provided, making it clear what needs to be fixed. There is enough information to attempt a solution, such as ensuring that arithmetic operations between ABmag quantities preserve the correct unit metadata.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8707": {
        "explanation": "The issue clearly describes the problem: Header.fromstring does not accept bytes in Python 3, even though the documentation suggests it should accept a 'byte string'. It also points out that Header.fromfile is more flexible, and suggests a solution: update Header.fromstring to accept both unicode (str) and bytes types, and similarly update Card.fromstring. The expected behavior is well described, and the scope of the change is clear. There is enough information to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8747": {
        "explanation": "The issue provides context: numpy's np.clip is now a ufunc as of version 1.17, and the codebase needs to ensure that np.clip works properly with a type called Quantity. The goal is to add or fix support for this new ufunc-based np.clip so that it works with Quantity, and to avoid test failures with numpy-dev. While it doesn't specify the exact implementation details, it is clear what needs to be achieved: ensure np.clip works with Quantity. This is a common type of compatibility issue and the required outcome is clear, though the implementation is left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-8872": {
        "explanation": "The issue clearly describes the unexpected behavior: when creating a Quantity from a np.float16, the resulting dtype is float64, whereas for other float types (float32, float64, float128), the dtype is preserved. The expected behavior is implied: the dtype of the Quantity should match the input type, as it does for other float types. There is a clear example and a comparison with other types, making it clear what needs to be fixed. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10097": {
        "explanation": "The issue clearly describes the problem: URLValidator currently accepts URLs with usernames and passwords that contain unencoded \":\", \"@\", or \"/\", which is against RFC 1738 section 3.1. It provides a concrete example (http://foo/bar@example.com) and references relevant test cases and the RFC. The required change is to make URLValidator reject such URLs unless those characters are percent-encoded. The only minor ambiguity is the exact scope of changes (e.g., whether to update only the regex or also the test suite), but the main requirement is clear and actionable. Therefore, this is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10213": {
        "explanation": "The issue clearly describes the desired feature: adding a --force-colors (or similar) flag to django-admin that forces colorized output even when Django would normally disable it (e.g., when output is piped). The motivation, real-world use case, and reference to similar behavior in other Unix commands are provided. The expected behavior is unambiguous: when the flag is present, Django should always emit color codes regardless of output destination. Implementation details (e.g., flag name, integration point) are left to the developer, but the requirements are clear enough for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10301": {
        "explanation": "The issue provides a clear example of code that triggers the crash (an annotate/values query using pow with NULL values), the error message (OperationalError: user-defined function raised exception), and the context (SQLite backend in Django). It is clear that the problem is that certain SQLite functions (like pow) are not handling NULL values gracefully and instead crash. The expected behavior is implied: these functions should not crash on NULLs, but should handle them gracefully (likely returning NULL or a safe value). This is enough information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10316": {
        "explanation": "The issue clearly describes the problem: when using settings.configure() instead of DJANGO_SETTINGS_MODULE, diffsettings raises a misleading exception due to an unconditional call to settings._setup(). The issue also suggests a concrete fix (checking settings.configured before calling settings._setup()). The expected behavior and the code change are both clear, making it straightforward to implement and test a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10390": {
        "explanation": "The issue provides a clear description of the problem: when truncating datetimes with Django's Trunc() function, ambiguous or non-existent times due to DST transitions can cause unhandled exceptions from pytz. The issue suggests that Trunc() should allow passing an is_dst parameter to resolve these ambiguities, or otherwise avoid checking the validity of truncated dates. A concrete test case is provided, along with the exact error encountered. While the issue leaves some room for interpretation (e.g., whether to expose is_dst directly or handle it internally), it is clear what the problem is and what a successful solution would look like: Trunc() should not raise these exceptions, and the test should pass. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10426": {
        "explanation": "The issue provides a clear request: to add app_label validation to the 'showmigrations' command, similar to what was done in the referenced tickets for other commands. While it does not specify the exact validation logic, it is reasonable to infer that the validation should be consistent with the other commands mentioned. The references to previous tickets provide a concrete guide for implementation. Therefore, the issue is reasonably specified, as there is a sensible interpretation of what is required, though some details must be inferred from the referenced tickets.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10531": {
        "explanation": "The issue clearly describes the current behavior (admin history uses field names) and the desired behavior (admin history should use verbose names, as in older Django versions). It provides a concrete example and the expected output. The scope is clear: update the admin history display to use verbose names for fields. No major ambiguities remain, and a developer familiar with Django admin would know what to change and how to verify the fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10554": {
        "explanation": "The issue provides a clear description of the problem, including a reproducible code snippet, the observed error, and the context in which the error occurs. It identifies the specific error message and the sequence of operations that lead to the bug. While it does not specify the exact desired fix, it is clear that the expected behavior is for the union queryset with ordering to work without raising a ProgrammingError. The issue is well-specified for a developer familiar with Django's ORM internals to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10606": {
        "explanation": "The issue provides a clear description of the bug: using RelatedOnlyFieldListFilter with a reverse ManyToMany relation causes a specific exception. It identifies the root cause (missing parameter in ForeignObjectRel.get_choices), references the relevant method, and provides both a pull request and a demo to reproduce the bug. This is sufficient information for an experienced engineer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10643": {
        "explanation": "The issue clearly describes the problem: searching for UUIDs in Django admin works on Postgres but not on Oracle due to differences in how UUIDs are stored and compared (with or without dashes). The expected behavior is also clear: users should be able to search for UUIDs as displayed (with dashes) regardless of the backend. The issue suggests that the icontains lookup should handle both dashed and non-dashed UUIDs. While the exact implementation is left to the developer, the requirements and context are sufficiently clear for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10737": {
        "explanation": "The issue clearly states the desired outcome: all FieldError exceptions raised in django/db/models/sql/compiler.py should, where possible, include the field that caused the error in the error message. While it leaves some discretion ('where possible'), the main requirement is clear and actionable: audit the file for FieldError raises and update their messages to include the relevant field. This is a sensible and implementable request, though some judgment is needed for cases where the field may not be easily available.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10853": {
        "explanation": "The issue clearly describes a problem: Django does not check for SQLite's support of window functions before generating SQL that uses them, leading to confusing OperationalError exceptions when running on older SQLite versions. The expected behavior is also described: Django should explicitly check for window function support and raise a clear, documented error if not available, rather than letting the SQL execution fail. The issue provides code samples to reproduce the problem and specifies the affected versions. This is sufficient information to attempt a solution, such as adding a version check and raising a more informative exception.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10880": {
        "explanation": "The issue describes a specific bug: when using Django's Count annotation with both a Case condition and distinct=True, the generated SQL is malformed due to a missing space between 'DISTINCT' and 'CASE'. The description includes the context (Django 2.2, any DB backend) and the nature of the error (malformed SQL). While it doesn't provide a code sample or the exact error message, the problem is clear and actionable for someone familiar with Django's ORM and SQL generation. The required fix is to ensure a space is present in the generated SQL between 'DISTINCT' and 'CASE'.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10904": {
        "explanation": "The issue provides a clear and actionable request: replace all uses of OSError aliases (IOError, EnvironmentError, WindowsError, mmap.error, socket.error, select.error) with OSError throughout the codebase. It also references the relevant Python documentation and explains the rationale for the change. The scope is well-defined, and the expected outcome is clear: code should use OSError instead of the listed aliases. There is also an additional note about SMTPException, but it is clear how to handle it. No major ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10910": {
        "explanation": "The issue provides a clear description of the bug, including the context (Django ORM with PostgreSQL), the problematic behavior (incorrect SQL query generated when using tzinfo=datetime.timezone(datetime.timedelta(...))), and a detailed example with code and output. It also references the relevant PostgreSQL documentation and explains why the current behavior is incorrect. The expected behavior is implied: the ORM should generate a query that PostgreSQL interprets correctly, so that the time zone offset is handled as intended. While the issue does not specify the exact fix, it is clear what needs to be addressed and what a successful solution would look like (correct handling of tzinfo objects with fixed offsets).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10914": {
        "explanation": "The issue describes a problem with inconsistent file permissions when uploading files, depending on the upload handler used. It requests a warning in the documentation about this behavior, specifically regarding the default FILE_UPLOAD_PERMISSIONS and how it can result in different permissions (0o600 vs 0o644) depending on the upload path. However, the issue title suggests setting the default FILE_UPLOAD_PERMISSION to 0o644, but the body focuses more on documentation and warning users. It is not entirely clear whether the request is to change the default, update documentation, or both. There is enough information to make a reasonable interpretation (e.g., update documentation to warn about this and/or consider changing the default), but some ambiguity remains about the exact desired outcome.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10939": {
        "explanation": "The issue describes a problem with Django's ModelAdmin where custom widgets, inlines, and filter_horizontal can cause media files (specifically JavaScript) to be merged in an incorrect order, resulting in a MediaOrderConflictWarning and inlines.js being loaded before jQuery. However, the description is somewhat unclear: it does not specify the exact ModelAdmin configuration, the custom widget code, or the expected vs. actual media order. While the general problem is understandable, there are ambiguities about the precise setup and the desired outcome. A reasonable interpretation is possible, but some assumptions would need to be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10973": {
        "explanation": "The issue provides a clear context: it wants to update the postgres client code in Django's backend to use subprocess.run (available since Python 3.5) and to set the PGPASSWORD environment variable via the custom environment feature of subprocess.run. The motivation is to simplify and make the code more reliable. The location (django.db.backends.postgres.client) and the desired approach (use subprocess.run and set PGPASSWORD) are both specified. While it doesn't provide exact code, the requirements and the goal are clear and actionable for an experienced engineer.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10989": {
        "explanation": "The issue provides a clear description of the problem: the use of the Unicode horizontal ellipsis character (\\u2026) in output strings causes a UnicodeEncodeError on Windows terminals using code page 437. The traceback is included, and the problematic string is identified. The issue also suggests a solution: replace the Unicode ellipsis with three ASCII dots ('...'). The scope is clear (find and replace all such occurrences in the codebase). This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10997": {
        "explanation": "The issue provides a clear feature request: to display the 'applied datetime' for each migration in the output of the 'showmigrations' command, specifically when using the --list option and with verbosity level 2 or higher. The motivation and use case are explained, and there is mention of an initial implementation approach. While there are some minor ambiguities (e.g., exact formatting of the datetime, whether to show for unapplied migrations, etc.), the core requirement is clear and actionable for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10999": {
        "explanation": "The issue provides a clear description of the problem: the regular expression used in parse_duration() does not correctly handle negative durations due to a missing '-?' in the lookahead for the hours group. It also provides a suggested fix for the regex. The context (Django's dateparse utility) and the specific code to change are both given, making it clear what needs to be done for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11003": {
        "explanation": "The issue provides a clear description of the problem: initializing a ModelChoiceField with a valid value and setting disabled=True causes a validation error upon form submission, even though the value is valid. It also points out that the error message is unhelpful and suggests it should include the invalid value. The issue mentions a workaround (using to_field_name) and references a related bugfix (#28387). While the issue is clear about the problem and the desired behavior (form should validate if the value is valid and field is disabled; error message should be improved), it does not specify exactly how the error message should be changed or whether the fix should be in the form validation logic, the error message, or both. However, as an experienced engineer, I can reasonably infer the required changes: ensure disabled ModelChoiceFields with valid initial values do not trigger validation errors, and improve the error message to include the invalid value. Thus, the issue is reasonably specified, though not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11019": {
        "explanation": "The issue provides a clear description of the problem, including a concrete example with code, the expected behavior, the actual behavior, and an analysis of why the current implementation is problematic. It specifies that unnecessary MediaOrderConflictWarnings are being thrown when merging 3 or more media objects, and it describes the desired order of JS files. The issue also points out the unintuitive result and the incorrect warning message. This is sufficient information to attempt a meaningful solution, such as adjusting the media merging logic to handle dependencies and order correctly, and to improve the warning reporting.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11057": {
        "explanation": "The issue describes a specific problem: the Django admin check for AuthenticationMiddleware is too strict and does not allow for subclasses or custom middleware that inherit from AuthenticationMiddleware. The user provides context (upgrading from 2.1 to 2.2b1, custom middleware subclassing AuthenticationMiddleware, error message admin.E408). The required solution is clear: modify the admin check so that it allows subclasses of AuthenticationMiddleware, not just the exact class. The technical direction is implied and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11062": {
        "explanation": "The issue provides a clear description of the problem, including the model, the code that triggers the error, the error message, and the environment (Django, Python, PostgreSQL versions). It specifies the expected behavior (aggregate by hour using Extract), the actual behavior (ProgrammingError), and the context (works in Django 1.8, fails in 2.1.7 and 2.2b1). While it does not specify the exact fix, it is clear what needs to be addressed: making aggregation on an Extract-annotated field work as expected. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11066": {
        "explanation": "The issue provides a clear description of the problem, including the relevant code, context about the user's setup, the error encountered, and a proposed solution. The user points to a specific line in the codebase and suggests a concrete change (adding 'using=db' to the save() call). The expected behavior is clear: the content type should be saved to the correct database, not the default one. This is sufficient information to attempt a solution and create a meaningful PR.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11070": {
        "explanation": "The issue clearly specifies that the autocomplete attribute should be added to the fields in contrib.auth's built-in forms, and provides examples of the values to use (username, email, current-password, new-password). It also references a related pull request and provides documentation links for further clarification. The intent and scope are clear: update the relevant forms to use appropriate autocomplete attributes to improve browser behavior. While it does not list every field or form explicitly, it is clear enough for an experienced developer to identify the relevant forms and fields in contrib.auth and apply the correct attributes.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11087": {
        "explanation": "The issue provides a detailed description of the problem, including the context (Django 1.11, Python 2.7 to 3.6 upgrade, MySQL, specific code locations), the observed error (UnicodeDecodeError), and the root cause analysis (unnecessary fields being fetched during .delete(), leading to decode errors on junk data). It also clearly states the desired outcome: optimize .delete() so that only required fields are fetched, which would avoid the error and improve performance. There is enough information to attempt a solution, such as modifying the queryset or the ORM's deletion logic to only select necessary fields. The issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11088": {
        "explanation": "The issue provides a clear description of the problem, including a step-by-step reproduction scenario, the error encountered, and the context (Python, Django, Postgres versions). It also points to the relevant code area and suggests a possible fix. The expected behavior is implied: migrations involving BinaryField with a default of '' (str) should not crash, and the migration system should handle defaults for BinaryField consistently in both forward and reverse migrations. While the exact implementation details are left to the developer, the problem and its context are well-specified for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11095": {
        "explanation": "The issue clearly describes the desired feature: adding a new hook method, ModelAdmin.get_inlines(request, obj=None), to allow dynamic determination of inlines based on the request or model instance. It explains the current workaround (overriding get_inline_instances and duplicating a for loop) and the motivation for the change. The required method signature and its purpose are specified. While some implementation details (e.g., exact return type) could be inferred from context or existing code, the request is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11099": {
        "explanation": "The issue clearly describes the problem (username validators allow trailing newlines due to use of $ in regex), explains the cause (quirk of Python regex $ matching trailing newline), and proposes a concrete solution (replace ^...$ with \\A...\\Z in the regex for both validators). It specifies exactly which files/classes are affected and what the new regex should be. This is sufficient information to implement and test a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11115": {
        "explanation": "The issue clearly describes a pain point: when a Django AlreadyRegistered exception is raised, it is difficult to determine which app or package is responsible for the duplicate registration. The user requests that the exception provide more details about the already-registered model, specifically which package or app registered it first. While the issue does not specify the exact format or mechanism for providing this information, the desired outcome is clear: improve the exception message to make debugging easier. As an experienced engineer, I can interpret this as enhancing the exception to include the source of the original registration, which is a sensible and actionable request.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11119": {
        "explanation": "The issue clearly describes a specific bug: Engine.render_to_string() does not honor the engine's autoescape attribute when creating a Context, resulting in output always being autoescaped even if autoescape=False was set. It also references a specific commit where the problem may have originated. The required fix is to ensure that render_to_string() respects the engine's autoescape setting when creating the Context. This is a well-scoped and actionable issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11129": {
        "explanation": "The issue describes a specific scenario involving Django's QuerySet.update() with an F expression referencing an annotated field that requires a join, resulting in a confusing or incorrect error message. However, the issue does not specify what the correct or improved error message should be, nor does it provide a clear acceptance criterion for what constitutes a 'better' error message. There is also mention of a 'simpler test attached,' but no test is actually included in the description. While the context and the problem are clear, the solution space is ambiguous: should the error message be more descriptive, should the operation be supported, or is there another expectation? Therefore, the issue is too vague to attempt a meaningful solution without further clarification.",
        "label": "VAGUE"
    },
    "django__django-11133": {
        "explanation": "The issue clearly describes the problem: HttpResponse does not handle memoryview objects as expected, and provides a reproducible example. The expected behavior is also specified: when passing a memoryview to HttpResponse, the response content should be the bytes represented by the memoryview, not the memoryview object itself. The context (difference between Sqlite and Postgresql) is given, but the core ask is clear: HttpResponse should accept memoryview objects and treat them like bytes. This is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11138": {
        "explanation": "The issue provides a detailed description of the problem, including configuration examples, expected vs. actual behavior, relevant documentation references, and even points to the specific line in the Django codebase where the problematic behavior occurs. The desired behavior is clearly stated: the database TIME_ZONE setting should be respected when making dates timezone-aware, and the conversion should be from the database timezone to the Django app timezone, not always from UTC. It also specifies that if the two timezones are the same, no conversion should be performed. This is sufficient information to attempt a meaningful solution, such as modifying the backend operations to respect the per-database TIME_ZONE setting in date lookups.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11141": {
        "explanation": "The issue provides a clear background, describes the current behavior (the migrate command checks for __file__ on the migrations package), explains why this is now unnecessary (migration discovery now uses pkgutil.iter_modules() and __path__), and states the problem (the __file__ check prevents migration on namespace packages). The required change is to remove or adjust the __file__ check so that migrations directories without __init__.py files (i.e., namespace packages) are supported. This is a well-specified issue with a clear objective and rationale.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11149": {
        "explanation": "The issue clearly describes a bug: users with only 'view' permissions are able to edit ManyToMany inlines in the Django admin, which should not be possible. It provides a concrete example with models and admin configuration, and references a specific regression and related discussion. The expected behavior is implied (view-only users should not be able to edit), and the current incorrect behavior is described. While it doesn't specify the exact code change, it is clear what needs to be fixed and how to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11155": {
        "explanation": "The issue clearly specifies the desired new settings (LANGUAGE_COOKIE_SECURE, LANGUAGE_COOKIE_HTTPONLY, LANGUAGE_COOKIE_SAMESITE), their default values, and the rationale for adding them. It is clear that the task is to add these settings to the language cookie functionality, defaulting to the current behavior, and to allow users to override them. There is enough detail to implement the solution without ambiguity.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11163": {
        "explanation": "The issue clearly describes the current behavior (model_to_dict returns all fields when fields=[]), the expected behavior (should return an empty dict), and pinpoints the problematic code and the suggested fix. It also provides a reference PR for context. This is sufficient information to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11166": {
        "explanation": "The issue provides a clear description of the problem: the admin app currently requires 'django.contrib.sessions' to be in INSTALLED_APPS, which prevents the use of alternative session backends that are API-compatible but not the same app. It suggests a concrete direction for a solution: remove the hard check for the app and instead check for the presence of compatible session middleware, similar to how other middleware checks are performed. The desired outcome is clear: allow alternative session backends that are compatible with the admin app without forcing the use of the default session app. There is enough information to attempt a solution by refactoring the check logic as described.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11169": {
        "explanation": "The issue provides a clear context: when a custom error handler (like handler404, handler500, etc.) is set to a view path that cannot be imported, the system currently raises an exception. The request is to catch this exception and instead add a new system check message (e.g., urls.E008) indicating the import failure. The desired behavior, the error code, and the context are all specified. The only minor ambiguity is the exact wording of the message and whether to handle both ModuleNotFoundError and ViewDoesNotExist, but these are reasonable to infer from the context and linked discussion.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11170": {
        "explanation": "The issue describes an error (UnicodeDecodeError) encountered when loading a Django debug template, specifically technical_500.html. The user provides some context about the error, their investigation, and a workaround (changing ellipsis to dotdotdot or setting encoding to utf-8). However, the issue ends with a question about whether the file was intentionally modified, rather than a clear request for a code change or bug fix. There is no actionable request or clear acceptance criteria for a pull request. It is unclear whether the user wants the encoding fixed, the template changed, or just an explanation. Therefore, the issue is too vague for a meaningful attempt at a solution.",
        "label": "VAGUE"
    },
    "django__django-11177": {
        "explanation": "The issue provides a clear description of the observed problem (autoreloader not consistently picking up file changes in Django 2.2), the environment (Python 3.7, Docker, not using watchman), and some steps already taken to diagnose or work around the issue. However, it does not specify what a successful solution would look like (e.g., should the autoreloader behave exactly as in 2.1.7? Is there a specific test case to reproduce?). There is enough information to start investigating and likely reproduce the issue, but some interpretation is required regarding the expected behavior and acceptance criteria.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11179": {
        "explanation": "The issue clearly describes the undesired behavior: after calling delete() on a model instance with no dependencies, the primary key (PK) field is not set to None, but it should be. It references a specific location in the Django codebase (django.db.models.deletion:276-281) and suggests that the model should be updated at line 280. The expected behavior is clear, and the code location is specified, making it possible to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11185": {
        "explanation": "The issue describes a problem with the Model.delete(keep_parents=True) method not preserving all parent reverse relationships, specifically for relationships toward parents of parents and beyond. It references a previous partial fix (#27407), indicating some context. However, it does not specify what 'preserving' means in this context, what the expected behavior is, or provide concrete examples or test cases. While a knowledgeable developer could infer the general area to investigate and possibly deduce the intended behavior from the referenced PR, there is still ambiguity about the exact requirements and edge cases. Thus, it is not fully well-specified, but a sensible interpretation is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11194": {
        "explanation": "The issue clearly describes the current behavior (RSS feeds always use settings.LANGUAGE_CODE for the language tag) and the desired behavior (allow specifying a different language for the feed). It also references an existing patch, which suggests a concrete direction for the solution. The requirement is clear: introduce a way to override the default language code for RSS feeds. This is enough information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11206": {
        "explanation": "The issue clearly describes the current behavior (small decimals rendered in exponential notation), provides a code example, and specifies the desired behavior: when decimal_pos is supplied and the number is too small to be represented with that precision, the output should be a string of zeros (e.g., '0.00'). The cause (hardcoded cutoff) is mentioned, and the expected output is unambiguous. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11211": {
        "explanation": "The issue provides a clear description of the problem: 'prefetch_related' does not work with a GenericForeignKey (GFK) to a model that uses a UUIDField as its primary key. It includes reproducible steps, code samples, and a specific example of the failure (Django ORM returns None for the 'foo' attribute when prefetching). The expected behavior is implied: 'prefetch_related' should work and not return None. While it does not specify the exact fix, it is clear what needs to be addressed and how to verify the solution. Thus, it is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11214": {
        "explanation": "The issue is described in detail, including the specific code that triggers the problem, the observed behavior (repeatedly re-adding the constraint), and the suspected cause (range being converted to tuple in migrations). The user also provides a sample project for reproduction. However, the issue raises a question about the correct fix, listing multiple possible solutions and explicitly asking for clarification on which approach to take. This means that while the technical details are clear, the desired direction for the fix is not specified, leaving ambiguity about what constitutes an acceptable solution. Therefore, while a sensible interpretation can be made (e.g., making migrations idempotent regardless of range/tuple conversion), the lack of a clear directive on the preferred solution means it is not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11216": {
        "explanation": "The issue provides a clear description of the problem: ManifestStaticFilesStorage leaves behind intermediate files after collectstatic runs, resulting in multiple hashed versions of the same file. It also explains why this is a problem (e.g., inefficiency with WhiteNoise's compression) and references prior discussions and related issues. The proposed solution is to add a keep_intermediate_files property to HashedFilesMixin, set appropriately for different storage classes, to control whether these files are kept. While there is some room for interpretation in the exact implementation details (e.g., naming, default values, and how/where to clean up files), the intent and the desired outcome are clear. Therefore, this issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11239": {
        "explanation": "The issue clearly describes the problem: the 'dbshell' command in Django does not currently support passing client certificate and key parameters required for mutual TLS connections to PostgreSQL, even though the Django database configuration supports them. The issue provides a concrete example of the configuration and specifies which parameters are missing from dbshell. The expected outcome is clear: update dbshell to support these parameters. There is enough information to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11260": {
        "explanation": "The issue describes a specific problem: Django's inspectdb generates a unique ForeignKey instead of a OneToOneField, which leads to fields.W342 warnings. The expected behavior is implied: inspectdb should generate a OneToOneField when the database schema indicates a unique foreign key constraint. While the issue does not provide a concrete example schema or the exact code location, the problem and the desired outcome are clear to an experienced Django developer. The solution would involve modifying inspectdb to detect unique foreign keys and generate OneToOneField accordingly. Thus, the issue is reasonably specified, though not exhaustively detailed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11265": {
        "explanation": "The issue provides a clear description of the problem: using 'exclude' on a queryset with an annotated FilteredRelation in Django raises a FieldError. It includes a concrete code example, the expected vs. actual behavior, and a traceback of the error. The issue also points to a likely problematic function in the codebase. While it doesn't specify the exact fix, it is clear what the bug is, how to reproduce it, and what a successful solution would look like (exclude should work as filter does in this context). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11270": {
        "explanation": "The issue clearly describes the problem: when using the 'startapp' command with a target argument, if the target directory name is invalid, the resulting app cannot be imported. The request is to add validation to the target directory's name, similar to how app name validation is already handled. While it does not specify the exact validation rules, it is reasonable to infer that the same rules as app name validation should be applied. Therefore, the issue is reasonably specified, as the intent and a sensible implementation path are clear, though some details (e.g., exact validation logic) must be inferred from existing app name validation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11276": {
        "explanation": "The issue clearly describes the desired change: replace the implementation of django.utils.html.escape() with Python's stdlib html.escape(). It provides context, rationale (performance, maintenance), and even notes a minor backwards-incompatible change (the encoding of single quotes). The only minor ambiguity is whether the function signature and all edge cases should match exactly, but the intent is clear and the tradeoff is acknowledged. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11278": {
        "explanation": "The issue clearly states that a system check should be added to ensure the uniqueness of names for constraints (check and unique) and partial indexes, as these names must be unique in the database scope. The scope of the check (database-wide), the objects involved (constraints and partial indexes), and the desired outcome (system check for uniqueness) are all specified. While it references a previous discussion (#30362), the core requirement is clear and actionable without needing to consult that discussion.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11279": {
        "explanation": "The issue clearly describes the desired feature: allowing the use of %(app_label)s and %(class)s placeholders in the 'name' argument for BaseConstraint and Index classes, with explicit instructions on what each placeholder should be replaced with. It also provides context for why this is needed (for abstract models) and references a related discussion. The requirements are concrete and actionable, making it clear what needs to be implemented and how to verify correctness.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11281": {
        "explanation": "The issue provides clear guidance on the desired typographic improvements: replacing straight quotes with curly quotes, using the correct apostrophe for contractions, replacing double hyphens with em dashes, and using the ellipsis character instead of three dots. While it does not specify exactly which user-facing strings or files are affected, the intent and the specific replacements are clear. An experienced engineer could reasonably interpret this as a request to audit and update all user-facing strings in the codebase to follow these typographic conventions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11292": {
        "explanation": "The issue clearly describes the desired feature: expose the existing 'skip_checks' stealth option as a '--skip-checks' command-line flag for management commands. The motivation and intended behavior are clear, and the implementation direction is straightforward (surface an internal option as a public CLI flag). There are no major ambiguities about what is required for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11299": {
        "explanation": "The issue provides a clear description of the bug, the context in which it occurs (Django migrations with CheckConstraint using OR and AND), the platforms affected (SQLite and Oracle), the incorrect and correct SQL outputs, and a minimal reproducible example. It also points to the likely cause (Col vs SimpleCol usage). This is sufficient information for an experienced developer to attempt a fix and write a test for the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11333": {
        "explanation": "The issue provides a clear description of the problem: multiple URLResolver instances are being constructed unnecessarily due to the timing of set_urlconf and get_resolver calls, leading to redundant and expensive computations. The root cause is explained, and a concrete solution is proposed: modify get_resolver to look up settings.ROOT_URLCONF before the memoized function call. The context, affected code paths, and the intended optimization are all specified. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11354": {
        "explanation": "The issue provides a clear description of the problem, including the models involved, the code to reproduce the bug, the error message, and an analysis of the root cause in the Django ORM code. It also suggests a possible fix and specifies the versions affected. While it leaves some room for implementation details (e.g., whether the suggested fix is the best approach), it is clear what the bug is and what a successful solution would look like: .count() should work on annotated querysets involving inherited fields. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11377": {
        "explanation": "The issue provides a clear description of the problem: a deprecation warning in Django 2.2 causes a crash when Meta.ordering contains query expressions (like F() or OrderBy objects), due to the way the warning message is constructed. It includes a code example, the traceback, a proposed (but insufficient) workaround, and a clear statement of the desired output (the warning message should generate valid .order_by() code, not just string representations). It also provides a regression test link. While there is some room for interpretation in the exact formatting of the output, the core requirement is clear: fix the warning message so it handles query expressions in Meta.ordering without crashing and outputs valid code. This is well-specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11396": {
        "explanation": "The issue provides a clear code example that triggers the problem, the error message from PostgreSQL, and the context (using Django ORM with annotate and order_by on a constant Value). The expected behavior is implied: the user expects to be able to order by a constant annotation without error. The issue is well-specified for a developer familiar with Django and PostgreSQL to attempt a fix or at least investigate the ORM's SQL generation for this case.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11400": {
        "explanation": "The issue provides a clear description of the problem: RelatedFieldListFilter and RelatedOnlyFieldListFilter in Django admin do not respect the related model's Meta.ordering when determining the order of choices. It points to the relevant lines in the codebase and explains the current behavior and the expected fallback to Meta.ordering. While it does not specify the exact code change, it is clear what the desired behavior is and where the changes need to be made. An experienced engineer could reasonably interpret and implement a solution based on this information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11405": {
        "explanation": "The issue provides a clear description of the bug: that Queryset ordering and Meta.ordering are being mutated when reverse() is called, and that this is revealed by running a specific test. It references specific commits and test files, and points to the likely location of the bug (SQLCompiler.get_order_by()). While it does not specify the exact fix, it is clear what the undesired behavior is and where to look. An experienced engineer could attempt a solution based on this information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11417": {
        "explanation": "The issue provides a clear objective: update the mail backend (specifically django.core.mail.message.sanitize_address) to use a more modern and less error-prone standard library function (email.headerregistry.parser.get_mailbox) instead of email.utils.parseaddr. The location in the codebase and the desired replacement are both specified. While there may be some minor details to clarify (such as handling edge cases or compatibility), the main task is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11423": {
        "explanation": "The issue provides a detailed description of the problem, including a reproducible example repository, clear steps to reproduce, the expected vs. actual behavior, and some investigation into the root cause (incorrect caching in _prefetched_objects_cache when using GenericRelation and cyclic prefetching). However, it does not specify exactly what a 'fix' would look like (e.g., whether to patch Django internals, suggest a workaround, or document the limitation). Still, the problem is concrete and reproducible, and a reasonable interpretation would be to fix the caching so that tag.content_object.tag returns the correct tag after prefetching. Thus, while some implementation details are left to the developer, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11428": {
        "explanation": "The issue provides a clear description of the problem, including how to reproduce it, the root cause (autoreload's attempt to reconstruct exceptions with a single argument), and the undesired consequence (TypeError masking the original exception). It also references the relevant code and commit that introduced the behavior. The expected behavior is implied: exceptions with custom signatures should not be masked by TypeError, and the original exception should be preserved. While it doesn't specify the exact fix, it is clear what needs to be addressed and what a successful solution would look like.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11433": {
        "explanation": "The issue provides a clear context: when a form is submitted in Django, if a field is missing from the payload but has a model default, the current behavior does not allow 'cleaned_data' to overwrite the model's default value. The issue references a specific code review comment for further context. The desired behavior is for 'cleaned_data' to be able to overwrite model defaults, aligning with the paradigm of modifying data in 'cleaned_data'. While the exact implementation details are not specified, the problem and the expected outcome are clear enough for an experienced Django developer to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11446": {
        "explanation": "The issue clearly describes the problem: the default 404 error page in Django 2.2.1 when DEBUG=False is not a complete HTML document, but just a fragment. It also provides an example of the current output and what is expected (a full HTML page). The expected output is specified, and the context (Django version, DEBUG mode) is given. This is sufficient information to attempt a solution, such as modifying the error handler to return a complete HTML document.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11451": {
        "explanation": "The issue provides a clear description of the current behavior, the undesired database query when username is None, and a concrete suggestion for how to fix it (short-circuiting the function when username or password is None). It also explains the rationale, including timing and performance considerations, and provides context about how the issue was discovered. The expected change to the code is explicit, and the success criteria (no unnecessary DB query when username is None) are clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11477": {
        "explanation": "The issue describes a problem with the translate_url() function when handling optional named groups that are missing in the URL pattern. However, it does not specify what the correct behavior should be, what the incorrect output is, or provide any concrete examples or test cases. The reference to a 'test case of the patch' is ambiguous, as no patch or test case is included. As a result, it is unclear what a successful solution would look like without further information.",
        "label": "VAGUE"
    },
    "django__django-11490": {
        "explanation": "The issue provides a clear description of the problem: when using composed queries (specifically union) and calling values()/values_list() with different columns, the list of columns does not change as expected. It includes a concrete example with code and points to a relevant section of the codebase (compiler.py#L428-L433). The expected and actual behaviors are both demonstrated. While it does not specify the exact fix, it is clear what the bug is and what the correct behavior should be, making it possible to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11514": {
        "explanation": "The issue clearly describes the current behavior of the never_cache decorator in Django, the problem (resources still being cached by CDNs like Fastly), and the proposed solution (add 'Cache-Control: private' to the response headers set by never_cache). The linked documentation provides further context. The required change is specific and actionable: modify the never_cache decorator to include 'Cache-Control: private' in addition to its current headers. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11517": {
        "explanation": "The issue provides a clear description of the problem: calling a Django management command with subparsers using call_command leads to a ValueError due to the way required options are handled in the argument parsing logic. The code sample, error message, and the specific line in Django's codebase are all provided. The expected behavior (that this worked in Django 1.11) is also mentioned. However, the issue does not specify what the desired fix should be\u2014should call_command support subparsers as before, or should it raise a more informative error? Still, the problem is concrete and reproducible, and a reasonable solution would be to restore the previous behavior or handle subparsers gracefully. Thus, the issue is reasonably specified, though not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11525": {
        "explanation": "The issue provides a clear description of the problem: when the MANAGERS (and by extension, ADMINS) setting is not in the expected format (should be a list of (name, email) tuples, not just a list of emails), mail_managers()/mail_admins() fails in a non-obvious way. The request is to raise an exception early if the settings are not in the expected format. The expected behavior is clear, and the code locations (mail_admins/mail_managers) and the settings involved are specified. There is a sensible and actionable interpretation for a solution: add validation and raise an exception if the format is wrong.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11527": {
        "explanation": "The issue clearly describes the current behavior (no output when there are no sequences to reset), the user confusion this causes, and a concrete suggestion for improvement: raise a CommandError with a message if there are no statements to execute. It also suggests an optional enhancement (sniffing the backend and reporting if the command is not necessary/available). The main requirement is clear and actionable, and the optional enhancement is well-motivated but not required for a minimal solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11532": {
        "explanation": "The issue provides a clear description of the problem (crash when non-ASCII domain is used with non-unicode encoding), steps to reproduce, a traceback, and a suggested fix (convert domain name to punycode before use in headers). It also provides a test case to verify the fix. The expected behavior is well-defined: the domain should be punycoded in the Message-ID header, and the crash should be avoided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11543": {
        "explanation": "The issue provides a clear description of the problem: when running Django's runserver with the --nothreading option, the server fails to close connections with keep-alive headers, causing subsequent connections to hang. The environment is specified (OS, Python, Django versions, browsers). The cause is explained in terms of how the server handles keep-alive connections without threading. While the issue does not specify the exact desired fix, it is clear that the goal is to ensure connections are closed or handled properly when --nothreading is used. This is a reasonably well-defined bug report, though some implementation details are left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11551": {
        "explanation": "The issue provides a detailed description of the problem, including the context (Django version upgrade), the specific error (admin.E108), the code that triggers the error, the root cause (change in _check_list_display_item logic), and a proposed solution with a code snippet. The expected and correct behaviors are clearly outlined, and the issue even includes a table of logical cases and their outcomes. This is sufficient information to attempt a fix: revert or adjust the logic in _check_list_display_item as described, and ensure the admin validation works as expected for fields like PositionField. No significant ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11555": {
        "explanation": "The issue provides a clear description of the bug (a crash when using order_by() on a parent model with Meta.ordering containing expressions in Django), and it includes a reproducible test repository and a stacktrace. While the issue does not specify the exact desired fix, it is clear what the problem is and where it occurs. An experienced engineer could investigate the linked repository and stacktrace to identify and resolve the bug. Therefore, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11559": {
        "explanation": "The issue provides a clear description of the bug (a crash when using order_by() on a parent model with Meta.ordering containing expressions in a multi-table inheritance scenario). It references a reproducible test repository and mentions a specific failure point (OrderBy object, not a string, in get_order_dir). However, it does not specify what the expected behavior should be, nor does it provide a concrete acceptance criterion for a fix. Still, with the linked test repo and stacktrace, an experienced engineer could reproduce and investigate the bug, making a sensible attempt at a solution possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11560": {
        "explanation": "The issue clearly describes a specific bug: using ExtractYear on a DurationField leads to an invalid query because the generated SQL compares an interval with a date, which is not valid. The issue requests that a ValueError be raised in such cases. The expected behavior (raising a ValueError when an unsupported Extract lookup is attempted on a DurationField) is clear, and the example provided makes it unambiguous what triggers the error. The solution is to detect this case and raise the appropriate exception.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11564": {
        "explanation": "The issue provides a clear description of the problem: when Django is deployed under a sub-path (using SCRIPT_NAME), static and media URLs generated by the {% static %} template tag and related storage backends do not include the SCRIPT_NAME prefix, leading to broken links. The issue also explains why the common workaround (prepending SCRIPT_NAME to STATIC_URL in settings.py) is insufficient when SCRIPT_NAME is dynamic. It suggests a high-level solution (modifying the static tag and storage backends to respect SCRIPT_NAME) and even provides a code snippet indicating where the change should occur. While it does not specify every detail (e.g., how to handle all possible usages or edge cases), it is clear what the desired outcome is: static and media URLs should include SCRIPT_NAME when present. This is enough for an experienced engineer to attempt a solution, though some implementation details will need to be decided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11584": {
        "explanation": "The issue provides a clear description of the problem (FileNotFoundError when running Django's runserver inside a Docker container with a volume-mounted source), a reproducible environment (Dockerfile, docker-compose, Django version), and a full stack trace. It also notes that the problem is present in Django 2.2.3 but not in 2.1.4, which gives a clue about a regression or change in Django. However, the issue does not specify what the expected behavior is (should the server start successfully?), nor does it specify what file or directory is missing, or what the user expects as a fix (documentation, code change, workaround). Still, a reasonable interpretation is to make the development server work as it did in 2.1.4 under the described setup. Thus, while not perfect, it is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11591": {
        "explanation": "The issue clearly describes the problem: after performing QuerySet.union(), intersection(), or difference(), only certain operations (LIMIT, OFFSET, ORDER BY) should be allowed, but currently, other operations (like count()) are not strictly disallowed and may appear to work, causing confusion. The task is to raise a descriptive error when unsupported operations are attempted on such QuerySets. The required behavior is well-defined, and the examples in the description and references to related issues provide enough context for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11592": {
        "explanation": "The issue clearly describes a specific enhancement: passing the FileResponse.block_size attribute to wsgi.file_wrapper in Django's WSGI handler. It provides relevant code references and context, making it clear what needs to be changed. The expected behavior is to utilize the block_size attribute when constructing the file wrapper, which is a concrete and actionable request.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11603": {
        "explanation": "The issue clearly states the desired feature: to add DISTINCT support for the Avg and Sum aggregate functions, similar to what was done in a referenced previous issue (#28658). It also explains the current behavior (exception thrown) and the expected change (these aggregates should allow DISTINCT). The note about Min and Max is informative but not required. The scope and expected outcome are clear, making it possible to implement a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11605": {
        "explanation": "The issue clearly describes the current behavior (Django only shallowly checks for window expressions in filters), provides concrete code examples where the check fails, and specifies the desired outcome (raise a descriptive error when a filter contains a window expression, even in combined or right-side expressions). The scope and expected behavior are clear, making it possible to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11612": {
        "explanation": "The issue provides a clear description of the problem: SQLite3 migrations fail when a model's Meta db_table is quoted (e.g., '\"table_with_quoted_name\"'), resulting in a syntax error. It includes a sample generated query, a traceback, and a reference to Django documentation. The reproduction steps are implicit but clear: create a model with a quoted db_table and at least one foreign key, then run migrations. The expected behavior (support for quoted names) and the actual error are both described. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11618": {
        "explanation": "The issue provides a clear description of the problem (PermissionError being cloaked by broad OSError handling in ManifestFilesMixin.read_manifest), the consequences (silent failure leading to confusing ValueError), and specific suggestions for improvement (stop catching OSError, catch more granular exceptions, or add logging). While it leaves some implementation details to the developer (e.g., what kind of logging, which exceptions to catch), the intent and desired outcome are clear. A reasonable solution can be implemented based on this information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11622": {
        "explanation": "The issue clearly describes the current problem: when invalid values are assigned to AutoField, FloatField, or IntegerField in Django models, the resulting exception is unhelpful and does not indicate which field caused the error. The desired improvement is to raise an exception that includes the field name, making debugging easier. While it does not specify the exact implementation details, the goal and the context are clear enough for an experienced developer to attempt a solution (e.g., by catching the relevant exceptions and re-raising them with additional context).",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11630": {
        "explanation": "The issue describes an error encountered after upgrading Django, specifically regarding multiple models across different apps (and databases) sharing the same db_table name. However, it is not a request for a specific code change or feature, but rather a question about whether this is correct behavior in Django 2.2. There is no actionable request or clear definition of what a 'solution' would be (e.g., code change, documentation update, workaround, etc.). Thus, it is unclear what a successful PR would look like.",
        "label": "VAGUE"
    },
    "django__django-11638": {
        "explanation": "The issue clearly describes the problem: when None is passed as data to the test client (specifically in a POST), an exception is raised, but the exception message is not informative enough. The request is to improve the exception message to include the offending key and value. The issue also references the relevant Django release note and provides a link to a proposed change, which further clarifies the intent. The scope and expected outcome (improved exception message) are clear, making it possible to implement a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11669": {
        "explanation": "The issue clearly describes a specific inconsistency: TemplateView automatically adds self.kwargs to the context, while ListView does not. The desired outcome is to stop TemplateView from this behavior to align it with ListView. The scope is clear (modify TemplateView's behavior), and the expected result is well-defined (remove automatic pushing of kwargs into the context).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11680": {
        "explanation": "The issue provides a clear description of the problem: when saving a new model instance with a primary key that has a default, Django issues an unnecessary UPDATE query before the INSERT. The expected behavior is demonstrated by the manager's create() method and by using save(force_insert=True), both of which only issue an INSERT. The issue also references a related Django ticket for context. The desired outcome is to remove the unnecessary UPDATE when saving a new instance. The code samples and expected/actual SQL queries make the problem and the solution path clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11692": {
        "explanation": "The issue provides a clear description of the problem: using OuterRef in a union Subquery raises a ValueError. It includes example code that reproduces the error and specifies the error message encountered. However, it does not specify what the expected behavior should be (e.g., should this be supported, or should a clearer error be raised?), nor does it clarify whether the goal is to fix the ORM to support this pattern or to improve the error message. There is also an aside about order_by, but it's not clear if that is part of the issue to be solved. Thus, while the technical context is clear, the desired outcome is not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11695": {
        "explanation": "The issue clearly describes the problem: the Ajax autocomplete widget (using Select2 in Django) currently sends a request on every keypress, which is inefficient. The solution is also specified: rate-limit the requests by adding a delay using the ajax.delay option provided by Select2. The reference to an existing pull request provides further context. The requirements and the method to solve the issue are clear, making it straightforward to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11701": {
        "explanation": "The issue provides a clear description of the problem: searching in the Django admin with a null character (%00) in the query string causes a crash with a specific error message. The environment, stack trace, and reproduction steps are all provided. While the issue does not prescribe a specific solution, it is clear that the expected outcome is to prevent the crash (likely by sanitizing or rejecting null characters in the search query). This is a common type of input validation/robustness fix, and the requirements are clear enough to proceed with a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11728": {
        "explanation": "The issue provides a clear description of the bug: the function replace_named_groups() (possibly called by simplify_regexp()) fails to replace the final named group if the pattern does not end with a '/'. An example input is given, and the expected behavior is implied (the final named group should be replaced regardless of a trailing '/'). The context (Django REST Framework, issue 6888) and a workaround are mentioned. While the exact replacement logic is not spelled out, the problem and expected fix are clear enough for a developer familiar with regex and the codebase to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11734": {
        "explanation": "The issue provides a concrete test case, code samples, and the specific error encountered. It clearly describes the context (using OuterRef in exclude() or ~Q() within an Exists subquery), what works, what fails, and the error message. While it does not specify the exact desired fix, it is clear that the goal is to make the exclude() and ~Q() cases work similarly to the filter() case. This is a common pattern in bug reports for ORMs, and the expected behavior is reasonably inferable from the working example. Therefore, the issue is well-specified enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11740": {
        "explanation": "The issue provides a clear description of the problem: when changing a UUIDField to a ForeignKey in a Django model, the generated migration does not create a dependency on the target app/model, which leads to migration errors. The user provides a minimal reproducible example, the expected behavior, the actual behavior, and even a test case that triggers the error. The only minor ambiguity is whether the solution should be a code fix, documentation update, or clarification, but the technical problem is well-specified and actionable for someone familiar with Django migrations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11742": {
        "explanation": "The issue clearly describes the problem: there is no check to ensure that the max_length of a Field is at least as long as the longest value in its choices. The desired solution is to add such a check, presumably to raise an error or warning if max_length is too small. While it does not specify exactly where or how the check should be implemented (e.g., at model definition time, at runtime, etc.), it is clear what the check should do and why. An experienced engineer could reasonably interpret and implement this requirement.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11749": {
        "explanation": "The issue provides a clear description of the problem: when using call_command with a keyword argument that is part of a required mutually exclusive group, Django raises a CommandError, even though the argument is provided. The issue includes code snippets, the error message, and an analysis of the relevant Django source code, pointing out that call_command does not handle mutually exclusive groups correctly. The expected behavior is implied: call_command should allow passing such arguments via kwargs and not raise an error. While the issue does not specify the exact implementation details of the fix, it is clear what needs to be addressed and what a successful solution would look like (i.e., call_command should work with mutually exclusive required groups when arguments are passed via kwargs).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11751": {
        "explanation": "The issue provides a clear description of the desired change: to make certain security headers (SECURE_CONTENT_TYPE_NOSNIFF, SECURE_BROWSER_XSS_FILTER, and X_FRAME_OPTIONS) enabled by default in new Django projects, and to consider changing their global defaults (with a deprecation period). It lists the specific settings and headers involved, references the current behavior, and provides rationale. While there is some room for interpretation regarding the exact implementation details (e.g., how to handle the deprecation period), the core requirements are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11754": {
        "explanation": "The issue provides a clear set of actionable items: (1) refactor technical_500_response to accept an ExceptionReporter subclass as a parameter, (2) provide an example (e.g., using functools.partial) for configuring a custom reporter, (3) update documentation to explain ExceptionReporter and its filter, and (4) add a DEFAULT_EXCEPTION_REPORTER setting. There is some room for interpretation in the documentation details and the example, but the core requirements are clear and actionable. The mention of possibly deprecating DEFAULT_EXCEPTION_REPORTER_FILTER is noted as a discussion point, not a requirement. Overall, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11772": {
        "explanation": "The issue provides a detailed description of the current problems with the make_template_fragment_key function, including unnecessary quoting, use of MD5, and inefficient string joining. It also references a proposed solution (with code and performance benchmarks) and discusses the implications of changing the hash function. However, the issue is written more as a discussion or proposal rather than a clear, actionable task. It asks for 'thoughts' and mentions related functions that could also be updated, but does not specify exactly what should be done in this PR. There is ambiguity about the scope (just this function, or all similar functions?), and whether the referenced solution should be adopted as-is. Thus, while there is enough information to make a reasonable attempt at a solution, some interpretation is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11790": {
        "explanation": "The issue clearly states that the AuthenticationForm's username field no longer renders with the maxlength HTML attribute, and identifies this as a regression introduced in specific commits. It provides references to discussions for further context. The required fix is to ensure that the maxlength attribute is rendered again for the username field. This is a concrete, actionable issue with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11797": {
        "explanation": "The issue provides a concrete code example, expected vs. actual SQL queries, and points out a specific problem: when using a queryset as a subquery filter, the generated SQL's GROUP BY clause changes unexpectedly. However, it does not specify what the correct behavior should be in all cases, nor does it clarify whether the problem is with Django's ORM, the user's query construction, or a misunderstanding of expected SQL. There is enough information to attempt a solution by aligning the GROUP BY in the subquery with the outer query, but some interpretation is required regarding the intended fix and its scope.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11808": {
        "explanation": "The issue clearly describes the current behavior of Model.__eq__, the expected behavior according to the Python data model, and the consequences of the current implementation. It provides a specific code location and a concrete example of the problem. The required change is to update Model.__eq__ so that it returns NotImplemented when it cannot compare to the other object, rather than always returning False. This is a well-specified behavioral change with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11810": {
        "explanation": "The issue provides a clear description of the unexpected behavior: chaining select_related (and prefetch_related) on a Django QuerySet mutates the original QuerySet, contrary to the documented expectation that these methods return a new, independent QuerySet. The issue includes a minimal reproducible example (models, test code, and output), and explicitly states the expected vs. actual behavior. The problem is well-scoped and actionable: ensure that select_related and prefetch_related do not mutate the original QuerySet when chained. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11815": {
        "explanation": "The issue provides a clear description of the problem: when using an Enum as a default value for a CharField, Django migrations serialize the Enum's value (which may be translated and thus change over time) instead of its name (which is stable). This leads to migration errors if the value changes due to translation. The issue includes a concrete example, the error encountered, and a proposed solution (use the Enum name in migrations). The expected behavior is well-articulated, and the code context is sufficient to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11820": {
        "explanation": "The issue describes a specific error (models.E015) that is raised when 'Meta.ordering' contains '__pk' of a related field, and provides an example ('option__pk'). It also references a regression and a specific commit. However, it does not specify what the correct behavior should be: should 'option__pk' be allowed, or should the error message be improved, or should something else happen? Without knowing the intended behavior, it is ambiguous what the solution should be.",
        "label": "VAGUE"
    },
    "django__django-11829": {
        "explanation": "The issue provides a clear description of the problem: the patch_cache_control function does not handle the 'no-cache' directive according to the HTTP specification, especially when field-names are specified. It also describes the expected behavior (how 'no-cache' should be treated as a list, and precedence rules for no_cache=True vs. no_cache='foo'). There is enough context, including references to the relevant RFC and a concrete example, to implement a solution that aligns patch_cache_control with the spec. While some implementation details (e.g., exact API or error handling) are not specified, the requirements are clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11848": {
        "explanation": "The issue provides a clear description of the problem: the current implementation of two-digit year parsing in django.utils.http.parse_http_date does not comply with the RFC 7231 requirement to interpret years more than 50 years in the future as the most recent past year with the same last two digits. It also describes the current (incorrect) logic and the expected behavior. While it does not specify the exact code change, it is clear what needs to be fixed and what the correct behavior should be, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11880": {
        "explanation": "The issue clearly describes a bug in the __deepcopy__ method of Django's Form Field, specifically that the error_messages dictionary is not being deep-copied, leading to shared state between form instances. It provides links to the relevant code, explains the consequences, and specifies the expected behavior (each form instance should have its own error_messages dictionary). This is sufficient information to attempt a fix: update the __deepcopy__ method to ensure error_messages is deep-copied. No further clarification is needed to proceed with a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11883": {
        "explanation": "The issue clearly specifies that the cache.delete() method should be modified to return a value indicating whether the deletion succeeded. It also provides context for why this is useful and references how this is handled in various backends (memcache, file, in-memory, database, Redis). The expected behavior is clear: after the change, cache.delete() should return a boolean or similar indicator of success. The implementation details for each backend are hinted at, but not fully specified, yet the intent and the required outcome are unambiguous. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11885": {
        "explanation": "The issue provides a clear description of the current behavior (multiple DELETE queries per table per foreign key) and the desired improvement (combining these queries by table using OR to reduce roundtrips). It gives concrete examples with models and the resulting SQL queries, both before and after the proposed change. The scope is well-defined: modify the deletion.Collector logic to combine fast delete queries by table. While some implementation details (e.g., edge cases, performance considerations) are left to the implementer, the core requirement is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11891": {
        "explanation": "The issue provides a clear description of the problem: ConditionalGetMiddleware returns a 304 Not Modified response if the ETag matches, even if the Last-Modified header has changed, which is incorrect behavior according to HTTP standards. It also explains the context (interaction with x-sendfile responses where ETag is always the same due to empty content) and the expected behavior (Django should not return 304 if Last-Modified has changed, even if ETag is the same). The issue is actionable: the logic in get_conditional_response() should be updated to account for changes in Last-Modified even when ETag matches. There is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11893": {
        "explanation": "The issue clearly describes the problem: DateTimeField does not accept ISO 8601 formatted date strings with a 'T' separator, which is standard in ISO 8601, while Django expects a space. The expected behavior is implied: DateTimeField should accept both 'T' and space as valid separators. The scope and the required change are clear, making it possible to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11894": {
        "explanation": "The issue clearly describes the problem: when using Django's .set_cookie and set_signed_cookie, the samesite argument defaults to None, but Django does not explicitly set the SameSite=None attribute on the cookie. Due to recent browser changes (specifically Chrome), cookies without an explicit SameSite=None attribute are not sent for cross-origin requests. The issue provides a link to the relevant Chrome documentation and explains the expected behavior. The required solution is to ensure that when samesite=None is passed, the cookie is set with SameSite=None explicitly. This is a well-specified issue with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11903": {
        "explanation": "The issue provides a clear description of the current behavior (the message 'No Django settings specified.' is printed even when settings are configured via settings.configure(...)), the relevant code snippet, and the desired behavior (the check should use settings.configured, as in the parent execute method). It is clear what needs to be changed: update the condition to check settings.configured instead of just the environment variable. The context and intent are sufficiently specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11905": {
        "explanation": "The issue clearly states that the __isnull lookup should not accept non-boolean values, and that an error should be raised if a non-boolean value is provided. The motivation and context are explained, and the expected behavior (raising an error for non-boolean values) is unambiguous. While it does not specify the exact error type or message, it is clear what needs to be changed in the codebase and what the outcome should be. This is sufficient for an experienced engineer to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11916": {
        "explanation": "The issue provides a clear description of the performance problem with Django's prefetch_related, including profiling data and a detailed explanation of where the inefficiency lies (the creation of related querysets). It also proposes two possible solutions, with a strong preference for the second: lazily instantiating the .all() queryset via a proxy object that mimics a QuerySet but doesn't incur the overhead unless needed. The technical challenge is acknowledged, but the desired outcome and constraints are well-articulated. While some implementation details are left to the developer, the problem and the preferred solution are clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11951": {
        "explanation": "The issue provides a clear description of the problem: the batch_size parameter in bulk_create currently overrides the compatible batch size calculation, which is inconsistent with the behavior in bulk_update. The issue references specific lines in the codebase and suggests a concrete solution (using min(batch_size, max_batch_size) logic). This is sufficient information for an experienced engineer to implement the fix as described.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11964": {
        "explanation": "The issue provides a clear description of the problem: when using Django's TextChoices/IntegerChoices for a model field, the value of the field on a freshly created instance is an enum member (e.g., MyChoice.FIRST_CHOICE), not the raw value (e.g., 'first'), which leads to inconsistent behavior compared to retrieved instances. The issue includes a reproducible example, test cases, and the observed vs. expected behavior. However, it does not specify exactly what the desired fix should be (e.g., should the model always return the raw value, or always the enum member, or provide a consistent interface?). There is a sensible interpretation: the getter should always return the raw value (str/int), matching the behavior of retrieved objects and the expectations in the tests. Thus, while there is some room for interpretation, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11999": {
        "explanation": "The issue provides a clear description of the problem: the inability to override the get_FIELD_display method in Django 2.2+, which worked in 2.1. It includes a code example, the expected behavior, and the observed behavior. However, it does not specify whether the solution should be a code change in the user's project, a workaround, or a patch to Django itself. There is also some ambiguity about whether the issue is a bug report or a feature request. Still, a sensible interpretation is to restore the ability to override get_FIELD_display as in 2.1, likely by investigating changes in Django 2.2. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12009": {
        "explanation": "The issue provides a detailed description of the current situation: both /usr/bin/django-admin and /usr/bin/django-admin.py are installed, explains why this happens (due to both 'scripts' and 'entry_points' in setup.py), and suggests that only one should be installed. However, it does not specify which one should be kept, nor does it discuss any compatibility or migration concerns. There is a clear, sensible interpretation: modify setup.py to install only one of the scripts, but some decisions (which one to keep, possible deprecation warnings, etc.) are left to the implementer. Thus, it is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12039": {
        "explanation": "The issue provides concrete examples of the current and expected SQL output for CREATE INDEX statements, specifically regarding whitespace placement. It references the relevant code area and describes the cause of the bug (empty strings in col_suffixes). The problem is clearly described, and the expected behavior is unambiguous. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12050": {
        "explanation": "The issue provides a clear description of the problem: Query.resolve_lookup_value is coercing input lists to tuples, which breaks exact value queries for certain ORM field types like PickledField that depend on the input type. The expected behavior is also specified: the return type should match the input iterable type. While it doesn't specify the exact code change, it is clear what needs to be fixed and what the expected outcome is. This is sufficient for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12062": {
        "explanation": "The issue clearly describes the desired feature: adding a setting DATABASE['TEST']['MIGRATE'] = False to disable all migrations for a specific database during tests. It also suggests a technical approach (hooking into MigrationLoader.migrations_module() and returning None). The context is clear, the expected behavior is defined, and there is a concrete implementation hint. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12073": {
        "explanation": "The issue provides a clear objective: deprecate the InvalidQuery exception in favor of more specific exceptions (FieldDoesNotExist and FieldError), and implement a deprecation shim that warns on usages of 'except InvalidQuery'. It also specifies where InvalidQuery is currently used and what should replace it. While some implementation details (such as the exact wording of the deprecation warning) are left to the implementer, the overall requirements and rationale are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12091": {
        "explanation": "The issue clearly states that the goal is to deprecate the HttpRequest.is_ajax property, provides context for why this is being done, and references similar actions in other frameworks. While it does not specify the exact deprecation process (e.g., whether to add a warning, update documentation, etc.), it is standard practice in Django to add a deprecation warning, update docs, and possibly add release notes. An experienced engineer familiar with Django's deprecation process would know the steps to take. Therefore, the issue is reasonably specified, with some minor blanks to fill in regarding the exact deprecation steps.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12113": {
        "explanation": "The issue provides a clear description of the problem: running the admin_views.test_multidb test with persistent SQLite databases (using --keepdb) results in a 'database is locked' error. It includes the relevant configuration, the command used, and the full traceback. However, it does not specify what the expected behavior should be (e.g., should the test pass, should the test be skipped, or should the code be changed to avoid the lock?). It also does not suggest a specific solution or approach. While the problem is clear, the desired outcome is not fully specified, but a reasonable interpretation is to make the test pass with persistent SQLite databases. Therefore, it is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12121": {
        "explanation": "The issue provides a clear description of the current behavior, the desired behavior, and two possible solutions: (1) change the code to handle ValueError in to_url as a signal to skip the route, or (2) update the documentation to clarify the correct way for a converter to decline a match. The context, motivation, and example code are all provided. While there is a choice between a code change and a documentation change, both are well-explained and actionable. As an engineer, I could proceed to implement either solution or open a PR for both, depending on project policy.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12125": {
        "explanation": "The issue provides a clear description of the problem, including a step-by-step reproduction scenario, code samples, and the expected versus actual output. It specifies that makemigrations generates incorrect import paths for inner classes used as fields in Django models, and gives concrete examples for both a custom CharField and an EnumField. The expected behavior is also clearly stated. This is sufficient information for an experienced engineer to attempt a solution, such as investigating how makemigrations serializes field classes and correcting the path resolution logic.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12143": {
        "explanation": "The issue clearly describes a specific bug: the use of unescaped user-supplied 'prefix' in a regex pattern in admin/options.py, which can cause data loss if the prefix contains regex special characters. It points to the exact line of code, explains why it's a problem, and suggests a straightforward fix (using re.escape()). The only ambiguity is whether to look for similar patterns elsewhere, but the main actionable item is well-defined. Therefore, a meaningful solution can be attempted by fixing this instance and, optionally, checking for similar issues elsewhere.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12148": {
        "explanation": "The issue describes an inconsistency between the FlatPage model's get_absolute_url() method and the reverse() function, particularly when a URL prefix is used for flatpages. It points out that the admin interface link does not work in this scenario, while the templatetag does. However, it does not specify what the expected behavior should be, nor does it provide details on the desired resolution (e.g., should get_absolute_url() use reverse()? Should the admin link be fixed in a particular way?). There is enough context to make a reasonable attempt at aligning the behaviors, but some implementation details would need to be inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12153": {
        "explanation": "The issue provides a clear description of the problem: a specific Django migration (0011_update_proxy_permissions) fails in a multi-database environment when the 'default' database is not configured, despite Django documentation suggesting this should be possible. The error and the relevant code line are provided, and the expected behavior (support for no default database) is implied. However, the issue does not specify exactly how the migration should be changed to support this scenario, nor does it specify what the correct behavior should be (e.g., should the migration run on all databases, or should it be skipped if 'default' is missing?). There is enough information to make a sensible attempt at a solution (e.g., making the migration database-agnostic or handling the absence of 'default'), but some implementation details would need to be inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12155": {
        "explanation": "The issue provides a clear description of the problem (docutils error when the first line of a docstring is not empty), the context (admindoc, Django docstrings), the error message, the relevant code snippet, and a proposed solution (skip the first line when calculating indentation). This is sufficient information to attempt a fix and verify its correctness.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12161": {
        "explanation": "The issue clearly describes the current behavior (callable values in through_defaults are not evaluated, leading to unexpected results), the expected behavior (callable values should be evaluated, similar to how defaults in get_or_create or model fields work), and provides context and references. The required change is straightforward: update through_defaults to support callables in the same way as other defaults mechanisms in Django. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12172": {
        "explanation": "The issue provides a clear description of the problem (Jupyter users encountering Django's async-unsafe checks), the motivation (ease of use in notebooks), and a concrete proposal for a solution (add a flag, preferably an environment variable, to disable the check). While it leaves some implementation details open (e.g., exact name of the variable, how/where to check it), these are minor and can be sensibly filled in by an experienced engineer. The success criteria are clear: provide a way to disable the async-unsafe check via a flag or environment variable, with a preference for the latter.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12187": {
        "explanation": "The issue provides a clear description of the problem: the staticfiles manifest generated by Django's ManifestStaticFilesStorage is always saved in a publicly accessible location, which can expose sensitive information. The request is to allow configuration of where the manifest is saved, so it can be placed somewhere non-public. While the issue discusses potential solutions and related concerns, it does not specify exactly how the configuration should be exposed (e.g., via settings, storage options, etc.), nor does it define the expected behavior for remote storage backends like S3. However, the core requirement\u2014to allow configuration of the manifest's save location\u2014is clear and actionable. There is some room for interpretation in the implementation details, but a sensible solution can be proposed based on the information given.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12193": {
        "explanation": "The issue provides a clear description of the bug: when using SplitArrayField with BooleanField and preexisting data, the 'checked' attribute is incorrectly set for all subsequent widgets after the first True value due to mutation of the attrs dict in CheckboxInput's get_context(). The cause is identified, and a relevant code reference is provided. The expected behavior is implied: each widget should reflect its corresponding value in the initial data, not inherit 'checked' from previous iterations. This is sufficient information to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12198": {
        "explanation": "The issue provides a clear description of the problem: decorating a function with sensitive_variables causes the function's signature to be lost, which breaks compatibility with inspect.getcallargs and leads to TypeError exceptions. The desired outcome is for sensitive_variables to preserve the original function's signature, as it did in version 1.6. While it doesn't specify the exact technical approach, the problem and the expected behavior are clear enough for an experienced engineer to attempt a solution (likely by using functools.wraps or similar).",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12209": {
        "explanation": "The issue provides a clear description of the observed change in behavior between Django versions, including a concrete code example and the resulting database operations. It also references a related Django ticket and discusses the impact on loaddata. However, it does not explicitly state what the desired behavior should be or what the fix should accomplish, though it hints at a more backwards-compatible approach. There is enough information to make a reasonable attempt at restoring the old behavior or clarifying the intended logic, but some interpretation is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12212": {
        "explanation": "The issue provides a detailed traceback, a clear description of the error ('local variable pk referenced before assignment'), the context in which it occurs (Django admin, reversion, deserialization), and a suggested fix (change 'for pk in field_value:' to 'for pk in field_value or []:'). The root cause is also analyzed (handling None values in a field that may have changed from FK to M2M). While the issue is somewhat exploratory and the reporter is not 100% certain of the root cause, the actionable fix is clear and testable. Therefore, a meaningful attempt at a solution can be made based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12225": {
        "explanation": "The issue clearly describes the current error message, the scenario in which it occurs (an inline with multiple foreign keys to the same parent model), and the desired improvement (the error message should recommend specifying fk_name). The context and the expected change are both clear, making it straightforward to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12231": {
        "explanation": "The issue describes a problem encountered when using Django's RelatedManager.set() method with a list of string representations of primary keys, which leads to unnecessary removal and addition of foreign keys due to type mismatch. The user suggests that converting strings to integers before calling set() resolves the issue. However, the issue is primarily a question about whether Django should handle this type conversion internally, raise an exception, or leave it to the user. There is no clear request for a specific code change or feature, nor is there a defined acceptance criterion for a solution. The issue is more of a discussion or feature request than a well-specified task.",
        "label": "VAGUE"
    },
    "django__django-12237": {
        "explanation": "The issue provides a clear description of the problem (slugify does not handle the Turkish '\u0130' correctly with allow_unicode=True), a reproducible example, an analysis of the root cause, and a proposed solution (change the order of lower() and re.sub() in slugify). The expected behavior is implied: slugify should produce a valid slug for strings containing '\u0130' without causing a ValidationError. While the issue references an external README for more details, the information in the issue itself is sufficient to attempt a solution. The only minor ambiguity is whether the proposed solution is the desired fix, but the problem and a reasonable fix are both clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12262": {
        "explanation": "The issue provides a clear description of the problem: custom template tags in Django raise a TemplateSyntaxError when using keyword-only arguments with defaults, and the error messages are incorrect in certain cases. It includes code examples that reproduce the issue, specifies the expected vs. actual behavior, and mentions that the problem affects both simple_tag and inclusion_tag. The scope is clear, and a solution would involve correcting the argument parsing and error reporting for these template tags. While the author mentions already having a fix, the issue itself is well-specified for someone else to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12273": {
        "explanation": "The issue provides a concrete code example, describes the expected and actual behavior, and specifies the Django version. It is clear that the reporter expects setting the primary key to None and saving should create a new object, but this does not happen for the child model. The code is sufficient to reproduce and understand the problem, and the desired outcome is clear. Therefore, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12276": {
        "explanation": "The issue provides a clear description of the undesired behavior (the 'required' attribute is displayed on FileInput even when initial data exists), references the relevant method and file, and suggests a concrete change (move the logic from ClearableFileInput.use_required_attribute to FileInput.use_required_attribute). The expected outcome is that the 'required' attribute should not be rendered when a file is already present. While there is a minor note about possible overlooked use cases, the main requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12286": {
        "explanation": "The issue provides a clear description of the problem: Django raises translation.E004 when LANGUAGE_CODE is set to a sublanguage (e.g., 'de-at') that is not explicitly listed in LANGUAGES, even though the base language ('de') is available. The expected behavior, as per Django documentation, is that the base language should be used if the sublanguage is not available. The issue includes a concrete example, the relevant error message, and the context (Django 3.0.2, settings.py). It is clear that the solution should involve modifying the system check so that translation.E004 is not raised in this scenario. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12299": {
        "explanation": "The issue describes a problem where update() and delete() operations are being called on QuerySets that have been combined using union(), intersection(), or difference(). It suggests that these operations should raise a descriptive error instead of proceeding. The example provided demonstrates the current (undesired) behavior. The required change is clear: prevent update()/delete() on such QuerySets and raise an appropriate error. The scope and expected outcome are well-defined, making it possible to implement a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12304": {
        "explanation": "The issue clearly describes the problem: Django's new enumeration types (Choices classes) are not usable in templates because they are callable, and Django templates call callables with no arguments, which fails for these classes. The issue also suggests a concrete solution: declare do_not_call_in_templates = True on the Choices classes. The context, the problem, and a proposed fix are all present, making it clear what needs to be done for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12308": {
        "explanation": "The issue clearly describes the problem: when a JSONField is readonly in the Django admin, its value is displayed as a Python dict (with single quotes) rather than as valid JSON (with double quotes). It also points to the relevant code area (django.contrib.admin.utils.display_for_field) and suggests a possible solution (using prepare_value of the JSONField). The expected behavior is clear: display valid JSON in the admin for readonly JSONFields. This is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12313": {
        "explanation": "The issue provides a detailed description of the problem, including the initial and modified code, the commands run, the output, and the problematic behavior (Django's makemigrations not detecting model name case changes). It also describes attempted workarounds and the resulting errors. However, the issue does not specify what the desired behavior should be (e.g., should makemigrations detect case-only renames and prompt for a RenameModel operation?), nor does it specify the expected user experience or acceptance criteria. While the problem is clear, the solution space is somewhat open to interpretation, especially regarding how Django should handle case-only renames. Therefore, while a sensible solution can be attempted, there are some blanks to fill in regarding the exact requirements.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12325": {
        "explanation": "The issue provides a concrete code example, the error message encountered, and a description of the unexpected behavior (that the order of OneToOneField declarations affects Django's model setup, even though parent_link is explicitly set). The user also expresses an expectation that ordering should not matter and references Django's behavior with other model components. While the issue is clear in terms of what is happening and what the user expects, it does not specify exactly what a 'fix' would look like (e.g., whether to change Django's model metaclass logic, or just document the behavior). However, as an experienced engineer, I can reasonably interpret that the goal is to make the order of OneToOneField declarations irrelevant when parent_link is used, or at least to provide a clear error or documentation. Thus, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12360": {
        "explanation": "The issue clearly describes a specific problem: when model names are very long, the generated permission codenames can exceed the database column limit, causing errors. The request is to add a system check (i.e., a Django check framework validation) to catch this before it causes a database error. The expected behavior is clear: warn or error if a model's name would cause default permission codenames to exceed the allowed length. The context, motivation, and desired outcome are all well-explained, making it clear what needs to be implemented.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12364": {
        "explanation": "The issue provides a clear context: the admin changelist's deterministic ordering detection currently only considers 'unique_together' and not 'UniqueConstraint' (without conditions). It references the relevant code and the desired change: update the detection logic to also consider UniqueConstraints, aligning with Django's current recommendations. The expected behavior is clear, and the scope is well-defined. There is enough information to attempt a solution by updating the _get_deterministic_ordering() logic to include UniqueConstraints without conditions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12394": {
        "explanation": "The issue provides a clear error traceback, including the file and line numbers, and describes the specific problem: a TypeError occurs because 'itertools.chain' is not subscriptable when raising an error about protected related objects during deletion. The context is Django's model deletion logic, and the error is reproducible in a test. The expected outcome is to fix the crash so that the error is raised correctly without a TypeError. While the issue does not specify the exact fix, it is clear what needs to be addressed and where in the codebase the problem lies.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12396": {
        "explanation": "The issue provides a clear description of the problem: when running a subset of tests, the test runner attempts to access a database without the 'test_' prefix, leading to a permissions error. The issue includes a concrete example with settings, a reproduction command, and a full traceback. The expected behavior (that the test database name should be prefixed with 'test_') is implied by the description and the comment in the settings. While the root cause is not fully diagnosed, the issue is specific about what is going wrong and how to reproduce it, making it possible to attempt a solution by ensuring the test database name is correctly prefixed in all relevant code paths.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12406": {
        "explanation": "The issue clearly describes the undesired behavior (a blank '-------' radio option is rendered for a required ForeignKey field using RadioSelect) and provides a concrete example with model and form code. It also specifies the expected behavior: when blank=False, there should be no blank radio option rendered. The context and the desired outcome are both clear, making it possible to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12407": {
        "explanation": "The issue provides a clear description of the current behavior (the {% include %} tag uses get_template and fails when given a list of templates), the desired behavior (it should use select_template to allow fallbacks, as other rendering functions do), and even points to the relevant code change. It also provides concrete examples and context for testing. The only minor ambiguity is how to handle Template instances, but the issue acknowledges this and suggests it's a detail to be considered. Overall, the requirements and expected outcome are clear enough for a meaningful solution attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12419": {
        "explanation": "The issue clearly describes the context (Django's SECURE_REFERRER_POLICY setting), the current state (ability to set Referrer-Policy header since Django 3.0), the proposed change (set the default to 'same-origin'), and the rationale (reduce information leakage to third parties). It also mentions the potential risk (breaking sites that rely on Referer header for verification) and provides relevant documentation. The required change is unambiguous: update the default value for SECURE_REFERRER_POLICY to 'same-origin'.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12453": {
        "explanation": "The issue provides a clear description of the problem, the context in which it occurs, the relevant code paths, and a suggested fix (including a code diff). It also explains why the problem occurs and how it differs from similar code (loaddata). The expected behavior is clear: wrapping the deserialization in a transaction to avoid integrity errors. While a minimal reproduction is not provided, the issue is specific enough for an experienced engineer to attempt a solution by following the described steps and applying the suggested fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12464": {
        "explanation": "The issue provides a clear description of the problem: Django's SQLite backend currently raises NotSupportedError for DISTINCT with aggregate functions like GROUP_CONCAT, but SQLite does support this in some cases. The issue references the relevant code, explains the context, and provides a code example. It also notes a caveat (DISTINCT is only allowed without a delimiter). The desired outcome is implied: to allow DISTINCT with GROUP_CONCAT in Django's SQLite backend, possibly by relaxing or modifying the check in django.db.backends.sqlite3.operations.py. While there is some room for interpretation (e.g., how to handle the delimiter case), the core request is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12469": {
        "explanation": "The issue describes a specific bug: when using the admin date_hierarchy filter by month in a non-UTC timezone (e.g., America/Los_Angeles), an extra day appears that actually belongs to the previous month. The expected behavior is implied: the filter should only show days belonging to the selected month. The context (admin date_hierarchy, timezone, extra day) is clear, and the problem is reproducible. While it doesn't specify the exact code location, it is clear what needs to be fixed and how to verify the fix. Therefore, it is well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12470": {
        "explanation": "The issue provides a clear description of the problem, including a code sample, the expected behavior, and the actual behavior. It specifies that the ordering defined in the Parent model's Meta class (ordering = ['-pk']) is not being respected in queries on the Child model, which instead orders by ASC. The expected outcome (ordering by DESC) is also clearly stated. This is sufficient information to investigate and attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12477": {
        "explanation": "The issue clearly describes a problem: Django's system check (fields.E310-E311) does not recognize UniqueConstraint (without conditions) as satisfying the uniqueness requirement, and still suggests using unique_together, which is deprecated. The expected behavior is that UniqueConstraint should be recognized by the checks, and the error should not be raised. The issue provides a concrete example, references the relevant documentation, and describes the desired outcome. This is enough information to attempt a solution: update the system checks to recognize UniqueConstraint (without conditions) as equivalent to unique_together for the purposes of E310/E311.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12484": {
        "explanation": "The issue clearly describes the current behavior (an error message from Django's system checks when a custom user model's USERNAME_FIELD is included in REQUIRED_FIELDS), the specific configuration that triggers it, and the desired improvement (adding a hint to the error message that explicitly names the problematic field and suggests removing it from REQUIRED_FIELDS). The suggestion for the hint is concrete, and the context is sufficient to implement the change in Django's system check framework. There is little ambiguity about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12496": {
        "explanation": "The issue provides a clear description of the problem, including the context (Django multi-table inheritance), the specific condition under which the bug occurs (parent model with a custom primary key, e.g., UUIDField), and a comparison of the correct and incorrect behaviors with example SQL queries. The expected behavior is implied: the extra UPDATE query should not occur when using a custom PK, and the behavior should match the case without a custom PK. While the root cause is not specified, the issue is well-specified enough to attempt a solution by investigating why the extra UPDATE occurs and ensuring the ORM behaves consistently regardless of PK type.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12503": {
        "explanation": "The issue describes a specific bug: when running 'makemessages' without any of the required flags (--locale, --exclude, or --all), the command should raise a CommandError but currently does not due to a bug in an if statement. The expected behavior is clear, and the area of code to investigate is specified. There is also a reference to a PR that claims to fix the issue, but the issue itself is clear enough to attempt a solution independently. The only minor ambiguity is whether the solution should exactly match the referenced PR, but the required behavior is unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12504": {
        "explanation": "The issue describes that the logout link in the admin app is currently implemented as a link (likely a GET request), which is not CSRF-protected. It suggests that, for purity and adherence to best practices (i.e., GET should not change state), the logout action should be protected against CSRF, even if the risk is low. The expected solution is to change the logout mechanism from a link (GET) to a form (POST) and ensure CSRF protection is applied. While the issue is somewhat informal and acknowledges the low risk, it is clear about what needs to be changed and why. Therefore, it is possible to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12508": {
        "explanation": "The issue clearly describes the desired feature: adding a '-c' argument to './manage.py dbshell' so that a user can pass a SQL command directly, similar to how shell commands work. The current workaround (using pipes) is described, and the expected behavior is clear. The implementation details (such as argument parsing and command execution) are left to the developer, but the goal and success criteria are unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12513": {
        "explanation": "The issue clearly states the goal: deprecate the 'providing_args' argument from the Signal class. It explains the rationale, notes that the argument is unused and purely documentational, and suggests that documentation should be handled elsewhere. The issue also references a related confusion (#19579). While it does not specify the exact deprecation process (e.g., whether to emit warnings, update docs, or remove usages), it is standard practice in software engineering to follow established deprecation procedures. Therefore, the issue is reasonably specified: the main task is clear, but some implementation details are left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12517": {
        "explanation": "The issue provides a clear description of the problem: inconsistent datetime formats in runserver logging due to different mechanisms used by WSGIRequestHandler and WSGIServer. It also points out the root cause (use of server_time vs asctime), references relevant documentation, and suggests a direction for a solution (making the formats consistent and possibly configurable, with asctime as the default). While there is some room for interpretation regarding the exact implementation (e.g., how to make it configurable), the core requirement\u2014make the datetime logging format consistent and preferably use asctime\u2014is clear. This is enough to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12518": {
        "explanation": "The issue provides a clear description of the problem: when using 'sqlmigrate' on a Django project with squashed migrations, the command either errors due to ambiguous migration names or fails with a KeyError when specifying the full migration name. The issue also provides a concrete reproduction case (a public GitHub repo), the exact commands to run, and the error messages encountered. Furthermore, it specifies two actionable improvements: (A) catch the error and provide a user-friendly message, and (B) allow inspection of individual migrations involved in a squash. While (B) is somewhat open-ended in terms of implementation details, the intent is clear and actionable. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12519": {
        "explanation": "The issue provides a detailed description of a regression between Django 2.2.9 and 3.0.2, including the exact SQL queries generated in both versions and the difference in results (490 vs 519). It identifies that subquery annotations are omitted from the GROUP BY clause in the newer version, leading to duplicate results. The expected and actual queries are both provided, making it clear what the correct behavior should be. While the root cause in the codebase is not specified, the problem is well-demonstrated and the desired outcome is clear: restore the previous (2.2.9) behavior where the subquery annotation is included in the GROUP BY clause. This is sufficient for an experienced engineer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12532": {
        "explanation": "The issue clearly describes an inconsistency between ModelMultipleChoiceField and MultipleChoiceField regarding the error message key used for invalid lists. It specifies that MultipleChoiceField uses 'invalid_list' while ModelMultipleChoiceField uses 'list', and implies that ModelMultipleChoiceField should be updated to use 'invalid_list' for consistency. The required change is clear: update ModelMultipleChoiceField to use 'invalid_list' as the error message key. This is a well-specified issue with a clear and actionable solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12553": {
        "explanation": "The issue clearly states the desired change: increase the default password salt size in BasePasswordHasher from approximately 71 bits to approximately 131 bits, referencing specific security guidelines and standards. The rationale is provided, and there is even a reference implementation (a linked PR), making the requirements and expected outcome clear. The only minor ambiguity is the exact method of increasing the salt size, but the intent and target are explicit.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12588": {
        "explanation": "The issue provides a clear request: to add an option (disabled by default) to the 'remove_stale_contenttypes' command so that it can also remove entries for nonexistent apps. The context is sufficient for an experienced engineer familiar with the codebase and the command in question. The reference to a discussion and a PR suggests there may be more context, but the core requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12591": {
        "explanation": "The issue provides a clear description of the problem: after a specific commit, it is no longer possible to replace a generic site-wide admin action with a specialized one per-admin, due to a system check error (admin.E130). It references the relevant code and provides a concrete example (qsessions app) and a potential workaround (overriding modeladmin.delete_queryset). However, it does not specify what the desired behavior should be, nor does it state whether the goal is to restore the previous ability to override actions, change the system check, or provide a new mechanism. There is enough information to make a sensible interpretation (e.g., restore the ability to override actions per-admin), but some decisions would need to be made by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12613": {
        "explanation": "The issue clearly describes a problem: the Django XML serializer fails when serializing a model with a JSONField, resulting in a TypeError because the serializer expects a string or bytes-like object, but receives something else (likely a dict or list). The error trace is provided, and the context (serializing a queryset with a JSONField) is clear. The expected behavior is implied: the serializer should handle JSONFields gracefully, likely by converting them to a string (e.g., via JSON serialization) before writing to XML. While the issue doesn't specify the exact desired output format for the JSONField in XML, it is a reasonable and standard expectation to serialize it as a JSON string. Thus, a sensible solution can be implemented.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12627": {
        "explanation": "The issue clearly describes the current behavior of the make_password function, the undesired consequences, and the desired change: make_password should raise a TypeError if passed anything other than bytes or str. It provides rationale, references to documentation, and even examples of similar behavior in other libraries. The required change is unambiguous: add type checking to make_password and raise TypeError for invalid types. This is well-specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12663": {
        "explanation": "The issue provides a clear description of the regression, including the code that previously worked, the code that now fails, and the traceback of the error. It specifies the context (use of SimpleLazyObject in a queryset filter with a subquery annotation), the expected behavior (it used to work), and the actual behavior (TypeError is raised). The test case is provided and is sufficient to reproduce the issue. The only minor ambiguity is the exact desired fix (should SimpleLazyObject be supported in this context, or should a more helpful error be raised?), but the regression context and prior behavior make it clear that restoring the previous support is the goal. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12669": {
        "explanation": "The issue provides a clear description of the problem: ordering by a UUID-annotated field in a Django QuerySet fails due to the current regex pattern used for validating order_by arguments. It also suggests a specific code change that resolves the issue for PostgreSQL and asks whether this change would be compatible with other databases. However, the issue does not specify exactly what 'proper field validation' should entail, nor does it define the expected behavior across all supported databases. There is some ambiguity about whether the solution should be limited to PostgreSQL or made cross-database, and whether the proposed regex change is the only acceptable fix. Still, a sensible interpretation would be to update the validation to support UUID fields in a way that is compatible with all databases, possibly by adjusting the regex and adding tests. Thus, the issue is reasonably specified, but not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12671": {
        "explanation": "The issue describes a specific behavior in Django management commands: OutputWrapper adds a newline to the end of the text, but currently, calling self.stdout.write() or self.stderr.write() with no message fails because 'msg' is a required positional argument. The suggestion is to allow these methods to be called with no arguments, which would result in just a newline being written. While the issue could be more explicit about the desired API (e.g., should an empty call always write a newline, or should it be optional?), the intent is clear enough to attempt a solution: modify OutputWrapper.write to allow an empty message and write a newline. Therefore, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12708": {
        "explanation": "The issue provides a clear description of the problem, including the Django version, steps to reproduce, the error encountered, and the relevant code location. It also outlines two specific concerns: (1) the inability to delete index_together when unique_together exists on the same fields, and (2) the undesired index re-creation when moving the declaration. While the second point is a bit more open-ended, the first is concrete and actionable. There is enough information to attempt a meaningful solution, especially for the first point.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12713": {
        "explanation": "The issue describes a problem where the 'widget' parameter does not work as expected when passed to the 'formfield_for_manytomany()' function, unlike its behavior in 'formfield_for_foreignkey()'. However, it does not specify what the expected behavior is, what the current behavior is, or what a successful solution would look like. There is no information about the context, the codebase, or any acceptance criteria. This makes it ambiguous and difficult to determine exactly what needs to be changed or implemented.",
        "label": "VAGUE"
    },
    "django__django-12733": {
        "explanation": "The issue clearly describes the desired change: modify the sql_flush() operation to use PostgreSQL's TRUNCATE ... RESTART IDENTITY syntax, which will both truncate tables and reset their sequences in a single query. The motivation (performance improvement) and the specific SQL feature to use are both stated. While it does not specify the exact code location, it is clear what needs to be changed and why, and an experienced engineer with access to the codebase can implement this.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12734": {
        "explanation": "The issue clearly describes a scenario where a CharField's max_length is changed on a model that is referenced by a ManyToManyField, and the migration does not update the corresponding field in the through table. The expected behavior (that the through table's field should also be updated) is implied, and the steps to reproduce are clear. However, it does not specify exactly what the correct behavior should be in all cases (e.g., should the migration system always update through tables automatically?), nor does it specify the Django version or any customizations. Still, a reasonable interpretation is possible: the migration system should detect and propagate such changes. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12741": {
        "explanation": "The issue clearly describes the current function signature, the proposed new signature, and the rationale for the change (the 'using' argument can be inferred from 'self.connection.alias'). It also provides an example of current usage and points out that some internal calls already use the new pattern. The required change is unambiguous: remove the 'using' argument from the method and update all internal calls accordingly. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12747": {
        "explanation": "The issue clearly describes the current inconsistent behavior of QuerySet.Delete when zero objects are deleted, provides concrete examples, and specifies the expected behavior (either always an empty dictionary or always a dictionary with model-label keys and zero values). This gives enough information to identify the code path, reproduce the issue, and implement a consistent return value as described. The only minor ambiguity is which of the two expected behaviors to choose, but the issue allows for either, so a reasonable choice can be made and justified in the PR.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12748": {
        "explanation": "The issue clearly states the desired feature: adding support to reset sequences on SQLite, and provides a reference to the internal 'sqlite_sequence' table, which is the mechanism SQLite uses to track AUTOINCREMENT values. While it does not specify the exact API or interface changes required, it is clear that the solution should involve manipulating the 'sqlite_sequence' table to reset sequence values. As an experienced engineer, I can reasonably infer the requirements and implement a solution that resets sequences, possibly by exposing a method or command to do so. There are some implementation details to decide, but the core requirement is clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12754": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the error encountered, and the root cause (migration operation order). It also suggests the expected behavior: the migration autodetector should generate operations in the correct order to avoid the error. While it references another issue (#21890), the current description is self-contained and actionable for someone familiar with Django migrations. The required solution is to adjust the migration autodetector to order operations correctly in this scenario.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12771": {
        "explanation": "The issue provides a clear motivation and rationale for the proposed change: switching ModelState.fields (and similarly .indexes and .constraints) from a List[Tuple[str, models.Field]] to a Dict[str, models.Field]. It explains why the original design was chosen, why it is no longer necessary, and what the benefits of the change would be. The scope of the change is well-defined, and the expected outcome is clear. There are no major ambiguities about what needs to be done or why.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12774": {
        "explanation": "The issue clearly describes the current behavior (in_bulk() only works for fields with unique=True), the desired behavior (it should also work for fields that are unique via UniqueConstraint), and provides a concrete example with code and the resulting error. The expected change is unambiguous: update in_bulk() to recognize fields that are unique due to UniqueConstraint. This is a well-specified feature request with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12796": {
        "explanation": "The issue clearly describes the current behavior (makemigrations requires a database connection due to a specific consistency check), the problem this causes (can't run makemigrations without a database), and a proposed solution (add a flag to skip the check or fail gracefully). The location in the code is referenced, and the desired outcome is clear: allow makemigrations to run without a database by skipping the check. There is some room for interpretation (e.g., exact flag name, CLI interface, error messaging), but the core requirement is unambiguous and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12821": {
        "explanation": "The issue provides a clear description of the current state (some admin static assets are minified, others are not), the rationale for why this is problematic (inconsistency, small file sizes, manual and error-prone process), and a concrete proposal: stop minifying admin files altogether, except for vendored libraries. The expected outcome is clear: remove the minification step for admin static assets that are not vendored, and ensure only vendored libraries remain minified. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12830": {
        "explanation": "The issue provides a clear description of the problem: the current 'max_num' parameter in Django's formset_factory limits the number of forms, even when more initial data is provided, and the user expects to be able to initialize more forms than 'max_num' allows. The request is to add an 'absolute_max' parameter to formset_factory to distinguish between the maximum number of forms allowed via POST (for security) and the maximum number of forms that can be initialized (for flexibility). The issue references documentation, provides a concrete example, and links to a related discussion. While the exact semantics of 'absolute_max' (e.g., how it interacts with 'max_num', what the default should be) are not fully spelled out, the intent and the required change are clear enough for a meaningful implementation attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12851": {
        "explanation": "The issue clearly states the desired action: remove 'ifequal' from the template language. While it does not specify every file or test that needs to be updated, the intent is unambiguous and the scope is clear for someone familiar with the codebase. The rationale is provided, and the expected outcome (removal of 'ifequal') is straightforward. Any additional details (such as updating documentation or tests) are standard practice when removing a feature.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12855": {
        "explanation": "The issue clearly states the objective: to deprecate django.conf.urls.url(). It references the documentation and the historical context, and asks if now is the time to deprecate the function. While it does not specify the exact steps (e.g., add a deprecation warning, update docs, etc.), it is standard practice in Django to follow a known deprecation process. An experienced engineer would know to mark the function as deprecated, add warnings, and update documentation. Thus, the issue is reasonably specified, with some blanks to fill in regarding the exact deprecation process.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12856": {
        "explanation": "The issue clearly describes a missing validation in makemigrations: when a UniqueConstraint is added to a model, there is currently no check that the fields specified in the constraint actually exist on the model. It also provides a comparison to existing behavior (unique_together raises E012 if fields are missing), and references a demonstration for context. The required solution is to add a check for UniqueConstraint fields similar to the one for unique_together, raising an appropriate error if the fields do not exist. This is a well-specified issue with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12858": {
        "explanation": "The issue provides a clear description of the error (models.E015) and the context in which it occurs (ordering by a related field's isnull lookup in the model's Meta.ordering). It also provides a working example in the ORM and references a specific Django issue (#29408) that may have introduced the regression. The relationships between the models are described. However, the issue does not specify what the desired behavior is: should the check be relaxed, or should the documentation be updated, or is there a code fix required? There is some ambiguity about what the 'fix' should be, but a reasonable interpretation is to make the check consistent with the ORM's behavior. Thus, it is reasonably specified, but not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12869": {
        "explanation": "The issue provides a clear description of the problem: the 'collectstatic' management command in Django does not run staticfiles system checks, which can lead to misconfigurations going unnoticed. It explains the context, the specific check that is missed, and even suggests a possible solution (invoking finder.check() for each finder and surfacing errors). While the exact implementation details for surfacing errors are not fully specified, the core requirement is clear: ensure that staticfiles checks are run when collectstatic is executed, and errors are reported. This is enough information for an experienced engineer to attempt a solution, possibly by setting requires_system_checks = True or manually invoking the checks and handling their output.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12906": {
        "explanation": "The issue clearly describes what needs to be documented (the admin's requirement on the django.template.context_processors.request context processor), why it is important, and where the documentation should be updated (release notes, admin docs, system check framework). It also specifies the desired behavior for the system check (warning, then error). While there is some room for interpretation in the exact wording or placement within the docs, the requirements are clear enough for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12910": {
        "explanation": "The issue provides a clear motivation and a concrete proposal: replace the boolean 'requires_system_checks' attribute on Command with a more flexible 'required_system_checks' attribute that can take values like '__all__', an empty list, or a list of tags. The intended behavior for each value is described, and the goal is to allow subclasses to specify which system checks to run, improving flexibility and reducing manual error-prone code. While some implementation details (e.g., exact handling of the attribute in the codebase) would need to be worked out, the requirements and desired outcome are clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12915": {
        "explanation": "The issue clearly describes a missing async method (get_response_async) in ASGIStaticFilesHandler, points out the relevant mixin (StaticFilesHandlerMixin), and provides a traceback showing the error encountered. The expected outcome is to implement the missing async response function so that ASGIStaticFilesHandler works correctly in async contexts. The context, the error, and the desired fix are all clear, making this a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12928": {
        "explanation": "The issue provides a clear description of the desired feature: enabling autoreload behavior for the cached template loader in Django, so that template changes are picked up without restarting the server. It references a related fix (#25788) and gives a concrete example of the configuration expected. While it doesn't specify every implementation detail, the requirements and expected behavior are clear enough for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12933": {
        "explanation": "The issue clearly describes the current behavior (writing to stderr) and the desired behavior (raising CommandError) when the clearsessions command is called on a backend that does not implement clear_expired(). The context is specific to the Django management command and its expected error handling. The required change is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12957": {
        "explanation": "The issue clearly identifies a specific template file (contrib/gis/templates/gis/admin/openlayers.js) and a concrete problem: float values rendered in the JS template may be affected by localization settings (like DECIMAL_SEPARATOR), which could break JS syntax. The proposed solution is to use the Django template filter '|safe' on float values. While it doesn't specify exactly which variables need this treatment, it is clear what the problem is, where it occurs, and what the fix should be. An experienced engineer can review the template, identify float values, and apply the suggested fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12961": {
        "explanation": "The issue provides a clear description of the problem, including code samples, the models involved, the queries being executed, and the specific error encountered. It also references related tickets and clarifies how this case is different. The expected behavior is implied: either support for expressions like F('dt').desc(nulls_last=True) in order_by() on union() querysets, or at least a more explicit error message. While there is a slight ambiguity about whether the reporter wants a fix for the functionality or just a better error, the issue is specific enough to attempt a solution (either by improving support or error reporting).",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12965": {
        "explanation": "The issue provides a clear description of the problem: a performance regression and compatibility issue caused by a change in the SQL generated by Django's ORM for Model.objects.all().delete(). It includes specific examples of the SQL generated in Django 3.0 vs 3.1, explains the impact on MySQL's LOCK TABLES, and provides benchmarks demonstrating the performance impact. The desired outcome is implied: revert or avoid the use of the subquery in the generated SQL for delete() to restore previous behavior and performance. However, it does not specify exactly how the ORM should be changed, nor does it discuss potential edge cases or reasons for the original change. Still, as an experienced engineer, I could reasonably interpret the required solution: restore the old SQL generation for this case. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12973": {
        "explanation": "The issue describes a scenario where the Django admin system checks for required middleware and emits errors E408, E409, and E410 if they are missing. However, if the user adds the required middleware in the wrong order, a runtime AssertionError is raised because AuthenticationMiddleware requires SessionMiddleware to precede it. The issue suggests that improving the hint or error message for E410 (or the order of system checks) could help users avoid this runtime error. While the issue is clear about the problem and the desired outcome (improve the hint for E410 regarding AuthenticationMiddleware's dependency on SessionMiddleware), it leaves some implementation details open (e.g., whether to change the error message, the check order, or both). However, it is reasonably clear what needs to be improved: the user experience around E410 and the dependency between these middlewares. Thus, a sensible solution can be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12983": {
        "explanation": "The issue clearly describes the current behavior of slugify (it leaves leading/trailing underscores and dashes), provides a code example, and specifies the desired output (those characters should be stripped from the start and end). The improvement is unambiguous and the expected result is clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13012": {
        "explanation": "The issue provides a clear description of the problem: when using ExpressionWrapper with a constant expression, Django incorrectly includes the constant in the GROUP BY clause, which leads to invalid SQL in Postgres. The issue includes a code sample, the generated SQL, the error message, and a comparison to the correct behavior. The expected behavior is implied: constants should not be included in the GROUP BY clause. While the issue does not specify exactly how to fix the problem, it is clear what the desired outcome is, and there is enough information to attempt a solution in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13023": {
        "explanation": "The issue clearly describes the current behavior (DecimalField.to_python() raises TypeError when passed a dict), the undesired effect (hard to track the problem to the field), and the desired behavior (should raise ValidationError instead). It also provides a concrete example scenario and a clear outcome for a successful solution. The reference to a proposed patch is extra context but not required for understanding the issue. The requirements for a fix are unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13028": {
        "explanation": "The issue provides a clear description of the problem: filtering a queryset using a ForeignKey to a model with a BooleanField named 'filterable' causes a NotSupportedError in Django 3.0.7, but renaming the field to 'filterable_test' resolves the issue. The models, the query, the error traceback, and the workaround are all provided. However, the issue is ambiguous about what the desired solution is: should the codebase be changed to avoid this error, should documentation be updated, or should a workaround be implemented? There is a sensible interpretation (e.g., document the issue or implement a code fix to avoid the error), but the exact expectation is not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13030": {
        "explanation": "The issue clearly describes the current behavior (NULL being included in the IN clause for prefetch_related on a ForeignKey), explains why this is unnecessary and potentially problematic, and provides a concrete example with models and SQL output. The desired outcome is to avoid passing NULL in such cases. There is also a suggestion to generalize the fix to all uses of __in with non-nullable fields, but the core requirement (avoid NULL in IN for FK prefetch) is clear and actionable. The implementation details are left to the engineer, but the problem and the expected change are well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13033": {
        "explanation": "The issue provides a detailed description of the observed behavior, including model definitions, example queries, the resulting SQL, and the user's expectations. However, it is not entirely clear what the desired outcome is: is the user asking for a code change to Django's ORM, a documentation clarification, or confirmation that the behavior is correct? The issue is well-documented from a reproduction standpoint, but the actionable request is ambiguous. There is enough information to investigate and potentially propose a fix or clarification, but some interpretation is required regarding the intended resolution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13066": {
        "explanation": "The issue provides a clear description of the desired outcome: Argon2id should be supported and become the default for Argon2PasswordHasher. It references specific upstream changes in the argon2-cffi library, including the addition of argon2id, its promotion to default, and updated default parameters. The task is to update Django's Argon2PasswordHasher to support argon2id, make it the default, and sync the default parameters (memory_cost and parallelism) with the upstream recommendations. The requirements are clear and actionable, with references to relevant upstream changes for implementation details.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13077": {
        "explanation": "The issue provides a detailed description of a problem encountered when running 'django-admin runserver' on Windows, including error messages, possible causes, and workarounds. However, it is not clear what the expected solution is, or even if the problem is within the Django codebase or an external tool. The issue author is unsure of the root cause and is not requesting a specific change, fix, or feature. There is ambiguity about where the fix should be applied and what constitutes a successful resolution. Therefore, while the problem is described in detail, the lack of a clear ask or acceptance criteria makes it too vague for a meaningful attempt at a solution.",
        "label": "VAGUE"
    },
    "django__django-13085": {
        "explanation": "The issue clearly describes the current behavior (compilemessages always runs msgfmt, even when .mo files are up-to-date), the desired behavior (only run msgfmt if the .po file is newer than the .mo file), and a concrete implementation approach (compare mtimes, handle missing .mo files as mtime=0, and consider the is_writable function's side effects). It also provides rationale and expected impact. While there is a minor open question about the is_writable function (remove or adjust usage), the issue is otherwise specific and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13089": {
        "explanation": "The issue provides a clear error message, a stack trace, the relevant code section, and a hypothesis about the cause (cursor.fetchone() returning None, leading to the 'NoneType' object is not subscriptable error). It also suggests a possible solution (check if fetchone() returns data before subscripting). While it doesn't specify the exact fix, the context and the expected behavior are clear enough for an experienced engineer to attempt a solution. There is little ambiguity about what needs to be addressed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13109": {
        "explanation": "The issue provides a clear description of the problem, including a code example, the current behavior, the reason for the undesired behavior, and a suggested solution. It points to the relevant code in Django and explains why the current use of _default_manager is problematic in certain scenarios. The desired change is explicit: ForeignKey.validate() should use _base_manager instead of _default_manager. This is a well-specified issue that can be addressed directly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13111": {
        "explanation": "The issue provides a clear request: add support for the '%V' ISO week format in WeekArchiveView, referencing Python's strptime support for '%G', '%u', and '%V' since 3.6. It also specifies that using '%V' requires using '%G' for the year, not '%Y', and suggests either handling this special case or improving the error message. The expected behavior and the context are clear, and the references to related issues and Python documentation make the requirements actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13112": {
        "explanation": "The issue provides a clear description of the error encountered, the context (Django version change), the relevant code snippets (models.py, settings.py, apps.py), and the specific problem (makemigrations fails due to a mixed-case app name in INSTALLED_APPS and ForeignKey references). The expected behavior (should work as in Django 3.0) and the actual error message are both included. The problem is well-scoped: resolve the makemigrations crash related to mixed-case app names in ForeignKey references. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13115": {
        "explanation": "The issue clearly specifies that an error should be thrown when UniqueConstraint.deferrable is combined with parameters that require an explicit unique index (specifically, include and opclasses). It also mentions that documentation should be updated to reflect this incompatibility. The required behavior is clear, and the scope of the change is well-defined, making it possible to implement a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13121": {
        "explanation": "The issue provides a clear description of the problem: duration-only expressions (specifically, adding a timedelta to a DurationField in a Django model) do not work on SQLite and MySQL, resulting in a specific traceback. The code sample, error message, and context (Django ORM, DurationField, annotate, F expressions) are all provided. The expected behavior is implied: the expression should work without error. While the issue does not specify the exact fix, it is clear what needs to be addressed and what a successful solution would look like (making the expression work as expected on SQLite and MySQL).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13128": {
        "explanation": "The issue clearly describes the current problem: attempting to subtract two DateTimeFields in a Django ORM annotation without using ExpressionWrapper results in a FieldError due to mixed types. The code sample and error message are provided, and the desired outcome is implied: make temporal subtraction work directly in annotations without requiring ExpressionWrapper. While the exact implementation details are not specified, the goal is clear and actionable for someone familiar with Django's ORM internals.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13158": {
        "explanation": "The issue provides a clear description of the observed behavior, including code samples and a step-by-step explanation of what happens when using the '|' operator versus the 'union()' method in Django QuerySets. It specifies the expected behavior (no objects should be added when the form is submitted empty) and the actual behavior (all objects are added when using union()). While the issue author is not 100% sure it's a bug, the problem is well-demonstrated and reproducible. The only minor ambiguity is whether the fix should be in Django itself or in the usage pattern, but as a software engineer, I could attempt to reproduce and address the issue in the codebase. Thus, it is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13162": {
        "explanation": "The issue clearly describes the current behavior (merge migration filenames use a timestamp), the problem (the names are opaque and do not indicate which migrations are being merged), and a proposed solution (combine the names of the merged migrations in the filename). It also provides an example of the desired output and allows for some flexibility in the exact naming scheme, but the core requirement is clear: filenames should reflect the merged migrations. This is enough information to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13170": {
        "explanation": "The issue clearly describes a current limitation (FilteredRelation's condition does not support nested relations), provides a concrete example of the error, and specifies the desired outcome (support for nested relations in the condition argument). While it does not specify implementation details, the goal is clear: enable the use of nested relations in FilteredRelation conditions. This is a well-specified feature request.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13192": {
        "explanation": "The issue provides a clear description of the problem: Django's email sending fails when the name portion of an email address exceeds 75 characters, due to a recent Python update that disallows newlines in encoded headers. It points to the relevant code, the method involved, and the upstream Python change. The expected outcome is implied: Django should be able to send emails with long names without error. However, it does not specify the exact desired behavior (e.g., should names be truncated, should an error be raised earlier, or should encoding be changed?), nor does it specify what the fix should look like. Still, a sensible interpretation is to update Django's email address sanitization to comply with the new Python restrictions, likely by avoiding newlines in the encoded name. Thus, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13195": {
        "explanation": "The issue provides a clear description of the problem: when using HttpResponse.delete_cookie(), the resulting Set-Cookie header does not preserve the SameSite attribute, which can lead to browser warnings and potentially cookies not being deleted as intended in modern browsers. The issue also explains the context, the observed behavior, and the expected behavior (that the SameSite attribute should be preserved or at least configurable when deleting a cookie). It even provides a code snippet as a suggested fix. While there is some ambiguity about exactly how the SameSite value should be determined (should it be preserved from the original cookie, or should it be configurable?), the intent is clear enough to make a meaningful attempt at a solution, such as adding a samesite parameter to delete_cookie and ensuring it is set in the Set-Cookie header. Therefore, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13199": {
        "explanation": "The issue provides a clear description of the observed problem (browser warning about SameSite and Secure attributes when deleting cookies), the suspected cause (delete_cookie not preserving SameSite or Secure attributes), and a concrete example of a workaround that resolves the warning. It also points to the relevant method (HttpResponseBase.delete_cookie) and describes the expected behavior (delete_cookie should preserve SameSite and Secure attributes when expiring cookies). While it does not specify the exact API or configuration for how to pass these attributes, it is clear that the solution should involve updating delete_cookie to allow specifying SameSite and Secure, and to ensure these are set appropriately when deleting a cookie. This is a sensible and actionable interpretation for a software engineer familiar with Django or similar frameworks.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13212": {
        "explanation": "The issue clearly describes the desired behavior: built-in validators should include the provided value in the ValidationError, so that custom error messages can use a %(value)s placeholder. It references existing documentation and provides a concrete example. The scope is clear (modifying built-in validators to pass the value), and the expected outcome is well-defined. There are no major ambiguities about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13218": {
        "explanation": "The issue provides a clear background, describes the current behavior (the migrate command checks for a __file__ attribute on the migrations package), explains why this is now unnecessary (migration discovery now uses pkgutil.iter_modules() and __path__), and states the problem (the __file__ check prevents migrate from working on namespace packages). The required change is to remove or adjust the __file__ check so that migrations directories without __init__.py files (i.e., namespace packages) are supported. This is a well-specified issue with a clear objective and rationale.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13220": {
        "explanation": "The issue clearly describes the current behavior (ValidationError instances with identical messages are not considered equal), the desired behavior (they should be considered equal if created identically), and even provides guidance on implementation (add an __eq__ method, and ideally make the comparison order-independent for field/non_field_errors). While there is some room for interpretation regarding the exact definition of 'identically' and how deep the equality should go, the intent and requirements are clear enough for a meaningful implementation attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13230": {
        "explanation": "The issue provides a clear objective: add a 'comments' argument to the 'feed.add_item()' method in 'syndication.views' so that 'item_comments' can be set directly. It also explains the motivation (to avoid using 'item_extra_kwargs') and references that 'comments' is already mentioned in the feedparser, suggesting alignment with existing standards. The location in the codebase and the desired change are both specified, making it clear what needs to be done for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13240": {
        "explanation": "The issue clearly describes the current behavior (raising ImproperlyConfigured during settings initialization if SECRET_KEY is missing) and the desired behavior (raise ImproperlyConfigured only when SECRET_KEY is accessed). It also provides a concrete use case and rationale for the change. The required change is clear: defer the error until the setting is accessed. This is a well-specified issue for a developer familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13250": {
        "explanation": "The issue provides a clear description of the problem: JSONField's __contains and __contained_by lookups do not work with nested values on SQLite, and the current implementation only supports basic examples. It also references the general principle for containment and provides concrete test cases that currently fail. While the issue mentions uncertainty about feasibility, it is clear what the desired behavior is (matching nested structures according to the described principle), and the test cases serve as a specification for what needs to be fixed. Therefore, a meaningful attempt at a solution can be made based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13251": {
        "explanation": "The issue clearly describes a bug: filtering on a field named 'negate' raises a TypeError, and provides a concrete example with a model and the exact error message. It also notes that 'negate' is not documented as a reserved argument, and provides a workaround. The expected behavior is implied: filtering on a field named 'negate' should work like any other field. This is sufficient information to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13265": {
        "explanation": "The issue provides a clear description of the problem: when using AlterOrderWithRespectTo and creating an index on the '_order' field in Django migrations, the migration fails because the '_order' field does not exist at the time the index is created. The user suggests that the order of operations in the migration is incorrect and that AlterOrderWithRespectTo should occur before AddIndex on '_order'. The expected solution is to ensure the migration operations are ordered so that the '_order' field exists before any index is created on it. This is a well-specified problem with a clear path to a solution: adjust the migration operation order.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13267": {
        "explanation": "The issue provides a clear description of the problem: instantiating an abstract model with a string ForeignKey raises a TypeError in Django 1.9.8, whereas it did not in Django 1.8. The code sample, traceback, and context are all provided. However, the issue does not specify what the desired behavior should be (should instantiation succeed, should there be a clearer error, or should the docs be updated?). There is enough information to investigate and potentially fix the error or improve the error message, but some interpretation is required regarding the intended outcome.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13279": {
        "explanation": "The issue provides context about a problem encountered during the transition to Django 3.1, specifically regarding session data encoding/decoding and the impact of a code change. It mentions a possible workaround (using the legacy encode() when DEFAULT_HASHING_ALGORITHM == 'sha1'), but does not specify exactly what the desired solution should look like, what constraints exist, or how the solution should be implemented or tested. There is ambiguity about whether the workaround is acceptable, if a more robust solution is needed, or if there are other considerations. Thus, while the problem is described, the requirements for a successful solution are not fully clear.",
        "label": "VAGUE"
    },
    "django__django-13281": {
        "explanation": "The issue provides a clear description of the observed behavioral change between Django versions, a reproducible test case, and even points to a specific commit that likely caused the change. It also references a suggested solution (deep-copying self._state) and the context (cloning model instances). While there are some open-ended questions about the 'correctness' of the new behavior, the actionable part\u2014modifying the model copy behavior to account for the new FK field caching\u2014is well-specified. The test case provides a concrete target for a successful solution: make the test pass on Django >=2.0. Thus, a meaningful attempt at a solution is possible.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13287": {
        "explanation": "The issue provides a clear description of the problem: an app with 'default_app_config' set in __init__.py and either missing or empty apps.py crashes with a specific traceback. The error is identified as an UnboundLocalError for 'app_config_name' in AppConfig.create. The issue also references a regression and provides a link to the related Django ticket. While it does not specify the exact expected behavior, it is clear that the app should not crash in these scenarios, and the traceback and context are sufficient for a developer familiar with Django to attempt a fix. The problem is reproducible and the failure mode is well-described.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13295": {
        "explanation": "The issue provides a clear description of the problem: when a non-concrete field is passed to update_fields in a model's save() method, Django silently ignores it without raising an error. The issue also points to the relevant code sections and suggests two possible solutions: (1) allow non-concrete fields to specify which concrete fields to save, or (2) raise an error when a non-concrete field is passed to update_fields. The reporter acknowledges that the first solution is complex and suggests the second (raising an error) as a simpler, preferred fix. This gives a clear, actionable path for a solution: update the save() logic to raise an error if update_fields contains a non-concrete field. The issue is thus well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13297": {
        "explanation": "The issue provides a clear description of the problem: in Django 3.1, kwargs passed to get_context_data contain SimpleLazyObjects, which cause errors when used directly in database queries, whereas in Django 3.0 this was not the case. The issue includes example code, the error message, and the context (Python and database versions). However, it does not specify what the desired fix should be: should the framework be changed to avoid passing SimpleLazyObjects, should documentation be updated, or should a utility be provided to handle this conversion? There is a sensible interpretation (ensure kwargs in get_context_data are not SimpleLazyObjects or are safely usable), but some blanks remain about the exact solution approach.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13300": {
        "explanation": "The issue provides a clear description of the current behavior (Django's Exists() subqueries select all columns, e.g., SELECT id, ...), the desired behavior (use SELECT 1 instead, as .exists() does), and the motivation (smaller, more debuggable queries). It also gives a concrete example and a workaround. The required change is to make Exists() generate SELECT 1 subqueries where possible. While there may be edge cases to consider (e.g., when the subquery needs to reference outer columns), the main request is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13301": {
        "explanation": "The issue clearly describes the problem: when running the createsuperuser command with the --database option, the check for whether the default username is already taken is performed against the default database, not the one specified by --database. The expected behavior is implied: the check should respect the --database option. The context (Django management command, multiple databases, default username logic) is clear, and the required change is well-scoped.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13315": {
        "explanation": "The issue describes a specific bug: using a Q object as limit_choices_to on a ForeignKey can result in duplicate options in a form field, especially when joins are involved. It references a regression test in a patch for a clear view of the problem, which suggests that an example or failing test is available in the codebase. While the exact desired behavior (e.g., how duplicates should be removed) is not spelled out, the problem and context are clear enough for an experienced engineer to attempt a fix, especially with the referenced regression test as guidance.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13321": {
        "explanation": "The issue provides a detailed traceback and a clear description of the problem: decoding invalid session data causes a server crash, preventing login in certain browsers. The expected behavior is implied (the server should not crash on invalid session data), and the context (Django sessions, version 3.1, Gunicorn/nginx, browser differences) is provided. While it doesn't specify the exact fix, it is clear that the solution should involve handling invalid session data gracefully to prevent crashes. This is a common class of bug and the required outcome is clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13325": {
        "explanation": "The issue provides a clear description of the problem: certain cache backend methods are missing key validation via self.validate_key(key). It lists the specific methods and classes affected, references a previous CVE and fix, and specifies that the solution should include tests to ensure validation is called for all relevant operations. The main concern (data loss) is also stated. While there may be minor implementation details to decide (e.g., exact test structure), the requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13341": {
        "explanation": "The issue provides a clear description of the inconsistency between TemplateView and ListView regarding how kwargs are pushed into the context. It also describes a regression/change in behavior between Django 3.0 and 3.1, specifically that TemplateView.get_context_data()'s kwargs now return SimpleLazyObjects, which can cause errors when used directly. The issue includes example code, the error message, and the context in which the error occurs. The desired outcome is to stop TemplateView from automatically passing kwargs into the context, aligning its behavior with ListView and preventing the described error. This is a well-specified issue with a clear problem statement and a clear direction for a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13343": {
        "explanation": "The issue provides a clear description of the problem: when a FileField is constructed with a callable storage parameter, the deconstruction process (used during makemigrations) incorrectly evaluates the callable and inlines the resulting Storage instance, rather than preserving a reference to the callable itself. The issue also specifies the correct behavior (return a reference to the original callable during deconstruction), and provides a parallel with the existing behavior for the upload_to parameter. There is enough information to attempt a solution: ensure that deconstruction of FileField with a callable storage parameter preserves the callable reference, not the evaluated result.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13344": {
        "explanation": "The issue provides a clear description of the problem: the first middleware's process_response receives a coroutine instead of an HttpResponse when using ASGI (with uvicorn), while subsequent middlewares receive the expected HttpResponse. It includes steps to reproduce, a minimal example, and a reference to a related issue in another package. The expected and actual behaviors are both described. However, the issue does not specify what the correct fix should be (e.g., should the framework await the coroutine before passing it to the middleware, or should middleware authors handle coroutines?), but it is clear that the current behavior is inconsistent and likely unintended. There is enough information to investigate and attempt a solution, though some implementation decisions will need to be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13346": {
        "explanation": "The issue provides a clear description of the problem: the __in lookup does not work as expected on key transforms for JSONField in Django, while it does for other fields. It includes a concrete example, the expected vs. actual behavior, and specifies the databases affected. However, it does not specify what the correct behavior should be for all possible cases, nor does it specify if the fix should be at the ORM level, database backend, or documentation. Still, a sensible interpretation is to make the __in lookup on key transforms for JSONField behave like it does for other fields, matching the single-value lookup. This is enough to attempt a solution, though some implementation details would need to be decided by the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13347": {
        "explanation": "The issue clearly describes the problem: SafeExceptionReporterFilter does not recurse into dictionaries with non-string keys, and provides a concrete example. It also points to the relevant code and explains the cause (the use of a regex on the key before recursing). The expected behavior is implied: the filter should sanitize values in dictionaries regardless of key type. While it doesn't specify the exact output or test cases, the intent and the area to fix are clear, making it possible to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13354": {
        "explanation": "The issue provides a clear description of the problem, including steps to reproduce, expected and actual behavior, and a traceback of the error. It also points to the suspected cause in the code (MigrationOptimizer mishandling operation order when app names contain uppercase letters). The desired outcome is implicit: MigrationOptimizer should correctly order operations regardless of app name casing. This is sufficient for an experienced engineer to attempt a fix and write a test for the scenario described.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13363": {
        "explanation": "The issue provides a clear description of the problem: TruncDate and TruncTime do not respect the tzinfo parameter, always using the global timezone instead. It points to the relevant code lines, describes the expected behavior, provides an example of usage, and explains the current workaround. The desired outcome is for TruncDate and TruncTime to accept and use the tzinfo parameter as expected. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13369": {
        "explanation": "The issue provides a clear description of the problem, including the code that triggers the error, the exact error message, and the context (a regression from Django 3.1 to 3.2). It also includes the stack trace and a minimal reproducible example. The expected behavior is implied: the query should work as it did in Django 3.1, or at least not raise the FieldError. The cause (mixed types in the expression) and the suggested workaround (set output_field) are also mentioned. This is sufficient information to attempt a fix or at least to investigate and propose a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13371": {
        "explanation": "The issue clearly describes a problem: django.db.models.query.Row objects, returned by QuerySet.values_list(..., named=True), cannot be pickled, which causes issues with packages like cacheops that rely on pickling. The issue provides a reproducible code example, the error message, and the context (the feature was introduced in Django 2.0). The expected outcome is that Row objects should be pickleable. While it does not specify the exact implementation, it is clear what needs to be achieved for a successful solution: make Row objects pickleable. This is a well-specified bug report.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13386": {
        "explanation": "The issue provides a clear motivation (max-age should be an integer per RFC6265), a concrete location for the change (the set_cookie() function), and describes the problem (users may pass floats, which can cause issues). However, it leaves open some important implementation details: whether to round, truncate, or only convert floats with zero decimal, and what to do if conversion fails. These are not specified, but the issue does enumerate them as things to consider, suggesting the implementer should make a reasonable choice. Thus, while not fully specified, it is possible to make a sensible, defensible implementation based on the information given.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13401": {
        "explanation": "The issue provides a clear description of the problem (field equality across models inheriting from an abstract base), demonstrates the current behavior with code and output, and suggests a concrete solution: modifying __eq__, __hash__, and __lt__ to consider the model as well as the creation_counter. It also gives implementation hints and notes a potential pitfall with ordering. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13406": {
        "explanation": "The issue provides a clear description of the problem, including the expected and actual behavior, a minimal reproducible example, and the traceback of the error. It specifies the context (pickling querysets with values()/annotate), the observed bug (wrong return type and crash), and the steps to reproduce. While it does not prescribe a specific solution, it is clear what needs to be fixed: ensure that unpickling a queryset with values()/annotate preserves the correct return type and does not crash. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13410": {
        "explanation": "The issue provides a clear description of the bug, explains the root cause (incorrect handling of the return value from fcntl.flock), and supplies a concrete code diff that demonstrates the expected fix. The requirements for a successful solution are unambiguous: update the posix implementation of lock and unlock in django/core/files/locks.py to correctly return True on success and False on failure, as per the provided diff. No further clarification is needed to implement and test the fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13417": {
        "explanation": "The issue provides a clear description of the problem: the QuerySet.ordered property remains True even when the resulting SQL query (after annotate and GROUP BY) does not include an ORDER BY clause, particularly when Meta.ordering is set. The example is concrete, showing both the code and the resulting SQL, and the expected vs. actual behavior is clear. The desired outcome is that QuerySet.ordered should accurately reflect whether the SQL query will be ordered. This is sufficient information to attempt a solution, such as adjusting the logic that determines the .ordered property for annotated/grouped queries.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13431": {
        "explanation": "The issue provides a clear description of the model, the query being executed, the SQL generated, the error encountered, and the expected SQL output. It is clear that the problem is with how Django's ORM is generating SQL when using both annotate and aggregate with renamed fields. The expected behavior is also explicitly stated. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13447": {
        "explanation": "The issue provides a clear motivation: the reporter wants to manipulate 'app_list' in a custom admin view and needs access to the model class, which is currently not available because the dictionary is a serialized model. The request is to include the model class in the context and to make the '_build_app_dict' method public, since it is used in multiple places. While there are some implementation details to decide (e.g., exactly how to expose the model class, naming, etc.), the core requirements are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13449": {
        "explanation": "The issue provides a clear description of the problem, including the model, the query that triggers the error, the error message, and the suspected cause (misplaced CAST in the generated SQL). It also specifies that the problem occurs with DecimalField and not with FloatField, and provides a workaround. The expected behavior is implied: the query should work with DecimalField without requiring a workaround. This is sufficient information to attempt a meaningful fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13454": {
        "explanation": "The issue provides a clear description of the problem: using admin.EmptyFieldListFilter with a GenericForeignKey in Django admin causes a crash due to the absence of the 'empty_strings_allowed' attribute on GenericForeignKey. The user provides code snippets, the error message, and a suggestion for a possible solution (extending GenericForeignKey to use the attribute from its underlying fields). While there is some room for interpretation regarding the exact implementation, the problem and the desired outcome are clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13460": {
        "explanation": "The issue provides a clear description of the problem (string formatting error when passing floats to blocktrans), a reproducible example, the error message, and two suggested directions for a solution (improved error messaging or type checking). While it leaves some implementation details to the developer (e.g., exactly how to check type compatibility or how to present the error), the core requirement is clear: prevent or better handle the formatting error when floats are passed to blocktrans. This is enough to make a meaningful attempt at a solution, though some minor decisions are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13466": {
        "explanation": "The issue provides a clear description of the problem: when using Django's call_command to invoke a management command with a mutually exclusive group containing a boolean flag, passing flag=True results in an error. The issue includes a minimal reproducible example, the expected behavior (should work as it does from the command line), and the actual error message. It is clear that the goal is to make call_command work with boolean flags in mutually exclusive groups, matching the behavior of the CLI. This is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13490": {
        "explanation": "The issue clearly describes a problem: when using get() with filters on a QuerySet resulting from union(), intersection(), or difference(), the error message is not descriptive enough. The issue provides a minimal reproducible example, specifies the expected improvement (a more descriptive error), and the context (Django QuerySet operations). It is clear that the task is to raise a more descriptive error in this scenario. The only minor ambiguity is the exact wording or type of the 'descriptive error', but the intent and the area of code to change are clear. Thus, it is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13495": {
        "explanation": "The issue description provides a clear explanation of the observed behavior, including a code example, the SQL generated, and the relevant code path. However, it does not explicitly state what the desired behavior is (e.g., should tzinfo be respected for DateField output?), nor does it specify what a successful solution would look like. The issue ends with a question about whether this is intentional or a missing feature, rather than a concrete request for change. Therefore, while a sensible interpretation can be made (that tzinfo should be respected for DateField as well), there is some ambiguity about the exact requirements.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13512": {
        "explanation": "The issue clearly describes the problem: Unicode characters (e.g., Chinese) in JSONFields are displayed as escaped ASCII sequences (e.g., \\u4e2d\\u56fd) in the Django admin, due to the default behavior of json.dumps. The desired outcome is for these characters to be displayed as their actual Unicode glyphs. The issue also references a potential solution (a PR) that addresses the problem. While it doesn't specify exactly how the fix should be implemented, the problem and the expected result are clear, and a reasonable solution can be attempted based on this information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13513": {
        "explanation": "The issue provides a clear description of the problem: the debug error view in Django does not respect exc.__suppress_context__ when displaying tracebacks, contrary to Python's standard behavior. It includes a concrete code example, references the relevant PEP, points to the exact function in the codebase, and even suggests a corrected implementation. The expected behavior is unambiguous, and the steps to resolve the issue are well-outlined.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13516": {
        "explanation": "The issue provides a clear description of the problem: output from management commands (specifically during migrations) is buffered and not flushed as expected, resulting in delayed feedback to the user. It also gives a concrete example of the current (broken) behavior and the expected behavior, making it clear that the solution should ensure that output is flushed in real time during long-running operations. While it doesn't specify exactly where in the code to make changes, it is clear what the end result should be, and an experienced engineer would know to investigate the use of flush() on stdout/stderr in the relevant management command code paths.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13528": {
        "explanation": "The issue provides a clear description of the problem: when chaining the floatformat (or floatvalue) and intcomma template filters in Django, the output does not use the correct thousands separator for some locales (e.g., Germany). It gives concrete examples of the input, the expected output, and the actual output. The context (Django templates, specific filters, and locale) is specified. While it doesn't specify the exact code location, the behavior and the desired fix are clear enough for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13530": {
        "explanation": "The issue provides a clear description of the problem: using KeyTransform in the ordering attribute of ArrayAgg with a JSONField produces invalid SQL. It includes a sample model, sample data, the exact query that fails, and the resulting invalid SQL. It also notes that this worked in previous Django versions, which gives context for regression. While it doesn't specify the exact expected SQL, it is clear that the goal is to make the query generate valid SQL as it did before. This is enough information for an experienced engineer to attempt a fix or regression test.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13537": {
        "explanation": "The issue describes a concrete problem: manage.py dbshell does not respect the charset/encoding specified in the DATABASES setting for MySQL, leading to encoding issues. It also suggests a solution direction (making mysql/client.py respect the encoding settings) and mentions a patch. However, it is not fully clear whether the patch is attached, and the issue leaves some ambiguity about the expected behavior for other backends and which database_options should be supported. Still, the core requirement\u2014ensure dbshell uses the charset/encoding from settings.py for MySQL\u2014is clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13551": {
        "explanation": "The issue clearly describes a security flaw: after a user changes their email, previously issued password reset tokens are still valid, which is undesirable. It provides a concrete sequence to reproduce the problem and suggests a specific fix (include the email in the token hash). It also notes a caveat about users not being required to have an email, and suggests a possible design improvement. The requirements for a successful solution are clear: ensure that password reset tokens are invalidated if the user's email changes. There is some room for interpretation regarding the optional email field, but the main fix is well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13553": {
        "explanation": "The issue provides a clear description of the bug, including the code that triggers it, the resulting SQL query, and the specific errors encountered on different databases. It also proposes a concrete solution: when union() is called with a single non-empty queryset, simply return that queryset instead of constructing a union query. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13556": {
        "explanation": "The issue clearly describes a problem: calling transaction.on_commit(None) leads to a TypeError because None is not callable. It suggests two possible solutions: either prevent registering None as a callback, or handle it gracefully (e.g., by ignoring or exiting). The context (Django, transaction.on_commit) and the error are clear, and the expected outcome (no crash, better handling of None) is understandable. There is some room for interpretation in how to \"exit gracefully,\" but the core requirement is clear: improve robustness against None being registered. This is enough to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13560": {
        "explanation": "The issue describes a problem where an exception (ProtectedError/RestrictedError) raised from Collector.collect contains an iterator (itertools.chain) in its arguments, resulting in an uninformative exception representation (repr) in Sentry. The core of the issue is clear: the exception should contain data that is informative when represented, rather than an iterator object. However, the issue does not specify exactly how the exception should be changed (e.g., should the iterator be converted to a list, or should a custom repr be provided?), nor does it specify if there are any constraints or edge cases to consider. Still, the intent is clear enough to make a reasonable attempt at a solution by ensuring the exception contains a more informative, materialized representation of the referenced objects.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13568": {
        "explanation": "The issue provides a clear description of the current behavior (auth.E003 is triggered when USERNAME_FIELD is not unique=True, even if a UniqueConstraint exists), the motivation (avoid extra index creation in PostgreSQL), and a concrete suggestion for a solution (extend the system check to also look for UniqueConstraint on USERNAME_FIELD). The expected change is well-defined: the system check should not raise auth.E003 if a UniqueConstraint exists for the USERNAME_FIELD. This is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13569": {
        "explanation": "The issue provides a clear description of the problem, including reproducible steps, code samples, expected vs. actual behavior, and a diagnosis of the root cause in the Django ORM's SQL compiler. It even suggests a code-level patch that resolves the issue. The only minor ambiguity is whether the proposed patch is the desired solution, but the problem and its context are well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13578": {
        "explanation": "The issue clearly describes a specific problem: when a formset is instantiated with an incorrect prefix, Django raises a generic ValidationError that is not helpful for debugging. The issue points to the relevant code location and suggests a concrete improvement: include the underlying form._errors in the exception output to aid debugging. The expected behavior and the desired change are both clear, making it possible to attempt a meaningful solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13589": {
        "explanation": "The issue provides a clear description of the problem, including a reproducible code example, the error encountered, and the context in which the error occurs. It also explains the expected behavior and the workaround currently required. However, the issue is phrased as a question about whether the behavior can be improved, rather than as a concrete request for a code change or feature. There is some ambiguity about whether the desired solution is a documentation update, a change to bulk_create(), or a change to model instance behavior. Still, a sensible interpretation would be to attempt to make bulk_create() handle unsaved related objects more gracefully, or at least to document the limitation more clearly. Thus, the issue is reasonably specified, but not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13590": {
        "explanation": "The issue provides a clear description of the problem: upgrading from 2.2 to 3.0 causes named tuples used as arguments to __range to error due to how Query.resolve_lookup_value reconstructs the tuple. It explains the root cause (passing an iterator to the namedtuple constructor instead of expanding it), and even suggests the fix (using * to expand the iterator). The context, error message, and location in the code are all provided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13606": {
        "explanation": "The issue clearly describes the current behavior (using NOT IN with a subquery to exclude multi-valued relationships), the problem with this approach (performance and NULL handling), and the desired change (use NOT EXISTS instead of NOT IN). It also references the relevant code location (sql.Query.split_exclude). While it doesn't provide an explicit example, the intent and the required change are clear for someone familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13615": {
        "explanation": "The issue describes a problem with inconsistent locale code formats passed to the makemessages command, and suggests normalizing the directory name to a standard format (e.g., zh_CN). However, it is not clear whether the request is to add a warning if the locale code is not correct, to normalize the directory name, or both. The issue also mentions uncertainty about the internal logic of makemessages and whether more checks are needed. There is ambiguity about the exact requirements and expected behavior, making it unclear what a successful solution would look like.",
        "label": "VAGUE"
    },
    "django__django-13616": {
        "explanation": "The issue describes a confusing error message when running 'makemessages' if the /locale directory does not exist under the app directory. The user suggests that the error message should be clearer, possibly indicating that a 'locale' directory is being searched for. There is also a mention of LOCALE_PATHS and a suggestion that it might include the directory where manage.py is located. However, the core actionable request is to improve the error message to make the problem and solution more obvious. While there are some side comments and possible feature suggestions, the main task\u2014improving the error message for this specific case\u2014is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13620": {
        "explanation": "The issue provides a concrete code example, describes the current behavior (an error when passing a list to an argument in a mutually exclusive group), and expresses a desire for support for list arguments in this context. However, it is not entirely clear what the expected behavior should be (e.g., should call_command('call_command_test', foo=[1,2,3]) work, and how should the parser handle multiple --foo arguments?). There is some ambiguity about the precise requirements and the desired user experience, but a sensible interpretation is possible: the user wants mutually exclusive groups to support nargs='+' arguments and for call_command to accept lists for such arguments. Thus, while not perfectly specified, it is reasonably clear what needs to be addressed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13658": {
        "explanation": "The issue provides a clear description of the problem: ManagementUtility parses the program name from argv but does not pass it to CommandParser, which instead uses sys.argv[0]. The issue also provides a concrete code example of the current behavior and a suggested fix (passing prog=self.prog_name to CommandParser). The motivation and expected outcome are clear: ensure that the program name used by CommandParser matches the one parsed by ManagementUtility, especially when sys.argv[0] is unreliable. This is sufficient information to implement and test a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13660": {
        "explanation": "The issue provides a clear description of the problem: when using 'python -m django shell -c' with multi-line code (such as function definitions), it fails due to the way exec is called in the Django shell command. The issue includes a minimal reproducible example, the traceback, and points to the relevant code in Django. It also suggests a concrete solution: passing a dictionary of globals to exec. This is sufficient information to attempt a fix and write a meaningful PR.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13665": {
        "explanation": "The issue provides a clear description of the problem: when using a custom manager as the default manager on a model, Django's test serialization logic (serialize_db_to_string) uses this manager, which may filter out records and cause test failures. The issue includes code samples, a migration scenario, and a reference to a proposed fix. The desired outcome is for the backend to use base_manager for serialization instead of _default_manager. While the issue assumes some Django knowledge, it is clear what needs to be changed and why, and the reference PR provides further context.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13667": {
        "explanation": "The issue provides a clear description of the current behavior, the desired optimization, and even an example of the SQL that should be generated. It also specifies the scope of the optimization (only for union, not intersection or difference) and explains why. The manual workaround is described, clarifying the intended effect. This is sufficient information for an experienced developer to implement the optimization in QuerySet.exists() for union queries.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13670": {
        "explanation": "The issue clearly describes the problem: Django's dateformat 'y' does not produce the expected two-digit year for years < 1000, and provides concrete examples comparing Django's output to Python and PHP. The expected behavior is well-defined (should match strftime('%y') and PHP's 'y'), and the scope of the fix is clear (change dateformat.y() to produce two-digit years with leading zeros as needed). There is even a reference implementation (a pull request), so the requirements are unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13682": {
        "explanation": "The issue clearly describes a specific problem: when a user forgets to call .as_view() on a class-based view (CBV) in Django's URL patterns, the resulting error is confusing and only appears at request time. The issue proposes that Django should detect this mistake earlier (at route construction time) and provide a clear, actionable error message. The desired behavior is well-articulated, and an example of the improved error message is provided. There is enough information to implement a solution that checks the type of the view argument in path() and raises a helpful error if a CBV class is passed instead of an instance returned by .as_view().",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13684": {
        "explanation": "The issue provides a clear description of the problem: when a database backend fails to load due to an ImportError, Django currently gives a misleading error message that suggests the backend is not available, rather than indicating there was an import problem. The issue includes a concrete example, a suggested improvement to the error message, and even a conditional for when to show the 'Try using...' advice. This is sufficient information to attempt a solution: update the error handling in the backend loader to distinguish ImportError from other cases and improve the error message accordingly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13689": {
        "explanation": "The issue provides a clear description of the problem: when grouping on an ExpressionWrapper in Django, the generated SQL omits the expression from the GROUP BY clause, instead grouping by the underlying field. The user provides a concrete code example and the resulting SQL, highlighting the discrepancy. However, the issue does not specify what the expected behavior should be (should the GROUP BY include the full expression?), nor does it specify what a fix would look like (e.g., should Django always include the full expression in GROUP BY when using ExpressionWrapper?). There is enough information to investigate and attempt a fix based on reasonable assumptions, but some interpretation is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13691": {
        "explanation": "The issue provides a clear description of the error, including the context (Django 3.1.3, Sqlite backend, JSONField path lookup), the specific operation that triggers the error, the types of values that cause the problem (integer, float, bool), and the stack trace. It also specifies that the issue does not occur with MySQL or Postgres, and that other types (strings, dicts, lists, None) work as expected. While it does not prescribe a solution, it is clear what the bug is and under what circumstances it occurs, making it possible to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13693": {
        "explanation": "The issue provides a clear description of the problem: 'django-admin runserver' crashes on Windows with Python < 3.8 due to get_child_arguments returning a WindowsPath object instead of a string, which subprocess.Popen cannot handle. The error traceback is included, the environment is specified, and steps to reproduce are given. The root cause is identified, and a monkey-patch workaround is provided. This gives enough information to implement a proper fix in the codebase (i.e., ensure get_child_arguments returns strings, not Path objects).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13708": {
        "explanation": "The issue provides a clear motivation and a high-level description of the desired feature: adding a 'durable' flag to transaction.atomic() that ensures all operations within the block are definitely committed at the end, and raises an error if called within an existing atomic block. However, it does not specify the exact API, error type, or how this should interact with existing transaction management in detail. There is enough information to make a sensible attempt at a solution by following the described behavior and referencing the linked blog post and Twitter discussion, but some implementation details would need to be decided by the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13710": {
        "explanation": "The issue clearly describes the current behavior, the inconsistency, and the desired change: when a verbose_name is specified for an Inline admin class, the default verbose_name_plural should be derived from it (as is done for models), rather than defaulting to the model's name. The expected behavior is well-articulated, and the scope of the change is clear. There is no ambiguity about what needs to be changed or how success would be measured.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13714": {
        "explanation": "The issue provides a clear description of the problem: when the SQLite database file path does not exist, Django raises a misleading error about JSONFields rather than a more appropriate error about the missing database file. The expected behavior is implied: Django should raise a more accurate error in this scenario. However, the issue does not specify exactly what the new behavior should be (e.g., what error message or exception should be raised), nor does it specify whether the fix should be in Django's core, the spatialite backend, or in documentation. Still, the problem and the desired direction for a solution are clear enough for a developer to attempt a meaningful fix, such as improving error handling for missing database files in the relevant backend. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13722": {
        "explanation": "The issue describes a new feature: adding a hook to InlineModelAdmin to provide initial data for inline formsets. It references the existing get_changeform_initial_data as a conceptual model, and notes the complexity due to the list nature of formset initial data. While it doesn't specify the exact method signature or name, it is clear that the goal is to allow customization of the initial data for inline formsets, similar to how get_changeform_initial_data works for single forms. As an experienced engineer, I can reasonably infer what is required: to add a method (e.g., get_inlineformset_initial_data) to InlineModelAdmin that can be overridden to provide initial data for each form in the formset. There are some implementation details to decide, but the intent and direction are clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13741": {
        "explanation": "The issue provides a clear description of the current behavior (the need for a clean_password method to prevent password changes in UserChangeForm), the recent addition of a 'disabled' argument to forms.Field, and a concrete proposal: set disabled=True by default on ReadOnlyPasswordHashField. The rationale and expected outcome are well-explained, and the change is scoped to a specific field. It is clear what code needs to be changed and why, and what the intended effect is. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13743": {
        "explanation": "The issue provides a clear context: on Windows, ConnectionAbortedError should be treated as a broken pipe error in Django's development server, specifically in the is_broken_pipe_error function in django/core/servers/basehttp.py. It references related upstream issues and fixes, and specifies the desired behavior change. The file and function to modify are named, and the intent is clear. While it doesn't specify the exact code change, the required logic is straightforward and unambiguous for an experienced engineer.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13744": {
        "explanation": "The issue clearly states the intention: deprecate the MemcachedCache backend in Django 3.2 and remove it in Django 4.1. It provides context (python-memcached is unmaintained, alternatives exist) and references related issues and PRs. The required action (deprecation) and the timeline (Django 3.2 for deprecation, 4.1 for removal) are specified. While some implementation details (e.g., exact deprecation warning wording) are left to the implementer, the overall requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13768": {
        "explanation": "The issue clearly describes the current behavior (exceptions in Signal.send_robust() are not logged), the desired behavior (add logger.exception() in the except clause), and the rationale for the change. It also references similar behavior elsewhere in Django for consistency. The implementation details are straightforward and unambiguous, making it clear what needs to be done for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13773": {
        "explanation": "The issue provides a clear description of the problem: the squashmigrations optimizer crashes when fields are renamed in a certain pattern, leading to duplicate field names in a CreateModel operation. The user describes the sequence of renames, the presence of an optimization barrier, and the suspected root cause in the optimizer's handling of RenameField operations. While the migration file that triggers the failure is not attached, the description is detailed enough for an experienced engineer to reproduce the scenario and investigate the optimizer's logic. The expected outcome is to prevent the optimizer from creating invalid CreateModel operations with duplicate field names. Thus, the issue is reasonably specified, though the missing migration file and some terminology uncertainty mean there are minor blanks to fill in.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13774": {
        "explanation": "The issue provides a clear description of the bug: when using prefetch_related_objects() with a list of model instances that may contain duplicates, the to_attr attribute is not set on all instances if the first instance already has it set. The issue references specific lines in the Django codebase that are relevant to the problem, and describes the expected behavior (to_attr should be set on all instances, not just the first). While it does not specify the exact fix, it is clear what the bug is and what the correct behavior should be, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13786": {
        "explanation": "The issue provides a clear description of the problem: when squashing migrations, specifically when an AlterModelOptions operation with empty options is squashed into a CreateModel, the resulting CreateModel does not have its options cleared as expected. The issue points to the relevant code location and describes the current and expected behavior. While it does not specify the exact test case or expected output, it is clear what needs to be fixed: ensure that options removed by AlterModelOptions are also removed from the squashed CreateModel. This is enough information for an experienced engineer to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13791": {
        "explanation": "The issue provides a clear description of the bug, including a reproducible example, expected vs actual behavior, and a likely location in the code where the bug occurs. It also explains the reasoning behind the suspected cause and provides enough context for a developer to attempt a fix. The only minor ambiguity is the precise requirements for ISO 8601 parsing, but the main problem (sign is ignored for days-only durations) is well-demonstrated and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13794": {
        "explanation": "The issue clearly describes a bug: the 'add' template filter fails when concatenating a regular string with a lazy string (specifically, a __proxy__ object), resulting in a TypeError. The expected behavior is implied: the filter should be able to concatenate these types without error. The context (template filter, lazy string, error message) is sufficient for an experienced engineer to identify the problem and implement a fix, likely by ensuring type compatibility or proper coercion. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13797": {
        "explanation": "The issue clearly states the desired feature: to add support for compressed fixtures in the 'dumpdata' command, similar to what 'loaddata' already supports. The reference to the Django documentation provides context and an example of the expected behavior. While implementation details are not specified, the goal is clear and actionable for someone familiar with Django's management commands.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13807": {
        "explanation": "The issue provides a clear description of the problem (loaddata crashes when table names are SQL keywords due to lack of quoting), steps to reproduce, the error message, the root cause (missing backticks/quoting in SQL statements), and even points to the relevant code lines. It is clear what needs to be fixed: ensure table names are properly quoted in the relevant SQL statements. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13808": {
        "explanation": "The issue provides a clear description of the desired feature: allowing PostgreSQL database connections in Django to be configured using only a service name, without requiring the database name to be specified redundantly in the DATABASES config. It explains the current workaround, the motivation for the change, and gives concrete examples of the desired configuration. While there is a minor ambiguity about whether the preferred syntax is via 'OPTIONS': {'service': ...} or a new 'SERVICE' key, the intent is clear and a sensible implementation path can be inferred. The issue is thus reasonably specified, with only minor blanks to fill in.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13809": {
        "explanation": "The issue clearly requests the addition of a '--skip-checks' option to the 'runserver' command, with rationale and a reference to similar behavior in other management commands. The expected behavior is clear: when this option is provided, system checks should be skipped when running the server. There is enough information to implement this feature in a Django-like codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13810": {
        "explanation": "The issue provides a detailed description of the observed problem, including a reproducible example, the context (ASGI, Django, custom middleware, django-debug-toolbar), and a hypothesis about the root cause in Django's middleware handling. However, it does not specify what the desired fix or outcome should be: is the reporter asking for a code change in Django, a documentation update, or something else? The issue is clear about the symptoms and the suspected cause, but leaves ambiguity about what a successful solution would look like (e.g., should Django change its middleware handling, or just clarify documentation?). Thus, while a sensible interpretation is possible (e.g., fix the middleware chain so that MiddlewareNotUsed does not leave side effects in ASGI context), there is some room for interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13814": {
        "explanation": "The issue provides a clear description of the problem: the get_package_libraries function in django/template/backends/django.py masks ImportError exceptions by raising InvalidTemplateLibrary, which makes debugging difficult. The issue includes a step-by-step reproduction guide, the observed and expected behaviors, and a rationale for the proposed change (removing or altering the exception handling to preserve the original stack trace). It is clear what needs to be changed and why, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13820": {
        "explanation": "The issue provides a clear summary of the current problem (the migration loader's check for namespace packages is too broad, excluding some valid packages in frozen environments), the motivation (compatibility with frozen Python environments), and a concrete proposal for how to improve the check (augmenting the getattr(m, '__file__', None) check with an isinstance(m.__path__, list) check). The scope is explicitly limited: it does not propose to allow namespace packages for migrations, nor to change any other Django behavior. The desired outcome is clear: allow migrations in non-namespace packages that lack __file__, specifically in frozen environments. There is enough detail to implement the change as described.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13821": {
        "explanation": "The issue clearly states the objective: to drop support for SQLite versions older than 3.9.0. It provides context about why this is being done (new features require 3.9.0+, old distributions are EOL, etc.). While it doesn't list every code location to change, it is standard for such a deprecation/removal ticket to require updating version checks, documentation, and possibly test environments. The requirements are clear and actionable for an experienced engineer.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13822": {
        "explanation": "The issue describes a specific error (fields.E305) that occurs when creating migrations in Django, particularly when multiple models with the same name in different apps have ManyToManyFields to the same model, even when using related_name='+'. The reporter provides a link to a test project that reproduces the issue. However, the issue does not specify what the expected behavior should be, nor does it clearly state what a successful solution would look like (e.g., should Django allow this pattern, or should the error message be improved?). There is ambiguity about the desired outcome, making it unclear how to resolve the issue without further clarification.",
        "label": "VAGUE"
    },
    "django__django-13837": {
        "explanation": "The issue provides a clear description of the current limitation in django.utils.autoreload.get_child_arguments, explains the motivation for the change, and outlines the proposed solution in detail, including the relevant Python documentation and the logic to be used. It also references a related issue (#32177) for context. The expected behavior and the technical approach are both well-specified, making it clear what needs to be changed and why. A successful solution would implement the described logic for detecting -m usage in get_child_arguments, as outlined in the issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13841": {
        "explanation": "The issue provides a clear description of the problem: using __file__ at the module (or class definition) level causes import errors in frozen Python environments. It lists the affected modules, explains the scope (not removing all uses of __file__, not touching test code, not updating documentation), and outlines proposed fixes for each module. The expected changes are well-defined, and the rationale for each is explained. There is enough detail to implement the solution as described, and the boundaries of the ticket are clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13884": {
        "explanation": "The issue provides a clear description of the bug: when changing the language, the set_language view incorrectly decodes an encoded '&' in a URL parameter, resulting in a malformed URL. The expected and actual behaviors are both described, and a reproducible Django test is provided that demonstrates the problem and the expected outcome. This is sufficient information to attempt a fix, as the root cause (unquoting of the next_url) and the desired behavior (preserve the original encoding) are both specified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13915": {
        "explanation": "The issue clearly states the objective: to drop support for Python 3.6 and 3.7. This typically involves updating documentation, setup/config files (like setup.py, pyproject.toml, tox.ini, CI configs), and possibly removing compatibility code. The references to Django's policy and EOL dates provide context. While it doesn't enumerate every file to change, the standard steps for dropping Python version support are well-understood in Python projects. Thus, a meaningful and standard solution can be attempted.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13924": {
        "explanation": "The issue describes a specific problem: after a certain commit, migrations are marked as applied even if deferred SQL (executed at SchemaEditor.__exit__) fails. It references a specific commit and a previous issue, and it clearly identifies the undesired behavior. However, it does not specify the expected behavior in detail (e.g., should the migration be rolled back, should an error be raised, etc.), nor does it provide test cases or acceptance criteria. Still, as an experienced engineer, I can reasonably infer that the migration should not be marked as applied if any part of it, including deferred SQL, fails. Thus, there is a sensible interpretation of what is required, but some blanks to fill in.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13925": {
        "explanation": "The issue provides a clear description of the problem: after upgrading to Django 3.2 alpha, warnings about auto-created primary keys (models.W042) are raised for models that inherit from a base model with a manually specified primary key. The reporter expects the inherited models to use the ancestor's primary key, as was the case in Django 3.1. The issue includes example warnings and a specific example (Entity -> User). However, it does not specify what the expected fix should be (e.g., code changes, documentation, or a workaround), nor does it provide code snippets of the model definitions. Still, the intent is clear: ensure that inherited models do not trigger W042 if the base model defines a primary key. There is enough information to attempt a solution, but some assumptions are needed about the model structure and the desired behavior.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13933": {
        "explanation": "The issue clearly describes the problem: ModelChoiceField does not include the invalid value in its ValidationError message, unlike ChoiceField and ModelMultipleChoiceField. It also suggests a solution: pass the invalid value as a parameter and modify the error message for 'invalid_choice'. The relevant code snippets are provided, making it clear what needs to be changed and where. The expected behavior is well-defined by comparison to other similar fields. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13952": {
        "explanation": "The issue clearly describes a problem: the migrate command's verbose output (when using --verbosity) is not directed to the provided stdout, making it hard to capture output during testing. It references specific functions (emit_pre_migrate_signal and emit_post_migrate_signal) and a related PR for context. The required solution is to ensure that these signals' verbose output respects the provided stdout, making it capturable (e.g., in tests using call_command). While it doesn't specify the exact code change, the intent and the expected behavior are clear, and an experienced engineer can implement a solution based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13964": {
        "explanation": "The issue provides a clear description of the problem, including a reproducible code example, the expected vs. actual behavior, and a hypothesis about the underlying cause. It specifies the models involved, the sequence of operations, and the observed bug. While it does not prescribe a specific solution, it is clear what needs to be fixed: assigning a related object with a non-numeric, non-auto primary key to a parent should result in the correct foreign key value being set, even if the primary key is set after assignment but before saving. This is sufficient for an experienced engineer to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13992": {
        "explanation": "The issue clearly describes a problematic and dangerous behavior: using .distinct() with .delete() on a Django queryset results in a full table deletion, which is unexpected and inconsistent with the behavior of .count(). The user provides a concrete example, the expected behavior, and two possible solutions (raise an error or generate the correct SQL). While the issue does not mandate a specific solution, it is clear what the problem is and what a successful solution would look like: prevent .delete() from wiping the table when .distinct() is used, either by error or by correct SQL. This is enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14007": {
        "explanation": "The issue provides a clear description of the problem: when using a custom BigAutoField with a from_db_value converter, the converter is not called for the returned primary key value on insert (including create and bulk_create), resulting in the field being a plain integer instead of the expected wrapper class. The issue includes code samples, expected vs. actual behavior, and specifies the affected methods. The required solution is to ensure that from_db_value is called for returning_fields on insert, matching the behavior of other query pathways. This is well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14011": {
        "explanation": "The issue provides a detailed description of the observed problem (database connections not being closed after each thread in LiveServerTestCase's ThreadedWSGIServer), the error message encountered, the context (Django 2.2.17, regression from 1.11), and a workaround that avoids the problem. However, it does not specify what the expected behavior should be, nor does it provide clear acceptance criteria for a fix. It also does not specify whether the solution should be a patch to Django itself, a workaround, or a test case. While a knowledgeable engineer could reasonably infer that the goal is to ensure all database connections are closed after each thread in ThreadedWSGIServer, there is some ambiguity about the exact requirements and scope of the fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14014": {
        "explanation": "The issue clearly states that during introspection (presumably of a database schema), the column order for unique constraints is not being returned when using SQLite. While it does not specify the exact expected output or the location in the codebase, it is clear what the problem is: the introspection process should return the order of columns in unique constraints, but currently does not. This is a concrete and actionable issue for someone familiar with database introspection and the codebase in question. The solution would involve ensuring that the introspection logic for SQLite includes and returns the column order for unique constraints.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14016": {
        "explanation": "The issue provides a clear description of the problem: using the | operator on Q objects in Django fails with a TypeError when the Q object contains a non-pickleable value (like dict_keys). The example is reproducible, and the expected behavior is implied: the operation should not crash, and ideally, Q(x__in={}.keys()) should work as expected. While the issue does not specify the exact desired behavior (e.g., should dict_keys be automatically converted to a list?), it is clear that the crash is undesirable and should be handled gracefully. There is enough information to attempt a solution, such as converting non-pickleable iterables to a pickleable type within Q or raising a more informative error. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14017": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the observed behavior (TypeError when using Q() & Exists(...)), the expected behavior (commutativity of & and | between Q and Exists), and a suggestion about the likely cause (missing __rand__ implementation). It is clear what needs to be fixed: make Q() & Exists(...) work the same as Exists(...) & Q(). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14019": {
        "explanation": "The issue clearly states that the __repr__ methods of Index, UniqueConstraint, ExclusionConstraint, and CheckConstraint have inconsistent formatting and that they should be made consistent. While it does not specify exactly what the consistent format should be, it is reasonable to interpret that the goal is to standardize the formatting across these classes. As an experienced engineer with access to the codebase, I could review the current __repr__ implementations, identify the inconsistencies, and propose a unified format. Therefore, the issue is reasonably specified, though not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14030": {
        "explanation": "The issue provides a clear description of the problem: the never_cache decorator is being used directly on instance methods (login and logout) in Django's admin site, but should be applied using method_decorator. It explains why this is not currently a bug for never_cache, but could be for other decorators. The expected change is to update the use of never_cache to use method_decorator for these methods. The context and the required change are both clear, making it possible to implement a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14034": {
        "explanation": "The issue provides a clear description of the problem, including code samples, the expected vs. actual behavior, and the context (Django forms, MultiValueField, required subfields). It specifies that when a MultiValueField contains a required subfield, the form should be invalid if that subfield is empty, but currently it is not. The issue is reproducible with the provided code, and the expected outcome is clearly stated. This is sufficient for a developer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14043": {
        "explanation": "The issue provides a clear motivation (security concerns with password handling in dbshell), references existing APIs (settings_to_cmd_args_env), and describes the desired mechanism (support for PostgreSQL passfile, specifically .pgpass, via a temporary file). It also links to a concrete implementation in another project and mentions a partial patch. However, it leaves some details open for discussion (e.g., exact API, whether to extend to other backends, and how the context manager should be integrated). While the core requirement\u2014add .pgpass support to dbshell\u2014is clear, some implementation details would need to be decided by the implementer. Thus, it is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14053": {
        "explanation": "The issue provides a clear description of the problem: HashedFilesMixin's post_process() yields the same original filename multiple times, which leads to incorrect stats and duplicate work in downstream consumers. It gives concrete examples, explains the impact, and clarifies the expected behavior (each original file should only be yielded once, with its final post-processed name). The context, motivation, and desired outcome are all well-articulated, making it clear what a successful solution would look like: modify post_process() so that each original file is only yielded once, with its final processed name, and no duplicates. There is enough information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14071": {
        "explanation": "The issue provides a clear description of a bug in Django's admin raw_id_fields validation: the check currently allows attname references (like 'author_id') to pass, but only field names (like 'author') are actually supported. It gives a concrete example, specifies the method to update (_check_raw_id_fields_item), and describes the expected behavior (raise admin.E002 if the field_name is not the field's name). This is sufficient for an experienced engineer to implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14077": {
        "explanation": "The issue clearly describes the problem: when querying a JSONField using values()/values_list() in Django, SQLite returns integers (1/0) instead of booleans (True/False), while Postgres returns the expected booleans. The model, query, expected and actual outputs, and environment are all specified. The desired behavior is clear: SQLite should return booleans like Postgres. This is enough information to attempt a solution, such as normalizing the output for SQLite.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14089": {
        "explanation": "The issue clearly states the desired feature: support for the built-in reversed() function on the OrderedSet class. It explains the motivation (OrderedSet is ordered, so this is natural), and specifies the technical approach (implement __reversed__()). There is no ambiguity about what is required for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14122": {
        "explanation": "The issue provides a clear description of the problem: fields specified in Meta.ordering are being included in the GROUP BY clause, which leads to incorrect aggregation. It references a previous ticket and commit, and even provides a PR with a test case. The expected behavior is clear: Meta.ordering fields should not be included in GROUP BY. While some implementation details may need to be inferred, the core requirement is well-defined and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14140": {
        "explanation": "The issue provides a clear description of the problem: Q objects with a single child are deconstructed differently, leading to a crash when the child is not subscriptable (e.g., an Exists object). It includes code examples that reproduce the bug, the traceback, and references an existing patch that removes the special case. It also discusses a possible alternative solution for backward compatibility. The expected behavior is clear: deconstructing Q objects should not crash, and the special case for single-child Q objects should be handled more robustly. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14151": {
        "explanation": "The issue clearly describes a specific problem: CsrfViewMiddleware does not handle ValueError exceptions that can be raised by urlparse() when parsing an invalid Referer header. It points to the exact line in the codebase where the problem occurs and gives an example of a problematic input. The required solution is implied: handle the ValueError to prevent unhandled exceptions. While it doesn't specify the exact handling strategy (e.g., reject the request, log an error), it is standard practice to handle such exceptions gracefully in middleware. Thus, a meaningful solution can be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14155": {
        "explanation": "The issue clearly describes the problem: ResolverMatch.__repr__() does not display useful information when the view is a functools.partial object. It specifies that the __repr__ should reveal the underlying function and arguments of the partial, and suggests that unwrapping partials in ResolverMatch.__init__() may be a solution. While it leaves some implementation details to the developer, the desired outcome is clear: improve __repr__ so that partials are represented in a more informative way. This is a sensible and actionable request.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14164": {
        "explanation": "The issue provides a clear description of the problem: the to_locale function is not idempotent, as demonstrated by the provided code example. The expected and actual outputs are shown, and the impact on other Django functionality is explained. The goal is to make to_locale idempotent, so that calling it multiple times does not change the result. This is a well-defined behavioral change with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14170": {
        "explanation": "The issue provides a clear description of the bug: the optimization in YearLookup incorrectly applies a BETWEEN filter for the '__iso_year' lookup, which should instead use EXTRACT('isoyear'). It gives concrete examples of the incorrect and correct SQL generated, references the relevant Django version and PR, and describes the expected behavior. The required fix is to ensure that the BETWEEN optimization is not used for '__iso_year', and that ExtractIsoYear is used instead. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14182": {
        "explanation": "The issue clearly describes the desired enhancement: adding support for a precision argument to Django's Round function, which currently only rounds to the nearest integer. It provides links to documentation for all supported backends, showing that they all support a second argument for precision. The user also provides an example workaround, indicating the expected API (a second argument to Round). The scope and requirements are clear: update the Round function to accept a precision argument and ensure it works across all supported backends. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14199": {
        "explanation": "The issue provides a clear description of the problem: using Value() with DecimalField in an annotation causes a crash on SQLite due to Django passing a string to create_decimal_from_float. It includes a code example that reproduces the issue and points to the relevant area in the codebase. The expected behavior is implied: Value() with DecimalField should not crash. While it doesn't specify the exact fix, the problem and context are clear enough for an experienced engineer to investigate and propose a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14238": {
        "explanation": "The issue provides a clear description of the problem: setting DEFAULT_AUTO_FIELD to a subclass of BigAutoField (or SmallAutoField) causes Django to crash, even though the subclass is valid. The traceback is included, and the error message is explicit. The issue also suggests a likely location for the fix (AutoFieldMeta.__subclasscheck__) and what needs to be changed (allow subclasses of those classes in the _subclasses property). This is sufficient information to attempt a solution: update the subclass check logic so that subclasses of BigAutoField and SmallAutoField are accepted as valid DEFAULT_AUTO_FIELD values.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14266": {
        "explanation": "The issue provides a clear description of the problem: after upgrading to Django 3.2, previously stored cookies for contrib.messages cause a crash due to a base64 decoding error. The traceback is detailed, pointing to the exact line in the code where the error occurs. The contents of the problematic cookie are included, and the context (Django and Python versions) is specified. However, the issue does not specify what the expected behavior should be (e.g., should the system gracefully handle old cookies, or is data loss acceptable?), nor does it specify if backwards compatibility is required or if a migration strategy is expected. There is enough information to attempt a solution (e.g., handle invalid/legacy cookies gracefully), but some implementation details would need to be inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14267": {
        "explanation": "The issue provides a clear description of the problem: deconstructing Q objects with a single child is handled differently, leading to a crash when the child is not subscriptable. It includes code examples that reproduce the issue, explains the root cause, and references an existing patch that addresses the problem. It also discusses a possible alternative solution for backward compatibility. The requirements for a successful solution are clear: fix the deconstruct method so that it does not crash with non-subscriptable children, ideally by removing the special case or by checking the type of the child. This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14271": {
        "explanation": "The issue provides a clear description of the problem: a specific queryset pattern involving a combined queryset in a subquery works in Django 3.1.8 but fails in Django 3.2 with a specific SQL error (\"no such column: V0.user_id\"). The issue includes a minimal reproducible example (models, query code), the SQL generated in both versions, the full traceback, and a sample project link. The expected behavior is implied (the query should work as in 3.1.8), and the failure is well-documented. This is sufficient for a meaningful attempt at a solution, such as investigating the ORM's SQL generation changes between versions and proposing a fix or workaround.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14311": {
        "explanation": "The issue provides context about a previous fix and describes a specific problem: when running 'python -m foo.bar.baz', the autoreloader incorrectly parses the module name, resulting in '-m foo.bar' instead of the intended module. It references a previous commit and describes the expected behavior (correct handling of dotted module names for autoreloading). While it doesn't specify the exact code changes required, it is clear what the bug is and what the correct behavior should be. An experienced engineer could attempt a solution based on this information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14313": {
        "explanation": "The issue provides a clear minimal reproduction, including model and admin definitions, the exact steps to reproduce the error, the error message and stack trace, and even a note about a version regression. The expected behavior is implied: deleting after searching should not crash. The root cause is also suggested. This is enough information for an experienced engineer to attempt a fix or at least to investigate and propose a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14315": {
        "explanation": "The issue provides a clear description of the bug: the PostgreSQL database client's 'runshell' command returns an empty dict for the environment instead of None, which causes os.environ not to be used and results in an empty environment being passed to the subprocess. It also references the specific commit where the bug was introduced and a related PR. The expected behavior (os.environ should be used) and the actual behavior (empty dict passed) are both described. This is sufficient information to attempt a fix: ensure that the environment is set to None (so os.environ is used) unless a specific environment is provided. The code location is also referenced.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14324": {
        "explanation": "The issue clearly describes a regression in the migration autodetector's behavior for self-referential ManyToManyFields. It specifies the expected behavior (related_name should include the model name) and the current incorrect behavior, and even references the commit and related issue where the regression was introduced. This provides enough context and detail to attempt a solution: restore the previous behavior for related_name generation in this case.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14334": {
        "explanation": "The issue provides a clear description of the problem: when both 'zh-hans' and 'zh-hant' are present in settings.LANGUAGES, the middleware is not selecting 'zh-hant' as expected for the 'zh-Hant-HK' Accept-Language header. It also points to the likely location of the bug in the codebase. While it doesn't specify the exact expected behavior in all edge cases, it is clear what the bug is and where to look, making it possible to attempt a meaningful fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14336": {
        "explanation": "The issue clearly describes a specific problem: inconsistent casing ('Col%d' vs 'col%d') in subquery alias generation in Django's SQL compiler. It provides direct links to the relevant code lines, a concrete example query, and explains the impact (failure on case-sensitive databases). The expected direction for a fix is also implied: standardize the casing of generated aliases. This is sufficient information for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14349": {
        "explanation": "The issue provides a clear description of the problem: certain URLValidator tests are failing on Python versions patched for bpo-43882 due to changes in how URLs with LF, CR, and tab characters are handled. It includes specific test failures and explains the root cause (the Python patch strips these characters before validation). However, the issue explicitly states uncertainty about the desired fix and presents two possible approaches without specifying which one to implement. This leaves ambiguity about the correct solution, as either approach could be valid depending on project policy. Therefore, while the technical context is clear, the required outcome is not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14351": {
        "explanation": "The issue provides a detailed description of the observed bug, including the difference in behavior between two similar queries, the specific error message encountered, the relevant code snippets, and even a hacky workaround. It also includes the generated SQL and debugging output, which helps clarify the root cause. However, the issue does not specify what the desired fix should look like (e.g., whether to patch Django internals, suggest a workaround, or document the change in behavior). Still, as an experienced engineer, I could reasonably interpret that the goal is to restore the previous behavior (from Django 2.2.5) or at least prevent the subquery from returning multiple columns. Thus, while some interpretation is needed, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14368": {
        "explanation": "The issue provides a clear description of the problem: Django's parse_datetime does not handle ISO-8601 datetimes with whitespace between the time and timezone, even though this is valid per the spec. It gives concrete examples of input that fails, references the relevant code, and suggests two possible solutions (use python-dateutil or inline its logic). It also mentions a short-term patch to support whitespace. While the spec itself is not public, the examples and the desired behavior are clear. The task is to update parse_datetime to accept whitespace between the time and timezone, matching python-dateutil's behavior. This is enough information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14372": {
        "explanation": "The issue provides a clear description of the problem (SuspiciousFileOperation raised when saving a FileField due to a full path being passed), the context (Django 3.2.1 security change), the cause (FileField.pre_save passes a full path), and steps to reproduce. It also references the relevant Django release note and an existing PR. The expected behavior is implied: FileField should not raise this exception when saving a file-like object with a full path in its name attribute. While the exact solution is not spelled out, the problem and its context are clear enough for an experienced engineer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14373": {
        "explanation": "The issue clearly describes the problem: the Y specifier in django.utils.dateformat.DateFormat should always return a four-digit year, zero-padded if necessary, but currently does not for years less than 1000. The expected behavior is unambiguous, and the area of the code to change is specified. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14376": {
        "explanation": "The issue clearly describes the problem: the Django MySQL backend is using deprecated 'db' and 'passwd' keyword arguments when connecting to mysqlclient, and these should be replaced with 'database' and 'password'. It provides links to the relevant code and the upstream deprecation notice, and specifies that all currently supported Django versions require a new enough mysqlclient to support the new keywords. The required change is unambiguous: update the code to use the new keywords. This is a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14382": {
        "explanation": "The issue clearly describes the problem (an error when using django-admin startapp with a directory name that has a trailing slash), the cause (os.path.basename returns an empty string if the path ends with a slash), and even suggests a specific code change to fix it (strip trailing slashes before calling basename). The expected behavior is also implied: startapp should work even if the directory name has a trailing slash. This is sufficient information to implement and test a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14387": {
        "explanation": "The issue provides a clear description of the observed problem, including the model, the queryset logic, the expected and actual results, and the generated SQL. It also includes a minimal reproducible example and references to related discussions. The problem is well-scoped: incorrect SQL generation when filtering OR-combined querysets in Django 3.2.1. The expected behavior is stated (that further filtering should work as with other querysets), and the actual behavior is described in detail. This is sufficient for a meaningful attempt at a solution, such as investigating the queryset combination and filter logic in Django's ORM.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14395": {
        "explanation": "The issue provides a detailed description of a behavioral change in the CacheHandler class's 'all' method after a specific commit, including code snippets of both the old and new implementations. The user explains the negative impact of the new behavior (unnecessary mass initialization of caches leading to IO lags) and provides a workaround. However, the issue is primarily a question: 'Which version of this method seems to be right?' It does not specify a clear desired outcome or action (e.g., revert to old behavior, add a flag, document the change, etc.). While the context is clear, the required solution is ambiguous\u2014should the code be reverted, made configurable, or something else? Thus, while a sensible interpretation could be made (e.g., restore lazy initialization), there is room for ambiguity about the exact fix expected.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14396": {
        "explanation": "The issue suggests changing the default value of Sitemap.protocol to 'https' and references the relevant Django documentation. However, it is phrased as a suggestion and includes a question ('What do you think?'), indicating it is not a finalized request. There is also mention of a deprecation timeline but no specifics are provided. While the technical change is clear, the lack of a concrete decision and missing details about the deprecation process make it ambiguous what exactly should be implemented and how. Therefore, the issue is too vague for a meaningful attempt at a solution without further clarification.",
        "label": "VAGUE"
    },
    "django__django-14399": {
        "explanation": "The issue describes a common problem with serving static files in Django when deploying with Hypercorn (ASGI), and asks whether it would make sense to integrate information about using Whitenoise into the Django documentation. However, it does not specify exactly what changes are being requested: Should the docs mention Whitenoise as an option? Should there be a step-by-step guide? Should the docs explain why static files are not served by default? The issue is more of a discussion starter or feature suggestion than a concrete, actionable request. There is room for ambiguity about what a successful solution would look like.",
        "label": "VAGUE"
    },
    "django__django-14404": {
        "explanation": "The issue provides a clear description of the problem: the catch_all_view function returns a redirect using request.path_info, which omits the script name, instead of request.path, which includes it. The context is Django and the FORCE_SCRIPT_NAME setting. The expected behavior is clear: the redirect should include the script name. There is even a reference to an existing patch, which further clarifies the intent. This is sufficient information for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14411": {
        "explanation": "The issue clearly describes a specific problem: in the admin, the label for ReadOnlyPasswordHashWidget has a 'for' attribute pointing to a non-labelable element (plain text, not an input). The required change is also clear: the label should not have a 'for' attribute in this case. The context, the widget, and the expected behavior are all specified. This is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14416": {
        "explanation": "The issue clearly describes the problem (incorrect language being displayed due to cached redirects that do not vary by Accept-Language or language cookie), the context (LocaleMiddleware, CDN, caching reverse proxy), and the root cause (missing Vary header). It also suggests a solution (adding a Vary header). As an experienced engineer, I can interpret this as needing to ensure that LocaleMiddleware adds the appropriate Vary headers to responses that depend on Accept-Language or the language cookie. The requirements and expected outcome are clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14430": {
        "explanation": "The issue provides a clear description of the problem, including reproducible code samples, expected vs. actual behavior, and references to relevant code in the Django codebase. It also suggests a direction for a solution (modifying the except EmptyResultSet clause to handle Coalesce with Value). While the exact implementation details are left to the developer, the requirements and context are sufficiently clear to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14434": {
        "explanation": "The issue provides a clear description of the problem: the function _create_unique_sql is causing references_column to always be false because it passes a Table instance to Columns when a string is expected. This gives enough context to identify the code path, the incorrect argument type, and the resulting bug. While it doesn't specify the exact fix, it is clear what needs to be corrected (the argument type passed to Columns). Therefore, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14441": {
        "explanation": "The issue clearly describes the problem: the get_image_dimensions() function crashes when given a non-existent file or path. The expected behavior is implied\u2014to prevent the crash, likely by handling the error gracefully. While it doesn't specify exactly how to handle the error (e.g., return None, raise a specific exception, log an error), it is a common and sensible interpretation to add error handling for this case. Therefore, a meaningful solution can be attempted, though some minor implementation details are left to the developer's discretion.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14447": {
        "explanation": "The issue provides a clear description of the current problem (redundant COUNT(*) queries causing performance issues), the relevant code locations, and a concrete proposal for a solution (passing the count into _cull to avoid a second query). It also discusses a possible further optimization (not running culling on every set), but this is presented as an open question rather than a requirement. The first change is well-specified and actionable; the second is more speculative but could be reasonably interpreted as a stretch goal. Overall, there is enough detail to make a meaningful attempt at a solution, especially for the first part.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14451": {
        "explanation": "The issue provides a clear description of the problem (syndication feeds do not use self-closing tags when appropriate), the historical context, the relevant classes (SimplerXMLGenerator, XMLGenerator), and the fact that a solution involves passing a specific argument to enable this behavior. It also references an existing PR as an example. While it doesn't specify the exact argument name, it is clear from the context (and Python's XMLGenerator documentation) what needs to be done. Therefore, a meaningful attempt at a solution is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14453": {
        "explanation": "The issue describes a specific scenario: when a user misspells 'urlpatterns' in a Django urls.py file, Django raises a generic ImproperlyConfigured error that does not help the user identify the real problem. The issue requests that a more helpful message be shown in this case. While the issue is clear about the problem and the desired outcome (a better error message when 'urlpatterns' is misspelled), it does not specify exactly how the detection should work (e.g., should it suggest the correct spelling, or just mention that 'urlpatterns' is missing?). However, as an experienced engineer, it is reasonable to interpret the request as: improve the error message to help users who misspell 'urlpatterns'. Thus, the issue is reasonably specified, though not exhaustively detailed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14463": {
        "explanation": "The issue provides a clear description of the desired feature: adding support for table and column comments in Django models, which would be propagated to the database schema via migrations or syncdb. It also includes a concrete example of the proposed API (using db_column_comment and db_table_comment). While there are some implementation details left open (e.g., how to handle different database backends, migration support, etc.), the core requirement and the expected interface are well-specified. A meaningful attempt at a solution can be made based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14471": {
        "explanation": "The issue provides a clear description of the current behavior, the inefficiency it introduces, and a concrete proposal for how to improve it. It references related work (#32795), specifies the relevant methods and exception handling, and outlines the desired control flow changes. While it assumes some familiarity with the codebase and the referenced issue, it is clear what needs to be changed and why. The steps for implementation are well described, making it possible to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14480": {
        "explanation": "The issue provides a clear feature request: add logical XOR support to Q() and QuerySet() in Django, similar to how AND, OR, and NOT are currently supported. It references relevant StackOverflow discussions and notes that most major databases support XOR, except for SQLite. However, it does not specify the exact API, expected behavior in edge cases (e.g., with SQLite), or how this should interact with existing query composition. There is enough information to make a sensible attempt (e.g., by following the pattern of existing Q object operators and handling SQLite as a special case), but some implementation details would need to be decided by the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14493": {
        "explanation": "The issue provides a clear description of the bug, including steps to reproduce, the relevant code snippet, the error message, and a link to the problematic code. The expected behavior is implied: setting max_post_process_passes to 0 should not cause a crash. The root cause (uninitialized variable) is identified, and the motivation for the use case is explained. This is sufficient information to attempt a meaningful fix (e.g., ensure 'substitutions' is always initialized).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14495": {
        "explanation": "The issue provides a clear description of the problem: Django's migration system is inconsistent when handling replaced (squashed) migrations. It gives concrete reproduction steps, including the file structure, commands run, and the resulting errors. The desired outcome is also described: either allow migrating to replaced migrations if they exist on disk, or improve the error handling and ambiguity resolution if that's not possible. While there is some flexibility in the exact solution (e.g., which of the two approaches to take), the problem and the criteria for a successful solution are well-articulated.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14500": {
        "explanation": "The issue describes a specific problem with the MigrationExecutor: when unapplying a squashed migration, and the replaced migration files are still present, the executor marks the squash migration as unapplied, not just the replaced migrations. The expected behavior is implied: only the replaced migrations should be marked as unapplied, not the squash migration. While the description could be clearer about the desired outcome, it is possible to infer what needs to be fixed and how to verify the solution. Therefore, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14508": {
        "explanation": "The issue provides a clear code example, a description of the observed error, and a specific suggestion for improvement: to emit a more precise warning or error message when a model field is being overwritten by a property in a base class. The reproduction steps are explicit, and the desired outcome (better error/warning messaging) is clear. While the exact wording of the warning is not specified, the intent is unambiguous and actionable for a developer familiar with Django's model system.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14513": {
        "explanation": "The issue provides a clear context: the current 'showmigrations' command in Django can misleadingly indicate that a squashed migration has been applied ([X]) when, in fact, it has only been 'soft' applied (i.e., the original migrations it replaces have been run, but the squashed migration itself has not been recorded as applied). The issue requests a change to the output of 'showmigrations' to better indicate this 'soft applied' state. While it does not specify the exact output format or symbol to use, it is clear what the problem is and what kind of solution is required: modify 'showmigrations' to distinguish between fully applied and 'soft applied' squashed migrations. There is some room for interpretation in the exact output, but the technical direction is clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14534": {
        "explanation": "The issue provides a clear description of the problem: BoundWidget.id_for_label ignores the 'id' set in the widget's attrs and instead reconstructs it, which can lead to inconsistencies, especially when auto_id is overridden. The issue also provides a concrete code example, explains the context (CheckboxSelectMultiple subwidgets), and suggests a specific fix (return self.data['attrs']['id'] in id_for_label). The scope and expected behavior are well-defined, making it clear what needs to be changed and why.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14539": {
        "explanation": "The issue provides a clear description of the problem with the urlize() function, including a concrete input example, the expected output, and the actual output. This makes it clear what the bug is and what the correct behavior should be. The requirements for a successful solution are well-defined: urlize() should handle HTML-escaped strings and trailing punctuation as shown in the example.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14559": {
        "explanation": "The issue clearly describes the current behavior (bulk_update returns None), the desired behavior (it should return the number of rows matched, similar to update()), and even points to the relevant code and suggests a straightforward implementation approach (sum the return values of the underlying update() calls). There is little ambiguity about what is required for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14580": {
        "explanation": "The issue provides a clear description of the bug, including the Django version, the relevant models.py code, the generated migration file, the error message encountered, and the expected vs. actual behavior. It also suggests a possible location of the bug in the codebase. The required fix is clear: ensure that the generated migration file includes the necessary import for 'models' so that 'models.Model' is defined. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14584": {
        "explanation": "The issue states that the logging output for 'django.db.backends' should include the database alias, and provides a clear rationale: it's essential for working with database routing. While it doesn't specify exactly where or how the alias should be included in the logging output, the intent is clear and actionable for someone familiar with Django's logging and database routing. The linked PR suggests there is precedent or an example to follow. There are some implementation details to decide, but the core requirement is clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14602": {
        "explanation": "The issue provides context about the current regular expression used in parse_time, points out that it is too permissive, and gives a concrete example of an input that is arguably invalid but currently accepted. However, it does not specify exactly which inputs should be considered invalid, nor does it provide a clear set of requirements for the new, 'tightened' regex. It also leaves open the possibility of not changing the regex at all, just documenting the current behavior. Therefore, while there is enough information to make a reasonable attempt (e.g., by analyzing current behavior, proposing a stricter regex, and/or improving documentation), there is some ambiguity about the exact desired outcome.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14608": {
        "explanation": "The issue clearly describes the desired feature: adding a 'nonform' CSS class to non form errors in FormSets, analogous to the existing 'nonfield' class for non field errors in Forms. It also explains the motivation and expected outcome, including updating documentation. The technical context (ErrorList, FormSets, CSS class usage) is sufficiently clear for an experienced developer to implement the change and update documentation accordingly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14631": {
        "explanation": "The issue provides a clear description of the desired refactor: BaseForm._clean_fields() and BaseForm.changed_data should access values via BoundField for consistency and code simplification. It also suggests moving per-field logic into a BoundField method (e.g., did_change), and even provides a code snippet for the new changed_data implementation. The issue references a related test that can be adjusted to cover the new behavior. While some implementation details (e.g., exact method signatures) are left to the implementer, the intent and scope are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14634": {
        "explanation": "The issue provides a clear objective: when an object is deleted, a success message should be shown, and this should be implemented via a mixin. While it does not specify the exact framework or the format of the message, it is a common pattern (e.g., in Django or similar frameworks) and the intent is clear. The implementation details (such as the message content or how the mixin is integrated) can be reasonably inferred or follow established conventions in the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14641": {
        "explanation": "The issue provides a clear description of the desired code change: move the special-case logic for time and datetime objects from BoundField.initial() to BaseForm.get_initial_for_field(), specifically under the 'if callable(value)' block. It also references the relevant lines in the codebase and gives context about a possible future refactor, but the immediate task is well-defined. The references to specific code locations and the nature of the logic to be moved make it clear what is required for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14667": {
        "explanation": "The issue provides a clear description of the problem, including the model structure, the specific QuerySet methods used, the observed SQL queries, and the expected behavior. It gives concrete examples of both the problematic and correct cases, making it clear what the bug is and what the correct output should be. This is sufficient information for an experienced developer to attempt a fix or at least to reproduce and investigate the issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14672": {
        "explanation": "The issue provides a clear description of the bug (a TypeError due to an unhashable list in the identity property of ManyToManyRel), the context (Django 3.2, proxy models, through_fields), a minimal reproducible example, and a concrete solution (add a make_hashable call to self.through_fields in ManyToManyRel). The expected change is unambiguous and the reproduction steps are clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14681": {
        "explanation": "The issue provides a clear description of the problem: when there is a storage issue (such as missing directories), the application incorrectly reports a CSRF failure instead of a filesystem/storage error. The steps to reproduce are detailed, including code changes, configuration, and the expected vs. actual behavior. The desired outcome is also specified: the error message should reflect the real storage problem, not a CSRF issue. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14725": {
        "explanation": "The issue clearly describes a desired feature: to provide a way for model formsets to be used in an 'edit only' mode, where new object creation is disallowed. It explains the current workaround (using extra=0) and why it is insufficient, mentioning that extra forms can still be added via JavaScript or POST data. While it does not specify the exact API or method to implement this, the intent and the problem are clear, and a reasonable solution can be designed (e.g., adding a flag to the formset to disallow new object creation and validating POST data accordingly). Thus, the issue is reasonably specified, though some implementation details are left to the developer's discretion.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14730": {
        "explanation": "The issue clearly describes the current behavior (that related_name is ignored for symmetrical/self-referential ManyToManyFields), the potential confusion for developers, and the proposed solution (raise an error if related_name is set in this context). The required change is unambiguous: detect when a ManyToManyField is both symmetrical/self-referential and has a related_name, and raise an error. No further clarification is needed to implement this.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14733": {
        "explanation": "The issue describes a feature request to allow overriding the deletion widget in formsets, similar to how ordering_widget and get_ordering_widget() were introduced in Django 3.0. It references a previous issue for context and provides a clear analogy for the desired functionality. While it does not specify exact API signatures or all edge cases, the intent and scope are clear enough for an experienced Django developer to propose and implement a solution (i.e., introducing deletion_widget and get_deletion_widget() methods/properties).",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14751": {
        "explanation": "The issue provides a clear description of the current limitations of the 'makemigrations' command regarding scripting: it writes progress to stdout instead of stderr, and does not provide a structured way to determine which files were created. The user describes a concrete use case (running in Docker, needing to copy new migration files out), and suggests a possible solution (outputting created file paths to stdout). However, the issue leaves some implementation details open, such as the exact format of the output, whether this should be a new flag or default behavior, and how to handle multiple files or errors. Still, the core requirement is clear: make it possible to programmatically determine which files were created by makemigrations. This is enough to make a sensible attempt at a solution, though some design choices will need to be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14752": {
        "explanation": "The issue provides a clear description of the current problem (difficulty in customizing autocomplete responses in AutocompleteJsonView), a concrete example of the workaround currently required, and a detailed proposal for a solution (refactoring to add a serialize_result method as an extension point). The expected code changes are described precisely, including before-and-after code snippets. The objective and the acceptance criteria are clear: refactor AutocompleteJsonView.get() to delegate result serialization to a new method that can be overridden. This is sufficient for a meaningful and unambiguous implementation attempt.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14765": {
        "explanation": "The issue clearly describes the context (a recent PR changed all calls to pass real_apps as a set), the current behavior (checking and converting real_apps to a set), and the desired change (replace the check/conversion with an assertion that real_apps is a set when non-None). It also specifies where this change should occur (ProjectState.__init__). The requirements and rationale are clear, making it straightforward to implement the solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14771": {
        "explanation": "The issue provides a concrete scenario, including commands and code snippets, demonstrating that when running Django's development server with the '-X utf8' option, the auto-reloader does not preserve the '-X' option when restarting the process. The expected behavior is implied: the auto-reloader should pass '-X' options to the child process, as evidenced by the difference in encoding output. The references to Python documentation clarify what '-X' options are. While the issue does not explicitly state the desired change in the codebase, the problem and expected outcome are clear enough for an experienced engineer to implement a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14779": {
        "explanation": "The issue clearly describes a specific bug: when a custom template tag is registered with takes_context=True but the context parameter is omitted and there are no other parameters, an unhelpful IndexError is raised instead of a meaningful TemplateSyntaxError. The expected behavior is also described (an informative error should be raised). The traceback and code samples make it clear where the problem occurs and what needs to be fixed. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14785": {
        "explanation": "The issue provides a clear description of the problem, including the environment, steps to reproduce, the model definition, the specific error encountered, and the context in which the error occurs (admin view, refresh_from_db). It also includes a traceback, which helps pinpoint where the error happens. The expected behavior is implied: storing NaN in a DecimalField should not cause an unhandled exception on retrieval. While it does not specify exactly how NaN should be handled (e.g., should it be rejected, converted to None, or handled in some other way), it is clear that the current behavior (crashing) is unacceptable and needs to be fixed. There is a sensible interpretation for a solution: prevent the crash and handle NaN values gracefully. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14787": {
        "explanation": "The issue provides a clear description of the problem: when using method_decorator with a decorator, the function passed is a functools.partial object, which lacks standard function attributes like __name__ and __module__. The issue includes a concrete example, the expected behavior (preservation of wrapper assignments), and the observed error (AttributeError). It is clear that the solution should ensure that method_decorator preserves these attributes, likely by using functools.update_wrapper or similar. The requirements and context are sufficiently specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14792": {
        "explanation": "The issue provides a clear description of the observed behavioral change between Django 3.1 and 3.2 regarding time zone handling in Trunc()/Extract() database functions, specifically with 'Etc/GMT-10'. It identifies the methods and classes involved, the difference in SQL generated, and the root cause (change in _get_timezone_name() return value). However, it does not explicitly state what the desired behavior is (should Django 3.2 revert to the old behavior, or is there a new correct way to handle this?). Still, the context and expected outcome can be reasonably inferred: the time zone conversion should match the pre-3.2 behavior, and the SQL should use the correct offset. Thus, a sensible solution can be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14802": {
        "explanation": "The issue provides a clear description of the problem (repetitive code for making and validating cache keys), a concrete proposed solution (add a helper function make_and_validate_key to the base cache backend), and even a code snippet for the new method. It also discusses an alternative approach and why it is less desirable. The requirements for a successful solution are clear: implement the helper method as described, ensure it is available on the base cache backend, and use it to reduce code repetition. There is no ambiguity about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14832": {
        "explanation": "The issue provides a clear description of the problem: the floatformat template filter in Django does not respect the {% localize %} template tag when its value is opposed to USE_L10N, due to not passing the use_l10n parameter to formats.number_format(). It also points to the relevant code and suggests that the value should be taken from the template rendering context. While it notes a challenge (filters do not take context), the desired behavior is clear: make floatformat independent of USE_L10N and allow forcing unlocalized format, ideally respecting the {% localize %} tag. This is enough information for an experienced Django developer to attempt a solution, though some implementation details (such as how to pass context to the filter) are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14855": {
        "explanation": "The issue provides a clear description of the bug (incorrect URL generated for readonly ForeignKey fields in a custom Admin Site), identifies the relevant code location, and even suggests a specific code change that appears to resolve the problem. The expected behavior is clear: the URL should use the custom admin site prefix, not the default '/admin/'. While the reporter expresses some uncertainty about side effects, the core requirements and context are well-specified for a meaningful attempt at a solution, including writing tests to ensure no regressions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14861": {
        "explanation": "The issue provides a clear minimal example with code snippets for both models and admin configuration. It describes the expected and actual behaviors, and specifies the error encountered when toggling the 'show_inlines' field from False to True. The problem is well-scoped: the admin form fails to render new inlines due to missing ManagementForm data. The desired outcome is implied: the admin should allow toggling inlines on and off without error. While it doesn't prescribe a specific solution, the problem and its context are clear enough for an experienced developer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14871": {
        "explanation": "The issue provides a clear description of the problem: Select2 translations are not loaded when the LANGUAGE_CODE includes a subtag (e.g., 'pt-BR'), due to language codes being converted to lowercase. It points to a specific line in the codebase where this happens. The expected behavior (translations should load for subtags) and the cause (lowercasing the language code) are both described. This is sufficient information to attempt a solution, such as adjusting how the language code is handled when looking up translation files.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14880": {
        "explanation": "The issue clearly describes the problem: when a reverse accessor clash is detected by RelatedField._check_clashes(), the error message does not include the name of the clashing field, making debugging harder. The desired improvement is to include the clashing name in the error message. The issue also references a related PR and mentions that unit tests may need to be updated. While it doesn't specify the exact wording for the new error message, the intent and scope are clear and actionable for a developer familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14894": {
        "explanation": "The issue provides a clear description of the problem, including model definitions, test data, the exact queries used, and the observed vs. expected outputs. It also explains the context and why the behavior is considered a bug. The only minor gap is that the minimal repro project is mentioned as an attachment but not included, but the code and steps provided are sufficient to reproduce and understand the issue. Therefore, a meaningful attempt at a solution can be made based on the information given.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14915": {
        "explanation": "The issue provides a clear description of the problem: after upgrading from Django 3.0 to 3.1, the user encounters a TypeError due to 'ModelChoiceIteratorValue' being unhashable when used as a key in a dictionary. The code snippets illustrate the context and the specific line causing the error. However, the issue does not specify what the expected behavior should be or how the value should be handled to restore compatibility. There is enough information to infer that the solution should make 'value' hashable or adjust the code to use a hashable representation, but some interpretation is required regarding the best approach (e.g., converting 'value' to its primary key). Thus, the issue is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14919": {
        "explanation": "The issue provides a clear description of the current problem (durability errors are ignored in TestCase but not in TransactionTestCase), the impact (errors not surfacing until later environments), and a proposed solution (track transactions with markers to selectively skip durability checks). It even references a proof-of-concept patch, which further clarifies the intended direction. While some implementation details may need to be worked out, the requirements and the desired outcome are clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14935": {
        "explanation": "The issue describes a specific problem: when using a callable as a default for a unique field in a Django model, the migration system only calls the callable once, resulting in all existing rows getting the same value. The request is to add a warning in makemigrations when a unique field has a callable default, to alert the developer to this potential pitfall. The context, example, and desired behavior (a warning) are all clear. The solution is to detect this pattern and emit a warning during makemigrations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14960": {
        "explanation": "The issue clearly describes a bug in the 'createsuperuser' command: when a REQUIRED_FIELD is passed via the command line in interactive mode, its value is not validated, leading to potential type errors or invalid data being passed to UserManager.create_superuser. The description points out where validation happens in both modes and what is missing. The expected behavior (that all fields, regardless of how they are provided, should be validated) is implied and standard. The solution would be to ensure that command-line provided values in interactive mode are also validated. This is a well-specified bug report with a clear path to a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14996": {
        "explanation": "The issue is well-specified: it describes the current and expected behavior, provides a concrete example with code, migration output, and SQL, and references a related issue (#31826) for context. The problem is clear: renaming a field and setting db_column to the old name should not emit unnecessary SQL, but currently does. The desired outcome (a SQL noop) is explicit. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14997": {
        "explanation": "The issue provides a clear description of the model, the migrations, the database backend (SQLite), the Django version, and the exact error message encountered. The problem is reproducible from the information given, and the error is specific. The expected behavior is implied: the migration should not crash. The root cause (use of F() expressions in UniqueConstraint on SQLite) is also evident from the traceback and model code. This is sufficient for a meaningful attempt at a solution, such as fixing the migration or improving Django's handling of this case.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14999": {
        "explanation": "The issue describes a specific scenario: when a RenameModel operation is performed and the model already has db_table defined, the operation should be a no-op (i.e., it should not perform any changes). It also provides context about the current behavior in Postgres and SQLite. While it does not specify the exact code location or framework, it is clear what the desired behavior is and what constitutes a successful solution: ensure that RenameModel with db_table set does nothing. This is enough information for an experienced engineer familiar with the codebase to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15018": {
        "explanation": "The issue provides a clear description of the problem: when using Django's call_command with a management command that has mutually exclusive arguments sharing the same 'dest', it is not possible to specify the '--for' argument, and the parsing fails. The code sample and the explanation of the two failure modes are concrete. However, the issue does not specify what the expected behavior should be for call_command, nor does it specify how the API should be changed or what the interface should look like. There is some ambiguity about the desired solution (e.g., should call_command accept a 'for_' keyword? Should it accept a dict? Should the command be refactored?). Thus, while the problem is clear, the requirements for a solution are not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15022": {
        "explanation": "The issue describes a performance problem in Django admin when searching with multiple words, due to unnecessary SQL joins generated by the search_fields configuration. It provides model and admin code, explains the root cause, and references an existing patch (though not included). However, it does not specify what the desired behavior is (e.g., should the joins be combined, or should the query be rewritten in a specific way?), nor does it provide the patch or a clear acceptance criterion. There is enough information to make a reasonable attempt at optimizing the query generation, but some interpretation is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15031": {
        "explanation": "The issue clearly describes a limitation in Django admin's filtering mechanism: when using the '__in' lookup in the query string, it is not possible to filter for values that themselves contain commas, because the splitting logic does not allow escaping or quoting commas. The issue points to the relevant function ('prepare_lookup_value') and provides a concrete example. However, it does not specify the desired behavior (e.g., should there be an escape character, quoting, or some other mechanism to allow commas in values?), nor does it specify how backwards compatibility should be handled. There is a sensible interpretation (e.g., implement a way to escape or quote commas), but some design decisions are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15037": {
        "explanation": "The issue provides a concrete example of a database schema where a foreign key references a non-primary unique field, and describes the incorrect behavior of inspectdb (generating a FK to the model rather than to the specific field). The expected behavior is implied: inspectdb should generate a ForeignKey referencing the specific unique field (foo.other_id) rather than the model's default primary key. While the issue does not specify the exact code changes or tests required, it is clear what the bug is and what the correct output should be. Therefore, a meaningful attempt at a solution can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15038": {
        "explanation": "The issue clearly describes the problem: using the --squashed-name option with squashmigrations can overwrite existing migration files, leading to inconsistencies and errors. It also specifies the desired behavior: squashmigrations should exit with an error instead of overwriting existing migration files. The expected outcome and the reason for the change are both well-articulated, making it clear what needs to be changed and why.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15044": {
        "explanation": "The issue provides a clear description of the problem: CacheMiddleware and FetchFromCacheMiddleware are not thread safe because they persist a cache instance on self.cache, which is not safe for concurrent access. The issue includes a traceback, a reproduction scenario, and references to the relevant code and history. However, it does not specify the desired solution (e.g., whether to use per-request cache instances, thread-local storage, or another approach), nor does it specify any constraints or acceptance criteria. Still, the problem is well-defined and a reasonable solution can be attempted based on standard thread-safety practices for Django middleware.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15061": {
        "explanation": "The issue provides a clear description of the current behavior (MultiWidget generates id_for_label with an index, e.g., 'id_0'), compares it to another widget (ChoiceWidget) that allows more control, and suggests a concrete solution: remove the id_for_label method from MultiWidget. While it doesn't specify every detail (e.g., whether to deprecate or just remove, or how to handle backward compatibility), the main request is clear and actionable. There is a sensible interpretation for a successful solution: remove the id_for_label method from MultiWidget and ensure labels no longer use 'for = ...' in this context.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15062": {
        "explanation": "The issue provides a clear description of the problem: Case() and ExpressionWrapper() do not work as expected with DecimalField on SQLite, with specific reference to type conversion inconsistencies. The author provides detailed test cases that reproduce the issue, including the SQL generated and the errors encountered. The expected and actual behaviors are described, and there is a reference to a similar previous issue (#18247) that may inform the solution. While the exact fix is not prescribed, the problem is well-demonstrated and the desired outcome (consistent and correct type handling for DecimalField annotations on SQLite) is clear. This is sufficient for an experienced engineer to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15087": {
        "explanation": "The issue clearly describes the current problem (using regexes to parse DDL for foreign keys in SQLite introspection) and suggests a specific alternative (using PRAGMA foreign_key_list, as is done in _get_foreign_key_constraints). The required change is to refactor DatabaseIntrospection.get_relations to use PRAGMA instead of regex parsing. The scope and intent are clear, and a successful solution would be to implement this refactor and ensure it works as intended.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15098": {
        "explanation": "The issue provides a clear description of the problem: Django's i18n_patterns does not support language locales that include both script and region subtags (e.g., en-Latn-US). It includes a reproducible example, expected vs. actual behavior, relevant code snippets, and references to the relevant RFC. The expected outcome is unambiguous: URLs with such locales should resolve as they do for simpler locales. This is sufficient for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15102": {
        "explanation": "The issue clearly describes the problem: the 'startproject' command in Django does not honor the user's umask when creating files, resulting in files with more permissive permissions than expected. It provides a concrete example, the expected behavior, and the current (incorrect) behavior. The goal is to ensure that files created by 'startproject' respect the user's umask. This is a well-defined requirement and can be addressed by modifying the file creation logic to honor umask.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15103": {
        "explanation": "The issue clearly states the desired change: make the 'element_id' argument of the 'json_script' function optional. The motivation and context are provided, and the expected behavior is clear (the function should work without requiring an id). There is no ambiguity about what needs to be changed, and the rationale is explained. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15104": {
        "explanation": "The issue provides a clear description of the problem (a KeyError in the migration autodetector when using a custom ForeignKey field that hardcodes and then removes the 'to' argument), a traceback, a minimal reproducible example, and a suggested fix (changing 'del deconstruction[2][\"to\"]' to 'deconstruction[2].pop(\"to\", None)'). This is sufficient information to understand the bug, reproduce it, and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15108": {
        "explanation": "The issue provides a clear description of the inconsistency between how order_by arguments are handled in QuerySet and aggregates (which support the leading dash syntax for descending order) versus Window (which does not). It suggests a specific solution: to reuse the logic from OrderableAggMixin.__init__ in Window.__init__, and even provides a code sketch for a possible abstraction. While there are some side notes about naming and API design, the core actionable request is clear: make Window(order_by) accept the leading dash syntax for descending order, matching the behavior elsewhere. This is enough to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15111": {
        "explanation": "The issue clearly describes the problem: when django-admin startproject or startapp is used with a remote template URL, the default urllib user agent is used, which can cause requests to be blocked by some sites. The desired solution is also specified: set the 'User Agent' header to 'Django/DJANGO_VERSION' when making these requests. The expected behavior and the change to be made are both clear, making this a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15127": {
        "explanation": "The issue describes a specific problem: when using @override_settings in Django tests to create new message tags, the LEVEL_TAGS setting is not updated as expected, resulting in Message.level_tag being an empty string. The context (Django messages framework, use of @override_settings, and the expected behavior) is clear. The desired outcome is that Message.level_tag should reflect the new tags set via @override_settings. While the issue does not specify the exact code to change, it is clear what the bug is and what the correct behavior should be, making it possible to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15128": {
        "explanation": "The issue provides a clear description of the bug, including the environment, code to reproduce, the exact error, and an analysis of the root cause. It also suggests a possible direction for a fix and requests improvements to documentation. The requirements for a successful solution are clear: fix the AssertionError in Query.change_aliases when combining QuerySets with OR, and improve documentation/comments around the assertion. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15135": {
        "explanation": "The issue provides a clear code example, describes the expected and actual behaviors, and suggests a possible area in the codebase (bulk_update logic) where the problem may lie. It is clear that the problem is with updating a ForeignKey field via bulk_update after assigning and saving a new related object. The expected outcome is also well-defined. While there is some room for interpretation in the implementation details, the requirements for a successful solution are clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15139": {
        "explanation": "The issue provides a clear objective: deprecate PickleSerializer and move it out of the core codebase, possibly into a third-party package. The motivation and context are explained, and the desired outcome (deprecation and removal from core) is explicit. While there may be some implementation details to decide (e.g., how to mark as deprecated, where to move the code), the main requirements are clear and actionable for an experienced engineer.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15161": {
        "explanation": "The issue provides a clear context: previously, the deconstruction of expressions like F() used a verbose path, but a recent PR simplified it for F(). The issue now requests applying the same simplification to other expressions, to improve migration code readability. While it doesn't list every expression to update, it is clear that the task is to identify other expressions that can be imported from a higher-level module (like django.db.models) and update their deconstruction paths accordingly. This is a sensible and actionable request for someone familiar with Django's codebase and migration system.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15166": {
        "explanation": "The issue describes a problem where the DatabaseCache backend does not quote all fields in queries, which causes issues with Snowflake since unquoted fields are treated as uppercase. However, the issue does not specify which fields are affected, what the expected quoting should look like, or provide details about the patch or how to integrate it. There is also mention of compatibility with different branches, but no clear instructions. While the general problem is clear, there are significant gaps in the specifics required for a solution.",
        "label": "VAGUE"
    },
    "django__django-15180": {
        "explanation": "The issue clearly describes a specific problem: when using Django's path()/re_path(), if the 'kwargs' argument is not a dict (e.g., a string is passed by mistake), the resulting error is confusing and not user-friendly. The reporter provides concrete examples, stack traces, and suggests that a TypeError should be raised early if 'kwargs' is not a dict. The desired solution is explicit: add a type check for 'kwargs' (and possibly 'name') to raise a clear error if the wrong type is passed. This is a well-scoped, actionable request.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15202": {
        "explanation": "The issue clearly describes the problem: when calling clean on a forms.URLField with a certain invalid input, a ValueError is raised instead of the expected ValidationError. The stack trace is provided, and the expected behavior (raising ValidationError) is implied by Django's form validation conventions. The input that triggers the bug is given, and the location in the codebase is clear. The fix would involve ensuring that ValueError is caught and re-raised as ValidationError in the relevant code path. This is a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15213": {
        "explanation": "The issue provides a clear description of the problem: using ExpressionWrapper with ~Q(pk__in=[]) causes a query generation failure, resulting in invalid SQL. It includes a working example and a failing example, showing the expected and actual outputs. The context (reducing Q objects, use of ExpressionWrapper, and BooleanField) is sufficient for an experienced developer to investigate and attempt a fix. The expected behavior is implied: the query should not break and should generate valid SQL, similar to the working case. While it doesn't specify the exact fix, it is clear what needs to be addressed and what a successful solution would look like.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15240": {
        "explanation": "The issue provides a clear description of the problem (stale entries in the django_migrations table after squashing and deleting migrations), the motivation for solving it, and a proposed solution (truncating the table to only include migrations that actually exist in the project, with a preference for an automatic approach). It also discusses potential pitfalls and how to avoid them (e.g., not removing migrations that are referenced in 'replaces' lists). While there are some implementation choices left open (automatic vs. manual, where to hook the logic), the requirements and constraints are clear enough to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15248": {
        "explanation": "The issue describes a desired feature: to provide context to pre/post delete signals about the origin of a delete operation (e.g., whether it was triggered by a model instance's delete() or a queryset's delete(), and which instance or queryset was responsible). However, it does not specify how this information should be passed, what the API should look like, or how to handle complex cases (e.g., cascading deletes, bulk deletes, etc.). There is a clear intent, but significant design decisions are left open, requiring the implementer to make assumptions. Thus, it is not fully well-specified, but a sensible interpretation is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15252": {
        "explanation": "The issue provides a clear description of the problem: MigrationRecorder is not respecting the allow_migrate rules defined in a custom database router, resulting in migration tables being created in databases where they should not be. The expected behavior is also clearly stated, and the relevant code paths and files are referenced. While the issue does not specify the exact code change required, it is clear what the intended outcome should be and where to look in the codebase. This makes it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15268": {
        "explanation": "The issue provides a clear description of the current behavior (multiple AlterFooTogether operations that could be combined), the motivation (to avoid unnecessary operations), and a concrete example of both the current and desired optimized output. It references relevant context (previous PRs and test cases) and specifies the goal: to optimize migration operations by combining redundant ones. The requirements and expected outcome are unambiguous, making it clear what a successful solution would look like.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15272": {
        "explanation": "The issue provides a clear description of the desired functionality: a management command named 'optimizemigration' that takes an app name and migration name, reads the specified migration, passes it through the optimizer, and rewrites the optimized migration to disk. It also specifies that this command should ignore the 'squashed-ness' of the migration, differentiating it from existing commands. While it references an attached patch (which is not present here), the requirements for the new command are sufficiently clear to implement it from scratch or improve upon an existing implementation. The context and expected behavior are well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15277": {
        "explanation": "The issue provides a clear description of the current behavior, the problem (unnecessary MaxLengthValidator with None max_length), and a concrete proposed solution (modify CharField.__init__ to only add the validator if max_length is not None). It also provides context, rationale, and even performance measurements. The required code change is explicitly described, and the expected outcome (no extraneous validator, micro-optimization) is clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15278": {
        "explanation": "The issue provides a clear description of the problem: adding a nullable OneToOneField via a migration on SQLite results in an OperationalError due to SQLite's inability to add a UNIQUE column with ALTER TABLE. It includes the error message, the migration code, and context about recent changes. However, it does not specify what the desired behavior or solution should be (e.g., should the migration be made to work, should there be a workaround, or should an error be raised earlier?). There is enough information to investigate and propose a sensible fix or workaround, but some interpretation is required regarding the expected outcome.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15280": {
        "explanation": "The issue provides a clear description of the models, the test case, the expected and actual behavior, and the specific queries being executed. It also includes diagnostic output and a hypothesis about the cause of the bug. While the reporter asks for confirmation that this is a bug and for guidance, the technical details are sufficient for an experienced engineer to attempt a fix: the problem is that deferred fields are not being handled correctly when following prefetches back to the parent object. The expected behavior is well-defined, and the test case can be used to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15292": {
        "explanation": "The issue clearly describes the problem: the technical 404 debug page is displaying the view name for class-based views (CBVs) in an incorrect format, showing the full class representation instead of the expected dotted path. It provides an example of the incorrect and correct output, and references the commit where the regression was introduced. This is sufficient information to identify the bug and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15297": {
        "explanation": "The issue provides a clear description of the problem (incorrect repr for ResolverMatch with CBVs), a concrete test case that demonstrates the expected behavior, a discussion of the root cause (change in how CBVs are handled after a specific PR), and even a suggested code fix. The context is specific to Django's URL resolver and CBVs, and the expected output is unambiguous. While there is some reference to related issues and PRs, the core requirement and the path to a solution are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15315": {
        "explanation": "The issue clearly describes the problem: the hash of a Field instance changes after it is assigned to a model class, which breaks its use as a dictionary key. It provides a minimal reproducible example, references the commit where the bug was introduced, and suggests a possible solution (reverting the __hash__ change from #31750). The expected behavior is also clear: Field.__hash__ should be immutable. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15316": {
        "explanation": "The issue identifies a specific function (simplify_regex) and a specific problem (it doesn't handle non-capturing groups), but it does not provide any examples of input/output, expected behavior, or details about what 'broken' means in this context. There is no information about what the correct handling should be, nor any test cases or error messages. While a knowledgeable engineer could make some assumptions, there is significant ambiguity about the requirements for a successful solution.",
        "label": "VAGUE"
    },
    "django__django-15318": {
        "explanation": "The issue provides a clear description of the current inconsistent behavior between reverse foreign key and many-to-many relations when querying unsaved models. It specifies the desired new behavior (consistent error or empty queryset), outlines the changes to be made, and even references an existing patch as a starting point. While there is some room for interpretation in the exact implementation details, the requirements and expected outcomes are clear enough for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15320": {
        "explanation": "The issue provides a clear description of the problem: Subquery.as_sql() generates invalid SQL due to incorrect handling of brackets, and setting the subquery attribute to True fixes it. It includes a code sample, expected and actual outputs, and a suggested fix (setting subquery=True in the Subquery constructor). The context (Django ORM) and the specific method to address are both clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15324": {
        "explanation": "The issue provides a clear description of the bug (crash when uploading a file with a null byte in the extension), a reproducible test case (Python script), the error encountered (ValueError: embedded null byte), and the relevant code context (Django view and form). It also hints at a possible solution (escaping or sanitizing the extension). The expected behavior is also stated: the server should not 500, but instead handle the invalid filename gracefully. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15334": {
        "explanation": "The issue describes a specific feature request: enabling prefetch_related to work with Queryset.iterator() in Django. It references the current documentation and explains the motivation for the change, including the technical context (chunking). However, it does not specify the expected API, edge cases, or how the feature should behave in detail (e.g., should prefetching happen per chunk, what about memory usage, etc.). Still, the core requirement is clear: make prefetch_related work with iterator(). There is a sensible interpretation for a solution, but some implementation details would need to be decided by the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15352": {
        "explanation": "The issue clearly states that session IDs should be cleansed from error reporting, similar to how other credentials are handled. It provides context, motivation, and even references an existing patch and related discussions. While it does not specify the exact technical steps, the intent and expected outcome are clear: ensure that session IDs are not included in error reports. As an experienced engineer, I can interpret this as needing to audit error reporting code paths and sanitize session IDs. Therefore, the issue is well-specified enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15368": {
        "explanation": "The issue provides a clear description of the bug, a reproducible example, the observed vs. expected behavior, and even points to the likely cause in the codebase with a suggested fix. The expected behavior is that assigning F('name') to a model field and using bulk_update should result in the SQL using the column name, not the string representation. The issue also suggests two possible solutions for the type check. This is sufficient information for an experienced engineer to attempt a fix and write a meaningful PR.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15375": {
        "explanation": "The issue provides a clear description of the bug: using the 'default' argument in an aggregate function after annotate() causes a SQL syntax error, while the equivalent operation using Coalesce works. The steps to reproduce are detailed, including the Django version, database backends, and code samples. The expected and actual behaviors are both demonstrated. The root cause (malformed SQL) is shown, and the workaround is identified. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15380": {
        "explanation": "The issue provides a clear description of the problem: the migration autodetector crashes when renaming a model and a field in a single step. It includes the exact command used, the user interaction, the full traceback, and even references the specific commit where the regression was introduced. However, it does not provide the exact model definitions before and after the rename, nor does it specify the field being renamed. While the general area of the bug is clear, some details would need to be inferred or reconstructed (e.g., by creating a test case that renames a model and a field simultaneously). Thus, it is possible to make a meaningful attempt at a solution, but some assumptions are required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15382": {
        "explanation": "The issue provides a clear description of the problem: when using a filter with a negated Exists subquery that is empty, the resulting queryset is empty and the WHERE block is missing from the SQL query. It also provides a reproducible code snippet and references a potentially related issue (#33018). However, it does not specify what the expected behavior should be\u2014should the WHERE block be preserved, or should the query return all objects, or something else? There is a sensible interpretation (that the WHERE block should not be removed), but some assumptions are required about the intended fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15388": {
        "explanation": "The issue provides clear reproduction steps, including the exact changes to make in the codebase and the observed behavior (dev server fails to restart when BASE_DIR is added to TEMPLATES[0]['DIRS']). It also describes the expected behavior (dev server should restart on file changes) and how the bug is resolved by removing BASE_DIR. While it does not specify the root cause or the desired fix, it is clear what the problem is and what a successful solution would look like: the dev server should restart as expected even when BASE_DIR is included in TEMPLATES[0]['DIRS'].",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15401": {
        "explanation": "The issue describes a behavioral change in Django regarding filtering with unsaved related objects, references a related ticket, and asks two questions: what should the intended behavior be, and is it documented? However, it does not specify what concrete change is required: Should the code raise an error, warn, or just document the behavior? Should the behavior revert to Django 1.11, or is documentation sufficient? There is no clear, actionable task or acceptance criteria. Thus, while the context is clear, the required solution is ambiguous.",
        "label": "VAGUE"
    },
    "django__django-15413": {
        "explanation": "The issue provides a clear description of the bug: the log message for 'Synchronous middleware ... adapted' is misleading or incorrect when dealing with synchronous middleware. It references the relevant documentation, the current test that is not correctly testing the intended behavior, and the relevant code in the Django codebase. Furthermore, it proposes a concrete fix, including a code diff, to handle middleware and method names separately in the log messages. The expected behavior is well-defined, and the steps to resolve the issue are clear. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15414": {
        "explanation": "The issue provides a clear description of the problem: QuerySet.bulk_update() is not using db_for_write when determining the database connection, because self._for_write is not set to True before accessing self.db. The expected behavior is that bulk_update should use db_for_write and ensure atomic writes on the correct database. The issue also references a specific Django ticket for further context. While the implementation details (e.g., where exactly to set _for_write) are left to the developer, the required outcome is clear: ensure bulk_update uses db_for_write and atomic writes are performed on the correct database. This is a sensible and actionable request.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15421": {
        "explanation": "The issue provides a clear description of the problem: the parallel test runner fails on macOS/Windows due to the change in the multiprocessing start method from 'fork' to 'spawn', leading to specific errors (AppRegistryNotReady, missing test DBs). It also references the relevant Python documentation and describes the root cause (lack of parent memory state in spawned processes). The issue further suggests a solution (run django.setup() and set the db name with the test_ prefix in the worker initializer) and mentions an attached patch, which would provide further guidance. Even without the patch, the requirements for a solution are clear: ensure the parallel test runner works with the 'spawn' method by initializing Django and setting up the test DB correctly in each worker. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15423": {
        "explanation": "The issue clearly describes a problem: LazyObject defines magic methods like __getitem__ and __iter__, which may not exist on the wrapped object, leading to misleading hasattr checks and subsequent TypeErrors. The expected behavior is implied: LazyObject should not expose these methods unless the wrapped object actually supports them. While the issue does not specify the exact implementation approach, it is clear what needs to be fixed and what the desired outcome is. Therefore, a meaningful solution can be attempted.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15433": {
        "explanation": "The issue provides a clear description of the problem (infinite migrations when using a ManyToManyField to a custom user model extending AbstractUser), includes a minimal reproducible example (GitHub repository), and specifies the relevant code and migration output. The expected behavior (no repeated migrations) is implied by the comparison to the built-in user model. While it does not specify the exact fix, it is clear what the problem is and what a successful solution would look like (no more infinite migrations in this scenario).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15438": {
        "explanation": "The issue clearly describes the current behavior (Select2 falls back to English if a regional language variant is not found) and the desired behavior (fallback to the base language, e.g., 'de' if 'de-ch' is not available). It specifies the context (Select2 translations in the autocomplete_field widget) and provides a concrete example. The implementation details are left to the developer, but the requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15467": {
        "explanation": "The issue provides a clear description of the problem (custom empty_label is ignored when using radio_fields in ModelAdmin), a reproducible code example, and a specific code change to fix the problem. The expected behavior and the location in the codebase to change are both specified. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15474": {
        "explanation": "The issue provides a clear description of the problem: when rolling back the contenttypes app to migration 0001 on a non-default database, the legacy name field is not populated as expected. The steps to reproduce are given, and the expected vs. actual behavior is described. While it does not specify the exact fix, it is clear what the bug is and what the correct behavior should be, making it possible to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15481": {
        "explanation": "The issue clearly describes the current behavior (TypeError when passing a timedelta to set_cookie/set_signed_cookie), the desired behavior (accepting a timedelta for max_age, as get_signed_cookie does), and provides concrete examples. The scope of the change is well-defined: update set_cookie and set_signed_cookie to accept timedelta objects for max_age. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15492": {
        "explanation": "The issue provides a clear description of the desired feature: adding a new method, synchronize_user, to the RemoteUserBackend class in Django. It specifies the method's purpose, when it should be called (after fetching the user in authenticate), its method signature (matching configure_user), and its intended use (to allow implementors to synchronize user attributes from a remote system on every authentication). The requirements are concrete and actionable, making it clear what needs to be implemented for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15498": {
        "explanation": "The issue clearly states the problem: previously, an empty string for the If-Modified-Since header was ignored, but after a specific commit (d6aff369ad3), it now raises an exception. The desired behavior is to restore the previous handling (ignore empty string). The commit hash provides a reference for when the behavior changed. This is enough information to attempt a solution: identify where the change occurred, and update the code to ignore empty strings for this header again.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15499": {
        "explanation": "The issue provides a clear description of the desired optimization: when a migration consists of a CreateModel operation followed by an AlterModelManagers operation, these should be combined into a single CreateModel operation that includes the manager changes. The analogy to the existing CreateModel + AlterModelOptions optimization provides a concrete example. While the exact implementation details are left to the developer, the goal and expected outcome are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15503": {
        "explanation": "The issue provides a clear description of the problem: JSONField lookups using has_key (and related lookups) do not work as expected with numeric keys on certain backends (SQLite, MySQL, Oracle), while they do work on PostgreSQL. The issue includes a reproducible test case, the expected and actual results, and specifies the environment and versions. The desired behavior is clear: the lookup should find entries with numeric keys, just as it does with string keys. This is sufficient information to attempt a solution, such as investigating how the lookup is implemented for these backends and adjusting it to handle numeric keys correctly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15521": {
        "explanation": "The issue provides a clear description of the problem: the makemessages command in Django is incorrectly skipping the 'nl_NL-x-informal' folder due to a locale validation check, while DjangoTranslation correctly recognizes it. The issue references the relevant commit that introduced the behavior and specifies the expected behavior (the check for '-' should only apply to the first section of the locale). This gives enough context and direction for a developer to attempt a fix, likely by adjusting the locale validation logic in makemessages.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15525": {
        "explanation": "The issue provides a clear description of the problem: 'loaddata' fails on a non-default database when using natural keys with foreign keys. It includes a reproducible test case (with a link to a GitHub repo), the relevant models, the fixture data, and the full traceback of the error. The expected behavior is also implied: 'loaddata' should work the same on non-default databases as on the default one. While the root cause is not specified, the issue is well-specified for a developer to investigate and attempt a fix, as all necessary context and reproduction steps are present.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15526": {
        "explanation": "The issue clearly describes the bug: the 'view_on_site' redirect in Django's admin does not respect custom admin sites due to a missing 'current_app' argument in a reverse() call. It specifies the file and method involved, the nature of the bug, and the impact. It even provides a link to a proposed patch and mentions the intent to add a regression test. This is sufficient information for an experienced engineer to attempt a solution, even if the patch were not provided.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15554": {
        "explanation": "The issue provides a clear description of the problem: using multiple FilteredRelation aliases on the same relation with different filters results in only one SQL JOIN being generated, which is unexpected. The user provides a code sample, describes the expected behavior (multiple joins, one per alias/filter), and the actual behavior (only one join is generated). The question is whether this is intended or a bug, and implicitly requests a fix if it is not intended. The problem is well-scoped and actionable: ensure that multiple FilteredRelation aliases on the same relation generate separate joins as expected.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15560": {
        "explanation": "The issue clearly describes the problem: when using a UniqueConstraint instead of unique=True on a custom user model's username field, the createsuperuser command does not check for existing usernames and allows duplicates, resulting in an IntegrityError. The expected behavior is implied: createsuperuser should validate the uniqueness of the username before attempting to create the user. The code sample and error message provide enough context to understand and attempt a solution (i.e., add validation for UniqueConstraint in createsuperuser).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15561": {
        "explanation": "The issue provides a clear description of the undesired behavior: when using SQLite, an AlterField operation that only changes 'choices' (which should be a database-transparent change) is generating unnecessary SQL operations, unlike on PostgreSQL. The expected behavior is that such changes should be a no-op on SQLite as well. The issue is specific about the context (AlterField, choices, SQLite), the current behavior, and the desired behavior, making it clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15563": {
        "explanation": "The issue provides a clear description of the problem, including a reproducible example with model definitions and shell commands that demonstrate the incorrect behavior. The expected and actual outcomes are both shown, and the root cause (update affecting the wrong table/instances due to multiple inheritance) is described. While it does not specify the exact desired fix, it is clear that the goal is to ensure that updates on Child queryset should only affect Child instances and not unrelated parent instances. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15569": {
        "explanation": "The issue clearly describes a specific problem: the _unregister_lookup method does not clear the lookup cache, while register_lookup does. It provides direct links to the relevant lines in the codebase, and mentions that a corresponding test may need to be updated. The required change is clear: ensure that _unregister_lookup also clears the cache, and update the relevant test if necessary. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15572": {
        "explanation": "The issue provides a clear description of the problem: autoreload breaks when TEMPLATES['DIRS'] contains an empty string, due to changes in how Django normalizes template directories. It explains the cause, the regression, and references relevant commits and previous reports. However, it does not specify exactly what the expected behavior should be (e.g., should empty strings be ignored, should there be an error, etc.), nor does it specify what kind of fix is desired (e.g., code change, documentation update, or error message). There is a sensible interpretation: the code should handle empty strings in DIRS gracefully, likely by ignoring them, to restore previous autoreload behavior. Thus, while not perfectly specified, it is reasonably clear what needs to be done.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15576": {
        "explanation": "The issue provides a clear description of the problem: an inefficiency in the .exists() method for distinct querysets due to all fields being selected, and a proposed condition for when fields should be left on the queryset. The desired code change is specified (a one-line change to the condition). However, there is some ambiguity regarding the testing approach: the author is unsure how to write a non-brittle test and is asking for suggestions or whether a patch without a test would be acceptable. While the code change itself is well-specified, the requirements for testing are not fully clear. Still, a sensible interpretation would be to implement the code change and, if possible, add a test that inspects the generated SQL or query structure, even if it's somewhat brittle. Therefore, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15586": {
        "explanation": "The issue provides context about enabling the cached template loader by default in development, referencing previous changes and discussions. It is clear that the task is to change the default behavior so that the cached template loader is enabled in development environments, unless explicitly overridden. While it does not specify the exact code location or configuration file, the intent and expected outcome are clear for someone familiar with the codebase and Django's template loader system.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15607": {
        "explanation": "The issue provides a clear description of the problem (an unnecessary redirect in LogoutView when an unsafe ?next=... parameter is provided), detailed reproduction steps, expected and actual results, and even points to the relevant code and historical context. The desired outcome is explicit: remove the unnecessary redirect so that when an unsafe 'next' parameter is provided, the user is redirected directly to LOGOUT_REDIRECT_URL without an extra round-trip. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15620": {
        "explanation": "The issue clearly describes the current behavior (no indication in SQL output when a migration operation is a no-op), the problem (confusion about which operations are no-ops), and the desired improvement (output an extra SQL comment such as '-- (no-op)' or similar when a migration operation is a no-op). It provides concrete examples of current and desired output, and the intent is unambiguous. The only minor ambiguity is the exact wording of the comment, but the issue suggests alternatives and leaves room for reasonable developer judgment. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15629": {
        "explanation": "The issue provides a clear description of the problem: when using db_collation on a primary key that is referenced by foreign keys in other models, the generated SQL does not propagate the collation to the foreign key columns, causing MySQL to fail when adding the foreign key constraints. The issue includes example models, the SQL generated, the error, and the correct SQL needed. The required solution is to ensure that the collation specified on the primary key is also applied to the referencing foreign key columns. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15643": {
        "explanation": "The issue describes a problem where, during migrations on SQLite, altering a primary key causes the explicit primary key defined on a model to be dropped and replaced with an implicit one. However, it lacks critical details: it does not specify the ORM or migration tool in use, does not provide example model definitions, migration steps, or expected vs. actual behavior. Without this context, it is unclear what the correct behavior should be or how to reproduce the issue. Therefore, the issue is too vague for a meaningful solution attempt.",
        "label": "VAGUE"
    },
    "django__django-15648": {
        "explanation": "The issue provides a clear description of the problem: decorating a method called by __get_dynamic_attr in views.Feed leads to a TypeError due to argument counting logic that doesn't work with decorators using *args, **kwargs. The relevant code snippet is provided, and the error is explained. The reporter also suggests a possible solution: standardizing on either attr(item) or attr(), not both. While there is some room for interpretation in the exact implementation, the problem and a direction for the solution are clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15651": {
        "explanation": "The issue provides a clear description of the problem (handling unnamed indexes for backwards compatibility in the context of deprecating index_together), and outlines the requirements for the RenameIndex operation, including its signature, behavior depending on parameters, handling of different database backends, error handling, and behavior in migration autodetection and backwards operations. While there are some implementation details left to the developer (e.g., exact SQL for different DBs), the requirements are specific enough to guide a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15669": {
        "explanation": "The issue provides a clear feature request: implement a --update flag for the makemigrations management command in Django, mimicking the behavior of South's schemamigration --update. It references South's documentation and describes the desired behavior (refining the latest migration for iterative development). While it doesn't specify every implementation detail, the intent and expected outcome are clear enough for an experienced Django developer to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15671": {
        "explanation": "The issue clearly describes the desired change: moving the error messages for 'too few' and 'too many' forms in a FormSet to default_error_messages, so that they can be overridden via the error_messages argument when instantiating the FormSet. The scope is well-defined, and the expected outcome is clear. There is a note about a preferred future enhancement (overriding via inlineformset_factory), but that is explicitly out of scope for this issue. Therefore, the requirements for a successful solution are clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15678": {
        "explanation": "The issue clearly states that the CryptPasswordHasher should be deprecated in Django 4.1 and removed in Django 5.0. It provides context for why this is necessary (the underlying crypt module is deprecated, it's not recommended, and is UNIX-only). The steps for a successful solution are clear: mark the hasher as deprecated in the appropriate version, update documentation, and prepare for its removal. No significant ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15682": {
        "explanation": "The issue provides a clear description of the current behavior, including code examples and the observed output. It also explains why the current behavior is problematic and proposes a specific change: deprecate passing False to nulls_first and nulls_last, and instead have them default to None, with the DB deciding when both are None. The desired behavior is well-articulated, and the steps to achieve it (deprecate False, default to None, update logic) are clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15689": {
        "explanation": "The issue provides a detailed description of a performance regression caused by a specific commit, including historical context, the nature of the regression, and empirical evidence (benchmarks) demonstrating the impact. However, it does not explicitly state what the desired outcome or fix should be. For example, should the middleware revert to the old behavior, or is a new approach desired? Should there be a configuration option? The issue is clear about the problem, but not about the expected solution, leaving some room for interpretation. Therefore, it is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15695": {
        "explanation": "The issue provides a clear description of the bug: RenameIndex() crashes when an unnamed index is moved backward and then forward, specifically when the index is auto-generated (as with unique_together). It specifies the expected behavior (restoring the old auto-generated name when moving backward), the current behavior (crash due to name conflict), and provides a code snippet and the error message. The context (test file, operation, and error) is sufficient to attempt a fix, such as ensuring the old name is restored or handling the name conflict. No major ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15698": {
        "explanation": "The issue provides a clear description of the problem: a template that previously rendered empty output when accessing a method on a non-existent variable now raises an exception after a Django upgrade. The error trace, expected behavior, and a proposed code fix are all included. The context, steps to reproduce, and the desired outcome are all well-detailed, making it clear what needs to be fixed and how success can be measured (restoring the old behavior of rendering empty output instead of crashing).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15703": {
        "explanation": "The issue clearly states the goal: deprecate Model.Meta.index_together in favor of Model.Meta.indexes. It also provides the rationale, noting that everything index_together does can be achieved with indexes and the Index class. While it does not provide step-by-step instructions, it is clear what needs to be done: update the codebase to deprecate index_together, update documentation, and possibly add warnings or migration notes. This is a common type of deprecation task in software projects and is well-understood by experienced developers.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15731": {
        "explanation": "The issue provides a clear description of the problem: inspect.signature returns an incorrect signature for manager/queryset methods in Django, specifically showing (*args, **kwargs) instead of the expected method signature. It identifies the root cause (manual copying of metadata instead of using functools.wraps) and points to the relevant code location. The proposed solution is also specified: replace manual assignment of __name__ and __doc__ with functools.wraps. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15732": {
        "explanation": "The issue provides a clear description of the problem: a unique_together constraint on a single field (which is also a primary key and has unique=True) cannot be dropped via migration because Django's migration logic expects only one unique constraint, but there are two. The database is specified (PostgreSQL), and the relevant schema details are given. The expected outcome is to be able to drop the erroneous unique_together constraint. While the issue does not specify the exact desired migration code or the expected state after the fix, it is clear what the problem is and what needs to be resolved. There is a sensible interpretation for a solution: allow the migration to drop the redundant unique constraint even when another (the primary key) exists.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15737": {
        "explanation": "The issue provides a clear description of the problem (unnecessary clearing of a cached reference leading to unwanted database queries), a concrete example, the context in which it occurs, and a specific suggestion for a code change to address it. It also references a related closed ticket for background. While the suggested code change is untested, the intent and the area of the code to modify are clear. There is enough information to attempt a solution and validate it by reproducing the described behavior and testing the proposed fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15738": {
        "explanation": "The issue provides a detailed description of the models involved, the migration steps attempted, the error encountered, and a workaround. However, it does not clearly specify what the desired outcome is for the PR: Is the goal to update documentation, to provide a code-based migration strategy, to fix a bug in Django, or to automate the multi-step migration? There is ambiguity about what a 'successful solution' would look like, as the issue is more of a report and discussion than a clear request for a code change or feature.",
        "label": "VAGUE"
    },
    "django__django-15741": {
        "explanation": "The issue clearly describes a regression caused by a specific commit, identifies the function involved (django.utils.formats.get_format), and provides a concrete example of the failure (using a lazy string in a template filter). The error message is included, and the desired outcome is implied: get_format should accept lazy strings as the parameter. While it doesn't specify the exact implementation, it is clear what needs to be fixed and how to reproduce the problem. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15742": {
        "explanation": "The issue provides a clear description of the problem: when using Django's blocktranslate with asvar, the resulting variable is double-escaped due to being stored as a plain string rather than a SafeString. The issue includes a minimal reproducible example, a reference to the documentation, and two possible solutions (assign SafeString or avoid escaping in the first place). The expected behavior is clear: prevent double-escaping when using blocktranslate asvar. This is sufficient information for a developer to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15744": {
        "explanation": "The issue provides a clear description of the problem: the ASGI handler is closing the request body prematurely, which leads to I/O operations on a closed file when accessing request.body in a view. It references a specific PR that introduced the problem, making it possible to identify the relevant code. The expected behavior is implied: the request body should not be closed before the response is generated. This is sufficient information for an experienced engineer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15747": {
        "explanation": "The issue provides a clear description of the problem: performing a queryset update in the Django admin raises a FieldError when the queryset is ordered by an annotated field. The user provides a code sample, describes the error message, and explains the context in which it occurs. However, the issue is primarily a bug report rather than a request for a specific code change or feature. While it is clear what the undesired behavior is, it is not entirely clear what the expected fix should be (e.g., should the code automatically remove ordering by annotated fields before update, or should it raise a more informative error, etc.). There is enough information to attempt a solution (such as stripping annotated field ordering before update), but some implementation choices are left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15752": {
        "explanation": "The issue clearly states that the function make_random_password() should be deprecated, and provides a rationale (it's unused since a specific commit). It also references a comment by a maintainer for further context. The task is straightforward: mark the function as deprecated, which is a common and well-understood process in software engineering. No further clarification is needed to proceed with the deprecation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15766": {
        "explanation": "The issue provides a clear description of the problem (on_commit handlers failing to execute if a previous handler raises an exception), the desired behavior (ability to mark some handlers as 'robust' so their exceptions are swallowed, similar to Django's send_robust for signals), and a proposed solution (add a 'robust' kwarg to on_commit). It also gives concrete examples of intended usage and specifies backward compatibility. This is sufficient for a meaningful implementation attempt.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15774": {
        "explanation": "The issue provides a clear description of the problem: Django currently parses the Accept-Language HTTP header in a case-sensitive manner, which is contrary to relevant standards (RFC2616, BCP47) that specify case-insensitive parsing. It gives concrete examples of the problem, identifies a specific function (parse_accept_lang_header()) as a likely place to fix, and also requests a documentation update to correct an inaccurate statement. The requirements for a successful solution are clear: (1) update code to parse language tags case-insensitively, and (2) update documentation to reflect the correct behavior and examples. There is little ambiguity about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15781": {
        "explanation": "The issue clearly describes the current behavior (help output formatting), the code that produces it, and the expected output (with improved formatting, specifically newlines and indentation in the help text). The problem is well-scoped: the help output for a management command should preserve the formatting (newlines and indentation) from the help string. The solution would involve ensuring that the help text is rendered as intended. No major ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15799": {
        "explanation": "The issue provides a clear description of the problem: the help text logic in ModelAdminForm does not check the allow_multiple_selected attribute of the SelectMultiple widget, leading to incorrect help text being displayed when allow_multiple_selected is False. It references specific code locations and describes the expected vs. actual behavior. While it doesn't specify the exact desired help text or the precise fix, it is clear that the solution should involve updating the help text logic to also check allow_multiple_selected, mirroring the widget's own logic. This is a sensible and actionable interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15814": {
        "explanation": "The issue provides a clear description of the problem, including the environment, the error traceback, the relevant models, the command that triggers the error, and even a suggested code change that resolves the issue. The expected behavior is implied: the query should not crash when using select_related() and only() on a proxy model. The root cause is identified, and a specific fix is proposed. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15819": {
        "explanation": "The issue describes a concrete problem: when using Django's inspectdb to generate models from a database, if there are multiple foreign keys to the same model, the generated code causes reverse accessor clashes. The error message and hint are provided, and the suggestion is to autogenerate a related_name for such fields. While the exact naming scheme for related_name is not fully specified, the suggestion to use the attribute name is clear enough to implement a reasonable solution. The intent and the desired outcome are clear, though some implementation details are left to the developer's discretion.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15828": {
        "explanation": "The issue identifies a specific problem: the deconstruct() and __eq__ methods of BaseConstraint do not consider the violation_error_message attribute. It also references a regression and provides a commit hash, which can be used to investigate the code changes that introduced the issue. While it does not specify the exact desired behavior, it is clear that the solution should ensure that violation_error_message is properly handled in both methods. This is a reasonably clear and actionable issue, though some implementation details are left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15851": {
        "explanation": "The issue clearly describes the problem: when using the dbshell command with additional parameters, those parameters are being passed after the database name, which is not compatible with how psql expects its arguments. The expected behavior is also described: all options should precede the database name. The solution is implied (reorder the arguments so that the database name is last), and the context (PostgreSQL, dbshell, psql) is clear. This is enough information for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15863": {
        "explanation": "The issue provides a clear description of the problem: the floatformat template filter in Django drops precision when used with Decimal numbers because it converts them to float, which cannot represent all decimal digits accurately. The issue includes a minimal working example (MWE) that demonstrates the problem, traces the cause to a specific implementation detail, and even references a prepared pull request. The expected behavior is implied: floatformat should preserve the precision of Decimal numbers. This is sufficient information for a developer to attempt a fix or review the proposed solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15869": {
        "explanation": "The issue provides a clear description of the motivation for deprecating the 'length_is' template filter in favor of using the 'length' filter with comparison operators in templates. It explains the historical context, the current usage, and the preferred alternative. However, it does not specify the exact steps required for deprecation (e.g., should the filter be removed immediately, should a deprecation warning be added, should documentation be updated, etc.). While the intent is clear, some implementation details are left to the developer's discretion, but a sensible interpretation is possible based on standard deprecation practices.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15902": {
        "explanation": "The issue describes a deprecation warning related to 'default.html' being raised when rendering ManagementForm's, even though the user never renders forms with {{ form }}. The user is asking whether it's worth special-casing this scenario to avoid the warning for management forms. However, the issue does not specify what the desired behavior is, what the acceptance criteria are, or what the expected solution should look like. It is more of a question or discussion prompt than a clear request for a code change. Therefore, it is too vague to attempt a solution without further clarification.",
        "label": "VAGUE"
    },
    "django__django-15916": {
        "explanation": "The issue provides a clear description of the current and expected behavior regarding how 'formfield_callback' should be handled when specified in the Meta class of a ModelForm. It includes a code example, describes the problem, and references previous related tickets. The desired outcome is explicit: modelform_factory should respect the formfield_callback specified in the Meta of the provided form class. This is sufficient information to attempt a solution, such as modifying modelform_factory to check for formfield_callback in the Meta class if not provided as an argument.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15925": {
        "explanation": "The issue provides a clear description of the problem, including the context (Django 4.1, SQLite 3.35.5+), the steps to reproduce, and the specific error encountered. It also details the minimal example and the types of fields affected. While it does not specify the exact desired fix, it is clear that the goal is to prevent the migration from crashing when removing an indexed field. This is sufficient for a meaningful attempt at a solution, as the expected behavior is implied: migrations should not fail in this scenario.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15930": {
        "explanation": "The issue provides a clear code sample that triggers the bug, the exact error message, the generated SQL, and the expected behavior. It also explains the context in which this pattern (~Q(pk__in=[])) arises. The problem is well-scoped: ensure that using ~Q(pk__in=[]) in a Case/When clause does not generate invalid SQL and instead produces the expected annotation. This is sufficient information for a developer to reproduce, diagnose, and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15957": {
        "explanation": "The issue clearly describes a problem: using Prefetch with a sliced queryset in Django raises an AssertionError, and this limitation is not documented. The user provides a code example, explains the use case, and references external discussion. However, the issue does not specify whether the desired outcome is to fix the underlying limitation in Django (which may be non-trivial and require design discussion), to improve documentation, or to provide a workaround. There is a sensible interpretation (e.g., document the limitation or raise a more helpful error), but the exact scope of the solution is not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15969": {
        "explanation": "The issue describes a performance problem with Django's on_delete=models.SET_NULL behavior, specifically that the generated SQL updates children by their IDs rather than by parent_id, leading to slow queries on large tables. However, the issue is more of a question or request for explanation about why Django behaves this way, rather than a request for a code change or a specific feature/fix. There is no clear acceptance criteria or actionable task for a PR, so it is not well-specified enough for a solution attempt.",
        "label": "VAGUE"
    },
    "django__django-15973": {
        "explanation": "The issue provides a clear description of the problem: defining a ManyToManyField with a 'through' model located in another app causes a specific AttributeError during migration. The models involved, their locations, and the error trace are all provided. The user also describes the workaround (putting all models in the same file works), which helps clarify the scope. The expected behavior is implied: the migration should work even when the 'through' model is in a different app. While the root cause is not given, the issue is well-specified for a developer to attempt a solution, such as fixing import paths, app registry usage, or migration dependencies.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15987": {
        "explanation": "The issue clearly describes a specific bug: when FIXTURE_DIRS contains Path instances, the duplicate check in loaddata does not detect duplicates. The context is likely Django or a similar framework, and the expected behavior is that duplicates should be detected regardless of whether the entries are strings or Path objects. The solution would involve ensuring that the duplicate check works for both types. The description is clear about the problem and the expected outcome, making it well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15993": {
        "explanation": "The issue describes a specific scenario: when a RenameModel operation is performed and the model already has db_table defined, the operation should be a no-op (i.e., it should not perform any changes). It also describes the current (undesired) behavior in both Postgres and SQLite. While it does not specify the exact code location or framework, it is clear what the desired behavior is and what is currently happening. An experienced engineer familiar with migration frameworks (such as Django) would be able to interpret and implement the required change.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15996": {
        "explanation": "The issue clearly describes a problem: when serializing a combination of Enum flags (such as re.UNICODE | re.IGNORECASE), the current EnumSerializer attempts to use the .name attribute, which fails if the value is a combination not represented by a single Enum member. The issue suggests a solution: use enum._decompose to obtain the list of names and generate an expression that 'ORs' the items together. The expected behavior and a high-level solution are both described, making it clear what needs to be changed in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16002": {
        "explanation": "The issue provides a clear description of the problem, including the environment (Python 3.10, Django 4.1, sqlite3), the model definition, the steps to reproduce (including code snippets), and the traceback of the error encountered. It also explains the root cause: 'nan' is accepted by DecimalField and stored, but cannot be retrieved due to a TypeError. The expected behavior is implied: either prevent 'nan' from being stored or handle it gracefully on retrieval. While it doesn't specify the exact desired fix (e.g., validation error, conversion, or special handling), it is clear what the bug is and what needs to be addressed. Therefore, it is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16032": {
        "explanation": "The issue provides a clear description of the bug, a reproducible test case, and the exact error message encountered. It specifies the context (Django ORM, use of annotate and alias, and filtering with __in), and what the expected behavior is (the test should pass, returning a list of publisher names). The error message also gives a strong hint about the nature of the bug (subquery returns too many columns). This is sufficient information for an experienced developer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16037": {
        "explanation": "The issue provides a clear description of the problem: after upgrading Django from 4.0 to 4.1.1, a QuerySet.annotate() call with Count('liked_by') now raises a database error in both SQLite and PostgreSQL. The error messages and stack traces are included, and the relevant code (models and manager methods) is provided. The issue is reproducible and the expected behavior (it worked in 4.0.7) is stated. While the root cause is not identified, the problem is well-scoped: fix the annotate(likes=Count('liked_by')) so it works as before. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16041": {
        "explanation": "The issue provides a clear description of the problem (KeyError when passing 'empty_permitted' in form_kwargs for a formset's empty_form), the expected behavior (empty_permitted should be ignored for empty_form), and detailed steps to reproduce the issue, including code snippets for views, URLs, and templates. The desired outcome is also specified. This is sufficient information to attempt a meaningful solution, such as modifying the formset or form handling code to ignore 'empty_permitted' for empty_form.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16046": {
        "explanation": "The issue clearly describes the error (IndexError: string index out of range) and the context (when str_number[0] == '-' is called on a null/empty value in numberformat.py, particularly when formatting for the admin list_display). It is clear that the solution should prevent this error by handling null or empty values before accessing str_number[0]. The expected behavior is implied: the code should not throw an IndexError in this case. While the exact fix is not specified, the problem and its context are clear enough for an experienced engineer to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16053": {
        "explanation": "The issue clearly describes the problem: the __str__ methods for ContentType and Permission mix localized and non-localized strings, resulting in inconsistent representations. It also suggests two possible solutions: either fully localize the string or not localize any part of it. While it does not prescribe exactly which approach to take, it is clear that the goal is to make the string representation consistent in terms of localization. As an experienced engineer, I can make a sensible decision (e.g., use only localized or only non-localized labels) and justify it in the PR. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16067": {
        "explanation": "The issue provides a clear description of the problem: DecimalValidator in Django 2.0.13 fails to validate decimals in scientific notation like '0E+1', which worked in 1.11.29. It includes code samples, a reproducible test case, and even points to the commit that introduced the change. The expected behavior is also clear: '0E+1' should be considered a valid decimal. The issue also suggests a workaround (calling normalize), but requests a proper fix. This is sufficient information to attempt a solution, such as updating the validator to handle this case.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16076": {
        "explanation": "The issue provides a clear description of the problem, including a minimal code example, the error message encountered, and the desired behavior (being able to use lookups like __startswith on the ForeignKey's raw field without forcing a join). The context, motivation, and constraints (cannot change schema, joins are too costly) are all specified. The expected outcome is clear: support for registering lookups on relation fields so that queries like parent_object_id__startswith work as expected. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16082": {
        "explanation": "The issue clearly describes a specific problem: when combining numeric expressions with the MOD operator in Django, the output_field is not resolved to Decimal when mixing Decimal and Integer types, unlike other mathematical operators. The expected behavior is implied (consistency with other operators), and the area of code to address (expression output_field resolution for MOD) is clear. While it doesn't specify exact test cases or the precise location in the code, it is sufficiently detailed for an experienced engineer to attempt a solution by following the pattern used for other operators.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16092": {
        "explanation": "The issue provides a clear objective: to add a 'db_default' attribute to Field for defining database defaults. It references a specific pull request and provides a code diff to apply, including the file and lines to modify. The requirement that it should work on any database is also stated. While there are some minor ambiguities (e.g., how to handle different data types or quoting), the intent and the main steps are clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16100": {
        "explanation": "The issue clearly states that the changelist_view in Django admin, specifically when using list_editable, should be wrapped in a transaction to prevent inconsistent database states if errors occur. The desired outcome is to ensure that all database changes made during this view are atomic. While it doesn't specify the exact implementation details (e.g., which transaction API to use), it is clear what needs to be achieved and where. This is a common pattern in Django and the solution is straightforward for an experienced developer.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16116": {
        "explanation": "The issue describes an inconsistency in the behavior of the 'makemigrations --check' command compared to similar commands like 'migrate --check' and 'optimizemigration --check'. It suggests that 'makemigrations --check' should not create migrations, but simply exit after checking, aligning its behavior with the other commands. The issue provides context, the current behavior, the desired behavior, and references to related discussions. While it does not specify exact implementation details, the intent and expected outcome are clear enough for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16117": {
        "explanation": "The issue provides a clear description of the problem: Django's migration file generation can produce filenames with dots ('.') if the constraint name contains dots, which causes Django to not detect the migration file. The issue also describes the steps to reproduce, the root cause, and three possible solutions, with a preference for the third (modifying Django's migration file naming logic to sanitize constraint names). The expected outcome is clear: migration files should be generated with valid filenames that Django can detect. This is sufficient information to attempt a solution in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16120": {
        "explanation": "The issue describes that when running 'migrate --check', Django emits pre_migrate and post_migrate signals even if the database is already up to date, and that the logs look similar to a real migration run, which can be confusing. However, the issue does not specify what the desired behavior should be: Should signals not be emitted at all for --check? Should the logs be different? Should both be changed? There is ambiguity about what the correct fix is, as the issue only describes the current behavior and its side effects, not the intended outcome. Therefore, while the problem is described, the solution is not clearly specified.",
        "label": "VAGUE"
    },
    "django__django-16136": {
        "explanation": "The issue provides a clear description of the problem, including the error message, stack trace, environment details (Django version, Python version), and a minimal reproducible example. It is clear that the expected behavior is for a GET request to an async-only View to return a proper 405 Method Not Allowed response, not a 500 error. The root cause is also indicated: HttpResponseNotAllowed is being awaited, which is not correct. The issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16139": {
        "explanation": "The issue provides a clear description of the problem (broken link to PasswordResetForm when accessing UserAdmin via to_field), the context (using to_field=\"uuid\"), the cause (hardcoded relative URL in help_text), and a proposed solution (change the URL construction to use the instance's pk). It even includes the relevant code snippet and a suggested fix. This is sufficient information for a developer to understand the problem and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16142": {
        "explanation": "The issue provides a clear description of the current behavior (get_language_from_request falls back to settings.LANGUAGE_CODE), the problem this causes (middleware cannot distinguish between a language from the request and the default), and a proposed solution (get_language_from_request should return None if no language is found in the request). The rationale is well-explained, and the expected change is specific. There are no major ambiguities about what needs to be changed or why.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16143": {
        "explanation": "The issue clearly describes a pain point: the TransactionManagementError in Django does not provide enough context about why 'needs_rollback' was set, making debugging difficult. The user suggests that transactions should track the reason for 'needs_rollback' and include that in the error message. While the issue does not specify the exact implementation, it is clear what the desired outcome is: enhance the error reporting to include the cause of the rollback. This is a reasonable and actionable request, though some design decisions (e.g., how to track and report the cause) are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16145": {
        "explanation": "The issue clearly describes the problem: when running 'python manage.py runserver 0:8000', the output says 'Starting development server at http://0:8000/' instead of 'http://0.0.0.0:8000/'. The expected behavior is specified, and the rationale (consistency with documentation and usability) is given. The required change is to ensure that the output normalizes '0' to '0.0.0.0' in the displayed URL, matching the behavior and documentation. This is a well-specified issue with a clear, actionable solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16208": {
        "explanation": "The issue clearly describes the current logging behavior (BEGIN is logged, but COMMIT and ROLLBACK are not), the problem (lack of visibility into transaction commits and rollbacks), and a proposed solution (log commits and rollbacks as well). It is clear what needs to be done: add logging for transaction commit and rollback events in the same way as BEGIN is currently logged. The context (Django, django.db.backends logging) is also provided, making it straightforward to identify where to make the changes.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16229": {
        "explanation": "The issue provides a clear description of the bug, including steps to reproduce, code samples, and a suggested workaround. It identifies the problem (validation bypass and field reset on resubmission), the context (ModelForm, ArrayField with callable default, Django admin inlines), and even points to a likely cause (hidden input with initial value). The workaround (setting show_hidden_initial=False) gives a strong hint at a possible solution. While it doesn't specify the exact desired fix, it is clear what behavior is broken and what is expected (validation should not be bypassed, and the field should not be reset). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16254": {
        "explanation": "The issue provides a clear description of the observed regression: adding a ManyToManyField on SQLite in Django 4.1 causes a table rebuild, which did not happen in Django 4.0. It includes detailed reproduction steps, example code, the SQL generated in both versions, and a suspected cause (a specific commit and code change). The expected behavior is implied: adding a ManyToManyField should not trigger a table rebuild on SQLite. While the issue does not specify the exact code change required, it is clear what the problem is, how to reproduce it, and what the correct behavior should be. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16255": {
        "explanation": "The issue provides a clear description of the problem (a ValueError is raised when calling max() on an empty sequence in get_latest_lastmod when there are no items), includes a traceback, and even suggests a concrete code change to handle the exception. The expected behavior (return None instead of raising an error) is implied by the suggested fix. This is sufficient information to implement and test a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16256": {
        "explanation": "The issue provides a clear description of the problem: async methods (acreate, aget_or_create, aupdate_or_create) were unintentionally added to related managers and do not behave as intended\u2014they call the QuerySet methods instead of the related manager's methods. The issue also points to the relevant code locations and suggests that proper versions of these methods should be implemented for related managers, covering both forward and reverse directions. While some implementation details (such as exact method signatures or edge cases) may need to be inferred from the codebase, the core requirement is clear: implement correct async versions of these methods on related managers. This is a sensible and actionable engineering task.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16260": {
        "explanation": "The issue provides a clear description of the problem: when calling refresh_from_db() on a model instance with a GenericForeignKey, the cached value of the generic relation is not cleared, leading to stale data and test failures. The expected behavior is that after refresh_from_db(), the cached generic relation should be cleared so that subsequent accesses reflect the refreshed state. The issue includes code samples, a failing test case, and an explanation of the underlying cause. It is clear what needs to be fixed: ensure that refresh_from_db() also clears cached GenericForeignKey values. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16263": {
        "explanation": "The issue clearly describes the current behavior (annotations are included in count queries even when not used), provides a concrete example, and suggests the desired behavior (Django should strip unused annotations from count queries). It also references similar existing behavior (select_related is ignored in count queries), which helps clarify the intent. The scope is clear: modify queryset.count() to ignore unused annotations. There may be some implementation details to decide, but the goal and expected outcome are well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16281": {
        "explanation": "The issue provides a clear, step-by-step reproduction scenario, including the initial model definitions, the migration steps, the change to the model, and the resulting error. The expected behavior is implied: the migration should correctly handle changing a ManyToManyField from referencing another model to referencing 'self', and the error should not occur. While it does not specify the exact desired schema or migration output, it is clear what the problem is and what a successful solution would look like (no OperationalError, migration works as expected).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16302": {
        "explanation": "The issue provides a clear description of the current behavior (CharField requires a positive integer max_length), the desired behavior (CharField should allow max_length=None to indicate unlimited length), and points to the relevant code section that enforces the restriction. It also mentions a related issue with FileField and the SQL generated. The expected change is well-motivated and the technical context is provided, making it clear what needs to be changed and why. There is some room for interpretation regarding the exact SQL to generate and how to handle migrations, but the core requirement is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16306": {
        "explanation": "The issue provides a clear description of the inefficiency in the BaseForm.__getitem__() method, points to the relevant code section, and suggests a concrete improvement: return self._bound_fields_cache[name] at the beginning and handle KeyError, instead of the current sequence of checks. The expected outcome is clear (refactor for efficiency), and the scope is well-defined. No major ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16311": {
        "explanation": "The issue provides a clear description of the BREACH attack, references to prior mitigations, and a specific new mitigation technique ('Heal The Breach') with a link to the relevant paper. The request is for Django to implement this mitigation by injecting random bytes in the gzip filename field, as described in the paper, with sensible defaults. While some implementation details (e.g., exact configuration options, how to expose toggles to users) are not specified, the core requirement is clear and actionable for an experienced engineer familiar with Django's middleware and response compression. Thus, the issue is reasonably specified, though not exhaustively detailed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16315": {
        "explanation": "The issue provides a clear description of the problem: when using bulk_create with update_conflicts, update_fields, and unique_fields, the generated SQL for ON CONFLICT uses the model's field names instead of the db_column names, causing a syntax error in PostgreSQL. The example includes the model definition, the method call, the incorrect SQL, the error message, and the expected correct SQL. This is sufficient information to identify the bug and implement a fix that ensures db_column names are used in the ON CONFLICT clause. No further clarification is needed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16322": {
        "explanation": "The issue is well-described: it provides a concrete example of code that fails, the error message, the difference in behavior between SQLite and MySQL, and the underlying SQL generated. The user also explains the root cause (name collision between annotate and aggregate), and suggests that either the ORM should handle this case or at least warn the user. While the issue does not prescribe a specific fix, it is clear what the problem is and what a successful solution would look like: Django should either prevent or correctly handle the case where annotate and aggregate use the same keyword, or warn the user. This is enough information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16333": {
        "explanation": "The issue clearly describes the problem: UserCreationForm does not save ManyToMany fields because it omits calling self.save_m2m(), unlike its parent ModelForm. The expected behavior is implied (that ManyToMany data should be saved when using UserCreationForm with commit=True), and the technical cause is identified. The solution is straightforward: ensure that save_m2m() is called appropriately. There is enough detail to implement and test a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16343": {
        "explanation": "The issue clearly describes the problem (accidental misuse of positional arguments in Signer leading to a security vulnerability), provides a concrete example, and proposes a specific solution (making all arguments after self keyword-only by adding a * in the __init__ signature). The expected change is unambiguous and the rationale is well-explained. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16379": {
        "explanation": "The issue clearly describes a race condition in the FileBasedCache's has_key method, including a code snippet and a stack trace. It explains the problem (file may be deleted between exists() and open()), and even points out that _is_expired() can delete the file. The expected direction for a fix is clear: make has_key robust against this race condition. While it doesn't prescribe a specific solution, the problem and its context are well-defined, making it possible to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16398": {
        "explanation": "The issue provides a specific context (QuerySet.select_related() with multiple filtered relations to OneToOneField), references a relevant commit and a line in the codebase, and gives a concrete suggestion ('should use partial, just like the remote_setter'). While it doesn't provide a full reproduction case or test scenario, it is clear what code needs to be changed and the intended direction of the fix. An experienced engineer could reasonably interpret and attempt a solution based on this information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16408": {
        "explanation": "The issue provides a concrete test case, including the code to add, the expected behavior (the assertion), and the actual failure output. It clearly identifies the problem: when using a multi-level FilteredRelation with select_related(), the wrong related object is set. The test case is specific and reproducible, and the failure message shows exactly what is going wrong. This is sufficient information for a developer familiar with the codebase to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16411": {
        "explanation": "The issue describes a desired feature: exposing a 'manifest_hash' property on ManifestFilesMixin (and thus ManifestStaticFilesStorage) that changes whenever the manifest changes. The use case is clear (enabling clients to detect asset changes via a header). However, it does not specify the exact API (property, method, etc.), the hash algorithm, or how/where it should be exposed. Still, the intent and the core requirement are clear enough for an experienced engineer to make a sensible implementation choice and open a PR for discussion, as the author suggests.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16429": {
        "explanation": "The issue provides a clear description of the bug (TypeError when calling timesince() with USE_TZ=True and a datetime more than one month in the past), the conditions under which it occurs, a traceback, a reproducible test case, a pointer to the relevant code, and a suggested fix. This is sufficient information to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16454": {
        "explanation": "The issue provides a clear description of the problem: subparsers created via CommandParser.add_subparsers().add_parser() in Django management commands do not inherit the error formatting improvements of the parent parser, resulting in stack traces instead of user-friendly error messages. It includes a concrete code example, the observed behavior, and the desired behavior. The solution direction is also suggested: ensure that the subparser action copies the relevant arguments to constructed subparsers. This is sufficient information for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16485": {
        "explanation": "The issue provides a clear description of the problem: calling the floatformat filter with the value '0.00' (as a string or Decimal) and precision 0 causes a ValueError. The error message is included, and the exact code to reproduce the issue is given. The expected behavior is implied: floatformat should not crash and should handle this input gracefully. While it doesn't specify the exact output expected, it is reasonable to infer that the function should return a formatted string without raising an exception. This is enough information for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16493": {
        "explanation": "The issue provides a clear description of the problem: when a callable is used as the storage argument for a FileField, and that callable returns default_storage, the deconstruction logic omits the storage kwarg in migrations, leading to inconsistent migration files. The example code and the explanation of the root cause (the deconstruct method's logic) make it clear what needs to be fixed: the deconstruction should reference the callable, not the result of calling it. The expected behavior is also described. This is sufficient for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16502": {
        "explanation": "The issue provides a clear description of the problem (runserver returns a response body for HTTP HEAD requests, which is non-compliant with RFC 2616), the context (change in behavior after #26052), steps to reproduce, expected and actual results, and relevant version information. It is clear what needs to be fixed: ensure that runserver does not return a response body for HEAD requests. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16511": {
        "explanation": "The issue provides a clear description of the desired feature: extending Django's update_or_create to allow specifying different defaults for create and update operations. It includes a concrete example, discusses possible API designs, and highlights a key design decision (parameter naming and backwards compatibility). However, the issue is not a direct request for implementation, but rather seeks guidance on which approach to take for the API. The actual implementation details are secondary to resolving the design question. Therefore, while the technical goal is clear, the issue is not actionable as a coding task until the API decision is made.",
        "label": "VAGUE"
    },
    "django__django-16514": {
        "explanation": "The issue clearly describes the current behavior (all AdminSites show all LogEntries) and the desired behavior (each AdminSite should only show LogEntries for models registered with that site). The context and motivation are provided, and the expected outcome is unambiguous. There are no major ambiguities about what needs to be changed, though implementation details (such as how to determine which models are registered to which AdminSite) are left to the implementer, which is reasonable. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16517": {
        "explanation": "The issue provides a clear description of the problem: using mixed-case view/template names in the :view:/:template: directive in Django admin docs leads to a 404 error, and the URL generated is lowercased. It references a specific file and line in the Django codebase, gives a concrete example, and points to a possible area of the code where the problem might originate. While the root cause is not fully diagnosed, the expected behavior (the link should work for mixed-case names) and the observed behavior (404 due to lowercasing) are clear. This is enough information for an experienced engineer to investigate and attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16527": {
        "explanation": "The issue provides a clear context: in the Django admin, the 'show_save_as_new' functionality should also check for 'has_add_permission' before being enabled. The file and line number are specified, and the suggested code change is provided. The rationale is explained: 'save_as_new' is an 'add' operation, so the permission should be checked. This is enough information to make a meaningful code change and create a PR.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16560": {
        "explanation": "The issue clearly describes the current limitation (cannot customize the 'code' attribute of ValidationError in BaseConstraint.validate), the desired enhancement (add a way to customize the error code, similar to how violation_error_message is handled), and the motivation (consistency with Django's recommendations and improved usability). The request is actionable: add a parameter to BaseConstraint to allow specifying the error code. The only minor ambiguity is the exact parameter name (the description says 'violation_error_message' but means a new parameter for the code), but the intent is clear and a sensible implementation is straightforward.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16569": {
        "explanation": "The issue provides a clear description of the bug, the conditions under which it occurs, the exception raised, the relevant code location, a suggested fix, and a self-contained reproducible example. The expected behavior is implied (no exception should be raised when calling empty_form), and the root cause is identified. This is sufficient information for an experienced engineer to attempt a solution and verify it.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16578": {
        "explanation": "The issue clearly states the desired change: update the signature of BaseConstraint to use keyword-only arguments. It references a specific discussion for context and notes that the subclasses already use keyword-only arguments for 'name' and 'violation_error_message'. The scope and intent are clear, and a successful solution would involve modifying the BaseConstraint __init__ method to enforce keyword-only arguments, updating any relevant documentation, and ensuring all usages are updated accordingly. There is little ambiguity in what is being asked.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16588": {
        "explanation": "The issue provides a clear description of the bug: the floatformat template filter in Django crashes with a ValueError when given '0.0000' (as a string or Decimal) and a precision of 2. It includes a minimal reproducible example, the traceback, the relevant code path, and even the commit that introduced the bug. The expected behavior is implied: floatformat should not crash and should handle this input gracefully. The root cause is also identified (precision of 0 is not allowed). This is sufficient information to attempt a fix and write a test for the case.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16595": {
        "explanation": "The issue provides a clear description of the problem: the migration optimizer does not reduce consecutive AlterField operations when AddField is not present. It includes a concrete example, the current and expected behavior, and even a suggestion for a code change. The only minor ambiguity is whether the desired behavior is to reduce all consecutive AlterField operations to the last one, but the context and the suggestion make it clear that this is the intent. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16599": {
        "explanation": "The issue provides a clear error scenario: querying a Django admin page with a user ID that exceeds SQLite's INTEGER range causes an OverflowError. The stack trace and reproduction steps are included. However, the issue does not specify what the desired behavior should be (e.g., should the app return a 404, validate the input, or handle the error gracefully?). While the problem is clear, the solution direction is not explicitly stated, but a reasonable interpretation would be to prevent the crash and handle such input more gracefully. Thus, the issue is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16603": {
        "explanation": "The issue provides a clear description of the problem: ASGI http.disconnect messages are not handled correctly when a request includes a body. It references the relevant code, explains the expected behavior, and even provides a failing test case that demonstrates the bug. The issue also hints at a possible solution direction (using something like await_many_dispatch). While the exact implementation is left to the developer, the requirements and context are clear enough to attempt a meaningful fix and validation via the provided test.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16612": {
        "explanation": "The issue clearly describes the problem: when APPEND_SLASH is True, AdminSite.catch_all_view() redirects URLs without preserving the query string. It provides a concrete example of the input URL, the expected redirect, and the actual redirect. It also points to the relevant code and suggests the likely cause (the redirect does not include the query string). The required fix is to ensure that the query string is preserved in the redirect. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16614": {
        "explanation": "The issue clearly describes the current behavior of URLField in Django forms: when a user enters a URL without a scheme, it defaults to 'http'. The request is to change this default to 'https'. The relevant code location is provided, and the desired behavior is unambiguous. There are no major ambiguities about what needs to be changed or how to verify the solution (i.e., entering 'example.com' should result in 'https://example.com' as cleaned data).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16631": {
        "explanation": "The issue provides a clear description of the problem: after rotating the secret key and using SECRET_KEY_FALLBACKS, users are logged out, which suggests that session validation does not use the fallback keys. The issue references specific Django internals (salted_hmac, get_session_auth_hash) and points out a possible documentation error or missing feature. However, it does not specify whether the desired solution is to update the documentation, change the code to support fallbacks for sessions, or both. There is a sensible interpretation: ensure that session validation uses SECRET_KEY_FALLBACKS as documented, or clarify the documentation if this is not intended. Some blanks remain, but a meaningful attempt can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16642": {
        "explanation": "The issue clearly describes a problem: when serving files with the extensions '.Z' or '.br', the FileResponse sets the content type as 'text/html' instead of the expected type. The examples provided make it clear what the current behavior is and what is wrong. While it does not specify exactly what the correct MIME types should be for these extensions, it is reasonable to infer that the solution should involve correctly guessing and setting the MIME type for these file types. This is a common enough problem that a software engineer can reasonably proceed to implement a fix, likely by updating the MIME type mapping for these extensions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16649": {
        "explanation": "The issue describes a behavior in Django's ORM where columns added via annotate() are appended to the end of the select list, which can cause problems when using union() with querysets that expect a certain column order. However, the issue does not specify a concrete action to take: it is unclear whether the reporter wants a code change, a documentation update, or simply to record the behavior. The reporter even states that the issue is reported 'mainly for posterity' and only suggests that documentation could be updated, or that the behavior could be changed if possible. There is no clear acceptance criteria or specific task to implement, making it ambiguous what a successful solution would look like.",
        "label": "VAGUE"
    },
    "django__django-16661": {
        "explanation": "The issue provides a clear description of the problem, including a failing test case that demonstrates the bug, the relevant models, and the admin configuration. It also includes an analysis of the likely cause of the bug. The expected behavior is specified (the lookup should be allowed), and the test asserts this. This is sufficient information for a developer to reproduce the issue and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16662": {
        "explanation": "The issue clearly describes the problem: newly generated migration files have import statements that do not follow Django's coding style or isort's default behavior. It provides a concrete example of the current (incorrect) output and the desired (correct) output. The references to Django's coding style and isort's documentation further clarify the requirements. The task is to ensure that migration file generation orders imports as specified. This is a well-specified issue with a clear, actionable outcome.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16667": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, the exact error message, and the root cause in the code (lack of validation for user-supplied year, month, and day values in SelectDateWidget). It also specifies where in the code the error occurs and how to reproduce it. The required solution is implied: add validation to prevent OverflowError when constructing a date from user input. This is sufficient for an experienced engineer to implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16686": {
        "explanation": "The issue clearly describes a problem: when using .annotate() and then .values() with a typo in the annotation name, the resulting FieldError does not mention annotations as valid choices. The expected behavior is implied: the error message should include annotation names as valid choices. The code example and the error message are provided, making it clear what needs to be fixed. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16693": {
        "explanation": "The issue clearly describes the current problem (a mysterious error message when a context processor returns the wrong type), provides a concrete example, and suggests a specific solution (catch the error and raise a more informative message, similar to how Django handles views returning the wrong type). The location in the codebase is indicated (around updates.update() in django.template.context), and the desired outcome is explicit. This is sufficient for an experienced engineer to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16745": {
        "explanation": "The issue clearly describes a mismatch between Django's StepValueValidator and browser behavior when both min and step are set on a number input. It explains the expected behavior (step should be calculated from min, not 0), the current behavior, and the resulting problem. While it doesn't specify the exact code change, it is clear what needs to be fixed: StepValueValidator should take min_value into account as the base for step increments, matching browser behavior. This is a well-specified bug report for someone familiar with Django forms and validators.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16746": {
        "explanation": "The issue clearly states that the Paginator.validate_number method raises error messages that cannot currently be customized or internationalized. While it does not specify the exact mechanism for customization or internationalization (e.g., via constructor arguments, class attributes, or integration with a translation framework), it is clear that the goal is to make these error messages configurable and translatable. An experienced engineer could reasonably interpret this as needing to refactor the method to allow error messages to be overridden or passed in, and to support i18n. Thus, the issue is reasonably specified, though some implementation details are left to the developer's discretion.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16750": {
        "explanation": "The issue provides a clear description of the problem: using the Chr database function in a Django annotation raises a ValueError due to an output field type mismatch. The error trace is included, and the user suggests a specific solution (setting output_field = CharField() in Chr). The context, expected behavior, and a proposed fix are all present, making it clear what needs to be done for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16757": {
        "explanation": "The issue is well-described: it explains the current behavior (system check error for ManyToManyField in list_display, but not for reverse foreign keys), the undesired result (500 error at runtime), and the desired behavior (system check should catch this at configuration time). It provides a concrete reproduction scenario (using Question and Choice models from the Django tutorial, adding 'choice_set' to list_display), and includes the traceback. The expected outcome is clear: extend the admin system checks to catch reverse foreign keys in list_display and raise an appropriate error, similar to the existing E109 check for ManyToManyField. This is a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16786": {
        "explanation": "The issue provides a clear description of the problem (FilteredRelation with Coalesce in the condition fails due to missing JOINs), a minimal reproducible example (with a link to a public repo and specific test file), and a demonstration of both the failing and workaround cases. The expected behavior is implied: FilteredRelation should correctly add necessary JOINs when Coalesce references fields that require them. While the exact implementation details are left to the developer, the problem and the desired outcome are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16801": {
        "explanation": "The issue provides a clear description of the problem: ImageField in Django always adds a post_init signal handler, which is unnecessary when width_field and height_field are not set. The performance impact is described, and the proposed solution is to avoid adding the signal handler unless those fields are set. While the issue does not specify exactly how to implement the change, it is clear what the desired behavior is and what constitutes a successful solution (i.e., only add the signal handler if width_field or height_field are set).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16816": {
        "explanation": "The issue provides a clear description of the problem: when 'choice' is included in list_display for QuestionAdmin, no error is raised at configuration time, but a runtime error occurs when accessing the admin page. The user suggests that error E108 (presumably a configuration validation error) should be updated to catch this misconfiguration earlier. The stack trace and reproduction steps are provided, and the desired outcome (improve E108 to catch this case) is clear. While the exact details of E108 are not included, it is reasonable to infer its purpose from context. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16819": {
        "explanation": "The issue states a general goal (reducing AddIndex/RemoveIndex operations during migration optimization) but does not specify what 'reduce' means, what the current behavior is, what the desired behavior is, or any constraints or acceptance criteria. There is no information about the context, the codebase, or what would constitute a successful solution. This leaves significant ambiguity and makes it unclear how to proceed or what to deliver.",
        "label": "VAGUE"
    },
    "django__django-16824": {
        "explanation": "The issue provides a clear description of the problem: the dbshell command crashes when DATABASES[\"NAME\"] is a PosixPath, and it includes a full traceback. The root cause is identified (PosixPath not being converted to str), and a suggested fix is given (coerce each item in e.cmd to string). The context, expected behavior, and a concrete solution are all present, making it straightforward to implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16830": {
        "explanation": "The issue provides a clear description of the problem: when saving inherited models with force_insert=True, the flag is not propagated to parent tables, resulting in an unnecessary UPDATE query. The issue includes a code example, the expected and actual behavior, and even points to the relevant code path (Model.save_base, Model._save_parents, Model._save_table). The desired solution is to ensure force_insert is passed through to parent saves. This is a well-specified issue with a clear, actionable request.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16865": {
        "explanation": "The issue describes a specific behavior: when using 'makemigrations --update', the custom migration name provided with '--name' is not respected, and the migration is renamed to an autogenerated name. The reporter suggests this may be a bug or at least a documentation gap. However, the issue also includes broader suggestions (e.g., adding a '--no-optimize' option, rethinking --update behavior) and is somewhat open-ended about whether the solution should be code or documentation. While the main actionable item (respect --name during --update) is clear, the issue mixes requests and is not fully precise about the expected outcome (should --update always respect --name? Only if supplied? Should docs be updated instead?). Thus, there is a sensible interpretation for a solution, but some blanks to fill in.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16873": {
        "explanation": "The issue provides a clear description of the problem, including the current and expected behavior of the `join` template filter when used inside an `{% autoescape off %}` block. It includes a code snippet, a test case that demonstrates the bug, and the actual vs. expected output. The desired behavior is unambiguous: when autoescape is off, the joining string should not be escaped. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16877": {
        "explanation": "The issue provides a clear description of the desired feature: a new template filter called 'escapeseq' that escapes each item in a sequence, analogous to how 'safeseq' works for marking items as safe. It gives a concrete example of usage and specifies the context (autoescape is off). While it does not specify the exact escaping mechanism, it is reasonable to infer that it should use the same escaping as the template engine's default. The reference to 'safeseq' provides a clear implementation analogy. Thus, the issue is reasonably specified, with only minor details to be filled in by referencing existing conventions in the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16888": {
        "explanation": "The issue provides a clear description of the problem: querying a Django DecimalField with a value exceeding max_digits now raises a confusing decimal.InvalidOperation error, whereas previously it would simply return DoesNotExist. The reporter suggests two possible solutions: either the backend should accept such queries and return 'not found', or the error should be made more descriptive. A reproducible test case is provided, along with a stack trace. The expected behavior is well-articulated, and the desired outcome (either graceful handling or a clearer error) is clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16899": {
        "explanation": "The issue clearly describes a specific problem: error messages for 'readonly_fields' in ModelAdmin do not include the field name, unlike similar messages for other fields. It provides an example of the current message and a comparison to the desired format. The requested change is to update the error message to include the field name, making it consistent with other fields and improving clarity. This is a well-scoped and actionable request.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16901": {
        "explanation": "The issue clearly describes the problem: Django's fallback SQL for XOR (^) with more than two Q objects is incorrect on databases that lack native XOR support. It specifies the expected behavior (parity/odd number of true arguments) and the current, incorrect behavior (exactly one true argument). It provides concrete examples and expected results, and references the relevant code path. This is sufficient for an experienced engineer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16903": {
        "explanation": "The issue provides a detailed description of the problem, including the inheritance structure, the specific error encountered, the relevant Django source code, and a proposed solution. The user has clearly identified the root cause (handling of primary key defaults in diamond inheritance scenarios), and even suggests a concrete code change. While there is some open-endedness about whether this is a bug or an unsupported feature, the technical details and the expected behavior are clear enough to attempt a meaningful solution. The only minor ambiguity is whether the maintainers agree with the proposed approach, but the technical requirements are well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16910": {
        "explanation": "The issue provides a clear description of the problem, including the Django version, the models involved, the exact query being run, and the difference in generated SQL between two Django versions. The expected behavior is implied by the Django 4.1.9 query, and the regression is well-demonstrated. It is clear that the goal is to restore the previous behavior where only the specified fields are selected from the related model. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16920": {
        "explanation": "The issue provides a clear description of the accessibility problem (lack of association between help text/errors and input fields for screen readers), references relevant accessibility standards, and gives concrete examples of the current and desired HTML output. It specifies the use of aria-describedby and unique IDs for error/help elements, and even notes a specific implementation detail (not using <ul> for error lists due to VoiceOver limitations). While there is some room for interpretation in how to generalize this across all fields in Django's form rendering, the core requirements and expected outcome are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16938": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example (models, manager, and serialization code), the exact error message, and the context in which the error occurs. It also references a related performance optimization (#33937) and describes a workaround. The expected vs. actual behavior is clear: serialization fails when a custom manager with select_related is used on an m2m field. The only ambiguity is whether this is a bug or expected behavior, but the technical problem to solve is well-defined: serialization should not fail in this scenario. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16950": {
        "explanation": "The issue provides a clear description of the problem, including the Django and Python versions, model and admin code, and a step-by-step reproduction scenario. The expected and actual behaviors are described, and a link to the exception is provided. While it is not 100% clear whether the solution should be a code fix, a workaround, or a test, the issue is specific enough for a developer to attempt to reproduce and address the bug in the codebase. The only minor ambiguity is whether the fix should be in Django itself or in the user's code, but the context suggests a Django core bug. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16952": {
        "explanation": "The issue provides a clear description of the problem: creating objects with nested multi-table inheritance (MTI) in Django crashes. It includes a code example (the ItalianRestaurantManyParents class inheriting from both ItalianRestaurant and Place), a test that triggers the crash, and the exact traceback of the error. The error message is specific (AttributeError: 'OneToOneField' object has no attribute 'attname'), and the context (Django's model inheritance system) is well-established. This is sufficient information for an experienced engineer to reproduce the issue and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17029": {
        "explanation": "The issue clearly describes a specific problem: the Apps.clear_cache() method does not clear the cache of get_swappable_settings_name, which is a functools.lru_cache. The expected behavior is that clear_cache should clear all internal caches, and the issue proposes a concrete solution: add self.get_swappable_settings_name.cache_clear() to the clear_cache method. The context, motivation, and a direct code reference are provided. This is sufficient information to implement and test a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17046": {
        "explanation": "The issue provides a clear minimal reproduction, including model and admin definitions, the exact steps to reproduce the error, the error message and stack trace, and even a note about a version regression. The problem is well-scoped: deleting objects after searching on a related many-to-many field in the Django admin causes a 500 error due to a .distinct()/.delete() incompatibility. The expected behavior is implied (deletion should work without error), and the cause is reasonably well-identified. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17058": {
        "explanation": "The issue provides a clear description of the new SQL feature (NULLS [NOT] DISTINCT in unique constraints), the motivation for supporting it (differences in backend behavior, especially with Postgres 15 and SQL Server), and a concrete proposal for the API change (adding a nulls_distinct: Optional[bool]=None option to UniqueConstraint). It also references relevant standards and discussions. While some implementation details (such as how to handle unsupported backends or migration implications) are left to the implementer, the core requirements and rationale are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-17084": {
        "explanation": "The issue provides a clear description of the problem: after upgrading to Django 4.2, aggregate functions over window functions in ORM queries now raise a specific database error. The error message, relevant versions, and a minimal code example are all included. However, the issue does not specify what the expected behavior should be (should this be supported, or should a clearer error be raised?), nor does it clarify whether the solution should be a code fix, a documentation update, or a workaround. There is enough information to investigate and attempt a solution, but some interpretation is required regarding the desired outcome.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-17087": {
        "explanation": "The issue provides a clear code example, describes the observed behavior (incorrect reference to the nested class method in the migration), and specifies what the correct behavior should be (the migration should reference appname.models.Profile.Capability.default instead of appname.models.Capability.default). The problem is well-scoped and the expected outcome is unambiguous. A solution would involve ensuring that the migration generator correctly references nested class methods for defaults.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-7188": {
        "explanation": "The issue provides a clear objective: enable the use of Django's authentication middleware and context processor without requiring 'django.contrib.auth' to be in INSTALLED_APPS. It references a discussion for context and specifies the version (1.9) and the need for tests. However, it does not detail the technical constraints, edge cases, or what 'using the auth machinery' precisely entails beyond the two components mentioned. There is some room for interpretation, but the main goal is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-7475": {
        "explanation": "The issue clearly describes the desired feature: extending manage.py diffsettings to allow comparison against an arbitrary settings module, not just the default Django settings. The rationale and a concrete example of usage are provided. The expected behavior is clear, and the implementation path is reasonably inferable from the description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-7530": {
        "explanation": "The issue describes a specific problem: makemigrations is calling allow_migrate() with (app_label, model) pairs that do not correspond (i.e., for each app, it is passing all models, not just the app's own models). It also references a previous ticket and a PR, which provide additional context. The expected behavior is clear: allow_migrate() should only be called with models belonging to the app in question. While some implementation details may need to be inferred from the codebase, the core requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-8119": {
        "explanation": "The issue describes a desired feature: allowing expressions in .filter() calls, similar to how some ORMs (like Django) allow expressions such as F('field').lower() == 'value'. The motivation is clear (enabling query syntax extensions and unifying the ORM experience). However, the issue does not specify the current limitations, the expected syntax, the types of expressions to support, or any constraints. There is enough context to make a reasonable attempt by looking at how expressions are currently handled and extending support, but some implementation details would need to be inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-8961": {
        "explanation": "The issue provides a clear description of the current behavior and the problems with how the program name is rendered in help messages for Django's command-line utility. It identifies specific inconsistencies (e.g., '__main__.py' being shown instead of 'python -m django', and 'manage.py' vs './manage.py') and asks what the correct behavior should be, especially across different platforms. However, it also poses open questions about what the correct replacement should be and what should happen in the documentation, without specifying a definitive desired outcome. This leaves some ambiguity about the exact solution, but a sensible interpretation would be to update the help output to display the most user-friendly and accurate invocation for each context and platform. Therefore, the issue is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-9296": {
        "explanation": "The issue clearly describes the desired behavior: the Paginator class should implement the __iter__ method so that iterating over a Paginator instance yields its pages, rather than requiring explicit use of page_range. The issue even provides a sample implementation of the __iter__ method. The objective and the expected outcome are both clear, making it straightforward to implement and test the solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-9703": {
        "explanation": "The issue describes a desired enhancement: when a user mistypes a management command, the system should suggest possible intended commands. The motivation is clear, and the general direction is understandable (suggest similar commands on typo). However, the issue does not specify the exact behavior, the matching algorithm, or how suggestions should be presented. It also references a screenshot and a patch, but these are not included in the description. While the core request is clear, some implementation details are left to interpretation, but a sensible solution (e.g., fuzzy matching on command names) can be inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-9871": {
        "explanation": "The issue provides a clear motivation: in the --help output of custom management commands, command-specific arguments are currently listed after boilerplate/global options, making them less visible. The request is to reorder the output so that command-specific arguments appear first. While the issue does not specify the exact code location or the precise output format, it is clear what the desired outcome is: prioritize command-specific arguments in the help output. As an experienced engineer with access to the codebase, I can identify where the help output is generated and adjust the ordering accordingly. There is a sensible interpretation of what is required, though some implementation details are left to the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-13859": {
        "explanation": "The issue provides a clear bug report: creating a figure with zero height (figsize=(3, 0)) and saving it as a PNG causes a crash in libpng. The code to reproduce the issue is provided, the error message is shown, and the relevant environment details are included. The expected behavior (not crashing) is implied, and the actual behavior (crash) is described. While the issue does not specify exactly how the library should handle zero-sized figures, it is reasonable to interpret that the library should either raise a more informative error or handle the case gracefully. Thus, the issue is well-specified enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-13908": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, actual and expected outcomes (with images), and version information. The problem is well-defined: minor ticklabels at positions coinciding with major ticks (e.g., 00:00) are missing, but they used to appear in previous versions. The expected behavior is explicitly described and illustrated. This is sufficient for a developer to attempt a fix and verify correctness.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13913": {
        "explanation": "The issue is well-specified. It clearly describes the unexpected behavior: `is_color_like` returns True for strings of integers, which are not valid colors for plotting. The issue provides a minimal reproducible example, the actual and expected outcomes, and relevant environment details. The desired resolution is also clear: `is_color_like` should return False for such strings. This is sufficient information to attempt a meaningful fix or investigation.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13959": {
        "explanation": "The issue provides a clear description of the inconsistency in how the scatter() function handles the shapes of its parameters: x and y are flattened and only need to have the same size, but c must match the shape of either x or y, not just the size. The issue includes concrete code examples that work and that fail, and points out the confusing error message. However, it does not specify what the desired behavior should be (e.g., should c be flattened like x and y, or should the error message be improved, or both?). There is a sensible interpretation: make c accept any shape as long as its size matches x and y, or at least clarify the error message. But some design decisions are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-13980": {
        "explanation": "The issue provides a clear description of the problem: when plotting data on a polar plot in Matplotlib, the default lower limit of the radial axis (r) can be negative, which is confusing and leads to a 'hole' in the center of the plot. The user expects the default minimum to be zero. The issue includes a reproducible code example, the observed output, and the expected behavior. The environment details are also provided. The only minor ambiguity is whether the fix should be a change in default behavior or an option, but the core requirement (default bottom y_limit should be zero for polar plots) is clear.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13983": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the observed and expected behavior, and a detailed analysis of the suspected root cause in the code. It also distinguishes between two similar methods (`remove()` vs `delaxes()`) and their differing effects. This is sufficient information for an experienced developer to attempt a fix or at least to investigate further in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13984": {
        "explanation": "The issue clearly describes the problem: the inability to set the color of tick marks (not labels) on Axes3D objects in matplotlib using the standard API. It provides a code example, references prior discussion, and specifies the expected vs. actual behavior. The desired outcome is for tick mark colors to be settable, matching the API for 2D axes. This is a well-specified feature/bug report that can be addressed by investigating and updating the relevant code to allow tick mark color customization.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13989": {
        "explanation": "The issue provides a clear description of the bug: when calling plt.hist with density=True and a specified range, the returned bin edges do not respect the range (i.e., the first and last bin edges are not equal to the specified range bounds). The expected and actual outcomes are both provided, along with a minimal reproducible code snippet and information about the environment. The issue also notes that the bug does not occur when density=False and provides a commit that introduced the regression. This is sufficient information to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-14471": {
        "explanation": "The issue provides a clear description of the problem, including the observed and expected behavior, a minimal reproducible example, and relevant environment details. It specifies that calling plt.figure() in Matplotlib v3.1.0 destroys existing subclassed FigureCanvasQT objects, which did not happen in v3.0.3. The issue also points to a likely area in the code (FigureCanvasQT.__init__) and provides a concrete way to reproduce the bug using the NeXpy application. While the root cause is not identified, the problem is well-defined and actionable for someone familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-14623": {
        "explanation": "The issue provides a clear description of the bug, including a summary, code to reproduce the problem, actual and expected outcomes, and relevant environment details. The expected behavior is explicitly stated: inverting the y-axis using set_ylim(y.max(), y.min()) should work for both 'linear' and 'log' scales. The code sample and screenshots make the problem and the desired fix unambiguous. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-17810": {
        "explanation": "The issue provides a clear and reproducible code snippet that demonstrates the problem, including the error traceback and the context in which the error occurs. It specifies the environment, versions, and the sequence of operations that lead to the bug. The expected behavior (no error when running each part alone) and the actual behavior (StopIteration when both are run together) are both described. This is sufficient information to attempt a meaningful investigation and solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-18869": {
        "explanation": "The issue clearly describes the current state (only __version__ is exposed), the desired improvement (expose a version_info tuple or a LooseVersion instance at the toplevel for easier version comparisons), and provides context and rationale. It also expresses a slight preference but leaves the implementation detail (tuple vs. LooseVersion) to the implementer, which is reasonable. The task is well-bounded and actionable: add a new attribute to the toplevel (likely matplotlib.__init__) that exposes the version in a more easily comparable format. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-19743": {
        "explanation": "The issue requests adding 'constrained_layout' support for 'figure.legend', which is a clear feature request. It is reasonably clear what is being asked: the user wants the figure-level legend to be included in the layout adjustments made by constrained_layout. However, there are some implementation details and edge cases that are not specified (e.g., how it should interact with existing axes legends, what happens with multiple legends, etc.), but the core request is understandable and actionable. Therefore, it is not perfectly specified, but a sensible interpretation is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-19763": {
        "explanation": "The issue provides a clear bug summary, expected and actual outcomes, a reproducible code snippet, and relevant environment details. It specifies the backend (nbagg), the widget (MultiCursor), the parameter in question (useblit=False), and the symptoms (multicursor disappears when idle, high CPU usage). The expected behavior is also clearly stated. This is sufficient information for a developer to attempt a meaningful fix or investigation.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20374": {
        "explanation": "The issue provides a clear description of the bug, including a summary, detailed reproduction steps, code samples, actual and expected outcomes, and a suggested direction for a fix. The root cause is identified (the out_of_date() function not considering :context: plots as out of date when included files change), and a concrete solution is proposed (always consider :context: plots as out of date when the parent file is rebuilt). This is sufficient for an experienced engineer to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20470": {
        "explanation": "The issue clearly describes the problem: when using the 'label' argument with ax.text, no legend entry is created for the text, even though one is expected. The code for reproduction is provided, along with the actual and expected outcomes. The expected behavior is unambiguous: a legend entry for the text should appear. The context (Matplotlib version) is also given. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20488": {
        "explanation": "The issue provides some context: a specific test (test_huge_range_log) is failing in CI but not locally, possibly due to a numpy change. It includes a traceback and references the PR where the test was introduced. However, it does not specify what the expected behavior is, what the test is supposed to check, or what a successful fix would look like. There is no information about the input data, the intended output, or how to reproduce the failure. As a result, there is significant ambiguity about what needs to be done to resolve the issue beyond 'make the test pass.'",
        "label": "VAGUE"
    },
    "matplotlib__matplotlib-20518": {
        "explanation": "The issue clearly describes the problem: the set_sketch_params() method has no effect when using the PGF backend in Matplotlib. It provides a minimal reproducible example, specifies the expected and actual outcomes (with images), and gives the relevant version information. The expected behavior is that sketch parameters should influence the output when using the PGF backend, just as they do with other backends. This is a well-specified bug report that can be meaningfully addressed by ensuring that the PGF backend respects sketch parameters.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20584": {
        "explanation": "The issue provides a clear description of the problem, a minimal working example (MWE), and visual evidence of the undesired effect. It specifies the expected behavior (that set_segments(get_segments()) should be a no-op) and the actual behavior (the lines become coarser). The context (matplotlib version, method calls, and code) is sufficient to attempt a meaningful investigation and solution. The only minor ambiguity is whether the fix should be in the documentation, the implementation of set_segments/get_segments, or elsewhere, but the core problem is well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20676": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible code snippet, a description of the actual and expected outcomes, and relevant environment details. The problem is well-scoped: when using an interactive SpanSelector, the axes limits are incorrectly expanded to include zero, whereas the expected behavior is for the limits to remain as set (with possible margins), matching previous behavior. This is sufficient information to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20679": {
        "explanation": "The issue provides a clear description of the regression: previously, the SpanSelector's onselect callback was triggered even when the selector was removed by an 'empty' click (click and release without dragging), but this is no longer the case. The expected and actual outcomes are described, and a minimal reproducible code snippet is provided. The desired behavior is explicitly stated: the onselect callback should be fired even for empty clicks, as before. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20693": {
        "explanation": "The issue describes a deprecation warning when updating selector colors in MNE due to changes in matplotlib, and asks for guidance on how to migrate the code. However, it does not specify what the new, recommended approach should be, nor does it provide enough information about the desired end state or the expected behavior after migration. The issue also suggests that the deprecation warning itself could be improved, but does not specify what information should be added. As such, there is ambiguity about what constitutes a successful solution: is it updating the code, improving the warning, or both? And what is the correct way to update the code? Without further clarification, it is unclear what exactly needs to be done.",
        "label": "VAGUE"
    },
    "matplotlib__matplotlib-20805": {
        "explanation": "The issue provides a clear description of the observed behavior, a minimal reproducible code sample, and a specific expectation: that labelrotation should be respected regardless of where the spine position command is placed. The environment details are also provided. While the user is unsure if this is a bug or intended behavior, the report is specific enough to attempt to reproduce, diagnose, and potentially fix the issue or clarify the behavior. Thus, it is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20826": {
        "explanation": "The issue provides a clear description of the bug, including a summary, code to reproduce the problem, actual and expected outcomes (with images), and version information. The problem is well-scoped: after calling ax.clear() on shared axes, extra ticks and tick labels appear that should be hidden. The expected behavior is demonstrated with a previous version of matplotlib. This is sufficient information to investigate, reproduce, and attempt a fix for the regression in ax.clear() behavior with shared axes.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20859": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible code snippet, the actual error message, and a description of the expected outcome. It also points to the relevant code section and suggests a possible fix. All necessary information to attempt a solution is present, and the requirements for a successful solution are clear: allow legends to be added to SubFigure objects, similar to Figure or Axes. Therefore, the issue is well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21042": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes, and relevant environment details. The user expects that deprecated rcParams should not be loaded, and the deprecation warnings should not occur when updating rcParams. While the issue does not specify the exact desired behavior (e.g., whether deprecated keys should be filtered out automatically or if a warning suppression is acceptable), it is clear that the goal is to prevent deprecated rcParams from being loaded and thus avoid the warnings. There is enough information to attempt a solution, such as filtering out deprecated rcParams before updating, or modifying the update logic to ignore them.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-21318": {
        "explanation": "The issue provides a clear bug report, including a summary, a minimal reproducible code example, the actual error message and traceback, and the expected outcome. It also specifies the environment and version information. The problem is well-scoped: the code works in an earlier version but fails in a later one, and the user expects the code to work as before. There is enough information to attempt a fix or at least to investigate the root cause and propose a solution or workaround.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21481": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual error traceback, and the expected outcome. The environment details (OS, matplotlib version, backend, Python version) are also specified. The problem is well-scoped: subfigures with certain Gridspec slices and constrained_layout cause an error, but should not. The expected behavior is unambiguous (no error, same as with constrained_layout=False). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21490": {
        "explanation": "The issue provides a clear description of the bug: Line2D does not copy its input arrays, which can lead to unexpected behavior if the input arrays are modified after being passed to Line2D. The issue includes a code sample that reproduces the problem, a comparison to the expected behavior (as seen in AxesImage), and a rationale for why the change is needed. The expected outcome is clearly stated: modifying the input array after creating the Line2D should not affect the drawn line. All necessary context (including version, platform, and related prior discussion) is provided. This is sufficient for a developer to attempt a solution by ensuring Line2D copies its input arrays upon construction or data update.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21542": {
        "explanation": "The issue provides a clear description of the current problem (colorbar uses old-style format strings, which is inconsistent with axes), references the relevant code locations, and outlines a proposed solution (removing the if statement and delegating to Axis, possibly with a deprecation cycle). It also discusses an alternative (checking the format string type in Axis._set_formatter). While there are some implementation details left to the developer (e.g., how to detect format string style), the overall goal and direction are clear. This makes the issue reasonably specified, as a sensible interpretation and solution path are provided, though some decisions are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-21550": {
        "explanation": "The issue provides a clear bug report: a specific example works as expected in matplotlib 3.4.3 but not in a later development version (3.5.0.dev). The user provides a full code sample for reproduction, screenshots of both the actual and expected outcomes, and detailed environment information (OS, Python version, Jupyter version, pip list, etc.). The problem is described as a regression between versions, and the reproduction steps are explicit. While the exact nature of the 'display not working well' is not described in words, the screenshots and code make the expected and actual behavior clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21559": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual error traceback, and a description of the expected outcome. It specifies the environment (OS, matplotlib version, Python version, installation method). The problem is well-scoped: eventplot should accept multiple series of datetime.date objects, but currently fails with a TypeError. The expected behavior is also described: each series should be plotted as a row on the y-axis. This is sufficient information to attempt a fix or at least to reproduce and investigate the problem.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21568": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, and visual evidence of the regression (with before/after screenshots). The expected outcome is also described: restore the datetime axis spacing as it appeared in version 3.3 when using usetex=True. The environment is specified (OS, matplotlib version, installation method). While the root cause is not identified, the problem and the desired result are clear, making it possible to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21617": {
        "explanation": "The issue provides a clear and reproducible bug report: it describes the problem (text missing in EPS output), the expected and actual outcomes, the specific versions of dependencies where the bug appears and does not appear, a minimal code sample to reproduce the issue, and relevant environment details. This is sufficient for a developer to attempt to reproduce, diagnose, and potentially fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22719": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes, and a detailed traceback. The reporter also analyzes the problem, points out the likely cause (broad exception handling and inappropriate warning for empty data), and suggests that the warning should be more specific or not triggered for empty data. The expected outcome is well described: either allow empty artists or provide a more accurate warning. This is sufficient for a developer to attempt a meaningful fix, such as adjusting the unit conversion logic to handle empty data gracefully or refining the warning logic.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22767": {
        "explanation": "The issue provides a clear and specific bug report: it identifies the function and file in question, the exact line and code that is believed to be incorrect, and what it should be changed to. It also includes a minimal reproducible example, the actual and expected outcomes, and relevant environment details. This is sufficient information to attempt a fix and verify it.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22835": {
        "explanation": "The issue provides a clear bug report, including a minimal reproducible example, the error traceback, and the context in which the error occurs. It also suggests possible directions for a fix (special check, try-except, or making BoundaryNorm approximately invertible), but does not mandate a specific solution. As an experienced engineer, I can make a sensible decision (e.g., add a check or handle the exception gracefully) to prevent the crash. The requirements for a successful solution are clear: prevent the crash when using BoundaryNorm with format_cursor_data. Therefore, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-22865": {
        "explanation": "The issue provides a clear description of the bug, including the context (matplotlib colorbar with drawedges=True and extend='both'), a minimal reproducible code example, and both actual and expected outcomes with images. The expected behavior is unambiguous: the colorbar should have black edges at the extremities when drawedges=True and extend='both'. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22871": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, the actual and expected outcomes, and relevant environment details. The expected behavior is explicitly stated: the year should appear in the offset text to the right of the x-axis when plotting less than a year and January is not included. This is sufficient information to attempt a solution, as it is clear what is wrong and what the user expects to see.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22883": {
        "explanation": "The issue provides a specific file and line number where a string is missing the 'f' prefix for an f-string, which is a clear and actionable code quality issue. The required fix is to add the 'f' prefix to the string so that variable interpolation works as intended. The context is sufficient for a developer to locate and resolve the problem without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22926": {
        "explanation": "The issue provides a clear bug report: initializing a RangeSlider with a custom value (via valinit) causes an IndexError in set_val due to an out-of-bounds assignment to xy[4]. The user identifies the problematic line and suggests commenting it out as a fix. The report includes a minimal reproducible example, the actual and expected outcomes, and even the relevant code section. The desired behavior (allow initializing the RangeSlider with user values) is clear. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22929": {
        "explanation": "The issue provides a clear bug summary, three concrete reproducible code examples, a traceback of the error, and a well-articulated expected outcome for each case. The desired behavior is specified for each scenario (raise an error for missing dependent value, ignore missing error values), and the relevant code location is referenced. This is sufficient information to attempt a meaningful fix or improvement to the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22945": {
        "explanation": "The issue provides a clear description of a regression: a specific commit where the bug is introduced, a minimal code sample that reproduces the error, the full traceback, and the expected outcome (no error). The context (animation, tight_layout, clip_path) and the error (AttributeError: 'NoneType' object has no attribute 'xmin') are all present. The only minor gap is that the code is not fully self-contained (as noted by the reporter), but it is sufficient for a developer with access to the codebase to reproduce and investigate the problem. The commit hashes and reference to the PR that caused the regression are also provided, making it straightforward to bisect and debug. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23031": {
        "explanation": "The issue provides a clear background and motivation: currently, matplotlibrc and style files are read using the locale encoding, but this leads to problems and inconsistencies, especially with non-UTF-8 encodings. The issue suggests that files should be read as UTF-8 (or, optionally, support encoding cookies). However, the 'Proposed fix' section is empty, and there is some ambiguity about whether encoding cookies should be supported or not. The main requirement is clear: switch to reading these files as UTF-8, but there is a minor open question about encoding cookie support. A reasonable interpretation would be to implement UTF-8 reading and possibly leave encoding cookie support for further discussion. Thus, the issue is reasonably specified, but not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-23057": {
        "explanation": "The issue provides a clear and detailed description of the bug, including the specific regression window (between 3.5.1 and 3.5.2), the affected functionality (figures not redrawing in IPython after the first plot), the exact steps to reproduce (including code and environment setup), the expected and actual outcomes, and even the commit that introduced the regression. The environment details are thorough, covering OS, Python version, backend, and package versions. This is more than sufficient for a developer to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23088": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes, and even points to the relevant code section and suggests a possible fix (adding a warning when a KeyError is caught). The problem is well-described: a KeyError is silently swallowed, leading to a confusing error message later. The user requests that a warning be issued instead. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23174": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible code example, the actual traceback, and the expected outcome. The environment details (OS, Matplotlib version, backend, Python version) are also specified. The problem is well-scoped: adding clabels to contour plots in subfigures causes a crash, and the error message is included. This is sufficient information to reproduce, diagnose, and attempt a fix for the bug.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23188": {
        "explanation": "The issue provides a clear summary of the current problem (default date limits are set to 2000-01-01 to 2010-01-01, which can cause issues for certain use cases), references a related issue for context, and proposes a specific fix (change the default limits to 1970-01-01 to 1970-01-02). It also clarifies the expected impact (should only affect empty date axes) and notes the effect on tests. The requirements and scope are clear, making it possible to implement a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23267": {
        "explanation": "The issue clearly describes the current behavior, the inconsistency between using the 'location' kwarg with auto-created vs. manually created Axes, and provides a code example. It also proposes a concrete solution: add a 'location' kwarg to the Colorbar constructor that sets both 'orientation' and 'ticklocation', and is mutually exclusive with them. Alternatively, it suggests improving documentation. The requirements for a successful solution are clear: implement the kwarg as described, or improve documentation if implementation is not feasible. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23299": {
        "explanation": "The issue provides a clear and reproducible code example, describes the observed and expected behavior, and explains the consequences of the bug. The context (matplotlib, Gcf, rc_context, get_backend) is well-defined, and the problem is isolated to a specific interaction. This is sufficient for an experienced developer to attempt a fix and verify it with the provided test case.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23314": {
        "explanation": "The issue provides a clear description of the problem: calling set_visible(False) on a 3D subplot does not hide the subplot as expected. It includes a minimal reproducible code example, specifies the actual and expected outcomes, and provides relevant environment details (Matplotlib version, backend, Python version). The expected behavior is unambiguous, and the code to reproduce the bug is provided. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23412": {
        "explanation": "The issue provides a clear description of the bug: when setting the linestyle on a patch object using a dash tuple, the offset has no effect. It includes a minimal reproducible example, actual and expected outcomes (with images), and even a comparison to the correct behavior in Line2D objects. The environment details are also provided. The expected behavior is clear: patch objects should respect the dash offset in their edge linestyle, just as Line2D does. This is sufficient information to attempt a fix or investigation.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23476": {
        "explanation": "The issue provides a clear and reproducible bug report, including a minimal code example, the actual and expected outcomes, and detailed environment information (OS, Python, Matplotlib version, backend, etc.). The problem is well-described: after unpickling a figure on an M1 Mac, the DPI doubles each time, leading to an eventual OverflowError. The expected behavior is that the DPI should remain constant. This is sufficient information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23562": {
        "explanation": "The issue provides a clear minimal reproducible example, the exact error message, and the context (matplotlib versions, method call, and traceback). It is clear that the user expects r.get_facecolors() to work, but it fails due to a missing attribute. The expected behavior is implied: get_facecolors() should return the face colors of the surface plot. The code location of the error is also given. This is sufficient information for a developer to investigate and attempt a fix or workaround.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23563": {
        "explanation": "The issue provides a detailed bug report, including a code snippet for reproduction, the exact error message, the expected outcome, and environment details. The problem is clearly described: after running code with certain numpy array inputs, a persistent AttributeError occurs in matplotlib's 3D plotting, only resolved by restarting the kernel. The goal is to fix this error so that the code works as expected and does not require a kernel restart. This is a well-specified bug report that allows for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23742": {
        "explanation": "The issue provides a clear bug summary, a traceback, and a description of the context (MNE-Python, widget abstraction, recent breakage after a pip upgrade). It specifies the environment (OS, Python, Jupyter, Matplotlib, ipympl versions) and includes a code snippet that is believed to trigger the bug. The expected outcome is clearly stated (no error). While the author mentions they will try to provide a minimal working example (MWE) later, the current information is sufficient to attempt a meaningful investigation and likely a fix, especially with the detailed traceback and reproduction context.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-23964": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes, and even a suggested fix. The stack trace and version information are included, making it straightforward to reproduce and understand the problem. The root cause is described, and a potential solution is proposed. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23987": {
        "explanation": "The issue provides a clear description of the bug, including a summary, code to reproduce the problem, the actual and expected outcomes, and relevant environment details. The problem is well-scoped: a UserWarning is emitted about layout incompatibility even when 'constrained_layout=False' is set, which is unexpected. The expected behavior is also clearly stated (no warning). This is sufficient information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24013": {
        "explanation": "The issue provides a detailed description of a namespace collision between a module and a function with the same name, using a concrete example (matplotlib.tri.tripcolor). It explains the problem, demonstrates the resulting confusion in object resolution, and suggests a possible solution (renaming the module). However, it also raises a broader question about whether this is a general concern in the codebase and if there is a desire to enforce a certain property (bijective mapping from fully qualified name to object). The issue is clear about the specific problem and a possible fix, but it is not clear whether the maintainers want to adopt the suggested change or if a broader policy should be implemented. Thus, while the technical problem is well-explained, the desired outcome (policy, code change, or documentation update) is not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-24026": {
        "explanation": "The issue provides a clear use case, a code example that reproduces the problem, and a traceback of the error. It also describes the desired behavior: stackplot should not change the Axes cycler and should allow CN color aliases like other plotting functions. The problem and the expected outcome are both well-articulated, making it clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24149": {
        "explanation": "The issue provides a clear and reproducible bug report: it describes the behavior of ax.bar when passed all-NaN data in matplotlib 3.6.1, provides code to reproduce the error, shows the traceback, and contrasts the behavior with the previous version (3.6.0). It also points to a likely related change in the release notes and provides further debugging information. The expected outcome is clearly stated. This is sufficient information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24177": {
        "explanation": "The issue provides a clear description of the bug, including the context (using matplotlib's Axes.hist with density=True and histtype='step'), the unexpected behavior (density axis not auto-scaled, and density values change with data rescaling), and a minimal reproducible example. It also includes actual and expected outcomes with images, and relevant environment details. The problem is well-scoped and actionable: ensure that density scaling is consistent and auto-scaling works as expected for histtype='step', matching the behavior of other histtypes. No significant ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24189": {
        "explanation": "The issue provides a clear bug report: when using plt.subplot_mosaic() with width_ratios (or height_ratios) and a nested mosaic, a confusing error is raised unless all mosaics have the same number of columns/rows. The report includes a minimal reproducible example, the actual error message, and an expected outcome (including a reference image). It also suggests two possible solutions: (1) support width_ratios/height_ratios for the outer mosaic (or nested layouts in general), or (2) provide a clearer error message explaining the limitation. This gives a clear path for a successful PR: either implement the feature or improve the error message. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24257": {
        "explanation": "The issue provides a clear objective: to document how to distribute style files (style dicts) in Python packages for matplotlib. It references an existing discussion and provides context on why this is useful. It also suggests creating a template package, similar to an existing one for colormaps, and mentions documenting the ability to distribute colormaps and custom logic alongside styles. While it does not specify the exact format or location for the documentation, it is clear what needs to be achieved: write documentation and possibly provide a template package. The requirements are clear enough for a meaningful attempt at a solution, though some implementation details are left to the developer's discretion.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-24334": {
        "explanation": "The issue clearly describes the current behavior (kwargs to set_xticks/set_ticks are only validated if labels are set), the undesired effect (invalid kwargs are silently ignored), and a proposed solution (always validate kwargs, possibly error if labels are not set). The scope and intent are clear, and the expected outcome is well-defined. There is little ambiguity about what needs to be changed and why.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24362": {
        "explanation": "The issue is well-specified: it clearly describes the bug (that 0 and 1 are not accepted as bool values for sharex/sharey), provides a minimal reproducible example, shows the actual and expected outcomes, and even suggests a concrete fix with a code diff. The context (Matplotlib version, OS, etc.) is also provided. There is no ambiguity about what is being requested: to allow 0 and 1 as valid bool inputs for sharex/sharey in subplots, matching the behavior of True/False.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24431": {
        "explanation": "The issue clearly describes the current behavior (an error when passing a list of alpha values to plt.eventplot), provides a minimal reproducible example, and specifies the desired behavior (support for a list of alpha values, analogous to how a list of colors is supported). The error message and context are provided, and the feature request is unambiguous: allow per-dataset alpha values in eventplot. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24538": {
        "explanation": "The issue clearly identifies a documentation gap: the 'legendHandles' attribute of the Legend class is not documented, nor is there a getter method for it. The user suggests that either documenting the attribute, adding a getter, or at least providing a pointer in the legend docs to related methods would be helpful. While there is some flexibility in how to address the issue (document the attribute, add a getter, or improve cross-references), the core requirement is clear: improve discoverability and documentation of 'legendHandles' for users. This is a reasonably specified issue, as the intent and possible solutions are clear, but the exact implementation is left to the developer's discretion.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-24570": {
        "explanation": "The issue provides a clear description of the bug: the 'align' parameter in HPacker appears to have its 'top' and 'bottom' options reversed. It includes a minimal reproducible code example, actual and expected outcomes (with images), and specifies the relevant version of Matplotlib. The expected behavior is unambiguous from the provided images and description. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24604": {
        "explanation": "The issue describes a desired enhancement to allow combining subplot_mosaic with axes of different projections, and suggests a possible API (figure.gridspec_mosaic) that would return a dict of SubplotSpecs. However, it does not specify exact requirements, acceptance criteria, or a clear definition of what the new method should do, what its signature should be, or how it should behave in edge cases. There is also no proposed solution or clear direction on whether to implement the SubplotSpec.add_subplot() idea. The issue is more of a discussion starter than a concrete, actionable ticket.",
        "label": "VAGUE"
    },
    "matplotlib__matplotlib-24619": {
        "explanation": "The issue provides a clear bug report, including a reproducible code sample, the observed error, and the expected outcome. It also suggests specific code changes to address the problem, and highlights where the current implementation fails. While the suggested fixes may not be complete, the problem is well-defined: pcolorfast (and likely quadmesh) should accept integer (including uint8) RGBA arrays, and the code should handle them appropriately. The requirements for a successful solution are clear: allow integer RGBA arrays, handle their value ranges correctly, and ensure the plotting works as expected. There is enough information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24627": {
        "explanation": "The issue clearly describes the current inconsistent behavior between removing an artist (which unsets its .axes attribute) and clearing the axes (which does not). It provides concrete code examples demonstrating the problem and specifies the desired outcome: that cla() and clf() should also unset the .axes and .figure attributes of deparented artists. This is a clear and actionable request.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24637": {
        "explanation": "The issue is well-described: it provides a clear use case, a minimal reproducible example, the expected behavior (the gid should be present in the SVG output), the actual behavior (the gid is missing), and references to related prior work. The problem is specific (AnnotationBbox gid not being rendered in SVG), and the desired outcome is clear (ensure the gid is present in the SVG output for AnnotationBbox). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24691": {
        "explanation": "The issue clearly describes the current limitation (alpha applies to the whole patch), the desired behavior (ability to set face and edge alpha separately), and provides both a use case and a proposed solution (allow alpha to take a tuple for patch methods). There is also a reference to prior art in the codebase. While some implementation details may need to be worked out, the requirements and the goal are clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24768": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes (including a traceback and a screenshot), and some analysis of the likely cause and relevant code locations. The problem is well-scoped: pcolormesh(rasterized=True) conflicts with set_rasterization_zorder(), leading to a crash. The expected behavior is also clear (the plot should render as in the screenshot, not crash). There is enough information to attempt a fix and to verify it works. No further clarification is needed.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24849": {
        "explanation": "The issue clearly describes the bug (gapcolor not supported for LineCollections), provides a minimal reproducible example, shows the actual and expected outcomes (including an image), and even suggests a possible approach for a fix (adding set_gapcolor and get_gapcolor to LineCollection, modeled on set_color/get_color). The requirements for a successful solution are clear: LineCollection should support the gapcolor property so that plotting methods like vlines/hlines work as expected. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24870": {
        "explanation": "The issue clearly describes the current behavior (contour() with boolean arrays defaults to 8 levels, which is not useful), provides a concrete example, and proposes a specific solution: autodetect boolean arrays and set the default levels to [0.5]. The expected behavior is well-motivated and the implementation direction is clear. There is little ambiguity about what is being requested, and a successful solution would be to implement this autodetection and default behavior.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24924": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the actual and expected outcomes, and even a suggested fix. The context is specific to the behavior of `Figure.set_tight_layout` in matplotlib, and the steps to reproduce and verify the fix are unambiguous. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24970": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual output (including the specific deprecation warnings), and the expected outcome (no warnings). It also includes relevant environment details. The deprecation warnings are specific and point to lines in the codebase that need to be updated to comply with NumPy 1.24's stricter type handling. The goal is clear: update the code to avoid these deprecation warnings. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25027": {
        "explanation": "The issue provides a clear context: it requests adding RGB(A) array support to the `pcolor` function, similar to what was done for `pcolormesh` in a previous issue. It also explains the motivation (Cartopy's use of `pcolor`), and references related issues and code locations. However, it does not specify the exact API, expected behavior for edge cases, or tests, and it mentions a dependency on another issue (#25027) without clarifying its status or content. Still, a reasonably experienced developer could infer the required changes by analogy to the `pcolormesh` implementation and the referenced code. Thus, the issue is reasonably specified, but not fully detailed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25052": {
        "explanation": "The issue is reasonably well-specified. The user has identified a discrepancy between the documentation and the actual behavior of the `bbox` parameter in `matplotlib.axes.Axes.table`. They provide a clear link to the documentation, a description of the problem, a traceback, and a suggested improvement. However, there is some ambiguity in the exact desired outcome: whether the documentation should be changed to clarify the expected input, or whether the code should be changed to accept a `Bbox` instance. The suggestion leans toward code change, but both options are presented. Thus, while the problem is clear, the precise solution direction requires some interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25122": {
        "explanation": "The issue provides a clear description of the bug: the use of np.abs in the window correction in mlab._spectral_helper is incorrect, especially for windows with negative values like 'flattop'. It references the relevant code lines, provides a code snippet to reproduce the problem, and points to the correct implementation in scipy for comparison. The expected and actual outcomes are specified, making it clear what needs to be fixed. This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25126": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible code example, actual and expected outcomes (including links to output files), and relevant environment details. The problem is well-described: after changing the y-axis scale from 'log' to 'linear', the plotted line disappears, which is not expected. The expected behavior is also clearly shown. This is sufficient information for a developer to reproduce, investigate, and attempt a fix for the issue.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25281": {
        "explanation": "The issue is well-specified. It clearly describes the bug: when passing a non-string, invalid value to the `loc` argument of `legend`, validation is not performed at the time of calling `legend`, but only later when `show()` is called, resulting in a confusing error. The issue provides a minimal reproducible example, the actual and expected outcomes, and even points to the relevant code section where validation is missing. The expected behavior is also clearly stated: errors should be raised immediately when invalid arguments are passed to `loc`, as is already the case for invalid string values. This is sufficient information to attempt a fix (i.e., add validation for non-string `loc` values at the appropriate place).",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25287": {
        "explanation": "The issue provides a clear description of the bug, including the current and expected behavior, a minimal reproducible example, and even a suggested patch. The problem is well-scoped: the offsetText (exponent label) color should follow tick.labelcolor, not tick.color. The expected outcome is illustrated with images, and the relevant code locations are identified. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25311": {
        "explanation": "The issue provides a clear and reproducible code sample, specifies the observed error, and describes the expected outcome. The environment details (OS, Python, Matplotlib version) are included. The problem is well-scoped: pickling a figure with a draggable legend fails, but works when the legend is not draggable. This is sufficient information to investigate and attempt a fix or at least to provide a meaningful workaround or explanation.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25332": {
        "explanation": "The issue provides a clear bug report: after calling fig.align_labels(), attempting to pickle the figure fails with a specific TypeError. The code to reproduce is provided, the error message is included, and the expected outcome (successful pickling) is stated. The environment is also described. The problem is well-scoped: ensure that figures with aligned labels can be pickled. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25346": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible code example, a screenshot of the actual outcome, and a description of the expected outcome. It also specifies the environment and version details. The problem is well-scoped: when using constrained layout with wrapped titles, not enough space is allocated for the titles. The expected behavior is that enough space should be left for wrapped titles, similar to when explicit newlines are used. This is a well-specified bug report that allows for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25404": {
        "explanation": "The issue provides a clear error message, the context in which it occurs (using LassoSelector and set_props), and points to a specific line in the codebase that causes the error. The user also suggests a workaround (commenting out a line) that prevents the error. However, the issue does not specify what the correct behavior should be (should set_props work for LassoSelector? Should _props be initialized? Should set_props be disabled for LassoSelector?). There is enough information to investigate and propose a fix, but some interpretation is required regarding the intended behavior.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25425": {
        "explanation": "The issue provides a clear context: it discusses the inconsistency between the current AxesImage.__str__ output and the numpy API, and suggests two possible directions for improvement. However, it does not specify which of the two alternatives should be implemented, nor does it provide a definitive proposed solution. The intent is clear (improve API consistency and possibly display more useful information), but the exact requirements are left open. A sensible interpretation would be to implement one of the suggested changes, but the lack of a firm decision means some assumptions would need to be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25430": {
        "explanation": "The issue provides a clear bug report: when calling savefig with a .jpg filename and the metadata kwarg, an inscrutable error is raised. The report includes a minimal reproducible example, the actual traceback, and a clear expected outcome: either support metadata for jpg, ignore the argument, or provide a more informative error message. This gives enough direction for a developer to improve the error handling or documentation, or to implement support if feasible. The requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25433": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the actual and expected outcomes, and relevant environment details. The problem is well-scoped: the on_changed callback for a RangeSlider blocks widget input after clearing and redrawing the figure, while the button's on_clicked does not. The expected behavior is also clearly stated. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25442": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the exact error traceback, expected outcome, and relevant environment details (OS, matplotlib version, backend, Python version, installation method). It is clear that the problem is an AttributeError when using mplcursor with matplotlib 3.7.1, and the goal is to prevent this error from occurring. This is sufficient information for a developer to attempt a fix or at least reproduce and investigate the issue.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25479": {
        "explanation": "The issue provides a clear and reproducible example of the problem: registering a colormap under a different name than its internal name leads to confusion and errors when using pyplot. The user expects that registering a colormap under a new name should allow it to be referenced by that name, regardless of the colormap's internal name. The issue describes the current behavior, the expected behavior, and the confusion caused. While it does not prescribe a specific solution, it is clear what the desired outcome is: after registering a colormap under a new name, it should be usable via that name in pyplot, regardless of the colormap's internal name. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25498": {
        "explanation": "The issue provides a clear description of the problem: after changing the norm of a mappable object in matplotlib, the colorbar does not update as expected. It includes a minimal reproducible example, the methods attempted (`update_normal`, `update_bruteforce`), and the error encountered (ZeroDivisionError). The expected behavior is implied: the colorbar should update to reflect the new norm. While the issue does not specify the exact desired API or error handling, it is clear that the goal is to make the colorbar update correctly after changing the norm. This is a reasonably well-specified bug report with a clear path to a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25547": {
        "explanation": "The issue description provides clear, reproducible code samples for two related bugs: (1) a UserWarning is thrown when all color values are NaN in plt.scatter, but not when only some are; (2) ax.errorbar raises StopIteration when yerr is all-NaN. For both, the expected behavior is stated (no warning/crash if only some values are NaN, so all-NaN should behave similarly). The stack traces and environment details are provided. The root cause is also identified for the errorbar case. This is sufficient to attempt a fix: ensure that all-NaN inputs are handled gracefully and consistently in both cases, without warnings or exceptions. The requirements for a successful solution are clear.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25565": {
        "explanation": "The issue provides a clear bug report: when using Poly3DCollection in a 3D plot and attempting to add a legend, an AttributeError is raised because the legend handler expects a numpy array but receives a tuple. The error traceback is detailed, the root cause is identified, and a suggested fix (converting colors to a numpy array) is proposed. The expected outcome is also described and illustrated. All necessary information to attempt a fix is present, including reproduction code and environment details.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25624": {
        "explanation": "The issue clearly describes a bug: when the rcParam 'figure.autolayout' is set to True, any kwargs passed to fig.tight_layout (such as w_pad) have no effect. The issue provides a minimal code example to reproduce the problem and specifies the environment (Matplotlib 1.5.3, Ubuntu Linux). The expected behavior is implied: kwargs to tight_layout should be respected even if figure.autolayout is set. The only minor ambiguity is whether the fix should be for all versions or just 1.5.3, but the core requirement is clear.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25631": {
        "explanation": "The issue provides a clear description of the bug: using bbox_inches='tight' with moviewriter.grab_frame() produces an incorrect animation (the 'snowflake movie'), while removing bbox_inches='tight' resolves the issue. The code to reproduce the problem is provided, as well as links to both the actual and expected outcomes. The expected behavior is clear: the animation should look like the 'expected outcome' video even when bbox_inches='tight' is used. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25667": {
        "explanation": "The issue provides a clear description of the bug, including a summary, code to reproduce the problem, actual and expected outcomes (with images), and the context (main branch of Matplotlib). The problem is well-scoped: bar/barh do not trigger the correct datetime unit conversion when given datetime-like parameters for left/bottom. The expected behavior is demonstrated with a workaround. This is sufficient for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25712": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, actual and expected outcomes, and a hypothesis about the cause. The expected behavior is well-defined (should match the documentation example, but with y-axis labels reduced by a factor of 10). The problem is specifically about how fill_between (and fill_betweenx) interact with axes transforms and axis limits. This is sufficient information to attempt a fix or at least to investigate the code path that sets axis limits in the presence of transforms.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25772": {
        "explanation": "The issue provides a clear bug summary, a detailed error trace, a minimal reproducible example, and full environment details (OS, Python, package versions, backend, installation method). The problem is well-scoped: importing matplotlib in VSCode interactive mode with '%matplotlib qt' fails to find Qt bindings, despite PySide6 being installed. The expected behavior is implied (the command should work if a Qt binding is present). This is sufficient for a developer to attempt a fix or at least reproduce and diagnose the problem.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25775": {
        "explanation": "The issue clearly describes the current limitation (Text objects only use the global antialiasing setting), the desired enhancement (add get/set_antialiased methods to Text objects, similar to other artists), and even hints at the necessary changes (adjust Annotations, update drawing code to use the new property, and modify backend code to use GraphicsContext state instead of rcParams). While some implementation details (e.g., exact backend changes) are left to the implementer, the requirements and scope are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25794": {
        "explanation": "The issue clearly describes the current behavior (both singular and plural forms of certain kwargs are accepted, with singular taking precedence), the problem this causes (complexity and confusion), and a concrete proposed solution (raise an error when both forms are specified). The scope of the change is well-defined and actionable. While there is some discussion of a larger change (deprecating one form), the 'small change' is clearly specified and can be implemented independently. There is enough information to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25859": {
        "explanation": "The issue raises a concern about Figure.add_axes() accepting arbitrary positional parameters, referencing a specific example and the official documentation. It proposes two possible solutions: document the behavior if the parameters are meaningful, or deprecate extra positional parameters if they are not. However, it is not fully clear what the expected behavior should be, nor is there a concrete specification of what 'not accepting arbitrary positional parameters' should look like (e.g., should an error be raised, or should a warning be issued?). There is also ambiguity about what constitutes 'extra' parameters and how to handle backward compatibility. While the intent is clear, the implementation details are left open, requiring some interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25960": {
        "explanation": "The issue clearly describes a bug: the 'wspace' and 'hspace' parameters in 'Figure.subfigures' do not have any effect. It provides a minimal reproducible example, describes the actual and expected outcomes, and even links to the relevant code section. The expected behavior is that changing 'wspace' and 'hspace' should affect the spacing between subfigures, but currently it does not. This is a well-specified bug report with enough information to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26011": {
        "explanation": "The issue provides a clear description of the observed bug: 'xlim_changed' callbacks are not triggered on shared axes when the change is induced via a shared axis (using sharex=). It points to the relevant code in _base.py, explains the suspected cause (emit=False), and even suggests a possible solution (moving the callback call outside the 'if emit:' block). The issue also notes a potential recursion problem if emit=True is used, but suggests that moving the callback is a working fix. While the issue is somewhat exploratory and asks if there is a reason for the current behavior, it is specific enough to attempt a solution: ensure that 'xlim_changed' callbacks are triggered for all axes, even when the change is induced by a shared axis. The code location and the desired behavior are both clear.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26020": {
        "explanation": "The issue provides a clear bug report: it describes the error encountered when using AxesGrid with a non-default axis class (GeoAxes from cartopy), includes a minimal reproducible code snippet, the full traceback, and a detailed environment specification. The cause is also analyzed (difference in the 'axis' attribute between mpl_toolkits.axes_grid1.mpl_axes.Axes and GeoAxes). While it does not specify the exact fix, it is clear that the goal is to make AxesGrid compatible with GeoAxes (or at least not fail with this error). This is a well-specified bug report for a software engineer familiar with matplotlib and cartopy.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26024": {
        "explanation": "The issue provides a clear list of missing mathematical Unicode code points that are not currently defined in the 'tex2uni' mapping in the '_mathtext_data.py' file. It also provides references for the corresponding symbols and LaTeX names, and suggests prioritizing those in standard LaTeX and AMS. While there is some room for interpretation regarding which symbols to add (\"be a bit discriminate\"), the core task\u2014adding missing mappings for the listed code points, especially those in standard LaTeX and AMS\u2014is clear and actionable. Thus, a meaningful attempt at a solution is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-26078": {
        "explanation": "The issue provides a clear description of the bug (parasite_axes does not properly handle units), a minimal reproducible example, actual and expected outcomes (including images), and a detailed analysis of the root cause with a suggested fix. The context is sufficient to understand what is wrong, what the correct behavior should be, and even offers a direction for a solution. This is well-specified and actionable for a developer familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26101": {
        "explanation": "The issue provides a clear description of the problem: mathtext markers (e.g., marker=\"$\\star$\") are not center-aligned in matplotlib plots, while standard markers (e.g., marker=\"*\") are. It includes a code snippet to reproduce the issue, a screenshot, and a clear expected outcome (center-aligned markers regardless of marker type). The only minor ambiguity is whether the solution should be a code workaround, a documentation update, or a fix in matplotlib itself, but the intent is clear: make mathtext markers center-aligned. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26113": {
        "explanation": "The issue is well-specified. It provides a clear bug summary, a detailed code snippet for reproduction, an explicit description of the actual and expected outcomes, and even points to the relevant lines of code and a recent commit that may have introduced the behavior. The desired resolution is also clearly stated: to make the behavior of the mincnt parameter consistent regardless of whether the C parameter is supplied, specifically by using len(vals) >= mincnt in both cases. This is a concrete, actionable request that can be implemented and tested using the provided information.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26122": {
        "explanation": "The issue provides a clear description of the current behavior of imshow() regarding the aspect kwarg and how it interacts with the transform kwarg. It specifies the undesired behavior (aspect is set even when transform != ax.transData) and the desired behavior (aspect should not be modified in this case). It also references related issues for context. While there is some mention of potential non-obvious parameter interactions, the main requested change is clear: imshow() should not modify the axes aspect if a non-default transform is used. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26160": {
        "explanation": "The issue provides a clear description of the desired enhancement: to add setters for the _xy1, _xy2, and _slope properties of the _AxLine class, following the normal Artist design. It also specifies a constraint: if the axline is specified by two points, you shouldn't be able to set the slope, and vice versa. The use case and rationale are explained, and the scope is clear. While the 'Proposed solution' section is empty, the problem statement and requirements are sufficiently detailed to proceed with an implementation without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26184": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes, and additional context. It identifies that AnnotationBbox does not return the correct window_extent before the first draw, which causes constrained_layout to fail. The user also provides a workaround and a link to a related discussion. The problem is well-scoped: ensure that using AnnotationBbox with constrained_layout does not produce a warning and works as expected. This is sufficient information for an experienced developer to investigate and attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26208": {
        "explanation": "The issue provides a clear and reproducible code sample, a detailed description of the unexpected behavior (ax1's dataLims being set to \u00b1inf after certain operations), the expected outcome (dataLims should not change), and relevant environment details. The problem is well-scoped and the success criteria (prevent ax1's dataLims from being replaced by inf when using stackplot with twinx) are clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26232": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes, and even points to the relevant code section that needs to be fixed. The error is well-described, and the desired behavior (no error when using a masked array with a read-only mask) is explicit. There is enough information to attempt a solution, such as ensuring that pcolor (or the underlying code) does not attempt to write to a read-only mask or makes a copy as needed. The context is sufficient for a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26278": {
        "explanation": "The issue provides a clear description of the current problem: the 'clip_path' kwarg is ignored in plt.contour/contourf, unlike other plotting functions. It also describes the desired behavior (that 'clip_path' and similar kwargs should be passed through to the underlying collections, making contour/contourf consistent with other plotting functions). However, it leaves open the exact implementation: whether to add a set_clip_path method to ContourSet or to overhaul the kwargs passing mechanism. While the high-level goal is clear, the specifics of the solution are not fully prescribed, but there is enough information to make a sensible attempt at a solution (e.g., by forwarding relevant kwargs to the collections).",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-26285": {
        "explanation": "The issue provides a clear bug report, including a code sample that reproduces the error, the actual and expected outcomes, and a discussion of the root cause (confusion between 'which' and 'minor' kwargs). However, the issue is not a direct request for a code change, but rather a report about a confusing error message and inconsistent API. It is not fully clear whether the desired solution is to improve the error message, harmonize the API, or both. There is enough information to attempt a solution (e.g., improve the error message when an unexpected kwarg is passed), but some interpretation is required as to what the 'fix' should be.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-26291": {
        "explanation": "The issue provides a clear bug report: it describes the expected behavior (an inset axis should appear as in the official demo), the actual behavior (an AttributeError is raised), and includes a minimal reproducible example. It also provides detailed environment information. The error traceback is included, and the code is based on an official example, so the expected outcome is unambiguous. This is sufficient for a developer to attempt to reproduce and fix the issue.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26300": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual and expected outcomes, and even a suspected cause and relevant code references. The desired behavior is unambiguous: calling fig.tight_layout() multiple times should not emit a warning. The context (regression between versions, suspected commit) is also provided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26311": {
        "explanation": "The issue provides a clear bug summary, a minimal reproducible example, the actual traceback, the expected outcome (including a screenshot), and even a link to a proposed fix. The only minor ambiguity is around writing a test, but the main bug and its resolution are well-specified. A meaningful attempt at a solution is possible based on the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26342": {
        "explanation": "The issue clearly describes the current workaround (mutating the paths list in place) and proposes a concrete API improvement: adding a set_paths method to ContourSet that would allow replacing the paths in a cleaner way. The goal is clear, the context is provided, and the expected outcome (a new set_paths method) is unambiguous. The only minor blank is the exact signature or docstring of set_paths, but as an experienced engineer, it's reasonable to infer it should accept a list of paths, similar to get_paths. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26466": {
        "explanation": "The issue provides a clear bug report, including a summary, a minimal reproducible example, actual and expected outcomes, and relevant environment details. The problem is well-described: mutating the array passed as the 'xy' parameter to 'annotate' after the call updates the annotation, which is unexpected. The likely cause (not copying the array) is suggested, and the expected behavior is specified. This is sufficient for a developer to attempt a fix (making a defensive copy of the input array).",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26469": {
        "explanation": "The issue provides a clear description of the current behavior, the rationale for change, and the desired end state: change the default of 'keep_empty' to False and eventually deprecate the option. However, it leaves some implementation details open, such as the exact deprecation strategy (e.g., how to warn users, whether to add a suppress_warning kwarg, and the timeline for removal). Despite these blanks, a sensible interpretation is possible: implement the default change, add a deprecation warning, and document the change. The issue is not fully prescriptive, but it is actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "mwaskom__seaborn-2457": {
        "explanation": "The issue provides a clear description of the problem: when calling sns.lineplot with ci=None, the function should warn and reformat the arguments to use errorbar=None instead. The expected behavior is specified, and the code example demonstrates the scenario. While there is a minor ambiguity about the exact wording or mechanism of the warning, the core requirement is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "mwaskom__seaborn-2766": {
        "explanation": "The issue describes a DeprecationWarning caused by the use of distutils' version classes in seaborn, triggered by setuptools >=60. It provides a traceback and points to the specific line in the codebase. However, the issue also notes that this has already been fixed on master (in PR #2466), and the main request is to release a new patch version to include the fix. As a software engineer, the actionable item is not to change code, but to cut a new release. The issue is clear in its request, but it is not a code change, rather a release management task.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-2853": {
        "explanation": "The issue description clearly states the problem: x_estimator error bars now inherit the alpha parameter from scatter_kws, whereas previously they were always opaque. The desired behavior is implied: error bars should not inherit the alpha from scatter_kws, and should remain opaque as before. The context (scatterplot, x_estimator, error bars, alpha parameter) is specific enough to identify the relevant code and make the necessary change. Therefore, the issue is well-specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3010": {
        "explanation": "The issue provides a clear description of the problem: PolyFit fails when the input data contains missing values (e.g., None). It includes a minimal reproducible example and a full traceback, showing that the error occurs due to missing data being passed to numpy's polyfit. The expected outcome is implied: PolyFit should be robust to missing data, i.e., it should handle or ignore missing values gracefully rather than raising an exception. The solution would involve modifying PolyFit to handle missing data. This is a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3069": {
        "explanation": "The issue provides a clear description of the current behavior for categorical axes in seaborn's categorical plots and suggests that nominal scales (including inferred ones) should behave the same way. It lists three specific behaviors to replicate and offers some implementation notes and open questions. However, there are some ambiguities, such as exactly where in the codebase to implement certain changes, how to handle user overrides for grid display, and whether some behaviors should be controlled by the scale object or elsewhere. While the overall goal is clear, some decisions are left to the implementer, and there are open questions about best practices and edge cases. Therefore, the issue is reasonably specified but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "mwaskom__seaborn-3187": {
        "explanation": "The issue provides a clear description of the problem (legend values for large ranges are missing the multiplicative offset), a reproducible code example, a screenshot of the incorrect output, and a pointer to the likely source of the bug in the codebase. It also mentions relevant matplotlib rcParams that may affect the behavior. The expected behavior is implied: the legend should include the offset value, matching the axis tick formatting. While it doesn't specify the exact desired output, the context and standard matplotlib behavior make the fix clear for an experienced developer.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3190": {
        "explanation": "The issue provides a clear, minimal code example that triggers the bug, a full traceback, and a concise description of the problem: color mapping fails when boolean data is provided. The expected behavior is implied (color mapping should work with boolean data), and the error is specific. This is sufficient for a developer to reproduce, diagnose, and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3394": {
        "explanation": "The issue provides a clear description of the problem (axis ordering is reversed when using pd.NA in plotting), a reproducible code example, expected behavior, and even a workaround. The expected outcome is unambiguous: NAs should be excluded without reversing axis order. The code and the screenshot make it clear what the bug is and how it manifests. This is sufficient information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3407": {
        "explanation": "The issue provides a clear description of the problem: calling seaborn's pairplot on a DataFrame with MultiIndex columns raises a KeyError. It includes a minimal reproducible example (MRE), the exact error traceback, and a workaround (flattening the columns). The expected behavior is implied: pairplot should work with MultiIndex columns or at least handle them gracefully. While it doesn't specify exactly how pairplot should behave with MultiIndex columns (e.g., should it flatten them, raise a more informative error, or support them natively), it is clear that the current behavior is a bug. There is enough information to attempt a meaningful fix, such as flattening columns internally or raising a clear error message.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pallets__flask-5014": {
        "explanation": "The issue clearly states the problem (Blueprints with empty names cause things to not work correctly) and the desired solution (raise a ValueError when a Blueprint is given an empty name). It references a related issue (#4944) for context. The required change is unambiguous: enforce a non-empty name for Blueprints and raise a ValueError otherwise. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1142": {
        "explanation": "The issue clearly describes the observed behavior (requests.get always adds a 'content-length' header), the expected behavior (it should not add this header automatically for GET requests or there should be an option to omit it), and provides a concrete example (amazon.com returns 503 when 'content-length' is present). This is sufficient information to investigate the code, confirm the behavior, and implement a fix or configuration option. The requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1327": {
        "explanation": "The issue provides a clear description of the problem: the current implementation of Session.get_adapter() in requests does not reliably select the correct adapter when multiple adapters are mounted, due to the unordered nature of the adapters dictionary. It also provides several possible solutions, discusses their trade-offs, and specifies the desired sorting order (longest key to shortest, then alphabetically). However, it leaves the final decision of which solution to implement up to the developer, rather than specifying a single required approach. This means there is some room for interpretation, but a sensible solution can be implemented based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1339": {
        "explanation": "The issue provides a clear description of the bug, a reproducible test case, the current (broken) behavior, the expected (correct) behavior, and even a suggested code fix. The problem is well-motivated and the desired outcome is unambiguous: CaseInsensitiveDict should store only one value per case-insensitive key, and the last assignment should win, regardless of the case used. This is sufficient for a meaningful and successful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1376": {
        "explanation": "The issue provides a failing test case, including the code and the specific assertion that fails. It shows the expected and actual behavior, and the context (testing unicode field names in multipart POST requests). However, it does not specify what the correct behavior should be (should the field name be bytes or str, or how should it be encoded), nor does it describe the intended fix. There is enough information to make a reasonable attempt at a solution by interpreting the test's intent (that the field name should not be encoded as b'stuff'), but some assumptions are required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1537": {
        "explanation": "The issue provides a clear description of the problem: a regression in handling non-string types (specifically float and datetime) in the 'data' dictionary when used with 'files' in multipart/form-data requests. It includes code samples that reproduce the error, specifies the versions affected, and provides the traceback. The reference to previous related issues gives additional context. While the issue could be improved by specifying the expected behavior (e.g., should non-string types be automatically converted to strings?), it is still clear that the goal is to restore the previous behavior where these types did not cause an error. Thus, a meaningful attempt at a solution is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1657": {
        "explanation": "The issue provides a clear description of the problem: session hooks and request hooks are being merged incorrectly, resulting in one overwriting the other due to both being dictionaries with lists as values. It references the relevant code location, and the expected behavior (proper merging rather than overwriting) is implied. While it doesn't specify the exact desired merging logic, it is reasonable to infer that the lists should be combined rather than replaced. This is enough information for an experienced engineer to attempt a solution, though some implementation details may need to be decided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1713": {
        "explanation": "The issue provides a clear description of a regression: passing a MozillaCookieJar instance to the 'cookies' parameter in requests.get() no longer works as expected in version 2.0.1. It includes a minimal reproducible example, the exact error message, and a link to the commit suspected of causing the regression. The expected behavior (that MozillaCookieJar should be accepted as a valid 'cookies' argument) is implied by the fact that this worked previously and is now broken. The only minor ambiguity is whether the fix should restore the old behavior or document the change, but the context strongly suggests the intent is to restore compatibility. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1724": {
        "explanation": "The issue provides a clear description of the problem: passing a unicode string as the HTTP method in requests.request() causes a UnicodeDecodeError in Python 2.7.2, while passing a byte string does not. The issue includes reproducible code samples, the full traceback, and a hypothesis about the cause (unicode leaking into headers due to lack of coercion to str). The expected behavior is implied: requests.request() should accept unicode method names without causing an error. The solution would involve ensuring the method is always a str/bytes type before being used internally. This is a well-specified bug report with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1733": {
        "explanation": "The issue clearly states the desired outcome: to allow the Response class (presumably from the requests library) to be pickled. It provides a concrete example (using pickle.dumps on a requests.get result) and the full error traceback, which points to the root cause (missing __getstate__ for a class with __slots__). The technical requirement is clear: modify the Response class so that it can be pickled without error. The context, expected behavior, and error are all provided, making this a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1766": {
        "explanation": "The issue provides context from RFC2617 about how the 'qop-options' directive in Digest Auth should be quoted with double quotes. It also references the behavior of the curl tool and mentions that some server-side implementations are sensitive to this. However, the issue does not specify which part of the codebase is affected, what the current behavior is, or what exactly needs to be changed (e.g., is the code currently not quoting qop-options and needs to be updated to do so?). There is no explicit instruction or acceptance criteria. While a reasonable interpretation is to ensure that qop-options are always quoted in Digest Auth headers, there is some ambiguity about the scope and exact requirements.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1776": {
        "explanation": "The issue clearly describes the problem: cookies sent with a request are being incorrectly persisted to the session after a previous fix. It points to the specific line of code where the problem occurs and references a related issue (#1630) and its test case. While it does not specify the exact solution, it is clear what the desired behavior is (request cookies should not be persisted to the session) and where to look in the code. This provides enough information for an experienced engineer to attempt a solution, though some interpretation is needed regarding how to preserve the fix for #1630 while resolving this issue.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1888": {
        "explanation": "The issue provides a clear description of the problem: a TypeError is raised when following a 301 redirect with the latest pyOpenSSL, due to a Unicode string being passed where bytes are expected. The issue includes the relevant package versions, a minimal test case, and a traceback. It also points to the suspected cause (URL decoding to Unicode and passing to pyOpenSSL). While it does not prescribe a specific fix, it is clear that the solution should ensure that the data passed to pyOpenSSL is of type bytes, not Unicode. This is a well-specified bug report for a software engineer familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1921": {
        "explanation": "The issue clearly describes the current behavior (setting a session header to None results in 'None' being sent as the header value) and contrasts it with the expected behavior (setting a session header to None should prevent the header from being sent, as per the documentation for method arguments). It also provides a workaround (deleting the header), but the main point is that the behavior for session headers is inconsistent with the documented behavior for per-request headers. The desired outcome is clear: setting a session header to None should prevent it from being sent. This is well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1944": {
        "explanation": "The issue provides a detailed description of a problem encountered when using the Requests library to follow a redirect from a server that incorrectly claims its response body is gzipped. It includes a specific URL, the observed error, and a comparison to other tools. However, the issue is primarily phrased as a question about the rationale for decoding the response body of a redirect, rather than as a concrete request for a code change or a specific behavior. There is no explicit acceptance criteria or clear instruction on what should be changed in the codebase (e.g., should Requests skip decoding bodies for redirects, or should it handle this error differently?). Thus, while the technical context is clear, the actionable outcome is ambiguous.",
        "label": "VAGUE"
    },
    "psf__requests-1962": {
        "explanation": "The issue clearly describes an inconsistency in the type of the 'history' attribute (sometimes a list, sometimes a tuple) and expresses a preference for it always being a list. It even points to a specific line in the codebase where this behavior occurs. While it doesn't specify exactly how to fix it, the intent is clear: ensure 'r.history' is always a list. This is a reasonable and actionable request, though some minor interpretation is needed for the implementation details.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2148": {
        "explanation": "The issue describes a concrete problem: a socket.error (specifically, 'Connection reset by peer') is being raised to the user instead of being wrapped in a requests.exceptions.ConnectionError or similar. The user provides a full traceback and expresses an expectation that requests should wrap such low-level exceptions. While the issue does not specify exactly where or how to implement the fix, it is clear what the desired outcome is: socket.error exceptions should be caught and wrapped in a higher-level requests exception. This is a reasonably clear bug report with a sensible interpretation for a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2153": {
        "explanation": "The issue provides a clear description of the problem: sometimes, instead of raising a requests.exceptions.Timeout, a socket.timeout is raised during a requests.post call. The user provides a full stack trace, context about the environment (Python 2.7, requests, urllib3), and the expected behavior (requests should catch socket.timeout and raise its own Timeout exception). The reproduction is not trivial due to hardware/service requirements, but the core bug (uncaught socket.timeout) is well-described and actionable for someone familiar with the requests codebase. The required fix is to ensure socket.timeout is always caught and wrapped in requests.exceptions.Timeout. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2193": {
        "explanation": "The issue provides a clear description of a regression: in requests version 2.4.0, a urllib3.exceptions.ProtocolError is not being wrapped as expected, and the raw exception is exposed to the user. The example code and stacktrace make it clear what the undesired behavior is. While it does not specify exactly how the error should be wrapped, it is reasonable to infer from context (and prior behavior) that the exception should be wrapped in a requests-specific exception, as was likely the case before the regression. This is a common pattern in requests, and the issue is specific enough to attempt a fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2317": {
        "explanation": "The issue provides a detailed description of the observed problem, including the relevant code line, the type conversion that is happening, the context (Python 3.4, requests 2.4.3, neutronclient 2.3.9), and the difference in behavior between Python 2.6 and 3.4. However, it does not specify what the desired behavior should be: should requests accept bytes as the method argument, or should it raise an error? Should it decode bytes to str, or is the bug in neutronclient for passing bytes? The issue also expresses uncertainty about which library is at fault. While a reasonable engineer could infer that requests should handle bytes method arguments gracefully (perhaps by decoding them), there is ambiguity about the correct fix and the expected behavior. Therefore, the issue is not fully well-specified, but a sensible interpretation is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2617": {
        "explanation": "The issue provides a clear, reproducible code sample, describes the environment (Python 2 with unicode_literals, Python 3, and Requests library), and specifies the exact error encountered (UnicodeDecodeError when sending a prepared request with a binary file and unicode_literals enabled). The expected behavior (should work as in Python 3 or Python 2 without unicode_literals) and the actual behavior (exception raised) are both described. The problem is well-scoped: ensure that prepared requests with binary files work even when unicode_literals is imported in Python 2. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2674": {
        "explanation": "The issue raises a question about whether urllib3 exceptions should be wrapped by requests exceptions, and reports that some exceptions are leaking through. However, it does not specify what the expected behavior should be, which exceptions should be wrapped, or how to reproduce the problem. There is no clear acceptance criteria or actionable task. As such, it is ambiguous what a successful solution would look like.",
        "label": "VAGUE"
    },
    "psf__requests-2678": {
        "explanation": "The issue raises a question about whether urllib3 exceptions should be wrapped by requests exceptions, and reports that some exceptions are leaking through. However, it does not specify what the expected behavior should be, which exceptions should be wrapped, or what the desired outcome is. There is no clear acceptance criteria or actionable request. As such, it is ambiguous what a successful solution would look like.",
        "label": "VAGUE"
    },
    "psf__requests-2821": {
        "explanation": "The issue provides a clear description of the problem: a TypeError occurs after upgrading from version 2.7.0 to 2.8.0, with a full traceback included. The user also provides the relevant and full list of installed packages and their versions, and a link to a full traceback. The error is reproducible (goes away after downgrade), and the context (Python 2.7.9, specific package versions) is specified. However, the issue does not specify what the expected behavior is, nor does it provide a minimal reproducible example or the code that triggers the error. While the traceback and environment details are helpful, some assumptions would need to be made about the usage pattern that triggers the bug. Thus, while a reasonable attempt at a solution could be made (e.g., investigating the change in requests 2.8.0 and its interaction with pyOpenSSL and memoryview), there are some blanks to fill in.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2873": {
        "explanation": "The issue provides a clear description of the problem: POST requests hang when the body is a StringIO object that has been written to but not seeked back to the start. It includes a minimal reproducible example, expected behavior (should get a 405 error), and a workaround (seek(0)). The root cause is also suggested (request isn't fully formed). While it doesn't specify the exact fix, it is clear what needs to be addressed: ensure that when a file-like object is passed as data, it is read from the correct position. This is enough information for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2931": {
        "explanation": "The issue provides a clear description of the problem: a request with a binary payload fails due to a call to 'to_native_string', and it references a specific change that introduced the problem. It includes a minimal reproducible example and specifies the versions where the behavior changed. While it does not specify the exact desired fix, it is clear that the goal is to restore the previous working behavior for binary payloads. This is enough information for an experienced engineer to investigate and attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-3362": {
        "explanation": "The issue is primarily a question about the behavior of the 'requests' library, specifically the difference between 'r.text' and 'r.iter_content(decode_unicode=True)'. The user is unsure if their understanding is correct and offers to provide an example if needed. There is no actionable request for a code change, bug fix, or feature implementation. The issue lacks a clear problem statement or desired outcome, making it ambiguous what a successful solution would entail.",
        "label": "VAGUE"
    },
    "psf__requests-4106": {
        "explanation": "The issue provides a clear error message, a reference to a specific commit that may have caused the problem, a code snippet to reproduce the error, and both expected and actual results. It also includes detailed system information. The problem is well-scoped: restoring or ensuring that 'requests.packages.urllib3' is available as expected. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-4356": {
        "explanation": "The issue provides a clear description of the problem: when the https_proxy environment variable is set with an invalid protocol (missing a '/'), the resulting exception is misleading and does not indicate the real cause. The expected result is a better, more informative exception. The issue includes a concrete reproduction case, the actual traceback, and relevant system information. While the issue author is unsure where the fix should be applied, the core requirement\u2014improving the error message for this specific misconfiguration\u2014is clear. As an engineer, I can interpret this as needing to detect malformed proxy URLs and raise a more descriptive exception. Thus, the issue is well-specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-5087": {
        "explanation": "The issue is well-described: it explains the current buggy behavior (an error in the response stream is only reported on the first access to response.content, and subsequent accesses return an empty string), the expected behavior (subsequent accesses should also raise the same exception), and provides a minimal reproducible example and even a test case. The environment is specified, and the problem is clearly scoped. There is no ambiguity about what needs to be changed: ensure that after a stream read error, subsequent accesses to response.content also raise the same error. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-5414": {
        "explanation": "The issue provides a clear description of the problem (UnicodeError is raised when an InvalidUrl should be), a concrete example to reproduce it, references to relevant code and prior issues/PRs, and a clear expected vs. actual result. The desired behavior is unambiguous: catching the UnicodeError and raising InvalidUrl instead. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-6028": {
        "explanation": "The issue provides a clear description of the problem (receiving a 407 error when using proxies in Python 3.8.12, but not in other versions), the expected behavior (should get a 200 status), and a minimal code sample to reproduce the issue. It also includes relevant system and package version information. However, it does not specify the exact proxy configuration or credentials, and assumes the reader has access to a paid proxy service for testing. While there are some minor blanks (e.g., the actual proxy details), the core of the issue is clear and actionable: investigate why proxy authentication fails in Python 3.8.12 with requests, and resolve it so that a 200 status is returned as in other Python versions. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-774": {
        "explanation": "The issue provides a clear traceback, identifies the problematic line of code, and suggests a concrete solution: wrapping the call to netloc.encode('idna').decode('utf-8') in a try/except block to catch UnicodeError. The context (Python 2.6, Requests 0.13.1) is specified, and the error to handle is explicit. While it does not specify exactly what to do after catching the exception (e.g., re-raise, log, return a specific error), the main requirement\u2014catching the UnicodeError at this location\u2014is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-863": {
        "explanation": "The issue clearly describes the current limitation (only one hook function per hook can be specified in the hooks argument to Request.__init__), the undesired behavior (passing a list of hooks fails), and the desired improvement (Request.__init__ should accept lists as values in the hooks dict). It also explains the current workaround and why it's inconvenient. The expected behavior is clear: allow specifying multiple hooks per event by passing a list of callables as the value for a hook key in the hooks dict. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-2905": {
        "explanation": "The issue provides a clear minimal example, describes the current and expected behavior, and points to the relevant code change that introduced the regression. It also explains the use case and the problem caused by the current behavior. The expected output is explicitly stated, and the context is sufficient to attempt a fix. While there may be some implementation details to decide (e.g., how to distinguish objects with a 'values' property that should not be coerced), the problem and the desired outcome are clear.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-2922": {
        "explanation": "The issue suggests adding an 'average' function, possibly with support for weighted averages, and mentions axes order and weights per dimension. However, it is not clear what the input data structure is, what 'ds' refers to, what the expected API should be, or how weights should be specified. There is ambiguity about the requirements and expected behavior, making it difficult to implement a solution without further clarification.",
        "label": "VAGUE"
    },
    "pydata__xarray-3095": {
        "explanation": "The issue provides a clear description of the regression, including the affected methods (Dataset.copy, DataArray.copy, copy.copy, copy.deepcopy), the specific problem (unicode indices being cast to object dtype), the versions affected, and a detailed code example showing the expected and actual behavior. The desired outcome is implied: copying should preserve the unicode dtype of indices, as in versions <=0.12.1. This is sufficient information to attempt a fix and write a regression test.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3114": {
        "explanation": "The issue provides a detailed description of the observed buggy behavior, including a step-by-step reproduction with code, the expected behavior, and a comparison to both the numpy API and a direct construction of the desired array. The problem is clearly stated: after using expand_dims, the underlying numpy array becomes read-only, and after manually setting it to writeable, .loc assignments behave incorrectly (broadcasting to all values instead of a single value). The expected behavior is also clearly described and demonstrated. The environment and versions are provided. While the root cause is not specified, the issue is well-specified for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3151": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the observed error, and the expected behavior (that combine_by_coords should not raise an error when identical coordinates are non-monotonic). It also references the relevant documentation and explains the discrepancy. This is sufficient information to attempt a solution: either by changing the implementation to allow non-monotonic but identical coordinates, or by clarifying the documentation if the current behavior is intended. The environment details are also provided for reproducibility.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3159": {
        "explanation": "The issue clearly describes the desired feature: allowing the 'data' argument of DataArray to accept a scalar default value, which would be broadcast to the shape implied by the coordinates, similar to how pandas.DataFrame works. The user provides a concrete example and explains the motivation. While there are some implementation details to decide (e.g., how to infer the shape from coordinates, what to do if shape is ambiguous), the core requirement is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-3302": {
        "explanation": "The issue provides a clear description of the current behavior of the `interpolate_na()` function and the limitation of its `limit` argument. It also describes the desired behavior: the ability to interpolate only small blocks of NaNs (e.g., single or double NaNs), while leaving larger blocks untouched. However, it does not specify the exact API or argument name for the new feature, nor does it specify edge cases or expected behavior in all scenarios. Still, the core requirement is clear and actionable: add an argument to `interpolate_na()` to only interpolate NaN blocks up to a certain size. The implementation details can be reasonably inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-3305": {
        "explanation": "The issue provides a clear description of the problem: DataArray.quantile does not honor the keep_attrs argument. It includes a minimal, complete, and verifiable example (MCVE) that demonstrates the bug, the expected output, and the actual output. The environment details are also provided. The required behavior is unambiguous: when keep_attrs=True, the output DataArray should retain the input's attrs. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3520": {
        "explanation": "The issue provides a clear minimal code example (MCVE) that demonstrates the problem, including the actual and expected outputs. The problem is described as a selection from a MultiIndex not working as expected in xarray, and the expected behavior is specified. The environment details are also included. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3631": {
        "explanation": "The issue provides a clear minimal code example that triggers the error, a detailed traceback, and a thorough explanation of the root cause. It also discusses the expected behavior and outlines a proposed solution direction (using NumPy directly for conversion, not being limited to nanosecond resolution, and supporting long cftime ranges). The requirements for a successful solution are well-articulated: fix the conversion of datetime.timedelta objects to numeric values in a way that supports long cftime coordinates, without relying on pandas' nanosecond resolution. There is enough information to attempt a meaningful fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3649": {
        "explanation": "The issue provides a clear description of the current behavior, the error encountered, and the desired behavior (allowing combine_by_coords to work with missing panels in the hypercube, filling missing tiles with fill_value). It includes a minimal code example, the expected output, and a concrete suggestion for how the code could be refactored (separating the dimension depth check from the hypercube completeness check). This is sufficient for a developer to attempt a solution, as the requirements and expected behavior are well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3677": {
        "explanation": "The issue provides a clear description of the problem: merging a DataArray into a Dataset using the Dataset's merge method fails, while the top-level merge function works. It includes a minimal reproducible example, the expected behavior, and the actual error message. The desired outcome is implied: ds.merge(da) should work the same as xr.merge([ds, da]). This is sufficient information to attempt a solution, such as updating the merge method to handle DataArray inputs as the top-level function does.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3733": {
        "explanation": "The issue provides a clear motivation, references to related discussions, and a concrete API proposal (a DataArray method called polyfit). It includes a code example showing expected usage and output, and discusses implementation considerations (vectorization, dask support, reshaping). However, some details are left open, such as the full API (e.g., handling of additional polyfit arguments, output format, error handling, etc.), and the expected behavior for edge cases. Still, there is a sensible and actionable interpretation of what is required: implement a DataArray.polyfit method that fits polynomials along a specified dimension, similar to numpy.polyfit, with at least the functionality shown in the example.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-3905": {
        "explanation": "The issue provides a clear description of the current and expected behavior: the string representation (repr) of xarray.DataArray objects is currently truncated based on array size, but not on the number of lines in the output. The user provides concrete examples of the current and desired output, and suggests a reasonable default (truncate at around 40 lines). The problem is well-motivated, and the scope of the change is clear: modify the repr logic to truncate based on line count. While some implementation details (e.g., exact truncation threshold, configurability) could be discussed, there is enough information to make a meaningful and sensible attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3976": {
        "explanation": "The issue provides a clear description of the problem, including a minimal code example that reproduces the bug, the expected behavior, the actual error message, and relevant version information. The request is for in-place addition (A += B) to behave like regular addition (A = A + B) when the coordinates are the same but in a different order. The desired outcome is unambiguous: make in-place addition work as regular addition does in this scenario. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3993": {
        "explanation": "The issue clearly describes an inconsistency in the API between DataArray.integrate and Dataset.integrate regarding the argument name ('dim' vs 'coord'). It specifies the desired change (use 'coord' for both), provides rationale, and even references prior discussion. The only ambiguity is whether a deprecation cycle is needed, but the main technical change is clear and actionable. Thus, a meaningful attempt at a solution can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-4075": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the actual and expected outputs, and an analysis of the root cause. It also suggests a direction for the fix (converting boolean weights to int or float before the dot product). This is sufficient information for an experienced developer to attempt a solution and write a meaningful PR.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4094": {
        "explanation": "The issue provides a clear description of the problem, including a minimal, complete, and verifiable example (MCVE) that demonstrates the bug. It specifies the expected behavior (a working roundtrip for stacking and unstacking single-dimension variables), the actual error encountered, and the context in which the error occurs. The environment details are also included. This is sufficient information to attempt a meaningful fix to the to_unstacked_dataset method for single-dimension variables.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4098": {
        "explanation": "The issue description is extremely brief and lacks context. It is not clear what 'groupby' refers to (is it a function, method, or class, and from which library or codebase?), nor is it clear what 'should work with name=None' means in terms of expected behavior, current behavior, or what constitutes 'working.' There are no examples, error messages, or acceptance criteria. As such, it is impossible to know what a successful solution would look like.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "pydata__xarray-4248": {
        "explanation": "The issue clearly describes the desired feature: displaying units for coordinates and data variables in the xarray.Dataset repr. It provides a concrete example of the current output and a mockup of the desired output, specifying where and how the units should appear. While it does not specify how units are to be determined (e.g., via an attribute like 'units'), it is a common convention in scientific datasets and xarray to store units in the 'units' attribute. Thus, a sensible interpretation is to display the value of the 'units' attribute if present. The issue is clear enough to proceed with a solution based on this interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-4356": {
        "explanation": "The issue provides a clear description of the problem (sum with min_count errors for multiple dimensions), a reproducible code example, a pointer to the relevant code, and a suggested fix. The expected behavior is implied: sum with min_count should work for multiple dimensions. While there is some room for interpretation in the exact implementation, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4419": {
        "explanation": "The issue is well-specified. It provides clear minimal, complete, and verifiable code samples that reproduce the problem, both with and without coordinates. The current and expected outputs are shown explicitly, and the problem is described in detail: xr.concat changes the order of dimensions/coordinates, and the expectation is that the original order should be preserved. The environment and version information is also provided. There is no ambiguity about what the bug is or what the correct behavior should be, so a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4442": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible code sample, the expected behavior, the actual error message, and the environment details. The problem is well-scoped: to_dataframe/to_series fails when a DataArray has a stacked (multiindex) dimension and at least one other dimension, and the user expects a combined multiindex in the resulting pandas object. The error is shown, and the working case (single stacked dimension) is also provided for contrast. This is sufficient information to attempt a fix or at least to investigate the root cause and propose a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4510": {
        "explanation": "The issue provides a clear description of the problem: when using xarray's rolling operation followed by mean, the resulting DataArray loses its name and attributes, even when keep_attrs=True is specified. The user provides a reproducible code snippet, the expected behavior (attributes and name should be preserved), and the actual behavior (they are dropped). The only minor ambiguity is whether the fix should be in rolling, mean, or both, but the core requirement is clear: ensure name and attributes are preserved when keep_attrs=True. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4629": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, an explanation of the expected vs. actual behavior, and even points to the likely problematic line in the codebase with a suggested fix. The environment details are also included. This is sufficient information to attempt a meaningful solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4684": {
        "explanation": "The issue provides a clear description of the problem (loss of millisecond precision in datetime64 during IO roundtrip), a minimal reproducible example (including a data file and code), expected and actual outputs, and relevant environment/version information. The problem is well-motivated and the expected behavior is unambiguous. This is sufficient for a developer to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4687": {
        "explanation": "The issue is well-specified: it clearly describes the problem (xr.where drops DataArray attributes), provides minimal reproducible code samples, states the expected behavior (attributes should be preserved), and even suggests a possible solution direction (e.g., passing keep_attrs=True). The environment details are included, and the problem is described from both a user and developer perspective. There is no ambiguity about what is being requested: ensure that xr.where preserves attributes, or provide a way to control this behavior. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4695": {
        "explanation": "The issue provides a clear, minimal reproducible example that demonstrates the bug: naming a dimension 'method' in an xarray DataArray causes .loc to fail with a specific error, while other names do not. The expected behavior is also clear: the dimension name should not affect .loc, and the error should not occur. The error message and the user's analysis suggest a likely cause (name collision with a parameter in underlying code). The environment and versions are specified. This is sufficient information to attempt a fix and to verify it with the provided code sample.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4759": {
        "explanation": "The issue provides clear code samples that reproduce the problem, a detailed description of the unexpected behavior (coordinate dtype changing from <U1 or |S1 to object), and a clear statement of the expected behavior (dtype should not change). It also includes environment details and references a possibly related issue. The problem is well-scoped and actionable: ensure that coordinate dtypes are preserved when adding variables to a Dataset or when concatenating DataArrays. There is no ambiguity about what needs to be fixed or how to verify the solution (by running the provided code samples and checking dtypes).",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4767": {
        "explanation": "The issue provides a clear description of the inconsistency between DataArray and Dataset when using transpose with Ellipsis and a non-existent dimension. It includes reproducible code samples, the observed error, and a hint at where in the codebase the problem may lie. However, it does not specify the exact expected behavior (e.g., should DataArray accept non-existent dims with Ellipsis like Dataset, or should Dataset error out like DataArray?), nor does it clarify what the correct error message should be. There is a sensible interpretation: align DataArray's behavior with Dataset's, and improve the error message. Some blanks remain, but a meaningful attempt at a solution is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-4802": {
        "explanation": "The issue provides a clear description of the problem: decode_cf fails when scale_factor and add_offset are length-1 lists, whereas it works when they are numpy arrays. The error traceback is included, and the root cause is identified (attempting to call .item() on a list). The issue also explains how such a situation arises (via h5netcdf and zarr). While it doesn't prescribe the exact fix, it is clear that the code should handle length-1 lists for these attributes, as it does for numpy arrays. This is a well-specified bug report with a clear expected outcome: decode_cf should work when scale_factor/add_offset are length-1 lists.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4819": {
        "explanation": "The issue describes a clear problem: the inability to drop indices from a dimension that lacks coordinates using the current `drop_sel` method in xarray. The user proposes a specific solution: the addition of a `drop_isel` method that would work analogously to `drop_sel`, but using integer-based indexing (like `isel` vs `sel`). The desired behavior is well-motivated and the analogy to existing API methods is clear. While some implementation details (e.g., exact API signature) are not specified, the intent and expected functionality are unambiguous and can be reasonably inferred from the context and existing API patterns.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4879": {
        "explanation": "The issue provides a clear description of the observed bug, the expected behavior, and a minimal reproducible example. It specifies the context (Jupyter notebook, xarray, netcdf files), the sequence of actions that triggers the bug, and the environment details. The problem is well-scoped: repr caching stale data after file deletion and recreation. There is enough information to attempt a fix or at least to reproduce and investigate the issue meaningfully.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4911": {
        "explanation": "The issue provides a clear description of the problem: using sum (and by implication, prod) with min_count on a lazy xarray DataArray forces evaluation, which is unexpected. The user provides a minimal, complete, verifiable example that demonstrates the problem, and specifies both the observed and expected behavior. The issue also suggests two possible solutions: either fix the implementation to avoid forced evaluation, or document the limitation. This is sufficient information for a developer to attempt a solution, either by addressing the forced evaluation or updating the documentation accordingly.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4940": {
        "explanation": "The issue provides a clear description of the problem: calling Dataset.mean() in xarray drops coordinates, whereas calling mean() on a DataArray preserves them. The issue includes a minimal, complete, and verifiable example (MCVE) with code and output, and explicitly points out the undesired behavior. The expected behavior is implied: coordinates should be preserved after mean() on a Dataset, similar to DataArray. The environment and version information are also provided. While the issue does not explicitly state the desired output format, the intent is clear from the comparison with DataArray behavior. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4966": {
        "explanation": "The issue provides a clear description of the problem: xarray handles the _Unsigned=True convention for netCDF3 signed bytes, but does not symmetrically handle the _Unsigned=False hack used by OPeNDAP and netCDF-c for signed bytes. The issue includes references to the relevant code location, links to conventions and hacks, and a concrete example demonstrating the discrepancy between the netcdf4 and pydap engines. The expected behavior is implied: xarray should handle _Unsigned=False for unsigned byte variables in the same way it handles _Unsigned=True for signed byte variables, so that the pydap engine returns the correct signed values. The solution is to add symmetric handling at the referenced code location. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4994": {
        "explanation": "The issue is well-specified: it clearly describes the missing feature (the lack of a 'date' attribute in the datetime accessor), provides a minimal reproducible example, explains the expected behavior, and even suggests a concrete code change to implement the feature. The environment and context are also provided. There is no ambiguity about what is being requested or how to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5033": {
        "explanation": "The issue provides a clear description of the current limitation (only being able to register new backends via setup.py), the desired behavior (allowing users to pass a function directly as the engine parameter), and even a code example of the proposed usage. While it doesn't specify every detail of the implementation, the intent and expected outcome are clear enough for an experienced engineer to attempt a solution. There may be some minor design decisions to make (e.g., how to handle function signatures or error messages), but the core request is well-articulated.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5126": {
        "explanation": "The issue clearly describes the desired feature: an option to control whether the data part of the HTML repr in xarray notebooks is collapsed or expanded, similar to how xarray.set_options works. The motivation, context, and a suggested solution are provided. While there are some open questions (e.g., about the text repr), the main request is clear and actionable. The request for guidance on implementation details is not a blocker for making a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5131": {
        "explanation": "The issue provides a clear description of the problem (trailing whitespace in the first line of the DatasetGroupBy string representation), demonstrates it with code examples, and explains the impact (conflicting constraints between flake8 and doctest). However, the issue ends by asking if it is OK to remove the whitespace, and does not explicitly request a code change or confirm that such a change is desired by the maintainers. As a result, while the technical problem and its context are clear, the required action (removing the whitespace) is not explicitly authorized. Therefore, the issue is not fully well-specified, but a sensible interpretation would be to remove the trailing whitespace from the string representation. This makes it reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-5180": {
        "explanation": "The issue is very well-specified. It describes the observed behavior, the expected behavior, provides a minimal reproducible example, and even suggests a concrete fix (using calendar.lower() in the relevant check). The environment is fully specified, and the links to documentation and code are provided. There is no ambiguity about what needs to be changed or how to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5187": {
        "explanation": "The issue provides a clear description of the problem: the bfill method on xarray DataArrays with dask-backed arrays does not fill NaNs as expected when the chunk size is small (specifically, chunk size of 1). The issue includes a minimal reproducible example, the expected and actual outputs, and relevant environment information. The expected behavior is explicitly stated ('absence of nans'), and the code demonstrates the discrepancy. While the root cause is not specified, the problem and the desired outcome are clear, making it possible to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5233": {
        "explanation": "The issue provides a clear description of the problems faced (handling cftime and numpy time coordinates), and proposes specific features to address them (calendar querying, conversion, and generation). It references existing implementations in xclim, provides links to relevant functions, and discusses possible API designs. While some implementation details (e.g., exact API signatures, error handling) would need to be worked out, the core requirements and user stories are well-articulated. A developer could make a meaningful attempt at a solution based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5362": {
        "explanation": "The issue provides a clear description of the observed behavior (weighted operations do not raise an error when a non-existent dimension is passed), the expected behavior (it would be less confusing and more consistent if an error was raised, as with unweighted operations), and a minimal reproducible example. The environment is also specified. While the issue is phrased as a question and does not explicitly demand a change, it is clear that the user is requesting that weighted operations should raise an error when a non-existent dimension is specified, for consistency. This is a reasonable and actionable request.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-5365": {
        "explanation": "The issue is primarily a feature request and a question about whether the maintainers want to include a vector cross product function in the xarray namespace. While the user provides a sample implementation and example usage, the issue is not a direct request to implement the feature, but rather a solicitation of feedback about whether such a feature should be added. There is ambiguity about the desired API (function vs. method), naming, and whether it should be in xarray or another package. Therefore, it is not well-specified enough for a meaningful attempt at a solution without further clarification from maintainers.",
        "label": "VAGUE"
    },
    "pydata__xarray-5455": {
        "explanation": "The issue provides a clear description of the current behavior, the desired new behavior, and specific changes to the plugin protocol (such as always registering backend entrypoints, modifying guess_can_open and installed methods, and clarifying open_dataset error behavior). It also gives a concrete example of the improved error message. While there is a question at the end about whether the proposal is reasonable, the technical requirements are sufficiently clear to attempt an implementation as described.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5580": {
        "explanation": "The issue clearly describes a request to increase the default value of `display_max_rows` in `xr.set_options()`. The motivation, context, and even suggested new values (100, 1000, 10000) are provided. The affected functionality (console output, doctests) is explained. While the exact new value to use is left somewhat open, the request is clear: increase the default from 12 to a much higher value. This is a well-specified change request, and a PR could be made to update the default and add a note in the changelog.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5731": {
        "explanation": "The issue provides a clear description of the problem (NameError due to missing 'dask' import when dask is not installed), the expected behavior (xarray.corr should work without dask being present), and a minimal reproducible example. The stack trace and environment details are also included. The required fix is to ensure that xarray.corr (and its dependencies) do not require dask to be installed for non-dask-backed DataArrays. This is a well-specified bug report with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6386": {
        "explanation": "The issue provides a clear description of the bug, including what was attempted, what failed, the expected behavior, a minimal reproducible example, and relevant traceback. It also discusses the ambiguity in the documentation and the change in behavior after a specific PR (#5692). The reporter outlines two possible resolutions: either restore the previous behavior (accept DataArray return values) or clarify/document/enforce that only Dataset return values are allowed, with appropriate error messages and documentation updates. This gives a clear path for a solution, even if some design choice is left to the maintainer. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6394": {
        "explanation": "The issue provides a clear description of the problem: DataArray groupby with a mapped function returning a Dataset is broken, which previously worked. It includes a minimal reproducible example, the exact error traceback, references to related issues and PRs, and even a suggestion that the solution may be similar to a previous fix (#6386). The environment details are also provided. While the expected behavior is not explicitly stated, it is strongly implied (the code should work as it did before #5692, without error). This is sufficient for an experienced engineer to attempt a fix and validate it against the provided example.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6400": {
        "explanation": "The issue provides a clear description of the problem (poor performance of the HTML representation for large multi-indexed data arrays in xarray), a minimal reproducible example, timing information, and relevant environment details. It also references a previous related PR, which may help guide investigation. The expected outcome is improved performance for the HTML repr of such arrays. While it does not specify a target performance metric, the problem and context are clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6461": {
        "explanation": "The issue provides a clear description of the problem: using xr.where with a scalar as the second argument and keep_attrs=True raises an IndexError. The code snippet and error trace are provided, and the workaround is mentioned. While the expected behavior is not explicitly stated, it is implied that the operation should not fail and should handle scalars gracefully. The environment is specified. There is enough information to attempt a fix: ensure xr.where works with scalars as the second argument when keep_attrs=True, without raising an error.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-6548": {
        "explanation": "The issue provides a clear description of the problem: xr.polyval requires the first argument (the coordinate DataArray) to have a name attribute matching its dimension, otherwise a KeyError is raised. The user provides a minimal reproducible example, the expected behavior, and context about why this is a problem. The issue is actionable: either improve the documentation to clarify this requirement, or modify xr.polyval to handle unnamed DataArrays more gracefully. The environment and version information are also provided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6598": {
        "explanation": "The issue provides a clear description of the problem (xarray improperly decodes times from a NetCDF when the time variable is a uint), a minimal reproducible example (including a link to the CDL file to generate the NetCDF, and Python code to demonstrate the issue), expected and actual outputs, and relevant environment details. The only minor gap is that the actual CDL file is not included in the issue text, but it is linked and can be accessed. The problem and the expected behavior are both clearly described, making it possible to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6599": {
        "explanation": "The issue provides a clear description of the problem: `polyval` with timedelta64 coordinates produces incorrect results in the latest version compared to a previous stable version. It includes a minimal, complete, and verifiable example (MVCE) that demonstrates the bug, expected and actual outputs, and relevant environment details. The expected behavior is explicitly stated (results should match between versions). This is sufficient information for a developer to reproduce the issue, investigate the cause, and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6601": {
        "explanation": "The issue describes an observed change in the dimension order of the object returned by the `polyval` function in xarray between two versions. It provides a clear, reproducible code example and specifies the exact difference in output. However, the issue is phrased as a question about whether this is expected behavior and whether it should be documented, rather than as a request for a code change or bug fix. There is ambiguity about what action is required: should the dimension order be reverted, should documentation be updated, or is no action needed? Thus, while the technical details are clear, the required solution is not specified.",
        "label": "VAGUE"
    },
    "pydata__xarray-6721": {
        "explanation": "The issue provides a clear description of the problem: accessing the 'chunks' attribute on a zarr-backed xarray dataset appears to load the entire array into memory, which is unexpected. The expected behavior is also specified: accessing 'chunks' should only inspect the encoding, not trigger data loading. The issue includes a minimal code example, a detailed traceback, and relevant environment information. While it does not specify the exact implementation required, it is clear what the bug is and what the correct behavior should be, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6744": {
        "explanation": "The issue clearly describes a discrepancy between the behavior of the 'center' kwarg when using the built-in rolling mean versus when manually iterating over a DataArrayRolling object in xarray. It provides reproducible code samples, expected and actual outputs, and a clear question: either fix the manual iteration to respect the 'center' kwarg, or provide a way to achieve center-justified windows when iterating. The problem is well-scoped and actionable for someone familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6804": {
        "explanation": "The issue is more of a feature request and a discussion starter rather than a concrete, actionable task. It expresses interest in using PyTorch as a backend for xarray, lists some motivations, and asks for thoughts and suggestions on where to start. However, it does not specify what 'using PyTorch as a backend' would concretely entail, what the expected API or user experience should be, or what parts of xarray should be targeted. There is no clear definition of 'done' or success criteria, making it too vague for a meaningful engineering solution without further clarification.",
        "label": "VAGUE"
    },
    "pydata__xarray-6823": {
        "explanation": "The issue is well-specified: it provides a clear description of the problem (a RuntimeError when formatting a sparse-backed DataArray in an f-string after upgrading xarray), a minimal reproducible example, expected and actual behavior, relevant traceback, and environment details. The expected outcome is also clear: either the code should not error, or the breaking change should be documented. This is sufficient for a developer to attempt a fix or to update documentation as appropriate.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6882": {
        "explanation": "The issue provides a clear description of the problem: reading a NetCDF file with a zero-length time coordinate fails with a ValueError during CF datetime decoding in xarray. It includes a minimal working example (MWE) that reproduces the error, the full traceback, and the relevant NetCDF file structure. The expected behavior is implied: xarray should be able to open such files without error, even if the time coordinate is empty. The root cause (reduction operation on an empty array) is also evident. This is sufficient for a developer to attempt a fix (e.g., by handling empty arrays in the decoding logic).",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6889": {
        "explanation": "The issue provides a clear description of the problem, including what was attempted, what failed, and what was expected. It includes a minimal, complete, and verifiable example (MCVE) that reproduces the bug, as well as the full traceback and environment details. The expected behavior is also stated. The only ambiguity is the precise technical approach to fix the bug, but the problem and its reproduction are well-specified, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6938": {
        "explanation": "The issue is well-specified: it describes the unexpected mutating behavior of the .swap_dims() method in xarray, provides a minimal, complete, and verifiable example (MVCE) that demonstrates the problem, and clearly states the expected behavior (that the original object should not be modified). The environment and version information are also provided, which is helpful for reproducing and debugging the issue. There is no ambiguity about what is being reported or what a successful solution would look like (ensuring .swap_dims() does not mutate the original object).",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6971": {
        "explanation": "The issue is primarily a discussion prompt about whether to add a `.set_xindex()` method to the public API, referencing previous discussions and a branch where this was experimented with. It does not specify concrete requirements, expected behavior, or acceptance criteria for the method. There is no clear directive to implement, document, or test anything specific. The issue is open-ended and seeks feedback rather than action, making it ambiguous what a successful solution would be.",
        "label": "VAGUE"
    },
    "pydata__xarray-6992": {
        "explanation": "The issue provides a clear description of the problem: after a recent index refactor, the internal assumption that all coord_names are present in _variables is no longer valid, which causes errors (e.g., in the repr). The issue includes a minimal, complete, and verifiable example that reproduces the bug, a traceback, and a pointer to the relevant code. The expected behavior is also described. While the exact fix is not prescribed, the problem is well-scoped and actionable for someone familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6999": {
        "explanation": "The issue is well-described, with a clear bug report, expected behavior, and a minimal reproducible example. The user describes the current behavior (rename_vars allows a coordinate to become a dimension coordinate but does not create an index), the expected behavior (either raise an error or create the index), and provides code that demonstrates the problem. The two possible solutions are clearly outlined, so as a developer, I could implement either (1) or (2) as a fix. The environment and context are also provided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7003": {
        "explanation": "The issue provides a clear description of the problem: calling ds.indexes.get_unique() raises a TypeError due to an unhashable type ('MultiIndex'), while ds.xindexes.get_unique() works as expected. The error is reproducible, and the expected behavior is implied (get_unique() should not raise an error for pandas indexes). However, the issue does not specify what the correct return value should be for ds.indexes.get_unique() in the case of a MultiIndex, nor does it clarify if the behavior should match xindexes exactly. There is enough information to investigate and propose a fix, but some interpretation is required regarding the intended output.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-7089": {
        "explanation": "The issue clearly describes a specific behavior: calling Dataset.copy(deep=True) does not deepcopy the .attrs attribute, but the user expects that it should. The expected behavior is implied: when deep=True, .attrs should be deepcopied. The method and attribute in question are named, and the desired change is clear. While there could be some minor ambiguity about edge cases (e.g., what if .attrs contains non-deepcopyable objects?), the main requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7101": {
        "explanation": "The issue provides a clear description of the bug, including what was attempted, what was expected, and what actually happened. It includes a minimal, complete, and verifiable example that reproduces the problem, as well as a traceback and a proposed test that demonstrates the failure. The root cause is hypothesized (incorrect updating of _coord_names), and the relevant code location is referenced. All necessary context (environment, versions) is provided. This is sufficient for an experienced engineer to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7105": {
        "explanation": "The issue provides a clear description of the problem: a change in the behavior of groupby on a multi-indexed DataArray between two versions of xarray. It includes a minimal, complete, and verifiable example, the expected and actual outputs, and the relevant environment details. The desired behavior is unambiguous (grouping by the first level of the MultiIndex should aggregate all sub-levels under the top-level key). This is sufficient to attempt a fix or a regression test.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7147": {
        "explanation": "The issue is well-described: it identifies a specific problem (error message does not include the problematic variable's name), provides a clear example that reproduces the issue, and specifies the desired improvement (include the variable name in the error message). The context, expected behavior, and current behavior are all clear, making it straightforward to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7150": {
        "explanation": "The issue is well-specified: it provides a clear description of the problem, a minimal reproducible example, the expected and actual behavior, and relevant environment details. The error is reproducible and the user has isolated the difference between calling decode_compress_to_multiindex inside the backend vs. after loading. The goal is clear: make the custom backend work equivalently to the manual approach, so that multiindices are handled correctly when loading via open_dataset. There is enough information to attempt a meaningful solution, such as investigating how xarray processes datasets returned by backends and why the error occurs when the multiindex is present at that stage.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7203": {
        "explanation": "The issue is well-specified: it describes the current behavior (repr loading all data for 'small' variables, causing slowdowns), the expected behavior (repr should be fast and avoid loading data), and provides a minimal, complete, and verifiable example. It even points to the relevant code section and describes a workaround that achieves the desired performance. The environment and context are also provided. The only minor ambiguity is the precise definition of 'avoid loading any data' (e.g., should all data access be lazy, or is some metadata access allowed?), but the intent is clear enough to proceed with a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7229": {
        "explanation": "The issue provides a clear description of the bug: when using xr.where(..., keep_attrs=True), coordinate attributes are being overwritten by variable attributes, which is not the expected behavior. The expected outcome is explicitly stated (coordinate attributes should be preserved). A minimal, complete, and verifiable example is provided, along with the actual and expected outputs. The environment details are included, and the issue references the relevant code change that likely introduced the bug. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7233": {
        "explanation": "The issue clearly describes the unexpected behavior (non-dimensional coordinates are demoted to variables after using ds.Coarsen.construct), the expected behavior (all variables that were coordinates before should remain coordinates after), and provides a minimal, complete, and verifiable example that demonstrates the problem. The environment and context are also specified. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7347": {
        "explanation": "The issue provides a clear description of the problem: when using assign_coords(), all dimension coordinates are reset to the default (pandas) index, but the expected behavior is that only updated or dropped coordinates should change, and others should be preserved. The issue references a specific example in another GitHub issue, which is stated to be a minimal, complete, and verifiable example (MVCE). The checkboxes confirm that the example is focused, self-contained, and verifiable. While the example itself is not included in the text, it is directly linked and confirmed to exist. This makes the issue reasonably specified, as a developer with access to the codebase and the linked example can understand and attempt a solution, though some interpretation is required from the external reference.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-7391": {
        "explanation": "The issue clearly describes the problem: when performing binary operations on xarray Datasets, the global attributes are not preserved even when the 'keep_attrs=True' option is set. It provides a minimal reproducible example, specifies the expected behavior (Datasets should keep attributes like DataArrays/Variables do), and suggests the direction for a fix. The requirements for a successful solution are clear: ensure Dataset binary operations respect the 'keep_attrs' option. No significant ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7393": {
        "explanation": "The issue is well-specified: it clearly describes the observed behavior (int32 coordinate is cast to int64 after stacking), the expected behavior (the dtype should not change), and provides a minimal, complete, and verifiable code example that demonstrates the problem. The environment is also specified, and the request is unambiguous: ensure that stacking does not upcast the dtype of the coordinate. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7400": {
        "explanation": "The issue provides a clear description of the required change: when concatenating two datasets with disjoint variables, instead of raising an exception, the code should add the missing variable(s) with np.nan as the default value. The checklist indicates that tests, formatting, and documentation are expected. The reference to issue #508 suggests further context, but the main requirement is explicit enough to proceed with a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4330": {
        "explanation": "The issue clearly describes the problem (spellchecker incorrectly flags code in comments/docstrings flanked by double backticks), the desired behavior (ignore words flanked by double backticks), and even hints at a possible implementation approach (add a filter similar to existing ones for Sphinx directives). The scope is clear, and the success criteria are well-defined: after the change, the spellchecker should not flag words inside double backticks, but should still flag them elsewhere. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4339": {
        "explanation": "The issue provides a clear motivation (outputting Pylint results to a single file when running under tox on CI), references prior related issues and removed features, and gives concrete examples of the desired outcome (single file output, similar to flake8's support). However, it does not specify exactly how the feature should be implemented (e.g., should it be a new command-line flag, should it support all output formats, etc.), nor does it specify edge cases or configuration details. Still, the core requirement\u2014supporting output to a single file when running under tox, without relying on shell redirection\u2014is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-4398": {
        "explanation": "The issue is reasonably well-described: it explains the current behavior, the problem encountered, and two possible solutions. It is clear that the goal is to add an option to pylint so that the process fails (returns a non-zero exit code) if specific issue types or codes are found, regardless of the overall score or the --fail-under option. However, there is some ambiguity about the exact desired behavior when both --fail-under and the new option are used together, and about the precise format of the new option (e.g., should it accept categories, message IDs, or both?). Still, a sensible interpretation is possible and a meaningful solution can be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-4421": {
        "explanation": "The issue provides a clear description of the problem: after upgrading to Pylint 2.8.2, the pylint-quotes plugin throws a TypeError due to a version comparison between a tuple and a string. The steps to reproduce are detailed, including a requirements file, a sample file, and the exact command to run. The expected behavior is also specified: Pylint should not crash but instead report on quote usage and variable naming. The probable cause is identified, and a link to the problematic line in the code is provided. This is sufficient information for an experienced engineer to attempt a fix, likely by updating the version comparison logic in pylint-quotes.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4516": {
        "explanation": "The issue provides clear steps to reproduce the problem, describes both the current and expected behavior, and specifies the relevant command-line options and their expected effects. It also includes version information and a concrete example of the bug. The desired outcome (that the --ignore and --ignore-patterns options should cause Pylint to skip files in specified directories, regardless of directory depth) is explicit. This is sufficient for a developer to attempt a fix and to know when the issue is resolved.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4551": {
        "explanation": "The issue clearly describes the current limitation (pyreverse does not use Python type hints for UML generation), provides a code example, shows the current output, and specifies the expected behavior (type hints like 'a : String' should appear in the UML output). The context (pyreverse, PEP 484 type hints) and the desired outcome are both clear. This is sufficient for a developer to attempt a solution, such as modifying pyreverse to parse and display type hints in the UML diagrams.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4604": {
        "explanation": "The issue provides a clear code sample that reproduces the problem, specifies the current and expected behavior, and identifies the relevant linter warning. It is clear that the goal is to prevent the 'unused-import' warning for modules used in type comments. The context (pylint version, Python version) is also provided. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4661": {
        "explanation": "The issue clearly describes the problem: pylint is storing data in a .pylint.d directory in the user's home folder, which is not compliant with the XDG Base Directory Specification. The issue provides a link to the specification and explains which directories should be used for data, cache, and config files. The required change is to update pylint so that it stores its data files in $HOME/.local/share/pylint (or the appropriate XDG_DATA_HOME location), rather than directly in the home directory. While it doesn't specify every file or code location, the intent and the standard to follow are clear, making it possible to implement a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4970": {
        "explanation": "The issue clearly describes the current behavior (setting min-similarity-lines=0 causes every line to be flagged as duplicate) and the desired behavior (setting it to 0 should disable the duplicate code check entirely). It also provides context by referencing similar behavior in other tools and a related open issue. The required change is unambiguous: update the logic so that a value of 0 disables the check. This is a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-5136": {
        "explanation": "The issue provides a clear description of the current problem (typing issues due to the mixin's relationship with PyLinter), the location of the relevant code, a concrete example of the problem (the add_message method), and a proposed solution (merging MessagesHandlerMixIn into PyLinter). It also mentions an alternative solution and provides a branch to reproduce the typing errors. This is sufficient information for an experienced engineer to attempt the proposed refactor and resolve the typing issues.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-5446": {
        "explanation": "The issue describes a desire to be able to disable the 'duplicate-code' (R0801) check in Pylint for portions of a file, similar to how other checks can be enabled/disabled with inline comments. However, it does not specify what the expected behavior should be, how the disabling should work, or what constitutes a 'portion' (e.g., function, class, arbitrary lines). There is also no information about edge cases, user interface, or acceptance criteria. While the general intent is clear, the specifics are not, leaving significant ambiguity in implementation.",
        "label": "VAGUE"
    },
    "pylint-dev__pylint-5595": {
        "explanation": "The issue raises a concern about the user experience of negative scores in pylint, especially for beginners, and suggests that the scoring system could be improved. However, it does not specify what the new behavior should be, nor does it provide concrete requirements or acceptance criteria. It also suggests looking at other tools for inspiration, but does not mandate any particular approach. As such, there is significant ambiguity about what a successful solution would look like, and multiple interpretations are possible.",
        "label": "VAGUE"
    },
    "pylint-dev__pylint-5613": {
        "explanation": "The issue clearly describes the current workaround required to use pyreverse programmatically and provides a concrete example of the desired API. The request is to provide a more convenient, Pythonic entry point for pyreverse, ideally as a function that can be called with parameters instead of manipulating sys.argv and handling SystemExit. The requirements are clear, and the expected outcome is well illustrated by the code snippet. There is little ambiguity about what is being asked.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-5839": {
        "explanation": "The issue clearly describes the problem: currently, Pylint allows reusing message IDs (msgid) or symbols that were previously deleted, which can cause confusion and bugs. The expected behavior is also clearly stated: it should be impossible to reuse an old message id or symbol. While the issue does not specify the exact implementation details (e.g., where to store the list of deleted IDs, how to enforce the check), it is clear what the end result should be. As an experienced engineer with access to the codebase, I can reasonably infer the steps needed: maintain a list of deleted msgids/symbols and prevent their reuse. Thus, the issue is reasonably specified, with some blanks to fill in regarding implementation details.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-6059": {
        "explanation": "The issue clearly describes a code location (`BaseChecker.__gt__`) and provides two possible actions: add a unit test if the method is required, or remove the method if it is not. However, it does not specify how to determine if the method is required, nor does it provide any context about its usage or intended purpose. The decision of whether to keep or remove the method is left to the implementer without clear criteria. This leaves room for ambiguity, as a contributor would have to investigate the codebase and possibly make assumptions about project requirements or conventions. Therefore, the issue is not well-specified enough for a straightforward solution.",
        "label": "VAGUE"
    },
    "pylint-dev__pylint-6196": {
        "explanation": "The issue is primarily a request for discussion and consensus on the intended use, naming, and best practices for the @check_messages decorator, rather than a concrete request for a code or documentation change. While it describes the current problems and some desired clarifications, it does not specify actionable steps or acceptance criteria for a PR, except that a PR could extend documentation and possibly make code changes after consensus is reached. As such, it is not well-specified enough for a meaningful attempt at a solution, since the required outcome depends on decisions and clarifications that have not yet been made.",
        "label": "VAGUE"
    },
    "pylint-dev__pylint-6386": {
        "explanation": "The issue clearly describes the problem: the short '-v' option for 'verbose' incorrectly expects an argument, while the long '--verbose' option does not. The expected behavior is that both options should behave the same (not require an argument). The issue provides the command used, the output, and the expected behavior, making it clear what needs to be fixed in the codebase (the argument parsing for the short '-v' option).",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6412": {
        "explanation": "The issue clearly describes the current mechanism (using __implements__) and proposes a concrete change: refactor the interfaces to use ABCMeta and update PyLinter to use isinstance checks for categorization, while maintaining backward compatibility. The steps are actionable and the expected outcome is clear, making it possible to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6506": {
        "explanation": "The issue clearly describes the problem: when an unrecognized option is passed to pylint, a traceback is printed, which is not user-friendly. The expected behavior is also specified: instead of a traceback, a user-friendly error message (similar to mypy's behavior) should be shown. The example output and the context are sufficient to understand what needs to be changed in the codebase. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6517": {
        "explanation": "The issue provides a clear description of the bug: when 'confidence=' is present (and empty) in the pylintrc file, pylint runs unexpectedly pass (i.e., do not report expected messages). The issue includes a minimal code sample, the relevant configuration, the command used, the actual output, and the expected output. The context about the default pylintrc and a reference to a related fix in pylint's own config is also given. This is sufficient to attempt a fix: ensure that an empty 'confidence=' in pylintrc does not cause pylint to skip messages or otherwise behave incorrectly.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6526": {
        "explanation": "The issue is a question about the behavior of pylint regarding stat file locations, not a request for a code change or a bug fix. It asks for an explanation or documentation about how pylint determines where to write stat files, especially when run in a certain way. There is no actionable change or feature to implement, nor a clear definition of what a 'successful solution' would be in terms of a pull request. Therefore, it is not well-specified enough for a meaningful attempt at a code solution.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "pylint-dev__pylint-6528": {
        "explanation": "The issue provides a clear description of the bug: Pylint's recursive mode does not respect the ignore options (`--ignore`, `--ignore-paths`, `--ignore-patterns`). It includes specific commands, expected vs. actual output, and references to the relevant documentation. The expected behavior is well-defined, and the problem is reproducible with the provided information. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6556": {
        "explanation": "The issue provides a clear description of the problem: the user wants to disable a specific pylint warning ('bad-option-value') that occurs when using a warning code not recognized by older pylint versions. The steps to reproduce, current behavior, and expected behavior are all described. However, the issue does not specify whether the solution should be a code change, a documentation update, or a workaround script. There is some ambiguity about the scope of the solution (e.g., should it be a change to pylint itself, or a recommended practice for users?). Still, a sensible interpretation would be to make pylint accept 'disable=bad-option-value' as a valid way to suppress this error, or to provide a mechanism for cross-version compatibility. Thus, the issue is reasonably specified, but not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-6820": {
        "explanation": "The issue clearly describes the problem: using the incorrect CLI argument '--load-plugin' instead of the correct '--load-plugins' causes the plugin not to load, and there is no warning or error message. The expected behavior is also specified: the user should receive a warning that 'load-plugin' is not the right argument. The context, commands used, observed and expected outputs, and version are all provided. This is sufficient information to attempt a solution, such as adding argument validation and user feedback.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6903": {
        "explanation": "The issue provides a clear description of the bug: running pylint with --jobs=0 in a Kubernetes pod leads to a crash because the CPU count is calculated as 0, which is invalid for multiprocessing. The relevant code locations are linked, the cause is explained (integer division resulting in 0), and a suggested fix (ensuring the value is at least 1) is proposed. The expected behavior is also specified. There is enough information to attempt a solution, such as modifying the CPU count calculation to never return less than 1. Some minor ambiguity exists about whether other code paths could also result in 0, but the main fix is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-7080": {
        "explanation": "The issue is well-specified. It clearly describes the problem: when running 'pylint --recursive=y src/', the 'ignore-paths' setting in pyproject.toml is ignored, resulting in files under 'src/gen/' being linted when they should not be. The configuration, command used, actual output, expected behavior, and environment details are all provided. The expected outcome is unambiguous: files matching 'src/gen/*' should not be checked. This is sufficient information to attempt a fix or investigation in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7114": {
        "explanation": "The issue provides a clear description of the problem, including steps to reproduce, current and expected behavior, and relevant version information. The root cause is described (pylint fails when a module contains a file with the same name as the module directory and no __init__.py is present), and the expected outcome is specified (pylint should succeed in this scenario). There is enough information to investigate and attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7228": {
        "explanation": "The issue provides a clear bug report: including '\\p{Han}' in a regex in the .pylintrc file causes Pylint to crash with a 'bad escape \\p' error. The configuration, command, output, and expected behavior are all specified. The root cause is that Python's 're' module does not support Unicode property escapes like '\\p{Han}'. The issue is well-specified: a successful solution would involve either supporting such escapes or providing a clear error message. The only minor ambiguity is whether the user expects full support for '\\p{Han}' or just a better error, but the expectation of 'not throw error' is clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7277": {
        "explanation": "The issue provides a clear description of the bug (pylint removes the first item from sys.path unconditionally when run via runpy), points to the exact line of code responsible, and specifies the desired behavior: only remove the first item if it is \"\", \".\", or os.getcwd(). It also provides a reproducible code snippet and explains the impact. The expected behavior is unambiguous, and the solution is well-scoped.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7993": {
        "explanation": "The issue provides a clear description of the problem: a message template using double braces (e.g., '{{ \"Category\": \"{category}\" }}') worked in an older version of pylint but fails in a newer version, with specific error messages and output included. The expected behavior is also described: the template should print a dictionary-like string with the category. The issue includes the exact command used, the output from both versions, and the relevant environment details. While it does not specify the exact root cause or desired implementation, it is clear what the user expects to work and what currently does not, making it possible to attempt a meaningful solution (e.g., restoring support for such templates or clarifying/documenting the change).",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8124": {
        "explanation": "The issue is well-described: it explains the context (explicit re-exports for mypy compatibility), the current behavior (pylint flags 'useless-import-alias' for these cases), and the expected behavior (no such error should be flagged for this pattern). It provides a concrete code example, a real-world reference, and the relevant linter output. The desired change is clear: update pylint so that it does not flag 'useless-import-alias' for explicit re-exports where the alias is identical to the original name, as this is a valid pattern for mypy compatibility. There is enough information to attempt a solution, such as modifying the pylint checker to recognize and allow this pattern.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8281": {
        "explanation": "The issue clearly describes the current pain point (having to list every source root individually) and the desired solution (supporting globbing patterns like 'src/*' in the --source-roots argument). It also specifies a preference for globbing over regex. While it doesn't specify every edge case or the exact syntax to support, the core requirement is clear: allow users to specify source roots using glob patterns. This is enough to make a meaningful attempt at a solution, though some implementation details may need to be decided by the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-8312": {
        "explanation": "The issue clearly describes the desired feature: adding globbing support for input specifications in pylint, specifically referencing a command-line example and the exact line in the codebase where the change should be made. The context is sufficient to understand what is required (supporting patterns like 'packages/*/src' in the CLI), and the links provide further background. While some implementation details are left to the developer, the goal and scope are clear.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8799": {
        "explanation": "The issue describes a performance problem: running 'pylint test.py --disable=all' is slow, even on an empty file. The user expects that if all checks are disabled, pylint should 'short circuit' and exit quickly. While the issue is clear about the problem (slow execution with all checks disabled) and the expected behavior (fast exit), it does not specify exactly how to implement the solution (e.g., where in the code to check for this, or what constitutes 'all checks disabled'). However, as an experienced engineer, it is reasonable to interpret that the solution should detect when all checks are disabled and exit early. Thus, the issue is reasonably specified, though not fully detailed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-8819": {
        "explanation": "The issue clearly describes a problem: in the rcfile, the order of 'enable' and 'disable' options affects the resulting configuration, which is counter-intuitive and inconsistent with user expectations. The user provides concrete examples and a desired outcome: specifying 'disable=fixme' and 'enable=all' in the rcfile should result in all checks enabled except 'fixme', regardless of their order. The issue is well-specified in terms of the problem, the current behavior, and the expected behavior, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8898": {
        "explanation": "The issue is well-described: it explains the bug (commas in regexes for bad-name-rgxs are incorrectly split, causing crashes), provides a minimal configuration to reproduce, the command used, the full stack trace, and a clear expected behavior (support valid regexes with commas, or provide a way to escape commas). The desired outcome is clear: allow valid regexes with commas in this config option, either by changing the parsing logic or supporting escaping. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8929": {
        "explanation": "The issue provides a clear description of the problem: when exporting pylint results to JSON format, the score is not included in the output, even when the --score=y option is specified. The steps to reproduce, current behavior, and expected behavior are all clearly outlined. It is unambiguous what needs to be changed: ensure that the score is included in the JSON output when --score=y is used. The only minor ambiguity is the exact structure of the JSON output with the score included, but it is reasonable to infer that the score should be present in the output, perhaps as an additional field or object. This is a sensible and actionable request.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-10051": {
        "explanation": "The issue provides a clear description of the problem: after calling caplog.clear(), caplog.get_records() becomes decoupled from caplog.records, leading to inconsistent behavior. It includes a detailed explanation of the root cause (records list being replaced rather than cleared), a reproducible example, and references to the relevant code. The expected behavior is implied: caplog.get_records() should remain consistent with caplog.records even after caplog.clear() is called. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10081": {
        "explanation": "The issue provides a clear and detailed description of the bug, including the expected and actual behavior, a minimal reproducible example, environment details, and references to a related issue. The problem is well-scoped: when running pytest with --pdb, tearDown() is executed for unittest.TestCase classes decorated with unittest.skip at the class level, which should not happen. The minimal example and output logs make it clear what is wrong and what the fix should address. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10115": {
        "explanation": "The issue clearly states the motivation (atomicwrites is unmaintained and its maintainer's actions have made it untrustworthy as a dependency), and the desired outcome: to stop depending on atomicwrites. It also provides context that atomicwrites is only used on Windows, is MIT-licensed, and is relatively simple. The issue even suggests a possible solution: copying the necessary parts into pytest. While it leaves some implementation details open (e.g., whether to vendor the code or reimplement it, and exactly which parts are needed), the core requirement\u2014removing the dependency on atomicwrites\u2014is clear and actionable. Thus, a meaningful attempt at a solution is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-10343": {
        "explanation": "The issue clearly describes a problem: a deprecation warning does not provide enough information to identify the source of the deprecated usage. The user requests that the warning message should include either the plugin name or the Python file where the deprecated hook implementation is found. The expected outcome is clear: modify the warning so it includes location information to help users identify the culprit. The technical context (pytest, hookimpl, deprecation warning) is specific enough to guide a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10356": {
        "explanation": "The issue provides a clear description of the problem: when using pytest markers on multiple base classes, inheriting from both causes one set of markers to be lost due to MRO resolution. The expected behavior is also clearly stated: the derived class should have both markers. A minimal reproducible example is provided, and the desired outcome is unambiguous. The issue also notes compatibility considerations for Python 2/3. While there is some room for interpretation in how to deduplicate or merge markers, the core requirement (ensure all base class markers are present on the derived class) is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10442": {
        "explanation": "The issue provides a clear high-level goal: change pytest's behavior so that only temporary directories for failed tests are kept, rather than for all tests, to save disk space. It also specifies that the current behavior of keeping the last 3 root temporary directories should remain unchanged. However, the issue references an external comment for the 'full proposal,' which may contain important implementation details or edge cases. Without the content of that referenced comment, there is some ambiguity about the exact requirements and edge cases. Still, the main intent and direction are clear enough for a reasonable attempt at a solution, assuming sensible defaults for any unspecified details.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-10552": {
        "explanation": "The issue provides a clear description of the problem: pytest's inconsistent discovery and running of staticmethod/classmethod tests, with specific examples and observed behaviors (warnings, silent failures). The user explicitly states the desired outcome: at minimum, pytest should emit a warning or error when such tests are not collected, rather than failing silently. The code samples, pytest output, and environment details are all provided, making it clear what needs to be addressed and how to reproduce the issue. The only minor ambiguity is whether the user wants the tests to be runnable or just a warning, but the request for a warning/error is explicit and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10758": {
        "explanation": "The issue describes a test function using the walrus operator and notes a difference in behavior between running in a Python terminal and with PyTest. However, it does not specify what the expected behavior is, what the actual outputs are, or what the desired resolution should be (e.g., is the code wrong, is PyTest wrong, is there a bug in numpy, or is it a misunderstanding of the walrus operator?). There is ambiguity about what the 'solution' should be: fix the test, fix the code, document the behavior, or something else. Without further clarification, it is unclear what a successful solution would look like.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-10893": {
        "explanation": "The issue description actually contains two separate issues: one about the deprecation of the 'onerror' parameter in 'rmtree' and another about the new 'addDuration' API in Python 3.12's test results. Both are described with context, links to relevant code and upstream issues, and the impact is explained (warnings as errors causing test failures). However, the issue does not specify which of these should be addressed, or if both, and does not specify the desired behavior (e.g., should pytest support both old and new Python versions, how to handle the deprecation, etc.). There is enough information to make a reasonable attempt at a solution for each, but some decisions would need to be made by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-10988": {
        "explanation": "The issue is well-specified. It provides a clear description of the bug (pytest crashes with OSError when a custom flag value is too long and is interpreted as a file path), includes a minimal reproducible example, relevant environment details (OS, Python, pytest version, pip list), and even points to the problematic code section in pytest. The expected behavior is also described (should not crash, should handle the error gracefully). The suggested fix (wrap the anchor.exists() call in a try/except) is clear and actionable. There is no ambiguity about what needs to be fixed or how to reproduce the problem.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11041": {
        "explanation": "The issue provides a clear description of the observed bug: a specific UnboundLocalError occurs in pytest 7.3.x but not in 7.2.x, when using the walrus operator in an assert statement. The code sample is provided, the error message is shown, the regression is identified, and a related PR is referenced. The environment (Python 3.11) is specified. This is sufficient information to reproduce the issue and investigate a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11044": {
        "explanation": "The issue is reasonably clear: when the 'testpaths' configuration option is set but does not match any files or folders, pytest should emit a warning (or possibly an error). The main ambiguity is whether it should be a warning or an error, but the issue leans towards a warning as the minimum, with an error as a possible stricter alternative. The behavior to implement is clear, and the context is well-explained. There is some room for interpretation (e.g., how exactly to word the warning, or whether to make it configurable), but the core requirement is actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-11047": {
        "explanation": "The issue is well-specified. It clearly describes the current behavior (pytest does not process the strftime %f specifier in log date formats), the desired behavior (pytest should support %f for microsecond precision in all --*-date-format options), and provides concrete examples, motivation, and even discusses workarounds. The scope is clear: update pytest's logging date formatting to support %f in the same way as other strftime specifiers. There is no ambiguity about what is being requested or how success would be measured (pytest log output should show sub-second precision when %f is used).",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11125": {
        "explanation": "The issue provides a detailed description of the problem, including the project structure, relevant configuration, the exact error message, and the context of when the error started occurring (after upgrading to pytest 7.3.2). However, the issue is primarily a question: is this a bug in pytest or a misconfiguration in the user's setup? There is no explicit request for a code change or a specific outcome, nor is there a clear acceptance criterion for what a 'solution' would be. The issue is more of a request for guidance or clarification rather than a concrete task to implement. Therefore, while the technical details are thorough, the actionable requirement is ambiguous.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-11143": {
        "explanation": "The issue provides a clear description of the bug: when the first expression in a file is a number, it is mistakenly treated as a docstring, leading to a TypeError during pytest's assertion rewriting. The error trace is included, showing exactly where the failure occurs. The expected behavior is implied: the code should not treat a number as a docstring. While the issue does not specify the exact code to change, it is clear what needs to be fixed and where. Therefore, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11160": {
        "explanation": "The issue is well-specified: it describes the current buggy behavior, provides a minimal reproducible example, points to the relevant code, and proposes a concrete fix. The expected and actual behaviors are clearly described, and the test case demonstrates the problem. This is sufficient to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5205": {
        "explanation": "The issue provides a clear description of the problem: pytest plugins using `record-property` generate JUnit XML reports with <properties> tags inside <testcase> elements, which is not compliant with the JUnit XML schema (the <properties> tag should be inside <testsuite>). It includes references to relevant standards, links to related issues, a minimal reproduction, and the expected correct structure. The required change is to ensure that <properties> are placed correctly in the generated XML. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5262": {
        "explanation": "The issue provides a clear and detailed description of the bug, including the root cause (EncodedFile's mode incorrectly includes 'b'), the observed exception, a minimal reproducible example, environment details, and the expected behavior (EncodedFile should not advertise binary mode if it only accepts str). The steps to reproduce and the context are all present, making it clear what needs to be fixed and how to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5281": {
        "explanation": "The issue provides a clear error traceback, identifies the problematic line (a unicode literal u\"\\uD83D\" in _pytest/terminal.py), and describes the environment (Jython 2.7.1, pytest 4.5.0). The reporter also suggests a possible fix (changing the unicode literal to a str literal) and asks if that would be appropriate. However, there is some ambiguity: the reporter is unsure whether the variable is bytes or unicode, and is not certain about the correct fix. While the core problem is clear (unicode handling in Jython), the exact requirements for the fix are not fully specified, and some investigation/interpretation is needed to determine the best solution. Thus, the issue is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5356": {
        "explanation": "The issue provides a description of the problem (tests being skipped in pytest 4.6.0), the environment, and a link to a CI job with logs and environment details. However, it does not provide a minimal reproducible example directly in the issue, nor does it specify which tests are skipped or any relevant configuration. The linked log may contain enough information for an experienced engineer to investigate, but there is some ambiguity and extra effort required to extract the necessary details. Thus, while a sensible interpretation is possible, the issue is not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5404": {
        "explanation": "The issue provides a detailed description of the error, including the exact error message, the context in which it occurs (using tox and pytest, importing from 'sty'), the relevant environment information (Python, pytest, OS versions), and a minimal working example (MWE) via a public GitHub repository. The steps to reproduce are clear: clone the repo and run tox or pytest. The error is reproducible and the conditions are well-specified. This is sufficient information for a developer to attempt to reproduce and debug the issue, making it well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5413": {
        "explanation": "The issue provides a clear description of the observed behavior (difference between str() on a caught exception and str() on the pytest.raises context variable), includes code samples, and asks if the behavior could be changed to eliminate the 'gotcha'. However, it does not specify exactly what the desired new behavior should be (e.g., should str(e) on the context variable return str(e.value)?), nor does it discuss potential backward compatibility or design considerations. There is a sensible interpretation (make str(e) on the context variable behave like str(e.value)), but some blanks remain about the exact requirements and acceptance criteria.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5479": {
        "explanation": "The issue provides a code snippet that demonstrates a failure when using pytest.raises with a unicode match string in Python 2. The error is a UnicodeEncodeError, and the context is clear: the user wants to be able to use unicode strings in the 'match' argument of pytest.raises in Python 2. The expected behavior is implied: pytest.raises should not crash with UnicodeEncodeError in this scenario. While the issue does not specify the exact fix, it is clear what the problem is and what outcome is desired (support unicode match in pytest.raises on Python 2). Thus, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5495": {
        "explanation": "The issue describes a confusing output message when comparing byte strings in assertion rewriting, providing concrete examples. However, it does not specify what the expected behavior or output should be, nor does it suggest a clear direction for a solution. The author explicitly states uncertainty about what should or could be done. This leaves significant ambiguity about what a successful solution would look like.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-5550": {
        "explanation": "The issue describes that when using pytest with the option junit_family=xunit2, the generated XML report does not contain a root <testsuites> tag, but instead uses the legacy format with a single <testsuite> root. The issue provides a sample command, sample output, and the expected difference (presence of <testsuites> root). However, it does not explicitly state what the desired output should be (e.g., should the xunit2 format have a <testsuites> root, or is the user expecting a different format?). There is some ambiguity about whether the problem is with pytest's implementation, the user's expectation, or a misunderstanding of the xunit2 format specification. While a reasonable interpretation is possible (modify the output to wrap <testsuite> in <testsuites>), there is room for ambiguity about the correct behavior according to the xunit2 spec.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5555": {
        "explanation": "The issue provides a concrete example (test file, pytest.ini, and command line) and demonstrates the observed behavior (xfail with strict mode and stepwise). However, it does not clearly state what the expected behavior is, nor does it describe what 'doesn't work' means in this context. Is the user expecting stepwise to stop after the first xfail strict failure? Should xfail strict failures be treated differently by stepwise? The issue is reproducible, but the desired outcome is not specified, so there is ambiguity about what a successful solution would look like.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-5559": {
        "explanation": "The issue description contains two unrelated topics: (1) a problem with pytest's stepwise and xfail strict failures, demonstrated with a code sample and output, and (2) a complaint about the recommendation to use pytest-runner in setup_requires, with links to related discussions. It is unclear which of these is the actual issue to be solved, or if both are. There is no explicit statement of the desired outcome or what a successful solution would look like. The presence of two distinct topics in one issue adds to the ambiguity.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-5631": {
        "explanation": "The issue provides a clear description of the problem, including the context (using pytest with unittest.mock.patch where 'new' is a numpy array), the error message, the version difference, and the suspected cause (a specific commit). It also explains why the error occurs (the use of 'in' with a numpy array). While it doesn't specify the exact fix, it is clear what needs to be addressed: ensure that the check 'p.new in sentinels' does not raise a ValueError when 'p.new' is a numpy array. This is enough information for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5692": {
        "explanation": "The issue clearly describes the missing properties (`hostname` and `timestamp`) in the generated JUnit XML reports from pytest. It provides concrete examples of both the current output and the desired output, specifying exactly which XML attributes are missing and where they should appear. The request is to add these properties to the <testsuite> element in the generated XML. This is a well-scoped and actionable enhancement request.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5787": {
        "explanation": "The issue clearly describes the problem: when running tests with pytest-xdist, chained exceptions are not fully serialized and displayed, whereas they are when running without xdist. The issue provides concrete test cases, expected output, actual output, and the relevant environment details. The desired outcome is implicit but clear: exception serialization in xdist should include chained exceptions, matching the behavior of non-xdist runs. This is a well-specified issue with a clear path to a solution (fixing xdist's exception serialization to include chained exceptions).",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5809": {
        "explanation": "The issue clearly describes a bug with the --pastebin feature when using the 'python3' lexer, provides a reproducible example, and suggests a concrete solution (change the lexer to 'text'). The context, affected code, and rationale for the change are all provided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5840": {
        "explanation": "The issue provides a concrete error message and a clear regression: after upgrading from pytest 5.1.1 to 5.1.2 on Windows, the path to conftest.py is converted to lower case, leading to an ImportError. However, it does not specify what the expected behavior should be (should the path casing be preserved? Should the import succeed regardless of casing?), nor does it provide details about the project structure or any relevant configuration. While the problem is described, the desired outcome and the context are not fully clear, leaving room for interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5980": {
        "explanation": "The issue provides useful context and motivation for replacing the deprecated --result-log option in pytest, and suggests that the new solution should produce a line-based log file, possibly with each line as a JSON object. It also mentions the existing resultlog.py plugin as a starting point. However, the issue is primarily a request for discussion and design input, not a concrete implementation ticket. There are open questions about the exact format, required fields, and whether replay functionality is in scope. As such, while there is a sensible direction, the requirements are not fully specified for a direct implementation without further clarification.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-6116": {
        "explanation": "The issue describes a desire for a single-character shortcut for the --collect-only CLI argument in pytest, but explicitly states that the character to use has not been decided and is subject to community feedback. The issue is more of a feature proposal and a call for discussion rather than a concrete request for implementation. There is no actionable specification for what the shortcut should be, so it is not possible to implement a solution without further clarification.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "pytest-dev__pytest-6186": {
        "explanation": "The issue clearly states that a deprecation warning should be shown if the user has not configured the 'junit_family' setting. It references a specific configuration key and a related issue number (#6178), which likely contains additional context. The requirement is straightforward: detect the absence of this configuration and display a warning. The expected behavior is clear, and the scope is well-defined for a software engineer familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-6197": {
        "explanation": "The issue provides a clear description of the regression: pytest 5.2.3 is collecting and importing __init__.py files that it did not in 5.2.2, leading to errors. It includes a minimal reproducible example using tox, specifies the environment, and shows the difference in behavior between the two versions. The expected behavior (do not collect/import random __init__.py files) is implied by the comparison to 5.2.2. This is sufficient for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-6202": {
        "explanation": "The issue provides a clear description of the bug ('.[' replaced with '[' in test report headlines), a minimal reproducible example, a detailed trace of the code path leading to the bug, and a specific suggestion for the code change required to fix it. The only minor ambiguity is the purpose of the original replacement, but the required change and validation steps (run tox, pass tests) are explicit. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-6323": {
        "explanation": "The issue provides a clear directive: the mktemp function should only accept normalized relative paths, and should disallow absolute and non-normalized paths. It also references a previous issue (#4202) for context, and explains the security motivation. While it doesn't specify the exact error handling or user feedback, the core requirement is unambiguous and actionable for a software engineer familiar with path handling and security best practices.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-6680": {
        "explanation": "The issue provides a clear list of improvements to be made to the deprecation documentation for Node.from_parent. It specifies that an example warning should be added, the warning should link to the docs, a before/after code example should be shown, and that from_parent should not support config/session. Each item is actionable and unambiguous, making it clear what changes are required for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-6926": {
        "explanation": "The issue description is extremely terse and lacks detail. It only mentions 'SysCaptureBinary: decode in writeorg' and references another issue for context. There is no explanation of the problem, expected behavior, or steps to reproduce. Without reviewing the linked issue (which is not provided here), it is impossible to know what needs to be done. Therefore, the issue is not specified enough for a meaningful attempt at a solution.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "pytest-dev__pytest-7046": {
        "explanation": "The issue provides a clear description of the observed behavior (the -k expression matches against the folder structure above the pytest root) and gives concrete, reproducible examples. However, it is phrased as a question about whether this is expected behavior, rather than as a request for a change or a bug report. There is no explicit statement of what the desired behavior is, or what the user expects to happen instead. As such, while the technical details are clear, the actual requirement for a solution is ambiguous: should the matching be changed, or is this just a request for clarification?",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-7122": {
        "explanation": "The issue provides a clear description of the observed behavior, including specific pytest commands, test code, and the actual vs. expected results. The user expects that '-k \"1 or 2\"' should select tests named 'test_1' and 'test_2', but it does not. The issue is reproducible and includes all necessary context (pytest version, Python version, example code, and command outputs). The only minor ambiguity is whether this is a bug or a documentation issue, but the technical ask is clear: make '-k \"1 or 2\"' behave as expected. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7151": {
        "explanation": "The issue provides a clear description of the problem: unittest.TestCase.addCleanup functions are not invoked on test failure in pytest >=5.4.0, but worked in 5.3.5. It includes a minimal reproducible example, expected vs actual output, and relevant environment details. The desired behavior is for cleanup functions to be called even when a test fails, matching unittest and previous pytest behavior. This is sufficient for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7158": {
        "explanation": "The issue clearly describes the problem: when running pytest from a subdirectory and specifying a test file using a relative path that points to a parent directory, the skip report displays the file path as if it were relative to the rootdir, not the invocation directory. The issue provides a concrete example, the current and expected output, and the context in which the bug occurs. It is clear what needs to be changed: the skip report should use the same relative path as was used in the invocation, or at least a path relative to the invocation directory. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7186": {
        "explanation": "The issue provides a clear description of the problem: pytest fails with a UnicodeDecodeError when setup.py contains UTF-8 characters and the locale is set to C. It includes a minimal reproducible example, the exact error message, steps to reproduce, and the environment details. The expected behavior is implied: pytest should not crash in this scenario. The root cause (file reading assumes ASCII encoding) is also evident. This is sufficient information to attempt a fix (e.g., ensure setup.py is read as UTF-8 regardless of locale).",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7205": {
        "explanation": "The issue provides a clear description of the problem, including the environment, the code to reproduce it, the command to run, the error message, and a suggestion for a fix (using saferepr instead of str() on bytes). The stacktrace pinpoints the problematic line. The expected behavior is also implied: avoid BytesWarning by not calling str() on bytes. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7231": {
        "explanation": "The issue describes a scenario where a logging call is made with a mismatched number of arguments, which causes an exception, but this exception does not result in a warning or a test failure. The desired behavior is for a warning to be issued when logging fails, so that the test can be marked as failed. While the issue does not specify exactly how the warning should be implemented (e.g., should it be a Python warning, a test assertion, or something else), it is clear that the goal is to make logging failures more visible in test results. This provides enough information to make a reasonable attempt at a solution, such as catching logging exceptions and issuing a warning or failing the test.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7236": {
        "explanation": "The issue provides a clear description of the problem: when running a skipped unittest.TestCase with pytest and the --pdb flag, the tearDown method is executed, which is unexpected. The issue includes a minimal reproducible example, the expected vs. actual behavior, the environment details (Python and package versions), and a note about a regression between pytest versions. While it does not specify the exact code change required, it is clear what the bug is and what the correct behavior should be. This is sufficient for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7283": {
        "explanation": "The issue provides a clear description of the problem: when running a skipped unittest.TestCase with pytest and the --pdb flag, the tearDown method is executed, which is unexpected. The issue includes a minimal reproducible example, the expected vs. actual behavior, the environment details (Python and package versions), and a note that this is a regression between pytest 5.4.1 and 5.4.2. While it does not specify the exact code change required, it is clear what the bug is and what the correct behavior should be. Therefore, it is well-specified enough to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7314": {
        "explanation": "The issue clearly describes the bug (UnboundLocalError for 'letter' in pytest_report_teststatus), the context (pytest 4.6.x branch, not master), the cause (variable may not be assigned in all code paths), and the expected fix (add a fallback/initial value for 'letter' before the if-blocks). It also references relevant PRs and provides rationale for not backporting the entire fix. The checklist is standard and does not affect the clarity of the technical requirement. This is well-specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7324": {
        "explanation": "The issue provides a clear description of the problem (interpreter crash on debug build for Python 3.8+), a minimal code snippet to reproduce the crash, and references a related upstream Python issue. However, it does not specify what the expected behavior should be, what changes are required in the codebase (e.g., should the crash be avoided, should an exception be raised, should the test be skipped, etc.), or whether the fix should be in the test suite, the Expression class, or elsewhere. There is enough information to investigate and reproduce the problem, but some interpretation is needed to decide on the appropriate fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7352": {
        "explanation": "The issue provides a clear error message, a stack trace, and context about when the error occurs (concurrent pytest processes). It references a related issue (#4181) and suggests a potential mitigation (using a random suffix for the numbered directory). However, it does not specify exactly what the desired behavior is, whether the random suffix approach is acceptable, or if there are constraints on changing the directory naming logic. There is also no explicit acceptance criteria or confirmation that the referenced code is the only place to change. Still, a reasonable interpretation is to address the race condition in temporary directory creation/cleanup for concurrent pytest runs, possibly by implementing the suggested mitigation. Thus, the issue is not perfectly specified, but it is reasonably clear what needs to be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7432": {
        "explanation": "The issue provides a clear description of the problem, including the expected and actual behavior, a minimal reproducible example, the relevant pytest versions, and a hint about where the bug is likely located. The objective is to ensure that skip location reporting is not broken by the --runxfail flag, and the expected output is well-defined. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7466": {
        "explanation": "The issue clearly describes the desired behavior for both the NO_COLOR and FORCE_COLOR environment variables, referencing the community standard and specifying the expected output logic for TTY and non-TTY contexts. The extension (FORCE_COLOR) is also well-defined. There is enough information to implement the feature as described, including the logic for when to emit colored output based on environment variables and TTY detection.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7468": {
        "explanation": "The issue clearly describes the problem: when using the --log-file option with a path that includes non-existent subdirectories, pytest crashes with a FileNotFoundError. It provides a concrete example and even suggests a code snippet to resolve the issue. The only ambiguity is a brief mention that perhaps users are not supposed to pass a directory path, but the main thrust of the issue is clear: improve --log-file so that it creates necessary subdirectories instead of crashing. This is a well-specified feature request with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7481": {
        "explanation": "The issue provides a detailed stack trace and a description of the problem (UnicodeEncodeError in terminalwriter on Windows when running a test that involves an emoji). It also includes the relevant test scenario and notes about the environment (pytest 6 rc, pytest-bdd, pytest-instafail, Windows, cp1252 encoding). However, the issue is not consistently reproducible, and the reporter is unsure if it's a regression in pytest 6 or related to their environment. There is no clear expected behavior or proposed solution, and the root cause is not definitively identified. While a reasonable attempt could be made to investigate and possibly fix Unicode handling in terminalwriter, there is ambiguity about the scope and the exact requirements for a successful solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7490": {
        "explanation": "The issue provides a clear description of the problem: dynamically adding an xfail marker to a test using request.node.add_marker(mark) worked as expected in pytest 5.x (the test was marked as xfail and did not fail the suite), but in pytest 6.x the test fails instead of being xfailed. The issue includes a minimal reproducible example, the expected and actual outputs, and the relevant environment details. The desired behavior is clear: restoring the previous behavior where dynamically added xfail markers are respected. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7499": {
        "explanation": "The issue provides a clear description of the confusing behavior encountered when using pytest.raises with the match argument, specifically when the string contains parentheses and is interpreted as a regex. The user suggests that pytest.raises should warn or provide a more helpful message when a non-regex string (with parentheses) is passed to match, to avoid confusion. The issue includes a concrete error message, a minimal code example, and a suggested improvement. While the exact implementation details are left to the developer, the intent and desired outcome are clear: improve the error/warning message in this scenario. This is enough to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7500": {
        "explanation": "The issue is well-described: it provides a clear error message, the context in which it occurs, the relevant code, and a proposed solution (swapping the order of two for-loops in a function). The user has tested this change and found it to resolve the issue in their environment. The problem is reproducible, and the code to be changed is identified. The only ambiguity is whether this is the best solution from a maintainability or design perspective, but the technical requirements for a fix are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7521": {
        "explanation": "The issue provides a clear description of the problem: in pytest 6.0.0rc1, the capfd.readouterr() method appears to convert carriage returns ('\\r') to newlines ('\\n'), whereas in pytest 5.x, the carriage return is preserved. The issue includes a minimal reproducible example, the expected and actual outputs, and the context (Fedora, Python versions). The expected behavior is that capfd.readouterr() should preserve carriage returns as it did in pytest 5.x. This is a well-specified regression report with a clear success criterion: after the fix, the provided test should pass as it did in pytest 5.x.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7571": {
        "explanation": "The issue provides a clear description of the problem: the caplog fixture in pytest 6.0 does not restore the log level after a test, contrary to the documentation and previous behavior. It includes a minimal reproducible example and specifies the expected vs. actual behavior. The desired outcome is implicit: caplog should restore the log level after each test, as documented. This is sufficient information for an experienced engineer to attempt a fix or investigation.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7637": {
        "explanation": "The issue provides a clear context: certain warnings were introduced near the 6.0 release but were suppressed due to policy, and now they should be reintroduced in 6.1. It references the specific PR where the warnings were suppressed, which would allow a developer to identify exactly which warnings need to be reintroduced. The task is clear: revert the suppression of those warnings for the 6.1 release. Therefore, the issue is well-specified for someone familiar with the codebase and the referenced PR.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7648": {
        "explanation": "The issue describes a warning from pylint about unimplemented abstract methods in a custom pytest collector class, after upgrading to pytest 6.0.0. It provides the relevant code, the error messages, and the context. However, the issue does not specify what the desired outcome is: Should the abstract methods be implemented? Should the pylint warning be suppressed? Is this a bug in pytest, or a request for documentation? The issue is more of a question or a request for clarification than a concrete task. Therefore, it is too vague to attempt a solution without further clarification.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-7939": {
        "explanation": "The issue clearly describes the desired feature: to add a shorthand CLI argument (like --sw) for the --stepwise-skip option in the pytest stepwise plugin. It provides the relevant code snippet and specifies the expected behavior. The implementation details are straightforward and there is no ambiguity about what is required.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7982": {
        "explanation": "The issue clearly describes the problem: since pytest 6.1.0, symlinked directories in test directories are not being collected, which is a regression from previous behavior. It identifies the specific commit that introduced the regression and points to the line of code responsible (`follow_symlinks=False`). The expected behavior is also clear: symlinked directories should be followed and collected as before. The only ambiguity is the mention of an upcoming PR, but that does not affect the ability to attempt a solution. The requirements and context are sufficiently clear for a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7985": {
        "explanation": "The issue description mentions deprecating the `--strict` flag, but it is not clear what the desired end state is. There is a comment about not removing and then reintroducing the flag, but no clear instruction on what should be done in the codebase. It is ambiguous whether the flag should be deprecated, removed, or left as is. There is also no information about what deprecation means in this context (e.g., warning, documentation update, etc.). Therefore, the issue is too vague to attempt a meaningful solution.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-8022": {
        "explanation": "The issue provides a clear description of the problem: when running doctest collection on an __init__.py file using pytest, only a single doctest is collected, which is not the expected behavior. It references the relevant code section and commit that introduced the behavior, and provides a link to failing tests that demonstrate the issue. However, it does not specify what the correct behavior should be (e.g., should all doctests in __init__.py be collected? Should it behave like any other module?). While the intent is clear, there is a small gap in the explicit expected outcome, but a reasonable interpretation is possible based on standard doctest behavior and the context provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-8055": {
        "explanation": "The issue provides a clear reference to a new Python 3.8 feature (sys.unraisablehook) and a link to the relevant CPython pull request. It also specifies the desired behavior: pytest should set sys.unraisablehook so that unraisable exceptions can be associated with tests. While it does not specify the exact implementation details, the intent and expected outcome are clear for an experienced developer familiar with pytest and Python's exception handling. Therefore, it is possible to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-8250": {
        "explanation": "The issue provides a detailed description of the problem, including the environment, the interaction between Twisted Logger and pytest's faulthandler, the specific error encountered, and even links to the relevant code sections. It also references a patch (#8250) that mitigates the problem. However, the issue itself does not specify what the desired behavior or solution should be, nor does it clarify if the referenced patch is the proposed solution or if further work is needed. While a knowledgeable engineer could reasonably infer that the goal is to prevent faulthandler from failing when sys.stderr.fileno() is invalid, there is some ambiguity about the exact requirements and acceptance criteria. Thus, it is not perfectly specified, but a sensible interpretation is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-8365": {
        "explanation": "The issue clearly describes a reproducible bug: when the username contains illegal characters for directory names (such as a backslash), the creation of temporary directories by pytest's tmpdir-related fixtures fails. The description includes the context, the root cause (use of getpass.getuser()), a concrete error message, and a minimal reproduction both in a real-world and a simplified environment. The expected outcome is implicit: tmpdir creation should succeed regardless of username contents, which implies the username should be sanitized or not used directly in directory names. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8399": {
        "explanation": "The issue provides a clear description of the problem: starting from version 6.2.0, unittest setUpClass fixtures generated by pytest are no longer named with a leading underscore, making them visible in the default output of `pytest --fixtures`. The expected behavior is that these fixtures should be 'private' (i.e., start with an underscore) and only shown with the `-v` flag. The issue includes a minimal reproducible example, the observed output, the expected output, and the impact (breaks a CI script). The required change is to restore the previous behavior of naming these fixtures with a leading underscore. This is a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8428": {
        "explanation": "The issue clearly states that an error should be generated if a `@pytest.mark` is applied to a fixture, which is a specific and actionable requirement. It also references a related issue (#1014) for context and mentions that documentation should be updated after the change. The expected behavior (error generation) is unambiguous, and the location (when a mark is applied to a fixture) is clear. Therefore, the issue is well-specified for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8447": {
        "explanation": "The issue provides a clear description of the error encountered (TypeError: __init__() got an unexpected keyword argument 'path') and the context in which it occurs (when using pytest-black and pytest-checkdocs plugins against the current main HEAD of pytest). It includes reproducible steps, specific error messages, and affected plugins. While it does not prescribe a specific solution, it is clear that the problem is the introduction of a 'path' keyword argument that is not expected by some plugins, likely due to a change in pytest's internals. A successful solution would involve ensuring compatibility so that plugins do not receive unexpected keyword arguments. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8463": {
        "explanation": "The issue description actually contains two unrelated issues: one about 'startpath/fspath parameter regressions' and another about 'Wrong color formatting output with FAILED test.' Each has some context, but they are mixed together, making it unclear which is the actual issue to address. The first part references internal API changes and suggests a solution (deprecate old hooks, introduce new ones, warn on old usage), but lacks concrete steps or a clear scope. The second part is a user-facing bug report about color formatting, with reproduction steps and environment details, but it's not clear if this is related to the first part or a separate issue. The ambiguity about which problem to solve and the mixing of two issues makes it difficult to know what a successful solution would look like.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-8641": {
        "explanation": "The issue clearly describes a bug: when a log format string in pytest.ini uses a precision-formatted levelname (e.g., %(levelname)-5.5s), colored output is not produced. It identifies the likely cause (the regex in ColoredLevelFormatter.LEVELNAME_FMT_REGEX does not match the format string due to the precision specifier) and provides a concrete example of the problematic format string. The expected behavior (colorful output) is implied, and the technical root cause is pointed out. While it doesn't specify the exact fix, it is clear what needs to be addressed: update the regex so that color output works with precision-formatted levelname. This is enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8906": {
        "explanation": "The issue provides a clear scenario: a test file uses Python 3.8-only syntax and needs to be skipped entirely on older Python versions, but the current pytest error message and recommended approaches are insufficient. The issue describes the current behavior, the error message, and why the suggested workaround (pytestmark) does not work. It also proposes two concrete improvements: (1) improve the error message to mention allow_module_level, and (2) introduce a new API for module-level skipping. While there is some room for interpretation in the exact implementation, the problem and the desired outcomes are clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-8952": {
        "explanation": "The issue describes a desire to enhance the `RunResult` class with a method to assert the number of warnings, providing a code example (`result.assert_warnings(count=1)`). It explains the motivation (current `assert_outcomes()` does not support warnings directly) and the intended usage. While it leaves some implementation details open (e.g., should the method support more than just count? Should it check warning types/messages?), the core requirement is clear: add a method to `RunResult` to assert the number of warnings. This is enough to make a meaningful attempt at a solution, though some minor decisions may need to be made by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9064": {
        "explanation": "The issue provides a clear and detailed description of the current behavior, the expected behavior, and a rationale for the change. It includes a minimal reproducible example, environment details, and step-by-step output for various flag combinations. The request is unambiguous: make `--stepwise-skip` behave as if `--stepwise` is also set, even when `--stepwise` is not explicitly provided. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9066": {
        "explanation": "The issue clearly describes a problem: when a deprecated direct Node construction occurs in pytest, the warning message does not indicate which class or plugin is responsible, making it hard for users to identify the source. The proposed solution is to enhance the warning message to include the fully qualified name of the offending class (e.g., 'pytest_relaxed.plugin.SpecModule'). This is a concrete and actionable request, and it is clear what needs to be changed and what the expected outcome is.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9249": {
        "explanation": "The issue clearly describes a problem with pytest's auto-generated test IDs containing characters (like '/') that are not compatible with the '-k' option, and requests that these IDs be sanitized to be shell-safe. It provides a concrete example and suggests a possible solution (using colon-separated IDs). However, it leaves some implementation details open (e.g., exact sanitization rules, whether to change the default ID format or just sanitize problematic characters, and how to handle backwards compatibility). Still, the core requirement is clear: make auto-generated IDs safe for use with '-k'. This is enough to make a reasonable attempt at a solution, though some decisions will need to be made by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9279": {
        "explanation": "The issue provides a clear description of the error encountered (TypeError: __init__() got an unexpected keyword argument 'path') and the context in which it occurs (using pytest-black and pytest-checkdocs plugins with the current main HEAD of pytest). It includes reproducible steps, the exact error messages, and affected plugins. However, it does not specify what the expected behavior should be or whether the fix should be in pytest, the plugins, or both. Still, a reasonable interpretation is that the goal is to resolve the compatibility issue so that these plugins work with the latest pytest. Thus, while some details are left to the implementer, the issue is specific enough to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9475": {
        "explanation": "The issue is primarily a discussion about a backwards-incompatible change and is asking for input on how to handle it for the current and future releases. It does not specify a concrete action or code change to be made, but rather seeks opinions and possible strategies. There is no clear, actionable task or acceptance criteria for a PR. Therefore, it is too vague for a meaningful attempt at a solution.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-9624": {
        "explanation": "The issue provides a detailed error traceback, environment information (pip list, pytest version, OS), and a link to the failing CI run. However, it lacks a minimal reproducible example and the reporter notes that the issue happened only once and is not reliably reproducible. The root cause is not clearly identified, and there is ambiguity about whether this is a transient CI/environment issue or a code bug. While there is enough information to start investigating, there are significant blanks to fill in regarding how to reliably reproduce and confirm a fix. Thus, it is not fully well-specified, but a sensible interpretation is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9646": {
        "explanation": "The issue provides a detailed description of the problem, including the context (Pytest 7, warning filtering via pytest.ini), a reproducible example (with commands to clone a repo, modify files, and run tox), the expected behavior (certain warnings should be suppressed), and the actual behavior (warnings are still shown). It also includes environment details and package versions. The problem is clearly stated: warning filters in pytest.ini are not suppressing specific warnings as expected. This is sufficient information to attempt a meaningful solution, such as investigating why the filters are not working and potentially fixing the filter syntax or Pytest's handling of them.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9681": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the expected and actual behavior, and the specific regression (change in behavior) between versions. It specifies the configuration, the file structure, and the error message. The only slight ambiguity is whether the fix should restore the old behavior or document the change, but the intent is clear: the existence of conftest.py should not break importlib-based package discovery as it did not in previous versions. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9709": {
        "explanation": "The issue provides a clear description of the problem: `pytest.approx()` does not work correctly with sets due to their unordered nature, and the current implementation misleadingly suggests support for sets. The issue includes a code example demonstrating the problem, an analysis of the relevant code, and two possible solution directions: (1) restrict `approx()` to only support sequence types and update documentation, or (2) implement proper support for arbitrary container (including set) comparisons. While the issue leaves some room for interpretation regarding which solution to implement, it is clear about the problem and the requirements for a successful solution (either restrict or extend support, but clarify behavior). Thus, a meaningful attempt at a solution is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9780": {
        "explanation": "The issue describes a problem observed after upgrading to version 7.1.0, where conftest.py is seemingly not picked up, leading to issues with test marks (specifically 'slow'). The user provides links to their CI logs and configuration file, and speculates about a possible cause (mark registration). However, the issue does not specify what the expected behavior is, what the actual error message is, or what a successful solution would look like. There is ambiguity about whether the problem is with pytest, the configuration, or something else. The issue is more of a bug report or a request for investigation rather than a clearly actionable task. Therefore, it is too vague for a meaningful attempt at a solution without further clarification.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-9798": {
        "explanation": "The issue is well-described: it provides a clear bug report, a minimal reproducible example, environment details, and a precise description of the undesired behavior (ModuleNotFoundError for Numpy when pytest.approx fails, even though Numpy is not required for the test to run). The expected behavior is also clearly stated: the error message should not require Numpy if pytest.approx does not. This is actionable and specific enough for a developer to attempt a fix (e.g., making the error reporting code not depend on Numpy).",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9911": {
        "explanation": "The issue provides a clear description of the current behavior (passing an empty tuple to pytest.raises(), pytest.warns(), or pytest.mark.xfail() results in an unhelpful error), the desired behavior (raise a more helpful error message), and even suggests the wording for the error messages. It also specifies that similar logic should apply to pytest.warns() and pytest.mark.xfail(). While there is a minor reference to another issue (#9002) for the 'no warnings' case, the main actionable items are well-defined and can be implemented without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10198": {
        "explanation": "The issue provides a clear request: implement a get_feature_names method for the CategoricalEncoder, similar to what exists in PolynomialFeatures. It references relevant discussions and issues for context, and specifies the desired behavior (consistency with PolynomialFeatures, including the ability to pass in original feature names). While some implementation details may need to be inferred from the referenced links and existing code, the core requirement is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10297": {
        "explanation": "The issue clearly describes a missing parameter ('store_cv_values') in RidgeClassifierCV, provides a reproducible code sample, details the expected and actual results, and references documentation that claims the parameter should exist. The request is to add support for this parameter, matching the behavior and API of similar classes (like RidgeCV). The requirements are clear: implement the 'store_cv_values' parameter, ensure it works as documented, and that the attribute 'cv_values_' is set appropriately. This is a well-specified feature request.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10377": {
        "explanation": "The issue provides a clear description of the bug, including the context (label indicator inputs, use of the labels parameter, and macro averaging in f1_score), a minimal reproducible example, expected vs. actual results, and relevant environment details. The problem is well-scoped: f1_score should only average over the label-specific scores indicated by the labels parameter, but currently behaves differently when 0 is present. This is sufficient information to investigate, reproduce, and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10382": {
        "explanation": "The issue description contains two separate concerns: (1) TfidfVectorizer's dtype argument is ignored, with a clear code example, expected and actual results, and environment details; (2) a suggestion to improve FitFailedWarning messages in cross-validation, with some discussion but less specificity. The first issue is well-specified and actionable. The second is more of a feature suggestion and is less precisely defined, but still gives enough context for a reasonable attempt. Overall, the issue is reasonably specified: the main bug is clear, and the secondary suggestion is interpretable, though not as tightly scoped.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10397": {
        "explanation": "The issue clearly describes a bug: passing a list of integer alphas to RidgeCV causes a ValueError, while using floats works. The user expects the code to handle integer alphas internally, so that both integer and float values are accepted without error. The expected behavior is clear: RidgeCV should accept integer alphas without requiring the user to cast them to float. The code to reproduce the issue and the error message are provided, making the requirements for a fix unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10427": {
        "explanation": "The issue clearly states that the function 'load_sample_images' uses the deprecated 'imread' function from SciPy, and provides the deprecation warning message. It is clear that the required solution is to update 'load_sample_images' to stop using the deprecated 'imread' function, likely by replacing it with a supported alternative. The scope and intent are clear, and a successful solution would be to remove the use of the deprecated function.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10428": {
        "explanation": "The issue clearly describes the desired invariant to test: for any estimator and any supported method, predictions (or transformations) on a subset of the data (using a boolean mask) should match the corresponding subset of predictions on the full data. It specifies the methods to check, the location for the test, and even provides a code snippet. While it leaves some implementation details (e.g., how to generate masks, which estimators to test, how to handle methods not implemented by all estimators), these are standard decisions for a contributor familiar with sklearn's testing framework. The intent and requirements are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10443": {
        "explanation": "The issue provides a clear description of the problem: the dtype argument to TfidfVectorizer is ignored, and the output is always np.float64. It includes a minimal reproducible example, expected and actual results, and relevant environment information. The required fix is to ensure that the dtype argument is respected in the output of fit/fit_transform. This is well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10452": {
        "explanation": "The issue points out that the 'PolynomialFeatures' functionality does not support sparse data and suggests that it should. However, it does not specify what 'support' should look like (e.g., should the output also be sparse? Should it accept all sparse matrix formats? Should it error gracefully or convert to dense?). There is no information about the expected API, edge cases, or tests. The issue is more of a feature request with minimal detail, leaving significant ambiguity about the requirements.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-10483": {
        "explanation": "The issue discusses a potential refactor of where imputation classes live within the codebase, suggesting that imputation be moved out of the 'preprocessing' module and possibly into a new 'impute' module, or distributed among other modules based on their algorithmic nature. However, it does not specify exactly what should be done, which classes should move where, or what the final structure should look like. There are open questions in the text (e.g., where should MICE go?), and the issue is more of a discussion starter than a concrete action item. As such, it is not clear what a successful solution would look like without further clarification.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-10495": {
        "explanation": "The issue clearly describes the current behavior (that check_array with dtype='numeric' does not fail on string arrays), provides an example, and states the desired behavior (it should fail or attempt to coerce, but not silently accept strings). It also mentions the possibility of a deprecation cycle, but the main requirement is clear: check_array(X, dtype='numeric') should not accept string arrays without error. This is enough information to attempt a solution, such as modifying the function to raise an error or coerce when non-numeric data is passed.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10508": {
        "explanation": "The issue provides a clear description of the problem, including the environment, the expected behavior, the actual behavior, and a minimal reproducible example. It specifies that LabelEncoder.transform([]) should return an empty array regardless of the data type used in fit, but currently fails for string classes. This is sufficient information to attempt a fix and to write a test for the expected behavior.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10558": {
        "explanation": "The issue provides a clear motivation for deprecating the 'axis' parameter in the Imputer class, and the intended change is to deprecate (and eventually remove) this argument. It references a specific issue to be fixed and describes the rationale for the change. However, there is a slight ambiguity: it is not 100% clear whether the task is to only deprecate (i.e., add a deprecation warning and documentation) or to remove the parameter entirely. Still, the most sensible interpretation is to deprecate the argument, as per the title and the '[MRG+1] Deprecate Imputer.axis argument' note. Thus, while there is a minor blank to fill in, the requirements are clear enough for a meaningful solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10581": {
        "explanation": "The issue clearly describes the problem: ElasticNet's fit method overwrites the input X even when copy_X=True, specifically when check_input=False. It provides a minimal reproducible example, explains the expected behavior (X should not be modified), and the actual behavior (X is modified). The title, description, and code sample make it clear what needs to be fixed. The only minor ambiguity is whether the fix should only apply to ElasticNet or also to related estimators, but for a PR targeting this issue, the requirements are clear.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10777": {
        "explanation": "The issue clearly describes a specific scenario: using CountVectorizer with ngram_range=(2, 1), and suggests that an error should be thrown if the lower bound of ngram_range is greater than the upper bound. The expected behavior is specified (raise an error in this case), and the context (CountVectorizer's ngram_range parameter) is clear. While the author is unsure of the current behavior, the desired change is unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10803": {
        "explanation": "The issue expresses a feature request: the ability to pass a weight for each sample in KDE (Kernel Density Estimation), referencing scipy.stats.gaussian_kde. However, it does not specify which codebase or function should be modified, what the API should look like, or any requirements for testing, documentation, or compatibility. While the general intent is clear (add weighted sample support to KDE), there are several blanks to fill in, such as how the weights should be handled, what the expected behavior is for edge cases, and how this should be exposed to users. Still, a sensible interpretation is possible: implement weighted KDE support similar to the referenced StackOverflow solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10844": {
        "explanation": "The issue clearly describes a bug in the fowlkes_mallows_score function, specifically a RuntimeWarning due to integer overflow when variables pk and qk become too large. The problematic line of code is identified, the error is described, and a concrete fix is proposed (using np.sqrt(tk / pk) * np.sqrt(tk / qk) instead of tk / np.sqrt(pk * qk)). The expected and actual results are provided, and the context is sufficient to attempt a solution. While the proposed fix should be verified for mathematical equivalence and numerical stability, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10870": {
        "explanation": "The issue is very well described: it identifies a specific bug (lower_bound_ not being set to the maximum across initializations when n_init > 1), provides a clear and direct code fix, and even points to the relevant location in the codebase. It also suggests an improvement to the test suite and provides a reproducible code snippet that demonstrates the bug. The expected and actual results are clearly stated. All necessary information to implement and verify the fix is present.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10881": {
        "explanation": "The issue clearly describes the problem: LogisticRegressionCV does not warn the user when it fails to converge within the default number of iterations. It provides a reproducible code example, expected behavior (a warning or error should be shown), and context about the observed behavior. The desired outcome is explicit: add a warning when convergence is not reached. The implementation details (e.g., how to detect non-convergence and how to emit the warning) are left to the developer, but the requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10899": {
        "explanation": "The issue provides a clear context: the user wants to persist and restore a TfidfTransformer by setting its idf_ attribute, similar to how CountVectorizer can be restored by setting vocabulary_. The user describes the current limitation (cannot set idf_), provides code examples, and explains the motivation and use case. The enhancement request is clear: allow setting idf_ (or provide a supported way to restore a fitted TfidfTransformer from saved parameters). While some implementation details (e.g., API design) would need to be decided, the core request is well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10908": {
        "explanation": "The issue provides a clear description of the current and expected behavior of CountVectorizer when a vocabulary is provided at initialization. It includes code examples demonstrating the problem and explicitly states the desired change: get_feature_names should not raise NotFittedError if the vocabulary parameter is provided, even if the vectorizer has not been trained. The reasoning is clear, and the required change is well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10913": {
        "explanation": "The issue clearly describes the current behavior of MultiLabelBinarizer when encountering unseen labels during transform, proposes a new parameter (ignore_unseen=True), provides a concrete code example with expected output, and references related user pain points. The desired behavior is unambiguous: unseen labels in transform should be ignored and set to all zeros, rather than raising an error. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10982": {
        "explanation": "The issue clearly describes the current behavior (an exception is raised if n_iter > search space size), the pain point (user wants to specify n_iter as an upper bound without worrying about the grid size), and a concrete proposal for a solution (add an option, off by default, e.g., allow_smaller_grid=True, to control whether the exception is raised). The location in the code is referenced, and the desired behavior is well-articulated. There is little ambiguity about what is being requested and how to implement it.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10986": {
        "explanation": "The issue is well-described: it provides a clear description of the bug, a minimal reproducible example, a diagnosis of the root cause (with a reference to the relevant code line), and even a suggested fix. The expected and actual results are specified, and the context (scikit-learn version, OS, etc.) is included. There is no ambiguity about what is wrong or what needs to be changed. This is sufficient for a meaningful and targeted attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11040": {
        "explanation": "The issue clearly describes a missing parameter validation for the 'n_neighbors' argument in the NearestNeighbors estimator. It provides a code example that currently fails with a TypeError when a float is passed instead of an integer. The issue also suggests the preferred solution: raise a more helpful error message early, rather than letting the error propagate from a lower-level function. It also mentions an alternative (casting to int), but expresses a preference for a clear error. Additionally, it specifies that the same validation should apply to the kneighbors method. This is sufficient information to implement a solution that checks the type of n_neighbors in both the estimator and the method, and raises a ValueError with a helpful message if a float is provided. Therefore, the issue is well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11042": {
        "explanation": "The issue provides a clear description of the problem: OneHotEncoder does not respect the specified dtype when producing a sparse matrix if the input data contains both categorical and real data types. It includes a minimal reproducible example, the expected and actual outputs, and relevant environment details. The required fix is to ensure that the output sparse matrix has the dtype specified by the user, even with mixed input types. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11043": {
        "explanation": "The issue describes a desired change in the default behavior of FunctionTransformer regarding how it handles DataFrames versus arrays, and suggests a new default and a deprecation cycle. However, it is phrased as a discussion or proposal rather than a concrete, actionable request. There is no clear specification of what code to change, what the new API should be, or what tests should be updated. It also ends with a question, indicating it is seeking consensus rather than requesting implementation. Thus, it is too vague for a meaningful attempt at a solution.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-11160": {
        "explanation": "The issue requests adding output options to the `classification_report` function in scikit-learn, specifically to allow returning results as a pandas DataFrame or xarray DataArray, instead of just a string. The issue also references a related PR that adds a dict-of-dicts output. While the request is clear in its intent (add more flexible output formats), it does not specify the exact API, parameter names, or expected behavior in detail (e.g., should the function gain a new argument, what should the DataFrame columns/indices be, etc.). However, as an experienced engineer, I could reasonably infer the requirements and propose a sensible implementation. Thus, the issue is not fully well-specified, but it is reasonably specified for a meaningful attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11206": {
        "explanation": "The issue description is very sparse and does not provide a clear explanation of what needs to be done. It mentions that 'increment_mean_and_var can now handle NaN values' and references another issue (#10457), but does not specify what the expected behavior is, what the current behavior is, or what changes are required. There is no description of the problem, no acceptance criteria, and no details about the implementation or testing requirements. Without further information, it is unclear what a successful solution would look like.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-11264": {
        "explanation": "The issue provides a clear description of the performance problem, reproducible code, and expected vs. actual results. It identifies that randomized_svd is much slower for dok_matrix and lil_matrix compared to other sparse formats, and suggests two possible solutions: either improve performance for these formats or warn users about the inefficiency. While it does not mandate which solution to implement, it is clear that either optimizing or warning is acceptable. This gives enough direction for a meaningful attempt at a solution, though some implementation choices are left to the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11281": {
        "explanation": "The issue raises a design question about whether mixture models should have an interface more compatible with clusterers, listing some differences in API and documentation. However, it does not specify what concrete changes are desired, nor does it provide acceptance criteria or a clear definition of 'more like clusterers.' It is unclear whether the task is to implement specific methods, refactor documentation, or something else. Therefore, the issue is too vague for a meaningful attempt at a solution without further clarification.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-11310": {
        "explanation": "The issue clearly describes the desired feature: adding an attribute (e.g., 'refit_time_') to BaseSearchCV (and its subclasses like GridSearchCV) that records the time taken to refit the best estimator on the full dataset after hyperparameter search. The motivation, current limitations, and a concrete use case are provided. The implementation details (e.g., how to measure and store the time) are left to the developer, but the requirement is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11315": {
        "explanation": "The issue clearly describes a bug: calling set_params on a ColumnTransformer with no estimators raises a ValueError. The error message and a minimal reproducible example are provided. The expected behavior is implied: set_params should not fail in this case. The context is sufficient to attempt a fix (e.g., handle the empty case in _set_params).",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11333": {
        "explanation": "The issue clearly describes a bug: calling set_params on a ColumnTransformer with no estimators raises a ValueError. The error trace is provided, and the expected behavior is implied (set_params should not fail in this case). The context is sufficient to attempt a fix: ensure that _set_params handles the empty case gracefully. No further clarification is needed to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11346": {
        "explanation": "The issue provides a clear description of the problem: when using SparseCoder with n_jobs > 1 and a large data_dims, a ValueError ('assignment destination is read-only') is raised. The user provides a minimal reproducible example, the versions of all relevant libraries, and the full traceback, which points to a specific line in the codebase. The error is reproducible and the conditions under which it occurs are well described. The expected behavior (no error) is implied. The only minor ambiguity is that the user does not explicitly state what the fix should be, but it is clear that the goal is to prevent the ValueError when using parallel jobs. This is a common enough bug report pattern that a fix can be attempted meaningfully.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11391": {
        "explanation": "The issue provides a clear description of the problem: a cryptic error occurs in imputers (specifically in the _get_mask function) when there is a dtype mismatch between the input array X and the missing_values parameter. The issue includes a minimal reproducible example, the error traceback, and a suggested solution (adding a check in _get_mask for dtype compatibility). The expected outcome is to raise a more informative error or handle the mismatch gracefully. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11496": {
        "explanation": "The issue provides a clear description of the bug, including the context (SimpleImputer with sparse matrices and explicit zeros), the expected behavior, the actual behavior, and a minimal reproducible example. The expected output is specified, and the code to reproduce the issue is given. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11574": {
        "explanation": "The issue specifies that the deprecation warning for the 'contamination' parameter in IsolationForest should be moved from the __init__ method to the fit method, and that the tests should be updated accordingly. While it does not provide detailed instructions, it is clear what needs to be done: relocate the deprecation logic and ensure tests reflect this change. The scope and intent are understandable for someone familiar with the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11578": {
        "explanation": "The issue is well-specified: it clearly describes the observed bug (LogisticRegressionCV with multi_class='multinomial' uses OvR for scoring due to not passing multi_class to the LogisticRegression instance in _log_reg_scoring_path), provides a detailed code-level analysis, points to the relevant lines in the codebase, and even suggests a concrete fix. It also includes a minimal reproducible example and describes the expected and actual behaviors. This is sufficient for an experienced engineer to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11585": {
        "explanation": "The issue provides a clear description of the bug (incorrect scaling in SparsePCA.transform()), points to the relevant code, and suggests two possible solutions. It also includes a minimal reproducible example and specifies the expected vs. actual results. While the reporter admits some uncertainty about the implementation details, the problem and the direction for a fix are clear enough for an experienced engineer to attempt a solution. The ambiguity about which of the two proposed fixes to use is minor, as both are well-motivated and the engineer can make a reasoned choice or implement both for review. Thus, the issue is well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11596": {
        "explanation": "The issue provides a clear request: implement a function in scikit-learn (sklearn) called show_versions(), similar to pandas.show_versions(). It also specifies that this function should include information about the BLAS binding used by numpy, as this can be relevant for debugging numeric issues. While it does not specify every detail of the output format, the reference to pandas.show_versions() provides a concrete example to follow, and the additional requirement about numpy BLAS info is clear. This is sufficient for an experienced engineer to proceed with a meaningful implementation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11635": {
        "explanation": "The issue raises a question about the necessity of using 'force_all_finite=True' in SelectFromModel.transform's check_array call, and suggests that this check could be relaxed, especially since some models can handle infinite or NaN values. However, it does not specify a concrete change to make, nor does it define the expected behavior (e.g., should force_all_finite be set to False, or should it be configurable?). It also mentions that this could apply to other feature selection methods, but does not specify which ones or how broadly to apply the change. Therefore, while the intent is clear, the exact requirements for a solution are not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12421": {
        "explanation": "The issue clearly describes a discrepancy between the documentation and the actual behavior of the OPTICS implementation in scikit-learn, specifically regarding the value of core distances for points that can never be core. It provides a reproducible code example, expected output (from R), and actual output (from scikit-learn). The required change is to ensure that points which will never be core have a core distance of 'inf', as stated in the documentation. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12462": {
        "explanation": "The issue provides a clear description of the problem: after upgrading scikit-learn and Dask, the .score() method in scikit-learn estimators fails when passed Dask DataFrames, due to shape/length checks that are incompatible with Dask objects. The issue includes a minimal working example, error trace, version information, and a workaround. The user also suggests a possible solution (handling shape checks more gracefully for Dask objects). While the exact implementation details (e.g., where to add the try-except, what fallback behavior is acceptable) are not fully specified, the problem and the desired outcome (make .score() work with Dask DataFrames as before) are clear. This is enough for a meaningful attempt at a solution, though some design choices may need to be made by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12486": {
        "explanation": "The issue provides a clear description of the bug: using GridSearchCV with scoring='roc_auc' and a GMM classifier results in an IndexError. The error message, code to reproduce, and the context (scorer.py, classifier check, num_classes >= 2) are all provided. The desired fix is also described: raise a ValueError if a non-classifier is used for a classification problem or if the dataset has only one class. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12583": {
        "explanation": "The issue clearly specifies that an 'add_indicator' parameter should be added to imputers, especially SimpleImputer, and describes the expected behavior: when enabled, the output of the imputer's transform should be stacked with the output of a MissingIndicator transform. The references to the relevant scikit-learn classes provide further context. While there may be some minor implementation details to decide (e.g., parameter default, exact stacking method), the core requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12585": {
        "explanation": "The issue provides a clear description of the problem: the `clone` function in sklearn fails when an estimator class (not an instance) is passed as a parameter. It includes a minimal reproducible example, the expected and actual results, and even suggests a possible fix. The context and motivation are explained, and the affected code location is identified. This is sufficient information to attempt a solution, as the required behavior (not erroring when a class is passed) is clear.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12625": {
        "explanation": "The issue provides a clear description of the problem: a TypeError occurs when passing a pandas.Series to preprocessing.scale in scikit-learn 0.20.0, whereas it worked in 0.19.1. The expected and actual results are both provided, along with a minimal reproducible example and relevant version information. The issue also references the documentation and explains why the user expects this to work. This is sufficient information to investigate, reproduce, and attempt a fix for the bug.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12656": {
        "explanation": "The issue description is mostly a pull request template and does not contain a clear, standalone problem statement or requirements. It references 'Fix Issue #10580' and describes some changes (removal of the 'labels' parameter from the hamming_loss function), but without access to the actual Issue #10580 or a clear summary of the problem, it is ambiguous what the original problem is, why these changes are needed, and what the acceptance criteria are. Therefore, it is not well-specified enough for a meaningful attempt at a solution without further context.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-12682": {
        "explanation": "The issue clearly describes a concrete problem: the inability to set the 'max_iter' parameter for the Lasso estimator used within SparseCoder, which leads to convergence warnings. It also suggests a direction for the solution: allowing users to specify additional parameters for the underlying estimator, not just those currently exposed in SparseCoder's __init__. While it leaves some implementation details open (e.g., whether to add a new parameter like 'estimator_params' or to explicitly add 'max_iter'), the intent and the required outcome are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12704": {
        "explanation": "The issue provides a clear description of a regression in the ColumnTransformer when using columns of type pd.Index, including a reproducible code snippet and the full traceback. The expected behavior is implied (that this should work as it did in previous versions), and the error is clearly identified. While it doesn't specify the exact fix, it is clear what needs to be addressed: ColumnTransformer should handle pd.Index as columns without raising this error. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12827": {
        "explanation": "The issue clearly requests an enhancement to the documentation of QuantileTransformer, specifically to add details about its implementation. It even references an external resource for guidance. While it does not specify the exact text to add, it is clear that the goal is to explain how QuantileTransformer works internally, especially regarding how it maps to a uniform distribution. This is a common type of documentation improvement issue and is actionable by someone familiar with the codebase and the algorithm.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12860": {
        "explanation": "The issue clearly describes a feature request: to add support for unpenalized logistic regression. It explains the current workaround (setting C to a large value in LogisticRegression) and references similar functionality in other classes. The desired outcome is clear: a straightforward way to perform unpenalized logistic regression, analogous to how LinearRegression provides unpenalized OLS. The implementation details (e.g., whether to add a penalty='none' option or another interface) are not specified, but the goal is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12908": {
        "explanation": "The issue is well-specified: it clearly requests an enhancement to OneHotEncoder to support 1-of-(k-1) encoding (i.e., dropping one category to avoid multicollinearity), with an option to specify which category to use as the baseline. It provides a concrete API suggestion (e.g., 'independent=True', 'baseline=24'), an example input and output, and discusses the motivation. It also references related issues and PRs, and notes an important edge case (interaction with missing values). This is sufficient for a meaningful implementation attempt.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12961": {
        "explanation": "The issue provides a detailed traceback, affected versions, and some context about the conditions under which the error occurs. However, it is primarily a bug report and a request for insight, not a clear request for a code change or a specific fix. The user suspects a cause and suggests that the error could be handled better, but does not specify what the desired behavior should be (e.g., a more informative error message, a fallback, or a fix to the underlying cause). There is enough information to investigate and possibly reproduce the issue, but not a clear specification of what a successful solution would look like. Therefore, it is best categorized as 'VAGUE'.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-12973": {
        "explanation": "The issue is well-described: it identifies a specific problem (duplicate 'copy_X' parameters in both the class and the fit method), explains the consequences (potentially contradictory values, silent overwriting), and proposes a concrete solution (make fit's copy_X default to None and only overwrite if explicitly set). The expected behavior is clear, and the rationale is provided. There is enough information to implement the proposed fix and write appropriate tests.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12983": {
        "explanation": "The issue provides a clear description of the problem: Gradient Boosting fails when the 'init' parameter is set to a scikit-learn estimator, due to unexpected output shape from predict. It also notes the lack of tests for non-default 'init' estimators. The edit further clarifies that initial estimators not supporting sample weights should be accepted if sample weights are not used in fitting. The references to specific issues provide additional context. While some implementation details (e.g., exact shape expectations) may need to be inferred from the codebase or related issues, the requirements are clear enough to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12989": {
        "explanation": "The issue clearly describes an inconsistency between the default 'init' parameter values of 'NMF' and 'non_negative_factorization', and suggests aligning them by changing the default in 'non_negative_factorization' with a deprecation process. The required change is well-scoped: update the default, implement a deprecation warning, and ensure consistency. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13010": {
        "explanation": "The issue provides a clear description of the problem (overflow in mean/std calculation when using float16 with StandardScaler), a reproducible code snippet, expected and actual results, and even hints at a possible solution (specifying dtype in mean/std calculation). The context and requirements for a successful solution are well-defined: StandardScaler should work with large float16 arrays without overflow, ideally without losing the memory benefits of float16. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13013": {
        "explanation": "The issue clearly describes the problem: manual checks for estimator fitting are being used instead of the standardized `check_is_fitted` utility, leading to inconsistent error messages. It provides concrete examples (with links to code), specifies the expected result (replace manual checks with `check_is_fitted`), and outlines the scope of the work (search for all such manual checks and replace them). The task is clear, actionable, and the success criteria are well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13017": {
        "explanation": "The issue clearly describes a specific ambiguity in the behavior of ColumnTransformer when negative column indexes are used. It provides a reproducible code example, shows both the expected and actual results, and specifies what the user believes should happen (either an error or consistent behavior with positive indexes). This is sufficient information to investigate, reproduce, and propose a solution or clarification in the code or documentation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13087": {
        "explanation": "The issue is well-specified: it clearly requests adding support for user-specified bin edges (arbitrary bin spacing) to sklearn.calibration.calibration_curve, similar to how numpy.histogram allows custom bins. The motivation, use case, and even a code example of the desired behavior are provided. The expected API change is clear (accepting bin edges as an argument), and the rationale is well-explained. There is no ambiguity about what is being requested or how to approach the solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13124": {
        "explanation": "The issue provides a clear description of the perceived problem: the behavior of the 'shuffle' parameter in sklearn.model_selection.StratifiedKFold does not match the user's expectation based on the documentation. The user provides a detailed code example, expected and actual results, and a specific claim about what is wrong (that shuffling does not affect the assignment of samples to batches, only their order). The issue is actionable: it either requires a code change to the shuffling behavior or a documentation update to clarify the current behavior. Thus, it is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13135": {
        "explanation": "The issue provides a clear description of the bug (unsorted bin_edges in KBinsDiscretizer with strategy='kmeans' causing np.digitize to fail), a minimal reproducible example, the expected and actual results, and relevant environment details. The root cause is identified, and the desired outcome (no error is thrown) is explicit. The solution direction is implied: ensure bin_edges are sorted before passing to np.digitize. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13142": {
        "explanation": "The issue provides a clear description of the problem: when using GaussianMixture with n_init > 1, fit_predict(X) and predict(X) yield different results, which is unexpected. It includes a minimal reproducible example, expected and actual results, and relevant environment details. The expected behavior is unambiguous (fit_predict and predict should agree), and the bug is well-demonstrated. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13143": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the expected and actual results, and the relevant environment details. The function in question (precision_score) and its parameters are specified, and the discrepancy is clearly outlined. This is sufficient information to investigate and attempt a solution, such as debugging the precision_score implementation or its usage with the provided input.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13157": {
        "explanation": "The issue clearly describes an inconsistency between the default value of the 'multioutput' parameter in two places: 'r2_score' and 'base.RegressorMixin'. It references a specific change (to 'uniform_average' in 0.19) and points out that 'base.RegressorMixin' still uses 'variance_weighted'. It also provides a code link for further context. The required action is to resolve this inconsistency, likely by updating the default in 'base.RegressorMixin' to match 'r2_score'. This is a well-specified issue with a clear objective.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13165": {
        "explanation": "The issue is well-specified: it references a specific bug (#13194), describes the context (monotonicity of bin edges in KBinsDiscretizer with the 'quantile' strategy), and proposes a concrete solution (enforcing monotonicity by applying a maximum to subsequent values after np.percentile). The expected change is clear, and the rationale is provided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13174": {
        "explanation": "The issue provides a clear motivation: AdaBoost and potentially other ensemble methods currently enforce that X must be a numeric array or sparse matrix, but in practice, the base estimator is responsible for handling X, so this restriction may be unnecessarily strict. The issue suggests relaxing this validation to allow for more flexible input types, such as lists of text blobs, as long as the base estimator can handle them. While it leaves some implementation details open (e.g., exactly which checks to relax and how to ensure backward compatibility), the core requirement is clear: reduce or remove the strict type validation of X in ensemble wrappers. This is a sensible and actionable interpretation for a PR.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13253": {
        "explanation": "The issue provides a clear enhancement request: to support pandas DataFrames as input to OneHotEncoder and OrdinalEncoder without converting the entire DataFrame to a contiguous numpy array. The motivation, expected behavior, and even some edge cases (like dtype changes in categories_) are described. It also clarifies that special handling for pandas categorical dtype is not required in this step, and that per-column coercion to numpy arrays is acceptable. While some implementation details are left to the developer, the requirements and scope are clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13283": {
        "explanation": "The issue provides a clear description of the problem (memory inefficiency in Isolation Forest's anomaly score calculation), points to the relevant code section, and suggests a possible solution (moving the calculation to the base estimator so that averaging is done per estimator, not all at once). It also describes the expected and actual results, and provides steps to reproduce the issue. While some implementation details would need to be worked out (e.g., exact refactoring steps), the problem and the direction for a solution are clear enough for a meaningful attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13302": {
        "explanation": "The issue description is primarily a work-in-progress (WIP) note and a TODO list, referencing other PRs/issues (#11642 and #11155) and mentioning a general goal about casting rules in Ridge regression with SAG/SAGA solvers. However, it lacks concrete details about the specific problem, expected behavior, or acceptance criteria. The instructions are high-level and depend on the context of other PRs/issues, making it ambiguous what exactly needs to be done without further information.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-13313": {
        "explanation": "The issue points out that two functions, 'check_class_weight_balanced_classifiers' and 'check_class_weight_balanced_linear_classifier', are defined but apparently never run, as determined by a 'git grep' search. However, it does not specify what should be done about this: Should the functions be removed, tested, integrated into a test suite, or something else? There is no clear instruction or desired outcome, making it ambiguous what a successful solution would look like.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-13328": {
        "explanation": "The issue provides a clear description of the problem: HuberRegressor.fit throws a TypeError when the input X is a boolean array, while other regressors like LinearRegression handle boolean inputs by converting them to float. The expected behavior is explicitly stated (automatic conversion of boolean to float), and reproducible code is provided. The error message and stack trace are included, as well as environment details. This is sufficient information to attempt a solution, such as adding a type conversion step in HuberRegressor.fit.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13333": {
        "explanation": "The issue provides a clear and detailed description of the problem: the documentation for the 'n_quantiles' parameter in QuantileTransformer is insufficient and potentially misleading. It explains the current behavior, why setting n_quantiles > n_samples is not meaningful, and provides code examples to illustrate the point. The request is to improve the documentation to clarify the impact of n_quantiles, especially regarding its relationship to n_samples and the effect on the transformation. The expected outcome is an improved docstring or documentation section for n_quantiles. The scope and intent are clear, making it possible to proceed with a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13363": {
        "explanation": "The issue provides a clear description of the problem: calling ridge_regression with return_intercept=True raises an UnboundLocalError. It includes a minimal reproducible code snippet, the expected behavior (no exception, return of coefficients and intercept), the actual error message, and the environment details. The problem is well-scoped and actionable: ensure that ridge_regression does not raise an exception and returns the expected tuple when return_intercept=True. No further clarification is needed to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13368": {
        "explanation": "The issue provides a clear description of the problem, a minimal reproducible code example, and specifies both the expected and actual results. The context (cross_val_predict with few samples in one class) is well-explained, and the code is sufficient to reproduce the bug. The only minor ambiguity is whether the fix should be in documentation, error handling, or algorithmic behavior, but the core issue is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13392": {
        "explanation": "The issue provides a clear description of the problem (missing multi-output checks in common tests), references related discussions and commits for context, and gives a concrete example of the kind of test that should be added. It also specifies that redundant individual tests should be removed. While it does not list exactly which estimators should be included or omitted, it is clear that the goal is to add a common test for multi-output support and clean up redundant tests. This is a sensible and actionable request, though some minor decisions (e.g., which estimators to include) are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13436": {
        "explanation": "The issue provides a clear example of the problem (confusing repr for nested pipelines), references documentation, and describes the undesired output. It also gives a high-level suggestion for improvement (trimming repr step by step to preserve outer structure). However, it does not specify exact acceptance criteria or a precise desired output format, leaving some implementation details open to interpretation. Still, the core problem and a reasonable direction for a solution are clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13439": {
        "explanation": "The issue clearly states that the Pipeline class should implement the __len__ method, and provides a code example that currently raises an error when len(pipe) is called. The expected behavior is that len(pipe) should work, presumably returning the number of steps in the pipeline. The context, motivation, and a reproducible example are all provided, making the requirements clear for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13447": {
        "explanation": "The issue provides a clear description of the bug: sample_weight is not applied to samples with zero true labels in label_ranking_average_precision_score. It references the relevant lines in the code, provides a minimal reproducible example, and specifies both the expected and actual results. The desired behavior is unambiguous: sample_weight should be respected for all samples, including those with zero true labels. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13454": {
        "explanation": "The issue clearly describes a confusing error message in OrdinalEncoder when a single list of categories is passed, provides a minimal reproducible example, and points out that the error message refers to an outdated parameter ('n_values'). The expected improvement is to update the error message to refer to the correct parameter and possibly detect and handle the case where a single list is passed. While the second point (detecting the case) is mentioned as a possible improvement, the main actionable item (fixing the error message) is well-specified. There is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13472": {
        "explanation": "The issue provides a clear description of the problem: using a Pipeline as the initial estimator for GradientBoostingRegressor fails due to a mismatch in the fit method's signature. The user provides a minimal reproducible example, the traceback, and points to the relevant line in the codebase, even suggesting a possible fix. The context (scikit-learn version, Python version) is also included. The expected behavior is clear: the Pipeline should be usable as an initial estimator. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13485": {
        "explanation": "The issue describes a problem encountered when using CalibratedClassifierCV with 3D input arrays, which are not supported due to the use of check_array with default parameters. The user suggests that removing or relaxing this check would allow their use case (3D pairs for metric learning) to work, as is possible with other meta-estimators like GridSearchCV. However, the issue is phrased as a question about the necessity of check_array, not as a concrete request for a code change. There is ambiguity about what the desired solution is: should check_array be removed, modified, or made configurable? There is also no specification of what the new behavior should be, how to handle edge cases, or what tests would be required. Thus, while the problem is clear, the solution is not well-defined.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-13496": {
        "explanation": "The issue provides a clear and actionable set of requirements: (1) expose the 'warm_start' parameter in the IsolationForest __init__ method, (2) document it in the same way as RandomForestClassifier, (3) add a test to ensure it works, and (4) possibly update the example documentation. The expected behavior is described, and there are references to similar implementations in the codebase. While there is a minor ambiguity in the last point ('possibly also mention...'), the core requirements are well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13497": {
        "explanation": "The issue description identifies a specific code location (_estimate_mi) and a problematic comparison (discrete_features == 'auto'), and explains why this is problematic (discrete_features can be an array or boolean mask, and this will error in future numpy versions). It also points out a lack of test coverage for non-'auto' values. While it does not specify the exact fix, it is clear that the solution should involve making the comparison robust to different types and adding tests for non-'auto' values. This is a sensible and actionable interpretation, though some implementation details are left to the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13536": {
        "explanation": "The issue clearly describes the current behavior (a confusing error message when passing 'sample_weight' directly to Pipeline.fit), provides a code example, and specifies the desired improvement: the error message should explicitly describe the correct way to pass 'sample_weight' (i.e., using the step name prefix and '__'). The expected outcome is clear, and the scope of the change is well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13549": {
        "explanation": "The issue describes a need for a utility function that can subsample data in a stratified way, even when the requested sample size is equal to or close to the total number of samples, which currently causes errors in train_test_split. The user suggests that adding a 'stratify' option to 'utils.resample' would be the most appropriate solution. While the issue does not specify the exact API or all edge cases, it is clear that the goal is to implement stratified subsampling in utils.resample, handling cases where the sample size is close to the dataset size. This provides enough direction for a meaningful attempt at a solution, though some implementation details would need to be decided by the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13554": {
        "explanation": "The issue is well-specified: it provides a clear description of the observed bug (numerical precision mismatch between sklearn and numpy for float32), a reproducible code snippet, expected and actual results, and relevant environment details. The problem is clearly demonstrated, and the expected behavior is unambiguous. There is enough information to attempt a fix or at least to investigate the root cause and propose a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13584": {
        "explanation": "The issue provides a clear code snippet that reproduces the bug, including the specific error message encountered. It identifies the context (print_changed_only in new repr) and the parameter (vector values, specifically Cs as a numpy array) that triggers the problem. The expected behavior is implied: printing the estimator should not raise an error. While it doesn't specify the exact fix, the reproduction steps and the nature of the bug are clear, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13620": {
        "explanation": "The issue provides a clear description of the observed bug (feature importances not summing to 1 in Gradient Boosting with many estimators), includes reproducible code, sample output, and details about the environment. The expected behavior (feature importances should sum to 1) is implied and standard for scikit-learn. The issue is well-specified for a developer to investigate, reproduce, and attempt a fix or clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13628": {
        "explanation": "The issue provides a clear description of the observed problem with brier_score_loss, including specific examples of input and output, and an analysis of the suspected cause (the behavior of _check_binary_probabilistic_predictions when the target contains only one class). The expected and actual results are clearly stated, and the user suggests that the behavior should be fixed. While the exact fix is not specified, the problem is well-defined and reproducible, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13641": {
        "explanation": "The issue provides a clear code example demonstrating the unexpected behavior: when using a custom analyzer with CountVectorizer and setting input='filename', the input argument is ignored. The issue is explicit about what is happening, and the expected behavior is implied (that input='filename' should be respected, or at least the behavior should be documented). While there is a minor ambiguity about whether to fix or document, the core technical issue is clear and actionable: investigate and address the interaction between custom analyzers and the input argument. This is enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13704": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, expected and actual results, and relevant environment details. The bug is well-demonstrated: VarianceThreshold should remove a column with zero variance, but in a specific case, it does not. The code samples and outputs make it unambiguous what is wrong and what the fix should achieve. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13779": {
        "explanation": "The issue clearly describes a bug: when an estimator is set to None in VotingClassifier and sample_weight is passed to fit, the code fails with an AttributeError. The code sample demonstrates the problem, and the error message is provided. The expected behavior is implied: the code should not fail in this way, and presumably, the VotingClassifier should handle None estimators gracefully when sample_weight is used. The area of the code to fix is also indicated (checking for None in sample_weight support). This is enough information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13780": {
        "explanation": "The issue clearly states that the VotingClassifier and VotingRegressor should accept the string 'drop' to drop an estimator from the ensemble, in the same way that passing None currently does. The reference to the discussion provides additional context, but the core requirement is clear: update the relevant classes to treat 'drop' as equivalent to None for dropping estimators. The expected behavior is well-defined, and the scope is limited to these two classes.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13828": {
        "explanation": "The issue clearly describes the current limitation (AffinityPropagation does not support sparse affinity matrices), provides a minimal reproducible example, specifies the expected and actual results, and references a related issue for context. The desired outcome is unambiguous: AffinityPropagation should accept and work with sparse affinity matrices in the same way it does with dense matrices. This is a well-specified feature request/bug report.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13910": {
        "explanation": "The issue provides a clear description of the problem (unexpected results from euclidean_distances in sklearn 0.21.1 with float32 input), a minimal reproducible example (including code and a data file), expected and actual results (with references to output files), and detailed environment information. The only minor ambiguity is the exact nature of the fix (e.g., whether to match scipy or previous sklearn behavior), but the expectation is to restore the previous correct behavior and handle float32 overflow. This is a well-specified bug report suitable for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13915": {
        "explanation": "The issue provides a clear description of the bug, including the context (32-bit Windows, csc_matrix with int64 indptr), a minimal reproducible example, expected and actual results, and relevant error traceback. The problem is well-scoped: min_max_axis should handle csc matrices with int64 indptr on 32-bit Windows without raising a TypeError. The expected behavior is also specified. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13933": {
        "explanation": "The issue raises a question about whether GBDTs should bin train and validation data separately, and describes the current behavior (binning before train_test_split). However, it does not specify what the desired behavior should be, nor does it clearly state what the expected outcome of a code change would be. It is more of a discussion prompt than a concrete, actionable issue. There is ambiguity about what exactly should be implemented, if anything.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-13960": {
        "explanation": "The issue is well-specified: it clearly describes the current limitation (IncrementalPCA does not accept sparse input), provides a minimal reproducible example that demonstrates the problem, specifies the expected behavior (no error should be thrown when using sparse matrices), and even suggests a possible fix with code. The requirements for a successful solution are clear: modify IncrementalPCA so that it can accept and process sparse input as described. There is no ambiguity about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13983": {
        "explanation": "The issue requests to add 'matplotlib' to 'show_versions()'. While it is not extremely detailed, it is clear that the function 'show_versions()' (likely a diagnostic or reporting utility) should be updated to include information about the installed version of matplotlib, similar to how it might already report versions of other dependencies. The task is straightforward for someone familiar with such utilities, and the expected outcome is clear: after the change, running 'show_versions()' should display the matplotlib version if installed.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14053": {
        "explanation": "The issue provides a clear description of the bug (IndexError in export_text when there is only one feature), a minimal reproducible code example, the actual error message, and relevant environment/version information. The expected behavior is implied: export_text should work even when there is only one feature. This is sufficient information to reproduce, diagnose, and attempt a fix for the problem.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14067": {
        "explanation": "The issue provides a clear description of the observed problem (accuracy degradation of ARDRegression when upgrading Scipy from 1.2.1 to 1.3.0), includes reproducible code snippets, and presents quantitative and visual evidence of the regression. However, it does not specify what a 'solution' would be: whether to patch ARDRegression, bisect Scipy, add a warning, or something else. The issue is well-documented for investigation, but the expected outcome is not fully defined. Still, a sensible interpretation is to investigate, isolate, and possibly fix or report the regression, making it reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14087": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the expected and actual results, and the relevant environment details. The error is specific (IndexError when using LogisticRegressionCV with refit=False), and the expectation is that no error should be thrown. This is sufficient information to investigate the code path, reproduce the bug, and attempt a fix so that the estimator works as expected when refit=False. No further clarification is needed to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14114": {
        "explanation": "The issue describes a specific problem: when using AdaBoostClassifier with the 'SAMME' algorithm and an SVC base estimator, calling predict_proba fails because SVC does not implement predict_proba unless probability=True. The issue points out that AdaBoost uses 'predict' during fitting but expects 'predict_proba' during prediction, which can cause errors if the base estimator does not support predict_proba. However, the issue does not specify what the desired behavior should be (e.g., should AdaBoost check for predict_proba support, should it fall back to predict, should it raise a clearer error, or should documentation be updated?). There is a clear bug, but the solution direction is not specified, so some interpretation is needed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14141": {
        "explanation": "The issue clearly states that 'joblib' should be included in the dependencies listed by the 'show_versions' utility or in the issue template, but only when the sklearn version is greater than 0.20. The intent is clear: update the output of 'show_versions' (or the issue template) to include joblib as a dependency, conditional on the sklearn version. The only minor ambiguity is whether both 'show_versions' and the issue template should be updated, or just one of them, but the main requirement is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14237": {
        "explanation": "The issue is well-described, with a clear explanation of the problem, a minimal reproducible example, and a precise statement of the expected vs. actual behavior. The root cause is identified (remainder columns are tracked by index, not name, leading to errors when DataFrame column order changes between fit and transform). The expected fix is clear: ensure that remainder columns are tracked by name when input is a DataFrame and columns are specified by name. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14450": {
        "explanation": "The issue provides a clear description of the problem: when fitting PLSRegression with a target matrix (yy) that contains a constant column (all zeros), an obscure error message ('array must not contain nan') is raised. The issue includes a minimal reproducible example and points out the root cause (constant column in yy). The expected improvement is to provide a clearer error message or handle this case more gracefully. While it doesn't specify the exact wording of the new error or the precise handling, it is clear what needs to be fixed and how to reproduce the problem.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14464": {
        "explanation": "The issue provides a clear description of the problem, a minimal reproducible example, expected and actual results, and relevant environment details. It is clear that the problem is with how cloning works for custom transformers when parameters are stored in a dictionary in __init__ rather than as explicit attributes. The expected behavior is also well-defined. This is sufficient information to attempt a meaningful solution, such as modifying the cloning logic or providing guidance on how to write custom transformers to avoid this issue.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14496": {
        "explanation": "The issue provides a clear bug report: passing a float for min_samples to OPTICS causes a downstream error because the value is not converted to int before being used as n_neighbors in NearestNeighbors. The error is described, the relevant code lines are referenced, and a concrete fix is proposed (convert min_samples to int after rounding). The context, expected behavior, and solution are all well-specified, making it straightforward to implement and test a fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14544": {
        "explanation": "The issue is presented as a request for comments (RFC) and is primarily a discussion prompt about design decisions and API consistency for ColumnTransformer, rather than a concrete, actionable task. It raises several open-ended questions about input validation, feature counting, and behavior with respect to column reordering and additional columns. There is no clear specification of what code should be changed, what behavior should be implemented, or what the acceptance criteria are for a solution. As such, it is not possible to make a meaningful attempt at a solution without further clarification and consensus on the desired behavior.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "scikit-learn__scikit-learn-14591": {
        "explanation": "The issue provides a clear description of the problem: LassoCV sets precompute to False before the final fit, even if the user specified precompute=True, leading to significant performance degradation. The user has identified the relevant code location and described the expected and actual behaviors. The desired outcome is implied: respect the user's precompute setting for the final fit, unless there is a mathematical or implementation reason not to. While the issue could be improved by explicitly stating the desired fix, it is clear enough for an experienced engineer to attempt a solution (e.g., remove or conditionally set the precompute flag).",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14629": {
        "explanation": "The issue is well-specified: it describes the bug, provides a minimal reproducible example, points to the likely problematic code, and explains the expected vs. actual behavior. The root cause is identified (incorrect access of 'classes_' on MultiOutputClassifier), and the fix is implied (iterate over estimators_). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14710": {
        "explanation": "The issue provides a clear description of the bug (mismatch between y_true and y_pred types during early stopping in HistGradientBoostingClassifier with string targets), a minimal reproducible example, the expected and actual results (including a full traceback), and even a suggested code diff for a potential fix. The requirements for a successful solution are unambiguous: ensure that early stopping works with string targets by encoding y_true appropriately. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14732": {
        "explanation": "The issue is primarily a question about whether multioutput support in `median_absolute_error` is a design choice or simply not yet implemented. There is no concrete request for a code change, feature, or bug fix. The user offers to submit a PR if it's not a design choice, but there are no details about the expected behavior, requirements, or acceptance criteria for such an implementation. Therefore, it is unclear what a successful solution would look like without further clarification.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-14764": {
        "explanation": "The issue is well-specified: it clearly describes the current behavior (make_classification only accepts list for weights), provides a rationale for change (other iterable types like np.array should be accepted, as in make_blobs), includes reproducible test code, expected and actual results, and even suggests a code correction. The scope of the change is clear (accept any sequence for weights and update the docstring accordingly).",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14806": {
        "explanation": "The issue provides a clear description of the current behavior of IterativeImputer when encountering features with missing values during transform that did not have missing values during fit. It also provides code examples demonstrating the behavior and suggests a concrete change: making the current behavior optional via a parameter (e.g., force-iterimpute). However, the issue does not specify the exact name or semantics of the new parameter, nor does it detail how the alternative behavior should be implemented in all edge cases. Still, the intent is clear and a sensible interpretation is possible: add a parameter to control whether iterative imputation is used in this scenario. Thus, the issue is reasonably specified, though some design decisions would need to be made by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14869": {
        "explanation": "The issue provides a clear code example that demonstrates the problem: using 'categorical_crossentropy' as the loss function in HistGradientBoostingClassifier for a binary classification task results in silent failure (incorrect predictions), whereas 'binary_crossentropy' works as expected. The issue also clearly states the expected behavior: either 'categorical_crossentropy' should generalize to binary classification or the code should raise an error. This gives a concrete direction for a solution\u2014either implement support for this case or add a check to raise an error. Thus, the requirements for a successful solution are well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14878": {
        "explanation": "The issue provides a clear code example that triggers the bug, a full traceback, and a description of the problem: SimpleImputer does not properly validate DataFrames with non-numeric data, leading to an AttributeError. The expected behavior is implied: the input should be validated and a more informative error should be raised, rather than an AttributeError. The context and the failure are both clear, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14890": {
        "explanation": "The issue provides a clear description of the problem: fitting a TransformedTargetRegressor with sample_weight in a Pipeline fails, and includes a full code example that reproduces the error. The expected and actual results are both specified, and the error traceback is included. The context (scikit-learn version, environment) is also provided. The desired outcome (support for sample_weight in this context) is clear. There is enough information to attempt a meaningful solution, such as investigating how sample_weight is passed through the pipeline and TransformedTargetRegressor, and proposing a fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14894": {
        "explanation": "The issue provides a clear description of the bug (ZeroDivisionError in _sparse_fit when support_vectors_ is empty), a minimal reproducible example, the expected and actual results, and relevant environment details. The expected behavior is also specified (no error, and self.dual_coef_ should be an empty sparse matrix). This is sufficient information to identify the problem and implement a fix that prevents the ZeroDivisionError and ensures the expected behavior.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14908": {
        "explanation": "The issue clearly describes the current error message, the context in which it occurs, and provides a concrete suggestion for how to improve the error message. It specifies that the improved error should include the provided constant value and the possible values from the training data. This is a well-scoped and actionable request for a code change.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14983": {
        "explanation": "The issue clearly describes the problem: the __repr__ method for RepeatedKFold and RepeatedStratifiedKFold does not return a string with the class name and parameters, but instead returns the default object representation. The expected and actual outputs are provided, and the steps to reproduce are clear. The solution would involve implementing or correcting the __repr__ method for these classes to match the expected output. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15028": {
        "explanation": "The issue provides a clear description of the problem: the 'classes_' attribute exists on DecisionTreeRegressor but is not documented and should be deprecated. It references related issues for context, describes previous attempts and their shortcomings, and suggests a solution (renaming to '_classes' in the parent and adding a test). While it doesn't specify every implementation detail, it is clear what needs to be done: deprecate or remove the 'classes_' attribute from DecisionTreeRegressor, ensure it is not exposed, and add appropriate tests. This is enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15084": {
        "explanation": "The issue provides a clear error message, a reproducible code snippet, and specifies the environment (scikit-learn version and Anaconda). The error is related to using VotingClassifier with a mix of regression and classification estimators and then calling roc_auc_score. The code and traceback are sufficient to understand the problem and attempt a solution (e.g., ensuring only classifiers are used in VotingClassifier, or handling label types appropriately).",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15086": {
        "explanation": "The issue provides a clear description of the observed problem: MultiTaskLassoCV (and MultiTaskElasticNet) with fit_intercept=True and binary features returns the same MSE for all alphas, leading to all coefficients being zeroed out, which does not happen with LassoCV or with fit_intercept=False or with normal features. The issue includes reproducible code snippets, expected vs. actual outputs, and details about the environment. However, the issue does not specify what the correct behavior should be or what a fix would look like\u2014should the model behave like LassoCV, or is there a deeper bug? There is enough information to investigate and attempt a fix, but some interpretation is needed regarding the expected outcome and the precise definition of 'correct' results.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15094": {
        "explanation": "The issue clearly describes the problem: MaxAbsScaler upcasts input pandas DataFrames of float32 to float64, which is undesirable for memory reasons. The issue provides a minimal reproducible example, expected vs actual results, and points out that other transformers (like OneHotEncoder) have a dtype argument, suggesting a possible solution direction. The request is clear: allow MaxAbsScaler to preserve input dtype or accept a dtype argument. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15096": {
        "explanation": "The issue provides a clear description of the observed bug: when using GridSearchCV with a pipeline and setting estimators as parameters in the param grid, the cv_results_['params'] contains fitted estimators, leading to high memory usage. The user also notes that this happens with n_jobs=1 and suspects that estimators are not being cloned before fitting. However, the issue does not specify what the expected behavior should be (e.g., should only parameter values be stored, not fitted estimators?), nor does it specify what a successful fix would look like (e.g., should cv_results_['params'] only contain parameter values, or should it clone estimators before fitting?). There is enough information to make a reasonable interpretation (that fitted estimators should not be stored in cv_results_['params']), but some assumptions are required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15100": {
        "explanation": "The issue provides a clear description of the bug, including the function involved (`strip_accents_unicode`), the context (strings already in NFKD form), and the expected vs. actual behavior. It includes a minimal reproducible example and specifies what the correct output should be. The problem is well-scoped and actionable: ensure that `strip_accents_unicode` removes accents even when the input is already in NFKD form. No significant ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15138": {
        "explanation": "The issue clearly describes a feature request: to add an option to use the original dataset when training the final_estimator in a stacking implementation. It references a concrete example (mlxtend's 'use_features_in_secondary' parameter), describes the expected behavior, and provides a default value. While it doesn't specify the exact code location, the intent and expected outcome are clear and actionable for someone familiar with stacking implementations.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15393": {
        "explanation": "The issue clearly states that the 'ascending' and 'descending' options for the 'imputation_order' parameter in IterativeImputer are inverted, and it points to a specific line in the codebase. It also describes the observed behavior: that the lists are appended in the wrong order. This provides enough information to investigate the code, verify the inversion, and correct the logic so that 'ascending' and 'descending' behave as expected. The expected behavior is clear from the context, and the location of the problem is specified.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15495": {
        "explanation": "The issue provides a clear objective: to replace custom and ad hoc sample_weight validation logic in a specific list of estimators/functions with the new _check_sample_weight utility. It lists the relevant classes, specifies the context (fit or partial_fit methods), and gives a rationale for the change. It also notes an explicit exclusion (linear_model module). While there may be minor implementation details to decide (e.g., exact placement of the call), the intent and scope are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15512": {
        "explanation": "The issue clearly describes a discrepancy between the documented and actual behavior of AffinityPropagation when it does not converge. It provides a reproducible code example, expected vs. actual output, and a concrete suggestion for improvement (returning -1 labels or exposing a convergence property). The problem and the desired outcome are both well-articulated, making it clear what a successful solution would entail: either fixing the return values to match the documentation or providing a reliable way to check for convergence.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15524": {
        "explanation": "The issue provides a clear description of the problem: nested cross-validation with a KNN classifier using a precomputed metric raises an error. It includes a minimal reproducible example, the expected behavior, the actual error traceback, and the environment details. The expected outcome is unambiguous (predictions should be returned as in the non-precomputed case). The error message and context are sufficient to understand what needs to be fixed: making nested cross-validation work with precomputed KNN metrics. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15625": {
        "explanation": "The issue requests adding a 'normalize' parameter to the 'metrics.classification.confusion_matrix' function, similar to what exists in other functions like 'accuracy_score'. The intent is clear: allow users to obtain a normalized confusion matrix directly from the function call, rather than normalizing manually. While the issue does not specify the exact API or normalization modes (e.g., 'true', 'pred', 'all'), it is reasonable to infer from similar functions (like sklearn's confusion_matrix) what is expected. Thus, there are some details to fill in, but the core request is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-24145": {
        "explanation": "The issue clearly describes the desired feature: adding an option to SplineTransformer to return a sparse matrix output, leveraging the natural sparsity of B-splines. It provides a code example showing the intended API (a 'sparse' parameter), and references a relevant upstream change in scipy that would enable the implementation. The workflow and expected behavior are clear, and the technical direction is outlined. There are no major ambiguities about what is required for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-24677": {
        "explanation": "The issue provides a clear context: fixing sklearn.metrics.silhouette_samples to work correctly with sparse matrices. It specifies two concrete failure points: (a) the pre-computed check for sparse matrices when getting diagonal elements, and (b) issues when indexing a sparse matrix to pass weights to np.bincount. While it does not provide step-by-step instructions, it is clear what the problems are and where in the code they occur. An experienced developer familiar with the codebase and the function in question would be able to attempt a solution based on this information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25102": {
        "explanation": "The issue provides a clear description of the problem (loss of dtypes when using pandas output in sklearn transformers), a concrete example demonstrating the undesired behavior, and a proposed solution that involves modifying the set_output mechanism to optionally preserve dtypes. It also suggests where in the codebase changes might be made (e.g., _SetOutputMixin, _wrap_in_pandas_container). While some implementation details would need to be worked out (e.g., handling of columns that are dropped or added), the intent and requirements are clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25232": {
        "explanation": "The issue clearly describes the missing feature (the lack of a 'fill_value' parameter in IterativeImputer), references the relevant documentation, explains the desired workflow, and specifies the expected behavior (add a 'fill_value' parameter that is used when 'initial_strategy' is 'constant', and allow np.nan as a valid value). The requirements are concrete and actionable, making it clear what needs to be implemented for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25363": {
        "explanation": "The issue provides a clear context: it is an alternative to a previous PR (#25242) and aims to fix a specific problem related to passing configuration explicitly to 'delayed' in the context of scikit-learn's parallelization. It specifies the desired behavior (pass config explicitly, raise a warning if not possible, ensure future compatibility with joblib's context/config). The references to related issues/PRs provide further context. While some implementation details are left to the developer, the requirements are clear enough for a meaningful solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25500": {
        "explanation": "The issue provides a clear description of the bug, including the context (CalibratedClassifierCV with isotonic regression and set_config(transform_output=\"pandas\")), a minimal reproducible example, the expected and actual results, and a traceback of the error. The root cause is identified: IsotonicRegression returns a DataFrame when transform_output is set to 'pandas', which is incompatible with the code expecting a numpy array. The expected behavior is also specified (it should not crash). This is sufficient information to attempt a meaningful fix, such as ensuring the output is converted to a numpy array where necessary.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25570": {
        "explanation": "The issue is well-described: it provides a clear bug report, a minimal reproducible example, the expected and actual behavior, and a detailed error traceback. The problem is specifically about how ColumnTransformer with pandas output handles transformers assigned to zero features, and the expected fix is to have such steps ignored. All necessary context (code, error, environment) is present for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25601": {
        "explanation": "The issue provides a clear description of the bug, including the specific parameter values that trigger the problem (`max_samples` near zero and `class_weight='balanced_subsample'`), a minimal reproducible example, the actual and expected results, and a stack trace. It also suggests two possible solutions: either prevent the error by enforcing a minimum of one sample, or fail gracefully with a descriptive error message. This is sufficient information to attempt a meaningful fix, such as adding a check for the minimum number of samples and raising a ValueError with a descriptive message if the condition is not met.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25694": {
        "explanation": "The issue provides a clear description of the bug, including the context (MLPRegressor.partial_fit with early_stopping=True), a minimal reproducible example, the observed error traceback, and a discussion of expected behavior (either graceful handling if unsupported, or a bug fix). The version information is also included. The only minor ambiguity is whether the desired solution is to support early_stopping in partial_fit or to raise a clear error, but the issue explicitly states that either a graceful error or a bug fix is acceptable. This makes the requirements clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25697": {
        "explanation": "The issue clearly states that the parameter `n_iter` should be deprecated and replaced with `max_iter` in the `BayesianRidge` and `ARDRegression` classes for consistency with other models. The required action is to deprecate `n_iter`, introduce `max_iter`, and ensure consistency. The scope is clear, and the expected outcome is well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25744": {
        "explanation": "The issue provides a clear description of the bug, the expected and actual behavior, a minimal reproducible example, and even points to the relevant code section. It specifies that setting min_samples_split=1 should raise an exception, but currently does not, and provides the expected error message. The context and requirements for a fix are unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25747": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, the expected and actual results, and the full error traceback. The code sample demonstrates the problem, and the desired outcome is explicitly stated: no error should be thrown when using pandas transform output. The environment details are also included. This is sufficient information to attempt a meaningful solution, such as investigating why FeatureUnion fails with pandas output and how to fix it.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25752": {
        "explanation": "The issue provides a clear description of the bug: KMeans does not use sample weights during initialization, leading to incorrect clustering results when sample_weight is provided. It includes a minimal reproducible example, expected vs actual results, and relevant environment details. The expected behavior is unambiguous, and the code to reproduce the problem is provided. The solution would involve ensuring that sample weights are properly considered during KMeans initialization. This is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25805": {
        "explanation": "The issue provides a clear description of the error encountered when using CalibratedClassifierCV with an LGBM model and fit_params containing an eval_set. It includes the error message, the relevant code, and the shapes of the input arrays. However, the issue does not specify what the expected behavior should be, nor does it clarify whether the goal is to fix a bug in sklearn, provide a workaround, or improve documentation. There is ambiguity about what constitutes a successful solution: should the code be modified, should documentation be updated, or should a new feature be added? Therefore, while the technical context is clear, the desired outcome is not fully specified.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-25931": {
        "explanation": "The issue provides a clear description of the bug, including the conditions under which it occurs (using a DataFrame and a non-default contamination value with IsolationForest), the unexpected warning message, and a minimal reproducible example. It also explains the likely cause and provides expected vs. actual results. This is sufficient information for a developer to investigate and attempt a fix, such as ensuring that feature names are handled correctly during the internal call to predict within fit. The environment details are also provided for reproducibility.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25969": {
        "explanation": "The issue provides a reasonable amount of context and a clear main objective: to add a CurveDisplayMixin with a _get_response_values method, reviving and superseding previous PRs. It also outlines a sequence of related refactorings to be done in subsequent PRs, but the current PR's scope is focused on the mixin and its method. The summary from the previous PR gives further details about the intended functionality, including the purpose of _check_response_method and _get_response, and mentions that unit tests should be included. However, there are some ambiguities, such as the exact signature and expected behavior of _get_response_values, and whether it should be implemented identically to the previously described _get_response. Still, with access to the codebase and the referenced PRs, a sensible interpretation and implementation can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25973": {
        "explanation": "The issue provides a clear description of the problem, including the expected and actual behavior, a reproducible code snippet, and the relevant version information. The error message is included, and the context (passing an iterable of splits to the 'cv' parameter of SequentialFeatureSelector) is well-explained. This is sufficient to attempt a meaningful solution, such as investigating why the iterable of splits is not handled as expected and fixing the bug or clarifying the documentation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26194": {
        "explanation": "The issue provides a clear description of the observed problem (roc_curve returning thresholds > 1 when given probability estimates), a reproducible test case, and points to the relevant code section. However, it explicitly states uncertainty about the best fix and only suggests a potential workaround, not a definitive solution. There is ambiguity about the intended behavior (should thresholds always be clipped, or is there a deeper fix needed?), and the issue author is not sure what the correct approach is. Thus, while a reasonable interpretation is possible (e.g., implement the suggested workaround and add the test), there is some room for interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26242": {
        "explanation": "The issue provides a clear description of the problem: the deprecation of 'base_estimator' in AdaBoostClassifier does not handle the explicit setting 'base_estimator=None' as it did previously, causing errors in code that was valid before. The issue includes a minimal reproducible example, the expected and actual results, and even suggests a solution (allowing None in _parameter_constraints). This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26289": {
        "explanation": "The issue provides a clear error message, code snippets, and a description of the problem: sklearn.tree.export_text fails when feature_names is supplied as a numpy array, while export_graphviz works. The error message points to a specific line in the codebase and suggests the cause (ambiguous truth value of a numpy array). While the issue does not explicitly state the desired fix, it is reasonable to infer that export_text should handle numpy arrays for feature_names in the same way as export_graphviz. Thus, a sensible solution would be to update export_text to properly handle numpy arrays for feature_names, matching the behavior of export_graphviz.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26318": {
        "explanation": "The issue clearly describes a mismatch between the documentation and the actual behavior of the code regarding the `warm_start` parameter in BaseForest-derived classes. It provides links to both the documentation and the relevant source code, and it explicitly outlines two possible solutions: either update the documentation to match the code, or update the code to match the documentation. This gives a clear and actionable path for resolving the issue, making it well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26323": {
        "explanation": "The issue provides a clear description of the bug: when using set_output on a ColumnTransformer, the output setting is not applied to the transformer defined in the remainder argument if it is an estimator. The issue includes a minimal reproducible example, expected and actual results, and points to the relevant code location. The expected behavior is well-defined, and the problem is clearly scoped. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26400": {
        "explanation": "The issue is well-described: it provides a clear bug report, a minimal reproducible example, the actual and expected behavior, and a stack trace. The expected result is also specified: either handle the all-NaN column gracefully (by preserving it) or raise a descriptive error. This gives a clear direction for a solution\u2014either improve error handling or support the case. The code context and environment are also provided. Thus, the requirements for a successful solution are clear.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26644": {
        "explanation": "The issue clearly describes the current behavior (arithmetic averaging in partial_dependence), the problem (does not respect sample weights for models trained with them), and the desired solution (add a sample_weight argument to allow weighted averaging). It specifies that ICE curves are not affected, only the aggregate. The required change is well-scoped and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-7760": {
        "explanation": "The issue raises a concern about the adequacy of common tests (such as check_estimator) in catching improper handling of init parameters in estimators, specifically when parameters are not stored as public attributes. However, it does not specify what the new or improved tests should do, what the expected behavior is, or what constitutes a passing or failing case. There is no clear acceptance criteria or actionable steps. As such, while the general area of concern is clear, the specifics required for a solution are missing.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-8554": {
        "explanation": "The issue is well-specified: it clearly describes a mismatch between the documentation (which claims that Isomap and LocallyLinearEmbedding should accept sparse matrix input) and the actual implementation (which does not accept sparse matrices due to the way check_array is called). The issue identifies the relevant code locations, provides links to the documentation and code, and even includes a backtrace showing the error. The required change is to update the check_array calls to accept sparse matrices where appropriate, in line with the documentation. This is a clear, actionable request.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-9274": {
        "explanation": "The issue provides a clear description of the bug (MLPRegressor/MLPClassifier with solver='lbfgs' is limited by the default maxiter value due to incorrect parameter mapping), a reproducible code example, expected vs actual results, and even a suggested fix direction. The only minor ambiguity is about how to test the fix, but the main technical requirement is clear: ensure that the 'max_iter' parameter is correctly passed to the 'maxiter' argument of the underlying optimizer. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-9288": {
        "explanation": "The issue is well-specified: it describes a reproducible bug (KMeans gives slightly different results for n_jobs=1 vs. n_jobs>1), provides a minimal code example to reproduce the problem, specifies the expected and actual results, and includes version information. The objective is clear: ensure that KMeans produces identical results regardless of the n_jobs parameter. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-9775": {
        "explanation": "The issue clearly states two actionable changes: (1) the function sklearn.manifold.t_sne.trustworthiness should allow a custom metric, and (2) the current 'precomputed' boolean parameter should be replaced by a more standard 'metric=\"precomputed\"' pattern. While it does not specify every detail (e.g., how to handle backward compatibility or what metrics to support), the intent and the required code changes are clear and follow established patterns in scikit-learn. Thus, a meaningful solution can be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10021": {
        "explanation": "The issue provides a clear description of the problem: when both 'autodoc_unqualified_typehints = True' and 'autodoc_typehints = \"description\"' are set, typehints in the info-field-list are not rendered in the short (unqualified) style as expected. The expected behavior is also clearly stated. While there is no code sample or screenshot, the configuration and the expected outcome are described well enough for a developer familiar with Sphinx and its autodoc extension to attempt a solution. The issue is specific about the settings and the undesired behavior, making it actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10067": {
        "explanation": "The issue clearly describes the accessibility problem (missing lang attribute in generated HTML), the cause (Sphinx's conf.py not setting the language by default), and the desired solution (ensure sphinx-quickstart sets the language in conf.py based on user input). It also suggests a possible enhancement (prompting to set language for existing installations), but the core actionable request is well-defined: update sphinx-quickstart so that the language entered by the user is always written to conf.py. This is a clear, actionable, and testable change.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10097": {
        "explanation": "The issue describes a specific problem: long option names in reStructuredText (RST) using the '.. option::' directive overflow in the generated LaTeX index when built with xelatex. It provides a concrete example, the RST snippet, and a screenshot of the resulting overflow. The environment (OS, Python, Sphinx version) is specified. However, the 'Expected behavior' is left blank, and there is no explicit description of how the output should look instead. While the problem is clear (avoid overflow of long option names in the LaTeX index), the exact requirements for the fix (e.g., should the text wrap, truncate, or use another formatting?) are not specified. Still, a reasonable interpretation is to prevent overflow, likely by wrapping or formatting the text. Thus, the issue is not perfectly specified, but it is clear enough to attempt a solution with some sensible assumptions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10137": {
        "explanation": "The issue describes a feature (extlink replacement suggestion) and provides a concrete example where the current suggestion is not ideal. It asks whether the check can be 'bailed out' in such cases, or if the suggestion should only apply under certain conditions (e.g., no '/' in the extlink). However, it is phrased as a question and discussion prompt, not as a clear request for a code change or a specific behavior. There is ambiguity about what the desired behavior is, and it is not clear what 'bail out' should mean in implementation terms. Therefore, it is too vague to proceed with a concrete solution.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-10191": {
        "explanation": "The issue provides a clear description of the bug (footnote marks rendered as '?' with no hyperlink in PDF output), a minimal reproducible example (the index.rst file content), the steps to reproduce (run 'make latexpdf'), the expected behavior (footnotes rendered correctly), and relevant environment details (OS, Python version, Sphinx version). This is sufficient information for a developer to attempt to reproduce and address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10207": {
        "explanation": "The issue description actually contains three separate feature requests/bug reports, each with its own context and expected behavior. While each sub-issue is described with some detail, the presence of multiple unrelated issues in a single ticket makes it ambiguous which one should be addressed. For example, the first is about adding a keyboard shortcut, the second is about how instance attributes are rendered by sphinx-napoleon, and the third is about abbreviating long dictionaries in autodoc output. Without knowing which of these is the intended target, it is unclear what a successful solution would look like. Therefore, the issue is too vague for a meaningful attempt at a solution.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-10323": {
        "explanation": "The issue provides a clear description of the problem: when using the Sphinx literalinclude directive with the prepend (and possibly append) option, the leading whitespace is not preserved, resulting in incorrect indentation in the output. The user provides a concrete example, expected vs actual output, and even discusses attempted workarounds and their limitations. The expected behavior is clearly stated: leading whitespace should be preserved so that prepended lines match the indentation of the included code. The context (Sphinx version, extensions, project) is also provided. This is sufficient to attempt a meaningful solution, such as modifying how literalinclude handles indentation for prepend/append content.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10325": {
        "explanation": "The issue provides a clear description of the current limitation (inherited-members only supports a single class or all classes), the desired behavior (support specifying a list of classes to include or exclude when documenting inherited members), and the context in which this feature would be useful. It also describes the current workaround and its limitations. The request is specific to the :inherited-members: option in automodule, and the expected behavior is well-articulated. While there may be some implementation details to decide (e.g., syntax for specifying the list), the core requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10435": {
        "explanation": "The issue provides a clear description of the bug: extra whitespace is being added at the start and end of inline code in LaTeX/pdf output after a specific enhancement (#10251). It includes a minimal reproducible example, the expected vs. actual LaTeX markup, screenshots of the output, and context about the environment. While the reporter says they have no strong opinion on whether the whitespace is good or bad, the issue is framed as a bug report and the expectation is that the whitespace should not be present. This is a reasonable and actionable interpretation for a fix. The only minor ambiguity is the lack of a strong opinion, but the rest of the report makes it clear what the desired output is. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10449": {
        "explanation": "The issue provides a clear description of the bug: when using 'autodoc_typehints = \"description\"', Sphinx's autoclass directive incorrectly includes a 'return type' for the class itself, which is not expected. The issue includes a minimal reproducible example, step-by-step reproduction instructions, expected behavior, and links to both a sample project and generated documentation. The environment (OS, Python, Sphinx version) is also specified. This is sufficient information to attempt a fix or at least to reproduce and investigate the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10457": {
        "explanation": "The issue provides a clear description of the bug, including the function involved (sphinx.domain.python.filter_meta_fields), the nature of the bug (only the first meta-field is removed, subsequent ones are not), and a minimal reproducible example. It also specifies the expected behavior (no meta-fields should appear in the generated documentation) and the context (Sphinx versions, relevant commit). This is sufficient information to attempt a fix and write a test for the described behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10466": {
        "explanation": "The issue clearly describes the problem (duplicate location entries in the generated .pot file), provides concrete examples, points to the suspected source of the problem (specific method in sphinx/builders/gettext.py), and even suggests a possible solution (deduplicating locations). It also provides steps to reproduce the issue and the expected behavior. While there is some mention that the solution may also need to be applied in other places, the core requirement\u2014removing duplicate locations from the .pot output\u2014is clear and actionable. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10481": {
        "explanation": "The issue provides a clear description of the problem (Sphinx errors out if 'language = None' is set in conf.py), the context (regression due to a recent change), the impact (affects older projects), and a concrete solution (treat 'language = None' as 'en' and log a warning). The error message is included, and the desired behavior is well-articulated. There is little ambiguity about what needs to be changed in the codebase and how the new behavior should work. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10492": {
        "explanation": "The issue clearly identifies a typo in a warning message related to the word 'language' being misspelled as 'langugae' and 'langauge' in a specific code snippet. The expected behavior is also clearly stated: the warning should mention 'language' without typos. The file and line numbers are provided, making it straightforward to locate and fix the issue. No further clarification is needed to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10504": {
        "explanation": "The issue provides a detailed error traceback, the context in which the error occurs (building Arch Linux kernel docs with Sphinx), the steps to reproduce, expected outputs, and relevant environment information. However, it does not specify which part of the codebase should be changed, nor does it indicate whether the fix should be in the Sphinx configuration, the documentation sources, or a custom extension. While the error is clear, the root cause is not identified, and there is ambiguity about where and how to apply the fix. Still, with the provided information, a knowledgeable engineer could attempt to reproduce and debug the issue, making a sensible interpretation of what is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10551": {
        "explanation": "The issue provides a clear description of the bug: in the HTML output of documentation for functions with default parameter values that are negative numbers, a spurious space appears between the minus sign and the digit (e.g., 'axis=- 1' instead of 'axis=-1'). Multiple concrete examples and links are provided, and the context is well-explained. The only ambiguity is whether the bug is in Sphinx or the PyData Sphinx theme, but the core problem and its manifestation are clear. A meaningful attempt at a solution can be made by investigating the HTML rendering of default parameter values for negative numbers and correcting the spacing issue.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10614": {
        "explanation": "The issue provides a clear description of the bug, including the context (SVG inheritance diagrams with sphinx.ext.inheritance_diagram), the conditions under which the bug occurs (when the diagram is not in the root directory), and a detailed reproduction guide with a minimal demo project. It also explains the expected vs. actual behavior, gives concrete examples of the incorrect and correct links, and references similar issues for context. The root cause is hypothesized (relative links in SVGs are not being resolved correctly when embedded in subdirectories). This is sufficient information for an experienced developer to attempt a fix, likely in the code that generates the SVG links.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10673": {
        "explanation": "The issue clearly describes a problem encountered when adding 'genindex', 'modindex', and 'search' to a Sphinx toctree, including the exact warning messages and user expectations. It provides context, examples, and references to related discussions. The desired solution is explicitly stated: allow these entries in the toctree without raising errors. While implementation details are not specified, the problem and expected outcome are clear, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10807": {
        "explanation": "The issue provides a clear description of the problem (Sphinx's automodule directive places members under the last header in the module docstring, which affects the ToC structure), a concrete example, a minimal reproducer, links to live builds, and even a suggested direction for a fix (making the module directive accept content like other directives). The expected behavior is well-articulated, and the context is sufficient to attempt a solution. While there is some room for interpretation in the exact implementation, the requirements and the desired outcome are clear.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11109": {
        "explanation": "The issue provides a clear description of the desired improvements to the formatting of type annotations in Sphinx's Python domain, specifically for Union, Optional, Literal, and constants. It specifies the new formatting style (PEP 604 syntax), gives concrete before/after examples, and references an existing implementation for guidance. It also mentions that this should be configurable via a Sphinx config option. While there is some room for interpretation regarding the exact implementation details (e.g., how to handle edge cases or the specifics of syntax highlighting for constants), the requirements are clear enough to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-11192": {
        "explanation": "The issue provides a clear description of the bug (smaller/wrong search index when using -j auto with sphinx-build), steps to reproduce (exact commands that fail and work), the environment, and a link to a minimal reproducible example. The expected and actual behaviors are well described, and the regression point (Sphinx 6.0.0 vs 6.1.0) is identified. This is sufficient information for an experienced engineer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11266": {
        "explanation": "The issue clearly describes a regression: in French LaTeX output, the 'seealso' directive now lacks a space before the colon after 'Voir aussi' since Sphinx 6.1.0. The expected and actual outputs are described, a screenshot is provided, and the relevant code changes that caused the regression are referenced. The reproduction steps are clear, and the desired fix (restoring the space) is unambiguous. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11311": {
        "explanation": "The issue provides a detailed description of the current behavior, the problems with it, and a set of concrete suggestions for improvement. It specifies the use of __init_subclass__ and a _sig_element meta-keyword for automatic registration, outlines the expected behavior for custom translators, and suggests a way to preserve node type information when falling back to inline nodes. The requirements are clear, actionable, and reference specific code locations. While some implementation details (e.g., exact attribute names) may need to be decided, the intent and scope are well-defined, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11312": {
        "explanation": "The issue provides a clear description of the observed problem: the non-deterministic ordering of set elements when a set is nested within another collection (such as a tuple) in the output of util.inspect.object_description, leading to non-reproducible documentation builds. It gives concrete examples, links to the relevant code, and references to reproducibility dashboards and logs. However, it also explicitly states that the exact circumstances causing the ordering to vary are not yet known, and that it is 'OK not to proceed until that's figured out.' This means that while the problem is well-motivated and illustrated, the root cause is not yet identified, and the expected solution is not fully specified. There is a sensible interpretation (make the output deterministic), but some blanks remain regarding the precise mechanism and scope of the fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-11316": {
        "explanation": "The issue provides a clear description of the bug, including the specific warning message, the context (Napoleon extension with dataclass attribute docstrings), and a minimal reproducible example that demonstrates both the failing and working cases. The environment and relevant Sphinx extensions are also specified. The expected behavior (no warning for valid references on the first line) is implied by the working example. This is sufficient for a developer to attempt a fix or at least reproduce and investigate the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11445": {
        "explanation": "The issue provides a clear description of the bug, including the conditions under which it occurs (when rst_prolog is set and a domain directive is used as the first heading), a minimal reproducible example, and the expected vs. actual behavior. The environment and configuration are also specified. While the root cause is not identified, the information is sufficient for an experienced developer to reproduce and attempt a fix for the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11489": {
        "explanation": "The issue clearly describes the problem (GitHub's Markdown renderer requiring JavaScript breaks anchor checks), the desired solution (a config option to disable anchor checks for URLs matching a regex), and even provides a code example of the expected configuration. The context and motivation are clear, and the requirements for a successful solution are well-defined. There is enough information to implement the feature as described.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11510": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the expected and actual output, and the context (Sphinx extension, version, and project). It specifies that the 'source-read' event does not modify included files as expected, and the desired outcome is for the replacement to occur in both the main and included files. The steps to reproduce and the expected HTML output are explicit. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11550": {
        "explanation": "The issue provides a clear description of the bug, including the context (Sphinx autodoc with autodoc_preserve_defaults = True), a minimal reproducible example (the mod.py, conf.py, and index.rst files), the exact error message, and the environment details. It also describes workarounds and the specific condition that triggers the bug (multiline lambda in property). This is sufficient information for a developer to reproduce the issue and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7234": {
        "explanation": "The issue describes a desired feature: automatically collecting and documenting all overloads of a @functools.singledispatch function, listing them together with the master function's documentation. However, it does not specify the expected output format, how the links should be presented, what constitutes an 'overload' in this context (e.g., only registered implementations?), or how this should integrate with the existing documentation system. There is also no mention of edge cases, configuration, or user-facing options. While the general intent is clear, there are significant ambiguities in the details required for implementation.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-7268": {
        "explanation": "The issue suggests that the 'sphinx.ext.autodoc.typehints' extension should be loaded automatically when the 'autodoc' extension is used, now that typehints are considered mature. It references a previous issue (#6418), which may provide additional context, but based on this description alone, the main requirement is clear: update the autodoc extension to automatically include typehints support. However, there are some minor ambiguities, such as whether this should be configurable or always-on, and if there are any compatibility concerns. Still, a sensible interpretation is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7305": {
        "explanation": "The issue provides a clear description of the bug: when using the '**' (power) operator in default argument values in Python method signatures within reStructuredText (rst) directives, Sphinx emits warnings that it cannot parse the argument list due to a 'BinOp' object. The environment is specified, and the difference in behavior between Sphinx versions is noted. While the 'Expected behavior' section is empty, it is reasonable to infer that the user expects Sphinx to correctly parse and document such signatures without warnings. The core of the issue is well-defined, and a solution would involve updating the argument parser to handle binary operations like '**' in default values. Thus, the issue is well-specified enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7350": {
        "explanation": "The issue provides a clear description of the bug: the :noindex: option is being ignored by the Attributes directive in sphinxcontrib-napoleon. It includes a minimal reproducible example, expected behavior, and environment details. The expected outcome (an empty document index) is clearly stated, and the steps to reproduce are well-documented. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7351": {
        "explanation": "The issue provides a clear description of the bug, steps to reproduce, the current and expected behavior, and even suggests a possible solution (configurable extension preference or a warning). The context is specific to Sphinx and the MyST parser, and the reproduction steps are concrete. While there is some room for interpretation in how to implement the config option or warning, the requirements are clear enough to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7356": {
        "explanation": "The issue clearly describes a regression: a change in anchor generation for Python functions from underscores to dashes, which breaks existing links. It provides a concrete example, references the relevant change (#7236), and explains the impact. However, it does not specify what the desired solution is: should the change be reverted, made configurable, or should a redirect mechanism be added? There is a sensible interpretation (restore the old behavior or provide compatibility), but some implementation details are left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7374": {
        "explanation": "The issue clearly describes a regression caused by a specific change (#7236) that altered the way fragment identifiers are generated for Python functions in documentation\u2014from using underscores to using dashes. It provides a concrete example, the expected vs. actual behavior, and the impact (broken links). However, it does not specify what the desired fix is: should the change be reverted, made configurable, or something else? While the intent is clear (restore or address the breaking change), there is some ambiguity about the exact solution expected. Still, a sensible interpretation would be to restore the previous behavior or provide a way to maintain backward compatibility.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7380": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, steps to reproduce, expected behavior, and a link to a repository with a setup that demonstrates the problem. The problem is specifically about the Sphinx C++ domain parser failing to handle parentheses in template parameter packs. The expected behavior is also clearly described. This is sufficient information for a developer familiar with the Sphinx C++ domain to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7395": {
        "explanation": "The issue clearly describes the problem: the 'index' directive with 'builtin' type and the 'function' directive generate inconsistent index entries in the documentation. It provides concrete examples of the input and the resulting output, enumerates the specific differences, and links to relevant documentation and the original bug report. The desired outcome is to unify and merge the index entries generated by both directives, making them consistent. While it does not specify exactly which format to use, it is clear that the goal is consistency, and as an experienced engineer, I can make a sensible decision or follow existing conventions. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7440": {
        "explanation": "The issue provides a clear error message from Sphinx about a duplicate glossary term ('mysql') in the documentation, and points to the relevant file and line. It also provides reproduction steps and context. The only ambiguity is whether the terms 'MySQL' and 'mysql' should be considered distinct (case-sensitive) or not, but the error and the question in the issue ('MySQL != mysql term right ?') suggest the reporter expects them to be treated as different. A reasonable solution would be to resolve the duplicate by either merging or renaming one of the terms, or clarifying the intended behavior. Thus, the issue is reasonably specified, though not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7454": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the expected behavior, and relevant environment details. It specifies exactly what is inconsistent (the presence or absence of a link for 'None' in different autodoc_typehints modes) and what the desired outcome is (that 'None' should always be linked). This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7462": {
        "explanation": "The issue provides a clear description of the bug, including the error message, the code snippet that triggers the error, steps to reproduce, expected behavior, environment details, and even a link to a reproducible repository. The problem is specifically about Sphinx failing to handle a function annotated with an empty tuple type, resulting in an IndexError. The expected outcome is also clear: documentation should build successfully and display the function with the correct type annotation. This is sufficient information to attempt a fix in the Sphinx codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7578": {
        "explanation": "The issue provides a clear description of the bug: autosummary links are broken in the master branch as of a specific commit, with a concrete warning message and a reproducible test case (the tinybuild example). It also mentions a possible cause and rules it out, and gives explicit steps to reproduce the problem. While it doesn't specify the exact fix, it is clear what the problem is and what a successful solution would look like (autosummary links should work and the warning should not appear).",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7590": {
        "explanation": "The issue clearly describes a problem: Sphinx's C++ domain does not support parsing or documenting C++ user-defined literals (UDLs), as evidenced by a specific code example and the resulting error message. It also references the relevant part of the Sphinx codebase. The request is to add support for UDLs in Sphinx's C++ domain. While the exact details of the implementation are not specified, the goal is clear: Sphinx should be able to parse and document C++ code that uses UDLs without error. This is a reasonably well-scoped feature request, though some design decisions (e.g., how to display UDLs in documentation) are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7593": {
        "explanation": "The issue provides a clear description of the current behavior (Sphinx converts :kbd: to a single <kbd> element), the desired behavior (parse the :kbd: value and convert it to individual or nested <kbd> elements as per MDN's suggestion), and even includes examples of both the current and desired HTML output. It also references relevant documentation and related issues/PRs. While there is a slight ambiguity about whether to use individual or nested <kbd> elements, the intent is clear: improve the HTML output to better represent key sequences. A sensible interpretation can be made, and a meaningful solution can be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7615": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the expected behavior (with explicit mapping of input to output), and even a screenshot. The environment is specified, and the difference from Docutils is explained. There is no ambiguity about what needs to be fixed: Sphinx should render consecutive backslashes in the same way as Docutils, following the RST spec. This is well-specified for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7670": {
        "explanation": "The issue requests support for C++20 'requires clauses', referencing the relevant C++ language feature and providing a link for more information. However, it does not specify what 'support' means in the context of the codebase: Is this a parser, a linter, a documentation generator, or something else? There is no information about the current state of support, what the expected behavior is, or what a successful implementation would look like. While the general direction is clear (add support for 'requires clauses'), the lack of context about the codebase and the specifics of the required support make it ambiguous.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-7738": {
        "explanation": "The issue provides a clear description of the bug (overescaped trailing underscore in attribute names when using napoleon), a minimal reproducible example, explicit steps to reproduce, the expected behavior, and relevant environment information. The expected output is also described, and the difference from the current output is specified. This is sufficient for a developer to attempt a fix and verify it.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7748": {
        "explanation": "The issue describes a specific problem: when using autodoc_docstring_signature with SWIG-wrapped C++ classes that have overloaded methods, only the first signature is picked up, but the convention is to include all signatures at the start of the docstring. The request is to enhance autodoc_docstring_signature to pick up all signatures. While it doesn't specify the exact output format or edge cases, the core requirement is clear and actionable: update the function to extract all method signatures from the docstring, not just the first. This is a sensible and implementable interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7757": {
        "explanation": "The issue provides a clear description of the bug: when documenting a function with a positional-only argument that has a default value, the default value is not shown in the generated documentation. The issue includes a minimal reproducible example, the expected behavior, and the actual result (with a screenshot). The environment is specified, and there is enough information to attempt a fix in the codebase responsible for rendering function signatures. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7760": {
        "explanation": "The issue provides a clear description of the current behavior (missing coverage is only written to a file, not surfaced as a warning), the desired behavior (a new config variable to print missing coverage as warnings to stdout/stderr), and even suggests a config name (`coverage_print_missing_as_warning`). The context (tox, sphinx coverage extension) and the motivation are well explained. There is some minor ambiguity about whether the warnings should go to stdout or stderr, but this is a small detail that can be sensibly decided by the implementer. Overall, the requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7762": {
        "explanation": "The issue clearly describes the current limitation (hardcoded Accept header in linkcheck), the problem it causes (inability to check certain URLs that require custom headers), and the desired solution (make HTTP headers configurable). It also provides context, examples, and alternative workarounds considered. The scope of the change is clear: allow users to configure HTTP headers for linkcheck, presumably via a configuration option. While the exact configuration interface is not specified, it is reasonable to infer that a new config option (e.g., linkcheck_headers) should be added, and the code should use these headers when making requests. This is a common pattern in Sphinx extensions. Therefore, the issue is well-specified enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7814": {
        "explanation": "The issue provides a clear description of the bug, including the exact warning message, a minimal reproducible example, the Sphinx and Python versions, and the relevant configuration. It also includes a proposed code change that partially solves the problem and describes its limitations (i.e., it doesn't work for custom classes). The expected behavior is clearly stated: Sphinx should resolve type annotations and link to the referenced classes. While the issue points out that the proposed fix doesn't fully solve the problem for custom classes, the overall problem and the desired outcome are well-specified, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7859": {
        "explanation": "The issue provides a clear description of the bug, including the exact configuration change that triggers it (setting autodoc_typehints = 'description'), the error encountered (KeyError: 'module'), and a full traceback. It also includes precise reproduction steps (git clone, checkout, edit conf.py, make html), environment details (OS, Python, Sphinx versions, extensions), and relevant logs. The expected behavior is implied: Sphinx should not crash with this configuration. The only minor ambiguity is whether the fix should be in the project or upstream in Sphinx, but given the context, a meaningful attempt at a solution (such as a workaround or patch) is possible.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7889": {
        "explanation": "The issue provides a clear description of the bug (TypeError when building docs for generically-typed classes due to string concatenation with a TypeVar in Autodoc's mock._make_subclass), steps to reproduce (including a public repository, environment setup, and commands), expected behavior, and relevant environment information. The log file is referenced for further details. The only minor ambiguity is whether the fix should be in the user's code or in Sphinx/Autodoc, but the context and phrasing suggest the expectation is for a fix in the extension. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7906": {
        "explanation": "The issue clearly describes the current behavior (that :noindex: prevents cross-referencing and permalinking), the expected behavior (that :noindex: should only suppress index entries, not permalinks or cross-references), and provides concrete input (ReST source) and output (HTML) examples. The desired output is explicitly shown. This is sufficient to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7910": {
        "explanation": "The issue provides a clear description of the problem: when __init__ is decorated (even with functools.wraps), Sphinx (with napoleon_include_init_with_doc=True) does not document it. The user has traced the problem to a specific line in Sphinx's code, explained why it fails (decorated function's __globals__ does not contain the class), and provided environment details. The expected behavior is clear: decorated __init__ methods should be documented if napoleon_include_init_with_doc=True. The root cause and context are well-explained, making it possible to attempt a solution (e.g., patching Sphinx to better handle decorated methods).",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7930": {
        "explanation": "The issue provides a clear description of the problem (a regression warning when using autodoc with dataclasses and custom types), steps to reproduce (including a sample project and exact commands), the expected behavior, and environment details. The only minor ambiguity is the precise nature of the warning, but since a reproducible project is attached, this can be directly investigated. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7961": {
        "explanation": "The issue expresses a desire to upgrade the Sphinx MathJax extension to support MathJax 3.0, referencing documentation and highlighting potential benefits. However, it does not specify what 'support' entails (e.g., full replacement, dual support for v2 and v3, configuration options, compatibility requirements, or testing criteria). It also frames the upgrade as a discussion point ('What do folks think?'), rather than a concrete request for implementation. As such, there is significant ambiguity about the scope and requirements for a successful solution.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-7975": {
        "explanation": "The issue clearly describes a bug in the HTML output of the index: when certain symbols are used as leading characters in index entries, two 'Symbols' sections are created, both with the same anchor, causing navigation issues. The expected behavior is implied: there should be only one 'Symbols' section, and the anchor should be unique so that links work correctly. The description provides enough detail to reproduce and understand the problem, and a successful solution would consolidate the 'Symbols' sections and ensure correct anchor behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7985": {
        "explanation": "The issue clearly describes the current behavior (linkcheck does not check local/internal links), provides a minimal reproducible example, and specifies the expected behavior (linkcheck should also check local links). The context is sufficient to understand what is required: extend linkcheck so that it reports broken internal references (e.g., references to anchors or documents that do not exist in the Sphinx project). While it does not specify every edge case, the main requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8007": {
        "explanation": "The issue clearly describes the desired feature: an option in autodoc to prevent the unfolding of user-defined type aliases in generated documentation. It provides a concrete example (Position = int, pos: Position) and explains the motivation. While it does not specify the exact configuration interface or name for the option, the intent and expected behavior are clear enough for a developer familiar with autodoc to implement a solution or at least make a meaningful attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8035": {
        "explanation": "The issue clearly describes the current limitation (all or nothing for :private-members:), the desired behavior (allowing :private-members: to take arguments specifying which private members to include), and references how :members: works as a model. It also mentions current workarounds and links to prior discussion. The requirements are clear and actionable for someone familiar with Sphinx autodoc internals.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8037": {
        "explanation": "The issue provides a clear error message, a specific code snippet that triggers the error, and detailed reproduction steps (including a repository branch, directory, and commands to run). The expected behavior is also stated: the build should succeed. While it does not specify the exact fix, it is clear that the goal is to make Sphinx's C++ parser accept valid C++14 syntax involving 'std::enable_if' in template parameter lists. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8056": {
        "explanation": "The issue clearly describes a rendering bug when using numpydoc-style docstrings with multiple parameters on the same line, specifically regarding the display of the 'optional' qualifier in the generated HTML. It provides a concrete example, a screenshot, and the expected output. The environment and relevant Sphinx extensions are also listed. While it does not specify which codebase or Sphinx theme is being used, the problem and expected behavior are clear enough to attempt a solution, likely in the Sphinx or numpydoc extension code.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8075": {
        "explanation": "The issue provides a clear description of the bug, including the context (Sphinx documentation with figures without captions), the configuration used, the test document, and the observed errors in both HTML and LaTeX outputs. It also specifies the expected behavior, including suggestions for possible solutions (e.g., making uncaptioned figures referencable, improving warning messages). The environment is detailed, and the problem is reproducible with the provided information. While there is some room for interpretation in the exact implementation (e.g., how to make uncaptioned figures referencable), the requirements are clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8117": {
        "explanation": "The issue provides a clear description of the bug, including the specific error message, the relevant configuration, and a minimal reproducible example. It also specifies the expected behavior (the macro should be ignored while parsing the function declaration) and provides enough context (Sphinx version, extensions, and a link to the project) to attempt a solution. The only minor ambiguity is whether the fix should be in the Sphinx domain parser or in documentation, but the context and error message make it clear that the parser should be updated to handle trailing macros listed in c_id_attributes. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8120": {
        "explanation": "The issue provides a clear description of the problem (custom locale translations in 'locale/<language>/LC_MESSAGES/sphinx.po' are ignored in favor of the official translations), steps to reproduce (including a public repository and specific commit to check out), the expected behavior (custom translations should override the official ones), and relevant environment details. There is enough information to attempt a solution, such as investigating how Sphinx loads translation files and why the local .mo file is not being prioritized. The references and screenshot further clarify the context.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8125": {
        "explanation": "The issue provides a clear description of the problem: autodoc-skip-member is not being called for all identifiers in a module if __all__ is defined, which prevents users from customizing documentation inclusion for private members. The desired solution is also specified: Sphinx should pass all members (not just those in __all__) to autodoc-skip-member, possibly with an indicator in the options object about __all__ exclusion. While there is some room for interpretation about the exact implementation (e.g., how to indicate __all__ exclusion), the core requirement is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8202": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example (both Python and rst), exact steps to reproduce, the expected behavior, and relevant environment details. The error message is specified, and the context (Sphinx doc generation with a specific code pattern) is well-explained. There is a clear, actionable goal: prevent the warning from appearing when generating docs for this code pattern. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8264": {
        "explanation": "The issue provides a detailed error log and context (Sphinx version, Python version, environment details, and the stack trace). However, it does not specify what the expected behavior is, what the user was trying to document, or what steps to take to reproduce the error. There is no information about the source files, configuration, or custom code that might be causing the problem. While the error is clear, the lack of reproduction steps or a minimal example makes it ambiguous what a successful solution would be (e.g., is the fix in user code, a Sphinx bug, or a documentation issue?). Thus, the issue is too vague for a meaningful attempt at a solution without further information.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-8265": {
        "explanation": "The issue provides a clear description of the bug (incorrect rendering of a default argument in the HTML documentation), a reproducible Dockerfile-based environment, the expected vs. actual output, links to the relevant code and documentation, and even a screenshot. The problem is well-scoped: fix the rendering of the default argument for a specific method in the generated HTML docs. There is no ambiguity about what is broken or what the correct output should be. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8269": {
        "explanation": "The issue provides a clear description of the current behavior, the expected behavior, and a reproducible example. It specifies that when linkcheck_anchors is True, HTTP errors (like 404) should be reported as such, rather than as 'Anchor not found'. The expected output is explicitly shown, and the environment is detailed. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8273": {
        "explanation": "The issue clearly describes the current behavior (all man pages are placed in a single-level directory), the problem (this does not work with MANPATH because man expects section directories like man1, man3, etc.), and the desired solution (sphinx should generate section directories and place man pages in the appropriate ones). The only ambiguity is how to determine the correct section for each man page, but in most Sphinx man page generation setups, the section is specified in the configuration or the document metadata. Therefore, there is a sensible interpretation for implementing this feature, even if some minor details may need to be inferred from the codebase or conventions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8278": {
        "explanation": "The issue clearly describes the problem: when documenting a function with a default argument specified in hexadecimal, Sphinx renders the value in decimal in the generated documentation. The expected behavior is also clearly stated: the documentation should preserve the number system (hexadecimal) as written in the source. There is a concrete example, screenshots, and a rationale for why this matters. The environment is specified, and the scope of the fix is clear (preserve the original number system in rendered documentation for default arguments). This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8282": {
        "explanation": "The issue clearly describes a bug: the 'autodoc_typehints' setting in Sphinx's conf.py does not affect overloaded callables as expected. It provides a minimal reproducible example, including the relevant configuration, source code, and the expected behavior (that autodoc_typehints should control the display of type hints for overloaded functions). The environment and versions are specified. While it does not specify the exact output or error, the expectation is clear enough for a developer familiar with Sphinx and autodoc_typehints to attempt a fix and verify it by checking if the type hints for overloaded functions are hidden or shown according to the setting. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8284": {
        "explanation": "The issue clearly describes the current behavior (all man pages are placed in a single-level directory), the problem (this does not work with MANPATH and the man program's expectations), and the desired solution (automatically create section directories like man/man1, man/man3, etc., and place each man page in the appropriate section). It also mentions an alternative workaround and notes a potential breaking change. The requirements are clear and actionable, and a successful solution would be to modify the man page generation so that it outputs to the correct sectioned directory structure.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8435": {
        "explanation": "The issue provides a clear description of the bug: the Sphinx configuration option 'autodoc_type_aliases' does not affect variables and attributes as expected. It includes a minimal reproducible example (example.py, index.rst, conf.py), specifies the expected behavior (that the type alias should be applied to both a module-level variable and a class attribute), and provides relevant environment information. The expected outcome is unambiguous, and the steps to reproduce are clear. This is sufficient for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8459": {
        "explanation": "The issue provides a clear description of the bug, including the relevant configuration, code samples, the observed and expected outputs, and the environment. It is clear that the user expects autodoc_type_aliases to be respected even when autodoc_typehints is set to 'description', and provides concrete examples of the current and desired behavior. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8474": {
        "explanation": "The issue describes a new warning message that appears after upgrading to Sphinx 3.3, specifically related to table numbering and possibly the 'numref' feature. However, the issue is primarily a request for help in identifying the cause of the warning, not a concrete request for a code change or bug fix. There is no clear acceptance criteria or actionable task beyond investigating the warning. As such, it is ambiguous what a 'successful solution' would be\u2014whether it's suppressing the warning, fixing a bug, or just providing information. Therefore, the issue is too vague for a meaningful attempt at a solution as a software engineer.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-8475": {
        "explanation": "The issue provides a clear description of the problem: when a HEAD request results in a TooManyRedirects exception, the current linkchecker does not fall back to a GET request, causing some valid links to be reported as broken. The desired behavior is to extend the fallback logic so that a GET request is attempted in this scenario. The issue also references a specific URL and a relevant change in Sphinx 3.2.0, giving context. The required change is to modify the exception handling in the linkchecker to include TooManyRedirects in the fallback logic. This is a well-specified bugfix request.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8506": {
        "explanation": "The issue provides a clear description of the problem: Sphinx 3.2 is now stricter about the syntax accepted by the 'option::' directive, causing previously valid documentation to fail to build. It includes the specific example that triggers the error, the error message itself, and a rationale for why the previous behavior (accepting more flexible option strings) is preferable. However, the issue is not a request for a specific change in the QEMU documentation or a codebase, but rather a complaint about Sphinx's behavior. It is not clear whether the solution should be to change the documentation to conform to Sphinx 3.2's requirements, to patch Sphinx, or to do something else. Therefore, while the problem is well-described, the required action is ambiguous.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-8509": {
        "explanation": "The issue clearly states the desired change: set uplatex as the default latex_engine for Japanese docs in Sphinx. It provides context about why this is beneficial, references relevant upstream issues and PRs, and notes compatibility. The scope is clear (change the default engine for Japanese docs), and there are no ambiguous requirements or missing details that would prevent a meaningful implementation attempt.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8539": {
        "explanation": "The issue provides a clear description of the problem, including configuration, steps to reproduce, expected behavior, and actual behavior. It also includes code samples and environment information. The desired outcome is well-articulated: type hints from __init__ should be reflected in the class docstring's parameter descriptions when autodoc_typehints='description', and no duplicate or extraneous 'Parameters' sections should be generated. There is a specific, actionable bug to address, and the expected behavior is clearly stated. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8548": {
        "explanation": "The issue describes a specific problem: autodoc's 'inherited-members' option does not work for inherited attributes (data members) because it only searches for docstrings using (namespace, attrname) and does not check base class namespaces. The expected behavior is implied: autodoc should find and document inherited data members' docstrings from base classes. While the issue does not specify the exact code location or desired output, it is clear what needs to be fixed and why. A reasonable solution would involve modifying autodoc's lookup logic to include base classes when searching for docstrings for attributes. Thus, the issue is reasonably specified, though not perfectly detailed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8551": {
        "explanation": "The issue provides a clear description of the bug, including the context (Sphinx autodoc cross-referencing), a detailed example to reproduce the problem, the observed warnings, and the expected behavior. The reproduction steps are concrete, and the expected fix is unambiguous: implicit xrefs in :type: and :rtype: should resolve unqualified names in the same way as explicit xref roles, prioritizing the current module and parent modules, not searching all modules. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8552": {
        "explanation": "The issue clearly states that support for the 'Receives' section of NumpyDoc docstrings should be added to Napoleon, and provides a link to the relevant documentation. It also explains the context: 'Receives' is similar to 'Yields', which is already supported, and gives a summary of how 'Receives' should be formatted and when it should appear. While it does not specify every implementation detail, it is clear what the desired outcome is: Napoleon should parse and render the 'Receives' section according to NumpyDoc guidelines. This is a well-specified feature request for someone familiar with Napoleon and NumpyDoc.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8579": {
        "explanation": "The issue provides a clear description of the bug (a crash in linkcheck with a specific exception), a full traceback, exact steps to reproduce (including repository, commands, and environment), and a link to a CI failure. The expected behavior is also stated. This is sufficient information for a developer to attempt to reproduce and debug the problem, and to know when it is resolved (linkcheck should not crash).",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8593": {
        "explanation": "The issue clearly describes the problem: the `:meta public:` directive in a variable's docstring is not causing the variable (with a leading underscore) to be included in the generated documentation by Sphinx autodoc, as the user expects. The reproduction steps are concrete, including a code sample and the relevant Sphinx configuration. The expected behavior is explicitly stated. The environment is specified. There is enough information to investigate and attempt a fix or at least to reproduce and analyze the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8595": {
        "explanation": "The issue clearly describes the bug: when autodoc processes a module with an empty __all__ attribute, it ignores it and documents all members, whereas the expected behavior is that no members should be documented. The reproduction steps are clear, including a code sample and the relevant Sphinx directive. The expected and actual behaviors are both explicitly stated. The environment is specified, and there is no ambiguity about what needs to be changed in autodoc's behavior. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8599": {
        "explanation": "The issue clearly describes the desired feature: allowing users to customize the HTML content of permalinks and viewcode links in Sphinx-generated documentation. It specifies the motivation, current behavior, and the proposed solution (two new configuration options: html_add_permalinks_html and viewcode_source_html). It also provides concrete examples of the expected configuration and output. While there is a mention of uncertainty about the best implementation approach, the requirements for the feature are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8611": {
        "explanation": "The issue provides a clear description of the problem: Sphinx is generating a warning/error about an undocumented inherited attribute (`staticMetaObject`) in a subclass (`Reenter`) of `QObject`. The issue includes the exact error message, a minimal reproducible example (with full setup and reproduction steps), the relevant code, environment details, and the expected behavior (no warning for inherited attributes). The only ambiguity is whether the solution should be a code change, a documentation configuration change, or a workaround, but the core requirement (suppress or resolve the warning for the inherited attribute) is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8621": {
        "explanation": "The issue provides a clear description of the bug, including specific examples of input and the incorrect HTML output generated. It also specifies the expected behavior for both single keystrokes and compound keystrokes involving the separator characters. The problem is well-scoped, and the requirements for a successful solution are unambiguous: the :kbd: role should correctly distinguish between separator characters used as keystrokes and as separators, generating the appropriate HTML. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8633": {
        "explanation": "The issue provides a clear description of the bug, including the error message, the context (building psf/requests documentation with Sphinx 3.4.1), steps to reproduce both in the documentation build and in an interactive Python console, and the versions of the relevant packages. The root cause is also indicated (AttributeError due to 'LookupDict' lacking a '__name__' attribute). This information is sufficient for a developer to reproduce the problem and attempt a fix, such as handling objects without a '__name__' attribute in Sphinx's restify function.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8638": {
        "explanation": "The issue provides a clear description of the bug: instance variables in Sphinx autodoc-generated documentation are incorrectly auto-linked to other variables of the same name elsewhere in the project, even when unrelated. It includes a concrete reproduction case with a public demo repository and step-by-step instructions to observe the problem. The expected behavior is also clearly stated: instance variable documentation should not be auto-linked to other variables of the same name. While the underlying cause (the 'referencing heuristic') is not fully detailed, the problem and the desired outcome are both clear, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8674": {
        "explanation": "The issue provides a clear feature request: enable passing options to Pygments, specifically mentioning the :startsinline: option for PHP snippets. It suggests two ways to specify options: globally in conf.py and per sourcecode block. While it doesn't specify the exact API or configuration syntax, the intent and the required functionality are clear. As an experienced engineer familiar with Sphinx and Pygments, I could reasonably interpret and implement this feature, making sensible decisions about configuration and documentation. Some details would need to be filled in, but the core requirement is clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8679": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, expected behavior, and links to both a live demonstration and the source project. The problem is well-scoped: references to rubric directives with quoted text are not rendered correctly, while titles with quoted text are. The environment is specified, and the expected output is clear. This is sufficient information for an experienced developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8684": {
        "explanation": "The issue suggests extending the ':dedent:' option for code-block and literalinclude directives to behave like Python's textwrap.dedent, specifically performing a full dedent when no value is provided. While the general intent is clear (make dedent default to full dedent if no value is given), there are some blanks to fill in: e.g., how exactly this should be implemented, whether it applies to both code-block and literalinclude, and if there are edge cases to consider. However, a sensible interpretation is possible and a meaningful solution can be attempted.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8707": {
        "explanation": "The issue describes a problem with the 'viewcode' extension in Sphinx not working as expected when building documentation with certain make targets. It provides steps to reproduce, expected behavior, and environment details. However, there is some ambiguity: the title and description mention both 'singlehtml' and 'html', and the expected behavior says 'always enabled for HTML builds (except singlehtml and epub)', but the reproduction steps include both 'singlehtml' and 'html' in the make command. It is not entirely clear whether the bug is that viewcode is enabled/disabled in the wrong targets, or if the problem is with the interaction between targets. Still, a reasonable interpretation is that viewcode should be enabled for 'html' but not for 'singlehtml' or 'epub', and currently it is not behaving that way. Thus, while there are some blanks to fill in, a sensible attempt at a solution can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8721": {
        "explanation": "The issue clearly describes the problem: when running 'make html epub', the viewcode extension creates module pages for the epub output even when the configuration option 'viewcode_enable_epub' is set to False. The expected behavior is also specified: module pages should not be created for epub by default. The environment and reproduction steps are provided. While there is no code snippet, the configuration option and the undesired behavior are clear, making it possible to attempt a solution by ensuring that viewcode respects the 'viewcode_enable_epub' setting when building epub output.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8729": {
        "explanation": "The issue provides a clear description of the bug, steps to reproduce, expected vs actual behavior, and even a sample project to test with. It also suggests a possible solution (introduce an option to not search recursively or disable recursive search with implicit namespaces). The environment is specified. While the exact implementation details are left to the developer, the requirements and the problem are well-defined, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8771": {
        "explanation": "The issue clearly describes the current behavior (Sphinx autodoc renders default argument values as their literal values, not as the constant names) and the desired behavior (autodoc should have a way to keep the constant name in the signature). It provides a concrete example and explains the motivation. While it does not specify exactly how the feature should be implemented (e.g., config option, autodetection, etc.), it is clear what the end result should be. There is a sensible interpretation for a solution: modify autodoc so that, when possible, it renders default values as their constant names rather than their literal values. This is enough to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8801": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the expected behavior, and relevant environment details. It specifies that an attribute with only a type annotation and a docstring in a superclass is not being documented in subclasses by autodoc unless :undoc-members: is used, and that this is not the desired behavior. The expected outcome is unambiguous: such attributes should be documented as inherited members. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8969": {
        "explanation": "The issue provides a clear description of the current behavior (absolute paths in the :file: option of the csv-table directive are treated as filesystem-absolute, not source-relative), a comparison to the behavior of the figure directive, and a concrete example of the problem. The expected behavior is also described: the user wants absolute paths in :file: to be interpreted as relative to the documentation source directory, matching the figure directive. While the user is unsure if this is a bug or a feature request, the technical change required is clear: modify the csv-table directive to support source-relative absolute paths, or provide an option to do so. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9015": {
        "explanation": "The issue provides a clear description of the bug, including a code example, the observed and expected behavior, and even a screenshot of the rendered documentation. It specifies that the problem is with how 'TracebackType' is rendered and linked in the generated documentation, and what the correct behavior should be (either showing 'TracebackType' and linking to the correct external documentation, or at least not linking to the local property). The environment and relevant Sphinx extensions are also listed. This is sufficient information to attempt a meaningful solution, such as adjusting the autodoc configuration or code to ensure correct linking of external types.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9104": {
        "explanation": "The issue provides a detailed reproduction path, including exact commands, environment, and the specific warning message from Sphinx. It also points to the relevant code and describes the user's uncertainty about the cause (possibly an ellipsis in the docstring/type annotation). However, the expected behavior is not fully clear: the reporter is unsure what the correct output should be, only suggesting that the warning should not occur and that perhaps the ellipsis should be rendered as plain '...'. There is ambiguity about whether the solution should suppress the warning, change the docstring, or link to external documentation. While a reasonable interpretation is possible (fix the docstring so Sphinx does not emit the warning and the output is as expected), some guesswork is required about the desired end state. Thus, the issue is not fully well-specified, but it is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9128": {
        "explanation": "The issue provides a clear description of the problem: Sphinx autodoc emits a duplication warning when documenting an aliased object (in this case, StringIO imported from io and referenced both as example.StringIO and io.StringIO). The steps to reproduce are explicit, including code snippets and the exact warning message. The expected behavior is also clearly stated: there should be no warning. The environment and context are sufficiently detailed. This is enough information to attempt a meaningful solution, such as investigating why autodoc emits the warning and how to suppress or resolve it for aliased objects.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9155": {
        "explanation": "The issue provides a clear description of the problem: when documenting C functions in Sphinx, using a builtin type like 'int' in :param: or :type: fields causes a warning about an unparseable C cross-reference. The expected behavior is also specified: builtin types should behave like unknown types (e.g., 'int123'), where no warning is generated and no link is created. The issue includes a minimal reproducible example, the exact warning message, and environment details. This is sufficient information to attempt a solution, such as modifying the cross-reference handling for builtin types.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9171": {
        "explanation": "The issue clearly describes the undesired behavior (the constructor signature being included in the class header by `autoclass`), the context (Python domain, Sphinx autodoc), and the desired outcome (suppressing the constructor signature in the generated heading). The user also explains why this is problematic for their use case. While the issue is phrased as a question, it is clear what change is being requested: a way to suppress the constructor signature in the class header generated by `autoclass`. This is sufficient for a meaningful attempt at a solution, such as documenting a configuration option, a workaround, or a code change to support this behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9207": {
        "explanation": "The issue provides a clear description of the problem, including the observed behavior (a new warning in Sphinx 4.0.0 that did not appear in 3.5.4), a minimal reproduction repository, exact steps to reproduce, the expected behavior (no warning), and relevant environment details. It also identifies the specific commit that introduced the issue. This is sufficient information for a developer to investigate and attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9229": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, step-by-step instructions to reproduce the problem, expected behavior, actual behavior (with screenshot), environment details, and a link to a gist with the project. The problem is well-scoped: type alias docstrings are inconsistently rendered in Sphinx-generated documentation. The expected outcome is also clear: all type alias docstrings should be rendered, not just some. This is sufficient information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9230": {
        "explanation": "The issue provides a clear description of the bug, including the specific docstring syntax that causes the problem, the incorrect and expected rendering, steps to reproduce, screenshots, and environment details. The expected output is explicitly stated, and a sample project is attached. This is sufficient information for a developer to attempt a fix and verify correctness.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9231": {
        "explanation": "The issue clearly describes the problem: the 'man_make_section_directory' option is now enabled by default, which breaks existing workflows by creating an unexpected directory structure for man pages. It also specifies the expected behavior (do not enable by default), provides concrete examples of breakage, and clarifies the correct output structure. While it does not specify the exact code change, it is clear what needs to be done: revert the default to disabled and ensure the output directory structure matches expectations. This is enough information for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9233": {
        "explanation": "The issue provides a clear motivation (customizing the formatting of base class lists, especially for parameterized types), references a related use case, and proposes a concrete solution: a new hook/event with a specific signature. It also describes the intended behavior and offers an alternative (returning a string for more freedom). The context (modifying autodoc output in Sphinx) is clear, and the requirements for the hook are well-defined. While there is some room for minor interpretation (e.g., exact integration details), the core request is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9234": {
        "explanation": "The issue provides a clear description of the problem (undesired redirects being allowed by the link checker), the desired solution (allow listing specific redirect pairs), and an example of both a good and bad redirect. It also describes an alternative workaround and gives context for why the feature is needed. The requirements are specific enough to implement a feature that allows specifying allowed redirect pairs and prohibits others, with error reporting for unlisted redirects. The only minor ambiguity is the exact format for specifying allowed pairs, but this can be reasonably inferred or implemented with a sensible default (e.g., a config file or command-line option).",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9246": {
        "explanation": "The issue provides a detailed reproduction scenario, including environment setup, code samples, configuration, and the exact error trace. It also analyzes the probable cause (the missing handling of 'pending_xref_condition' in a Sphinx extension), references related code and previous issues, and asks a specific question about whether the current behavior is intended or an antipattern. However, the issue is not a straightforward bug report or feature request; it is more of a design discussion or request for clarification about Sphinx's event API and best practices for extension authors. While a solution could be attempted (e.g., improving documentation, refactoring the event API, or providing a utility function), the issue leaves some ambiguity about the desired outcome (documentation update, code change, or just confirmation). Thus, it is not fully well-specified, but there is a sensible interpretation for a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9258": {
        "explanation": "The issue clearly requests support for union types using the '|' syntax in type specifications within docstrings, provides a concrete example, and references existing tooling that already supports this feature. The expected behavior is well described, and the scope is clear: update the system to recognize and handle the '|' union type syntax in docstring type annotations. There are no major ambiguities about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9281": {
        "explanation": "The issue provides a clear description of the problem (Enum values in function signatures are rendered with their full repr instead of a cleaner name), a minimal reproducible example (a GitHub repo), the expected vs actual output, and relevant environment details. The desired output is explicitly stated, and the context (Sphinx autodoc rendering) is clear. This is sufficient for a developer to attempt a solution, such as customizing the rendering of default Enum values in autodoc.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9289": {
        "explanation": "The issue provides a clear description of the bug, including the configuration used, a minimal reproducible example, the observed behavior, and the expected behavior. It specifies the relevant settings (`python_use_unqualified_type_names` and `autodoc_typehints='description'`), the Sphinx version, and the Python version. The expected output is also described in detail. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9320": {
        "explanation": "The issue provides a clear description of the problem, including the steps to reproduce, the expected behavior, and the actual behavior. It also includes a screenshot for further clarification. The desired outcome is explicitly stated: pressing Enter at the prompt should exit sphinx-quickstart when a conf.py already exists. The environment details are also provided. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9367": {
        "explanation": "The issue clearly describes the bug (incorrect rendering of 1-element tuples), provides a concrete example of the expected vs. actual output, and specifies exactly where to add a test case. It also references related issues and PRs for context. The required change is clear: ensure that (1,) is rendered with the trailing comma. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9386": {
        "explanation": "The issue provides a clear description of the bug: type hints are still rendered in autodoc property documentation even when 'autodoc_typehints' is set to 'none'. It includes steps to reproduce (with a downloadable zip), expected behavior, actual behavior (with screenshot), and relevant environment details. The expected outcome is unambiguous: no type hints should appear in the output. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9461": {
        "explanation": "The issue provides a clear description of the problem: Sphinx does not document methods decorated with both @classmethod and @property. It includes a minimal reproducible example, a test repository, specific methods that are not documented, the expected behavior, and relevant environment details (OS, Python version, Sphinx version, extensions used). There is also context about Python version changes and links to related discussions. This is sufficient information to attempt a meaningful solution, such as investigating and patching Sphinx's autodoc behavior for these cases.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9464": {
        "explanation": "The issue clearly describes the bug (PEP 585 generics like list[str] are not rendered correctly by autodoc), provides a minimal reproducible example, specifies the expected behavior, and even points to the likely location in the codebase where the fix should be made. The environment and versions are specified, and a test repository is provided. This is sufficient information for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9467": {
        "explanation": "The issue provides a clear description of the problem: a specific type of GitHub anchor link that previously passed Sphinx's linkcheck now fails in version 4.1.0. It includes a concrete example of the link, steps to reproduce the issue, the expected behavior, and references to the relevant code change and failing test logs. While it does not specify the exact technical solution, it is clear what the desired outcome is (the link should pass linkcheck as before). This is sufficient for an experienced engineer to investigate and attempt a fix or regression handling.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9547": {
        "explanation": "The issue provides concrete examples of currently unsupported GNU C extension types (`_Complex`, `_Fract`, etc.), the parsing errors encountered, and references to relevant code sections and documentation. It also suggests a possible direction for a fix (handling these types similarly to how 'unsigned' is handled) and provides links to the relevant GCC documentation. While there is some room for interpretation in the exact implementation details, the requirements are clear: extend the C domain parser to support these GNU extension types so that the provided function declarations are parsed without errors. This is a reasonably specified issue, as the main goal and context are clear, but some design decisions are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9591": {
        "explanation": "The issue provides a clear description of the bug (cross-references not working in property's type annotations), a minimal reproducible example, steps to reproduce, expected behavior, and even a sample project and generated documentation for reference. The environment (OS, Python, Sphinx version, and extensions) is specified. It is clear what is required: ensure that documented types in property type annotations are cross-referenced in the generated documentation. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9602": {
        "explanation": "The issue clearly describes the problem: Sphinx's nitpick mode incorrectly treats Literal values in type annotations as needing a py:class reference, which causes warnings/errors for values like True. The reproduction steps are detailed, including a code sample and a link to a failing project. The expected behavior is also specified: Literal values should not trigger nitpick warnings. All necessary context (Sphinx version, Python version, extension used) is provided. This is sufficient for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9654": {
        "explanation": "The issue provides a clear description of the bug (incorrect base class shown in documentation), a concrete example (class D showing A instead of C as base), exact reproduction steps (including repository, file, and commands), and a statement of expected behavior (should show direct superclass). It also provides relevant context (Sphinx version, extension used, related issues). This is sufficient for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9658": {
        "explanation": "The issue provides a clear description of the bug, including the context (Sphinx documentation for inherited classes from mocked classes), the specific symptom (the 'Bases' section in the generated docs is incorrect), and the expected behavior (the base class should be 'torch.nn.Module', not 'torch.nn.'). It also provides detailed reproduction steps, links to example builds, relevant environment information, and even a demo PR. This is sufficient information to attempt a meaningful solution, such as investigating the Sphinx autodoc configuration, mocking setup, or possible workarounds.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9665": {
        "explanation": "The issue provides a clear description of the desired feature: Sphinx should automatically insert a reference to the documentation page of a base class if it is documented somewhere. It also discusses an alternative solution (allowing `autodoc-process-signature` to return a list of strings to customize the 'Bases:' line), and gives context about previous related issues and why they were insufficient. While there are some implementation details left to the developer, the core requirement is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9673": {
        "explanation": "The issue provides a clear description of the problem: the Sphinx config option 'autodoc_typehints_description_target = \"documented\"' does not work as expected when used with the Napoleon extension (Google style docstrings). It specifies the expected behavior (return type should be present in the documentation if the return is documented), provides a minimal configuration and example docstring to reproduce the issue, and even includes a screenshot. The project and environment details are also given. This is sufficient information to attempt a meaningful solution, such as debugging the interaction between autodoc_typehints and Napoleon, and ensuring the return type is rendered as expected.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9698": {
        "explanation": "The issue clearly describes the bug: when using the `py:method` directive with the `:property:` option in Sphinx, the generated index entry incorrectly includes parentheses, which is not expected for properties. The reproduction steps are provided, including the exact reStructuredText input. The expected behavior is also clearly stated: the index entry for the property should not have parens. There is even a screenshot showing the current (incorrect) output. All necessary context to attempt a fix is present, including Sphinx version and Python version. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9711": {
        "explanation": "The issue clearly describes a bug: the 'needs_extensions' check compares version strings lexicographically rather than as version numbers, leading to incorrect behavior (e.g., '0.10.0' is considered less than '0.6.0'). The expected behavior is also specified: version comparisons should be numeric, so '0.10.0' should be accepted if the minimum is '0.6.0'. The reproduction steps, expected outcome, and context are all provided. It is clear what needs to be fixed: update the version comparison logic in 'needs_extensions' to use proper version parsing (e.g., using packaging.version or similar).",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9799": {
        "explanation": "The issue clearly describes the problem: when using autodoc to document a function with a hexadecimal default argument, the generated HTML shows the value in decimal rather than hexadecimal. The expected behavior is also specified: the default value should be shown in hexadecimal, or there should be an option to enforce this. The reproduction steps are clear, and the code sample is provided. While the issue does not specify exactly how the option should be configured (if an option is to be added), it is clear what the user wants to achieve. Therefore, a meaningful attempt at a solution can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9828": {
        "explanation": "The issue provides a clear problem statement (fuzzy translations are not currently included in mo files), a concrete proposed solution (add a configuration parameter 'use_fuzzy_translations'), and even points to the relevant line of code and the supporting library feature. While there may be some minor implementation details to decide (e.g., how the configuration parameter is exposed to users), the core requirements and the expected behavior are clear. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9829": {
        "explanation": "The issue provides a clear description of the problem (difficulty configuring MathJax 3 with Sphinx due to script loading order and config serialization), references relevant prior issues and PRs, and outlines three concrete solution directions. It also explains why the current approach is insufficient and what the desired outcome is. While there is some flexibility in implementation details, the requirements and context are clear enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9902": {
        "explanation": "The issue provides a clear description of the problem: when using the .. py:data:: directive with the :type: option in Sphinx, specifying a type by its short name (e.g., 'Unit') does not create a clickable link, whereas specifying the full module path does. The expected behavior is well-articulated, including code samples, the desired output, and the context in which the bug occurs. The project repository is provided, and relevant environment details are included. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9931": {
        "explanation": "The issue clearly describes the current behavior, the desired behavior, and provides concrete examples of both. It also suggests possible solutions (extending the existing config option or adding a new one) and references prior art and alternatives. While there is some flexibility in the exact naming of a new config option, the core requirement\u2014controlling the display of module names in type annotations in autodoc output\u2014is unambiguous. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9997": {
        "explanation": "The issue clearly describes the bug: autodoc typehints do not generate links for parametrized types (such as typing.Literal), while they do for simple types (like int). It provides a minimal reproducible example, expected behavior, and even a screenshot. The environment and relevant Sphinx extensions are specified. The desired outcome (Literal should link to typing.Literal in the CPython docs) is unambiguous. This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11384": {
        "explanation": "The issue provides a clear description of the current and desired behavior for the `fps` function's output: it should print as a formal power series (i.e., as a sum or formula), not as a truncated normal series. The example output and the string representation are given, and the user requests that the printed output reflect the underlying formal power series. There is also a secondary question about error handling when the series cannot be computed, suggesting an exception should be raised instead of returning the function. Both requests are actionable and specific enough to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11400": {
        "explanation": "The issue clearly describes the current behavior of ccode(sinc(x)), shows the output, and provides a reasonable suggestion for what the output should look like (i.e., expand sinc(x) into its definition using a Piecewise expression, similar to how the user manually wrote it). The expectation is clear: ccode should generate valid C code for sinc(x) by expanding it, since C does not have a built-in sinc function. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11438": {
        "explanation": "The issue provides a clear example of a misclassification in the 'classify_diop' function, where an equation with mixed even powers is incorrectly classified as a 'general_sum_of_even_powers'. The issue also specifies the desired behavior: a check should be added to ensure all powers are the same, not just even. The code context and the expected change are both clear, making it possible to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11618": {
        "explanation": "The issue clearly describes a bug in the distance calculation between two Point objects of potentially different dimensions. It provides a concrete example, the expected vs. actual output, and identifies the root cause (zipping dimensions ignores extra coordinates). This is sufficient information to attempt a fix: ensure all dimensions are considered in the distance calculation, possibly by raising an error or handling points of different dimensions appropriately.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11788": {
        "explanation": "The issue clearly describes a problem: Singularity function powers do not print correctly in the qtconsole. It provides a reproducible code snippet, specifies the environment (qtconsole), and includes a screenshot showing the incorrect output. The expected behavior is implied: the singularity function powers should print correctly. While it does not specify exactly what 'correctly' means, the context and screenshot make the problem and the desired outcome clear enough for a developer familiar with SymPy and its printing system to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11794": {
        "explanation": "The issue states that ASCII printing for Singularity Functions is needed, but it does not specify what 'ASCII printing' should look like, what a 'Singularity Function' is in the context of the codebase, or what the expected output or interface should be. There is significant ambiguity about the requirements and the desired outcome.",
        "label": "VAGUE"
    },
    "sympy__sympy-11822": {
        "explanation": "The issue provides a clear error traceback, the exact input that triggers the exception (Symbol('')), and a hint that the problem is related to the unicode pretty printer. It also notes that a workaround exists (using use_unicode=False). The stack trace points to a specific function (split_super_sub) and line where the error occurs. This is sufficient information to reproduce the bug and attempt a fix, such as handling the case where the symbol name is an empty string. The expected behavior is implied: Symbol('') should not raise an exception when printed, even with unicode pretty printing enabled.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11862": {
        "explanation": "The issue provides a clear description of the problem: when using lambdify with the mpmath module, high-precision Floats (with more than 16 digits) are being converted to double precision, resulting in a loss of precision. The user provides a reproducible code example, demonstrates the difference in output, and specifies the expected behavior (that mpmath should preserve high precision). The only minor ambiguity is the exact location in the codebase where the conversion happens, but the problem and the expected outcome are clear. Therefore, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11870": {
        "explanation": "The issue provides concrete code examples, expected outputs, and a clear description of the desired behavior: that expressions involving exponentials should be simplified to their trigonometric equivalents using a function like trigsimp. The corollary further clarifies the expectation for a related case. While it does not specify the exact implementation details, it is clear what the user wants to achieve, and a solution can be attempted based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11897": {
        "explanation": "The issue provides concrete examples where the LaTeX printer and the pretty printer produce different outputs for the same expressions. It also states a general expectation: the LaTeX printer should match the pretty printer unless LaTeX can provide a better output. However, the definition of 'better output' is subjective and not clearly specified. Additionally, the issue mentions that assumptions (like 'positive=True') should not affect printing, but does not clarify what the expected output should be in those cases. While the examples help, there is ambiguity in what is considered 'better' and what the exact output should be, making the requirements open to interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-11989": {
        "explanation": "The issue provides concrete examples of input expressions for the 'diophantine' function, the expected solutions, and the actual (incorrect) outputs. It also suggests a possible area of the code (elliptic case) that may be problematic. While it does not specify the exact cause or the desired algorithm, it is clear that the function should return all integer solutions for the given quadratic forms, and currently does not. This is enough information to reproduce the bug and attempt a fix, though some implementation details may need to be inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12088": {
        "explanation": "The issue provides a clear description of the problem: Poly does not use the correct precision for coefficients unless mpmath.mp.dps is set, and even specifying a custom domain with higher precision does not work as expected. The issue includes reproducible code examples, expected vs. actual behavior, and context about the confusion between base-10 and base-2 precision. While it does not specify the exact implementation approach, it is clear what the desired outcome is: Poly should respect the specified precision, either from the domain or from the input, regardless of mpmath.mp.dps. This is enough to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12096": {
        "explanation": "The issue provides a clear description of the problem, including a reproducible code example, the expected vs. actual output, and a pointer to the relevant code location (`Function._eval_evalf`). It also explains the underlying cause: `evalf` is not being called recursively on the result of `_imp_`. This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12108": {
        "explanation": "The issue clearly describes the current behavior (the string representation of logic expressions uses function-like notation) and the desired behavior (the string representation should use symbolic operators: ~, &, |). It also specifies that correct parenthesization is important. The scope is clear: update the str (and non-Unicode pretty) printer for logic expressions. There is enough information to proceed with a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12171": {
        "explanation": "The issue clearly describes two specific problems with the Mathematica code printer: (1) incorrect handling of derivatives, and (2) incorrect formatting of floats with exponents. It provides concrete examples of the current (incorrect) output and the expected output. Furthermore, it suggests specific code changes to address both issues. While the suggested code may need refinement or adaptation to fit the codebase, the requirements and expected behavior are clear, making it possible to implement and test a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12194": {
        "explanation": "The issue clearly describes the desired feature: a function or option in sympy that returns the list of prime factors of an integer, including multiplicity (e.g., [2, 2, 2, 3] for 24), rather than the current dictionary format. It even suggests two possible solutions: an option to factorint or a new function. The expected output is unambiguous, and the motivation is clear. This is sufficient information to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12214": {
        "explanation": "The issue describes a problem encountered when using the 'spline_basis' function (or 'bspline_basis_set') with B-splines of degree greater than 1, providing code snippets and an example of knot vector construction. However, it is not clear what the expected behavior is, what the actual error or output is, or what the desired 'basis matrix' should look like. The issue also contains a question about whether the user is missing something, rather than a clear request for a code change or bug fix. There is ambiguity about whether the problem is with the function, the knot vector construction, or user misunderstanding. Therefore, it is too vague to attempt a meaningful solution.",
        "label": "VAGUE"
    },
    "sympy__sympy-12227": {
        "explanation": "The issue provides a clear description of the current behavior, the problem (inability to specify binary precision directly in the Float constructor), and a proposed solution (allow both dps and prec as keyword arguments, with precedence rules and error handling if both are specified). It also discusses backwards compatibility concerns and the need for a deprecation cycle. While some implementation details (such as the exact deprecation mechanism) are left to the implementer, the requirements for the new API and its behavior are well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12236": {
        "explanation": "The issue provides a clear, reproducible code example, including the Python and SymPy versions, and demonstrates the unexpected behavior: 'bug.apart(t)' returns '-a*t' instead of the expected partial fraction decomposition (as seen when 'a' is substituted with 1). The problem is well-isolated, and the expected vs. actual outputs are shown. While the issue does not explicitly state what the correct output should be for the general case, it is clear from the context and the behavior after substitution what is expected. Therefore, a meaningful attempt at a solution can be made.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12270": {
        "explanation": "The issue provides concrete examples of the unexpected behavior, including both cases that work and those that do not, and pinpoints the relevant code area (core/expr.py) and the mechanism (primitive decomposition and non-evaluated Mul). It also references a specific commit that may be related. The only ambiguity is whether the behavior is truly a bug or an intentional design, but the technical details and expected/actual outputs are clear enough to attempt a fix or at least a diagnostic PR. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12307": {
        "explanation": "The issue clearly describes a problem: Sympy's Fortran code generation for the sign function produces invalid Fortran code. It provides a reproducible example, references the correct Fortran syntax, and suggests a preferred output format (e.g., sign(1, x) or sign(1d0, x)). While there is a minor uncertainty about the exact behavior for x=0, the main requirement is clear: update the code generation so that the output is valid Fortran syntax. This is enough information for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12419": {
        "explanation": "The issue provides a clear description of the observed bug: when summing all elements of an identity matrix (resulting from M.T * M for an orthogonal matrix M), the result is 0 instead of the expected 'n'. The code sample is complete, demonstrates the problem, and specifies the expected vs. actual behavior. The context (SymPy, symbolic matrices, assumptions) is clear. The only minor ambiguity is whether the bug is in the summation logic, the matrix construction, or the assumptions system, but the core issue is well-defined and reproducible.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12454": {
        "explanation": "The issue provides a clear description of the bug: Matrix.is_upper raises an IndexError for non-square (specifically tall) matrices. It includes a reproducible code snippet, the traceback, the relevant code, and an analysis of the problematic indices. The expected behavior is implied: is_upper should not raise an error for a 4x2 matrix (or any non-square matrix), and should handle such cases gracefully. The information is sufficient to attempt a fix and write a test for the described scenario.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12472": {
        "explanation": "The issue provides a concrete example of the problematic behavior (how sqrt splits out non-real factors), explains why this is incorrect, and references a previous fix (PR #12472). The desired behavior is implied: sqrt should not split out non-real factors, and this should be tested. While it doesn't specify the exact test to write, it is clear that the solution involves ensuring sqrt does not split out non-real factors and that this is covered by a test. Thus, there is a sensible interpretation for a successful solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12481": {
        "explanation": "The issue clearly describes the current behavior (raising a ValueError when non-disjoint cycles are passed to the Permutation constructor) and the desired behavior (the constructor should accept non-disjoint cycles, apply them in left-to-right order, and return the resulting permutation). The example provided is concrete, and the expected semantics for non-disjoint cycles are specified. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12489": {
        "explanation": "The issue provides a clear description of the problem: subclassing combinatorics.Permutation is not possible due to the way instances are created internally, always returning Permutation instances rather than instances of the subclass. The issue also suggests a direction for a solution (using classmethods and proper use of the class reference in __new__). While it does not specify the exact code changes required, it is clear what the problem is and what a successful solution would look like: refactor instance creation so that subclassing works as expected. There is some room for interpretation in the implementation, but the requirements are clear enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12798": {
        "explanation": "The issue provides a clear reference to a specific change (the PR link) and states that the change should be reverted. While it does not specify the exact code to change, the instruction to 'revert the change' is unambiguous given the context and the reference. An experienced engineer can review the linked PR, identify the changes to 'ff' and 'rf' definitions, and revert them. Therefore, the issue is well-specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12881": {
        "explanation": "The issue clearly describes the current incorrect behavior (Poly(x,x,y).free_symbols returns {x, y} instead of just {x}), the desired behavior (only free symbols of generators that actually appear in the polynomial expression should be included), and provides a code snippet that attempts to address the problem. While there is a minor uncertainty expressed about the 'domain part', the main requirement is clear and actionable. The uncertainty about the domain can be reasonably interpreted or left unchanged if not relevant to the main fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12906": {
        "explanation": "The issue provides a concrete code example demonstrating the problem: calling .canonical on a relational object does not yield a canonical result, as evidenced by the fact that r.canonical != r.canonical.canonical. The expected behavior is implied: .canonical should be idempotent (i.e., applying it multiple times should not change the result after the first application). The context (Relational.canonical) and the specific failure are clear, so a developer can attempt to fix the method to ensure idempotency. No further clarification is strictly necessary.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12977": {
        "explanation": "The issue provides a clear description of the problem: calling a function with zero arguments (a nullary function) currently raises a ValueError due to an empty sequence being passed to max(). The stack trace and code context are provided, and the desired behavior is stated: nullary functions should be allowed, including for explicit subclasses of Function. While it does not specify the exact implementation, the expected outcome is clear and testable. There is enough information to attempt a solution by modifying the relevant code to support nullary functions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13001": {
        "explanation": "The issue provides a concrete code example, the observed behavior, and the suspected cause (that cse leaves behind a 'hollow Mul' with evaluate=False). However, it does not specify what the expected behavior should be: should cse always return an equivalent expression (in terms of ==)? Should it avoid returning unevaluated Mul? Should it force evaluation? Without a clear statement of the desired outcome, there is ambiguity in how to resolve the issue. Thus, while the problem is described, the solution requirements are not fully specified.",
        "label": "VAGUE"
    },
    "sympy__sympy-13031": {
        "explanation": "The issue provides a clear description of a behavioral change in the sympy Matrix.hstack function between versions 1.0 and 1.1, with concrete code examples and the observed outputs. However, it does not specify what the desired or correct behavior should be (i.e., whether the old or new behavior is correct, or what the expected output should be for the given inputs). Without this clarification, it is ambiguous what the 'fix' should be: should the code revert to the old behavior, keep the new one, or do something else? Therefore, while the issue is well-documented, it is not fully specified in terms of what constitutes a successful solution.",
        "label": "VAGUE"
    },
    "sympy__sympy-13043": {
        "explanation": "The issue clearly identifies a specific function (decompose in intpoly), describes the problematic behavior (returns a list of arbitrary order), and suggests that this is causing test failures. It also raises questions about the intended use and possible solutions (sorting, returning a set, or just using the dict). However, it does not specify exactly how the output should be ordered or which solution is preferred. There is enough information to make a reasonable attempt (e.g., by sorting the output), but some interpretation is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13091": {
        "explanation": "The issue provides a clear description of the problem (incorrect return value in rich comparison methods), the rationale (to allow Python's comparison delegation to work as intended), and a concrete example (line 316 in basic.py). It also references relevant documentation and explains the expected behavior. While it mentions uncertainty about whether other locations need to be changed, the main requirement is clear: update rich comparison methods to return NotImplemented when the type is unknown. This is a well-specified and actionable issue.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13146": {
        "explanation": "The issue provides a clear code example, the observed output, and the desired output (simplification to 0). It is clear that the user expects the symbolic simplification to recognize that the two terms cancel each other out. The problem is well-scoped: improve the simplification so that such expressions are reduced to 0. The only minor ambiguity is whether the fix should be in the simplify function, evalf, or elsewhere, but the main requirement is clear.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13173": {
        "explanation": "The issue describes a problem with the 'degree' function when applied to multivariate expressions: it returns the degree with respect to the first generator, which is chosen in a way that may not be obvious to the user, leading to ambiguity. The issue provides concrete examples and suggests that the function should require the generator to be specified to avoid this ambiguity. However, it does not specify exactly what the new behavior should be (e.g., should the function raise an error if the generator is not specified? Should it require a keyword argument? Should it return a tuple of degrees for all generators?). There is a clear problem and a general direction for a solution, but some implementation details are left to the developer's judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13185": {
        "explanation": "The issue provides a detailed code example, the observed output, and highlights the specific problem: cse() produces substitutions where x0 and x2 are just copies of the input matrices a and b, respectively, which is described as 'strange behaviour.' However, the issue does not specify what the expected or desired behaviour is, nor does it state what would constitute a correct or improved output. Without a clear statement of what the correct output should be, or what the user expects instead of the current behaviour, it is ambiguous what a successful solution would look like. Therefore, while the problem is demonstrated, the requirements for a fix are not fully specified.",
        "label": "VAGUE"
    },
    "sympy__sympy-13236": {
        "explanation": "The issue provides a clear description of the desired behavior: that expressions like factorial(n) % n (and more generally, factorial(n) % k for 1 <= k <= n) should automatically simplify to 0 in a symbolic mathematics library. It also provides a concrete example of current behavior and references related issues and fixes. While it does not specify the exact implementation, the requirements for a successful solution are clear: ensure that the simplification logic recognizes the divisibility property of factorials for symbolic positive integers. Thus, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13259": {
        "explanation": "The issue provides a clear example of incorrect symbolic simplification in SymPy involving trigonometric functions with complex arguments. It specifies the input, the observed (incorrect) output, and the expected behavior (that the simplified expression should be numerically equivalent to the original). The mathematical context and the specific transformation that is wrong are described. While it does not specify the exact fix, it is clear what the bug is and what a correct solution should achieve: ensure that simplification preserves numerical equivalence for such expressions. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13264": {
        "explanation": "The issue provides a clear description of the problem: the 'simplify' function in SymPy does not always reach the most reduced form in a single call, but does so if called recursively. The user provides a concrete example, describes the expected behavior, and even suggests a code change (adding a 'max_rec_steps' parameter for recursive simplification). While the attached .zip file and images are not accessible in this context, the textual description is detailed enough to understand the problem and the proposed solution. Therefore, a meaningful attempt at a solution can be made based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13265": {
        "explanation": "The issue provides a clear description of the problem: the simplification routines in the codebase fail to recognize and simplify an expression for sin(1) when it is expressed in terms of exponentials, while similar expressions for cos(1) and -sin(1) are recognized. The issue includes a reproducible code example, the expected and actual outputs, and a specific context (matrix exponentials and simplification functions). It is clear that the goal is to improve the simplification so that sin(1) is recognized in the same way as cos(1) and -sin(1). This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13286": {
        "explanation": "The issue provides a clear description of the problem: the periodicity function in SymPy returns 2*pi for Abs(sin(x)), but the expected result is pi. It includes a code example demonstrating the current (incorrect) behavior. The issue also references related issues (#13205 and #13207) and mentions that the fix should prevent infinite recursion for relational expressions and improve the periodicity calculation for absolute trigonometric functions. While there is some ambiguity about the exact implementation details, the desired outcome is clear: periodicity(Abs(sin(x)), x) should return pi, and recursion errors for relational expressions should be avoided. This is enough information for an experienced developer to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13301": {
        "explanation": "The issue clearly states that the current str and srepr representations of AccumBounds are not recreatable (i.e., they do not return a string that can be used to reconstruct the object). It also specifies that only the pretty printer should display the pretty form, implying that str and srepr should return a string that can be evaluated to recreate the object. The expected behavior is clear, and the examples provided make it unambiguous what needs to be changed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13361": {
        "explanation": "The issue provides a clear description of the bug, including a minimal reproducible example, the observed incorrect output, the expected behavior, and a hypothesis about the cause. It also suggests a possible direction for a fix. The context (Python and SymPy versions) is specified. This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13364": {
        "explanation": "The issue clearly describes the desired behavior: the __pow__ method (and thus pow()) should support a third, optional argument for modular exponentiation, as per Python's numeric protocol. It specifies that this should be implemented at least for Integer, and ideally for arbitrary expressions, referencing a related issue for Mod support. The current error is shown, and the expected behavior is implied by the Python documentation link. While there is some room for interpretation regarding 'arbitrary expressions,' the core requirement (support ternary pow for Integer) is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13369": {
        "explanation": "The issue provides a clear code example that fails, the error message, and a comparison with an alternative approach that works. The expected behavior is that M.eigenvals() should return the symbolic eigenvalues, as solve(det(lam*eye(3) - M), lam) does. The problem is well-demonstrated, and the desired outcome is clear: fix .eigenvals() so that it returns the symbolic eigenvalues for this matrix. There is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13372": {
        "explanation": "The issue provides a clear error (UnboundLocalError in evalf), a minimal reproducible example, and a suggested direction for a fix (adding an else clause to raise NotImplementedError if reprec/imprec are not set). The context (changing Mul arg order) and the relevant code location are given. While the root cause analysis is not complete, the problem and a plausible solution are well described, making it possible to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13429": {
        "explanation": "The issue provides a clear description of the problem: comparisons between rational numbers and irrational numbers (specifically pi) in SymPy can yield inconsistent or logically impossible results (e.g., neither less than, equal to, nor greater than). The issue includes a reproducible code example, the environment details, and an analysis of the likely cause (use of evalf with limited precision). The expected behavior is also implied: comparisons between rationals and irrationals should be logically consistent and decidable. This is sufficient information to attempt a meaningful fix or at least to improve the comparison logic.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13441": {
        "explanation": "The issue provides a clear description of the problem: the count_ops function is slow for large expressions, and it includes a reproducible example (using n_link_pendulum_on_cart with 3 and 4 links) that demonstrates the performance degradation. The stack trace shows that count_ops is being called deep within the symbolic differentiation process, and the user reports that the script hangs for larger expressions. However, the issue does not specify what the expected performance should be, nor does it suggest any particular approach for improvement. Still, the problem is well-defined: count_ops is too slow for large expressions, and the goal is to make it faster or avoid the hang. This is a common type of performance issue report, and a meaningful attempt at a solution can be made (e.g., profiling, optimizing, or refactoring count_ops).",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13471": {
        "explanation": "The issue provides a clear description of the problem: pickling a SymPy expression containing a float in Python 2 and unpickling it in Python 3 fails with a specific error. It includes a minimal working example, the exact error message, and the context (SymPy, Python 2.7, Python 3.6). The expected behavior is implied: the pickled object should be loadable in Python 3 without error. While it does not specify the exact fix, it is clear what needs to be addressed and how to reproduce the problem, making it well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13480": {
        "explanation": "The issue provides a clear and reproducible error, including the code to trigger it, the error message, and the context (substitution on coth(log(tan(x))) for certain integer values). The root cause is also hinted at: a NameError due to an undefined variable 'cotm' in the code. The expected behavior is that the substitution should not raise this error. This is sufficient information to attempt a fix (define or correct 'cotm' in the relevant function).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13551": {
        "explanation": "The issue clearly describes a specific symbolic computation in SymPy that is producing an incorrect result. It provides a concrete example, the expected correct value, and a reference to the mathematical concept (q-Pochhammer symbol) that should be used. The required fix is to correct the evaluation of Product(n + 1 / 2**k, [k, 0, n-1]) so that it matches the mathematically correct result. This is sufficient information for a developer familiar with symbolic computation to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13581": {
        "explanation": "The issue provides a concrete example (Mod(Mod(x + 1, 2) + 1, 2) should simplify to Mod(x, 2)) and references a related StackOverflow discussion, which gives context. It also suggests a more general recursive simplification for expressions like Mod(foo*Mod(x + 1, 2) + non_mod_terms + 1, 2). While the generalization is a bit open-ended, the intent is clear: improve the simplification logic for nested Mod expressions, especially modulo 2. There is enough information to make a meaningful attempt at a solution, though some implementation details would need to be decided by the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13615": {
        "explanation": "The issue provides a clear example of the problem, including the input, the current (incorrect) output, and the expected output. It specifies that the Complement operation does not work as expected when the input set is a mixture of symbols and numbers. The expected behavior is also clearly described. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13647": {
        "explanation": "The issue provides a clear example of unexpected behavior with the Matrix.col_insert() method in sympy. It includes the code to reproduce the problem, the expected and actual outputs, and the version of sympy used. While it does not specify exactly what the correct output should be, the context and the example make it clear that the col_insert method is not behaving as intended. This is sufficient for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13678": {
        "explanation": "The issue provides a clear set of failing assertions that describe the expected behavior for the is_real (and in one case, is_negative) property of various trigonometric and hyperbolic functions when applied to symbolic variables with certain assumptions. The expected output (None or True) is explicitly stated for each case. This gives a concrete and testable specification for what needs to be fixed in the codebase. While the underlying reason for the current behavior is not discussed, the required outcome is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13682": {
        "explanation": "The issue provides a high-level overview of the desired feature (ordinal arithmetic in SymPy), suggests two possible approaches (new object or new assumptions), and lists several required features (arithmetic operations, relationals, Cantor normal form, etc.). However, it lacks detailed specifications for the API, expected behaviors, edge cases, and integration with existing SymPy features. There is also ambiguity about which approach to take and how deep the implementation should go. While a motivated developer could make reasonable assumptions and start a prototype, there is significant room for interpretation and ambiguity.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13744": {
        "explanation": "The issue provides a clear symptom (doctest failure due to stack overflow) and a traceback that pinpoints the recursion in the 'eliminate_word' function in 'sympy/combinatorics/free_groups.py'. It also references a specific CI job for context. While it does not specify the exact input that triggers the failure, the stack trace and file references are sufficient for a developer with access to the codebase and test suite to reproduce and investigate the problem. The goal is to fix the stack overflow in the coset_table doctest, which is a clear and actionable objective.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13757": {
        "explanation": "The issue provides a clear description of the problem, including specific code examples that demonstrate the inconsistent behavior when multiplying a Poly object by another expression, depending on the order of operands. The expected and actual outputs are shown, making it clear what the user expects (that the multiplication should evaluate to a Poly in both cases) and what currently happens. The context (Python versions, library version) is also provided. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13761": {
        "explanation": "The issue provides a clear code example that triggers an exception, including the stack trace and the specific error (KeyError: sinc). The user clarifies that they do not expect a mathematical simplification, but rather that the simplification function should not error out. The expected behavior is that simplify(x + csch(sinc(1))) should return the unevaluated expression or handle it gracefully, not raise an exception. This is a well-specified bug report with a clear reproduction path and expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13773": {
        "explanation": "The issue clearly describes the current behavior (that __matmul__ allows scalar operands by delegating to __mul__), the desired behavior (that __matmul__ should only allow matrix-matrix multiplication and fail otherwise), and provides both current and expected examples, including reference to NumPy's behavior. The required change is unambiguous: update __matmul__ to raise an error if either operand is not a matrix. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13798": {
        "explanation": "The issue clearly describes the current limitation (the latex() function's mul_symbol kwarg only accepts four hardcoded choices), the desired behavior (allowing arbitrary user-supplied mul_symbol values, specifically to allow a thin space '\\,'), and provides concrete examples. The request is to make the mul_symbol argument accept any string, while maintaining backwards compatibility. This is a well-scoped and actionable feature request.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13806": {
        "explanation": "The issue describes a lack of support for the LaTeX expression \\[{90^^\\circ }\\], which is intended to represent '90 degrees' (as in cos(90 degrees)). The user requests that this be supported. While the exact context (e.g., which parser, renderer, or application) is not specified, it is clear that the user wants the system to correctly interpret and render the LaTeX degree symbol (\\circ) in math expressions. There is some ambiguity about the double caret (^^), which is not standard LaTeX, but the intent is reasonably clear: support for degree notation in LaTeX math rendering. A sensible interpretation would be to ensure that expressions like \\(90^\\circ\\) are rendered correctly. Thus, while not perfectly specified, it is possible to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13840": {
        "explanation": "The issue description is vague and lacks clear requirements. It mentions a problem with converting Max & Min using SymPy and references issues with JavaScript and R, but does not specify what 'conversion' means, what the expected behavior is, or what part of the codebase is involved. There is also no clear definition of what a successful solution would look like. The attached image may provide some context, but without further information or clarification, it is not possible to determine the exact nature of the problem or how to resolve it.",
        "label": "VAGUE"
    },
    "sympy__sympy-13852": {
        "explanation": "The issue provides a clear example of the current behavior (polylog(2, 1/2) not evaluating to the expected closed form), the expected output, and a rationale for why the current expansion for polylog(1, z) is problematic (unnecessary use of exp_polar). It also provides mathematical justification and test cases. While there are two related but distinct requests (improving polylog evaluation and fixing exp_polar in expansion), both are well-motivated and concrete. The issue is detailed enough to attempt a solution: implement correct evaluation for polylog at rational arguments and ensure expand_func for polylog(1, z) returns -log(1-z) without exp_polar. The requirements are clear and testable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13877": {
        "explanation": "The issue provides a clear description of the problem: computing the determinant of a symbolic matrix in SymPy fails for certain sizes, resulting in 'nan' or an exception. It includes a reproducible code example, the observed outputs, and a traceback. The user also suggests a possible cause (Bareiss algorithm being used on non-integer matrices) and implicitly requests that the determinant computation should not fail in this way for symbolic matrices. While the issue could be improved by specifying the expected behavior (e.g., should it raise a more informative error, or use a different algorithm?), it is clear enough to attempt a fix: ensure that determinant computation for symbolic matrices does not result in 'nan' or an invalid comparison error. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13878": {
        "explanation": "The issue provides a clear and detailed description of the problem: for several continuous distributions in SymPy, the CDF is currently computed by integration, which often fails or is inefficient. The issue specifies the desired solution: implement internal _cdf methods with precomputed CDF formulas for the listed distributions. It provides concrete test cases for each distribution, references to documentation and Wikipedia for formulas, and even suggests a method for testing correctness (differentiate the CDF and compare to the PDF). The requirements and success criteria are explicit and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13895": {
        "explanation": "The issue provides a concrete example of a symbolic expression in SymPy that, when simplified, produces an expression that is not equivalent to the original for certain values of x. It includes code snippets, the specific expressions involved, and numerical evaluations that demonstrate the inequivalence. However, the issue does not specify what the expected behavior should be: Should simplify avoid this transformation? Should it warn the user? Should it only simplify under certain conditions? There is a clear bug or unexpected behavior, but the desired resolution is not stated. Therefore, while a sensible interpretation is possible (e.g., 'fix simplify so it does not produce inequivalent expressions'), there is some ambiguity about the exact requirements.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13915": {
        "explanation": "The issue describes a specific behavior in SymPy where substituting b with a in the expression r yields 1, even though mathematically the expression becomes undefined (division by zero). The user points out that the correct mathematical behavior would be for the result to be undefined, or perhaps to return NaN or raise an exception. The issue also mentions that the limit as b approaches a is -1, which is different from the substitution result. However, the issue does not specify what the expected behavior should be in terms of SymPy's API: should subs() return NaN, raise an exception, or something else? There is also ambiguity about whether this should be a global change to subs(), or only for certain types of undefined expressions. Thus, while the problem is clear, the desired solution is not fully specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13962": {
        "explanation": "The issue is reasonably well-specified: it describes the current behavior (quantities print with their full name, e.g., 'meter'), the desired behavior (they should print with their abbreviation, e.g., 'm'), and points to the relevant code and test locations. It also notes that a test will need to be changed. However, there is a small amount of ambiguity about whether all quantities should always use the abbreviation, or if there are exceptions, and whether this should be configurable. Still, the main intent is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13971": {
        "explanation": "The issue clearly describes a problem with the LaTeX rendering of the output of SeqFormula in Jupyter: the square brackets are being backslash-escaped (\\[ and \\]), which causes issues when copying the output to a markdown cell. The desired behavior is for the output to use unescaped square brackets ([ and ]) so that it renders correctly in markdown. The issue provides a concrete example and a clear expected outcome. It is clear what needs to be changed in the code: the LaTeX representation of SeqFormula should not escape the square brackets. This is well-specified enough to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13974": {
        "explanation": "The issue provides a clear description of the problem: powers of tensor product expressions are not being evaluated as expected by either the 'expand(tensorproduct=True)' method or the 'tensor_product_simp' function. It includes a detailed example session showing the current (incorrect) behavior and what the expected output should be in certain cases. The examples make it clear that the goal is for expressions like 'tp(1,1)*tp(1,1)' to be simplified to 'tp(1,1)' (or '1x1'), and similarly for other tensor products. The expected results are shown in the session, making it clear what a successful solution would look like. Therefore, the issue is well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13978": {
        "explanation": "The issue clearly describes a problem: SymPy generates Octave code for imaginary numbers in the form 'sqrt(3)i', which is not valid in Octave 4.0. It also suggests a specific fix: use 'sqrt(3)*i' instead. The scope is clear (code generation for imaginary numbers in Octave), and the expected output is well-defined. There is little ambiguity about what needs to be changed and what the correct output should look like.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14024": {
        "explanation": "The issue provides a clear description of an inconsistency in the simplification behavior of an expression involving powers and negative bases in a symbolic mathematics library. It gives concrete code examples, expected and actual outputs, and highlights the difference in results when using a symbolic variable versus a concrete value. The problem is well-demonstrated, and the desired outcome is implied: the simplification should be consistent regardless of whether 'a' is a symbol or a concrete value, given the same assumptions. This is sufficient information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14031": {
        "explanation": "The issue provides a concrete code example that triggers the error, includes the full error message, and references a related previous issue (#13970) for context. It is clear that the problem is with coercion of certain expressions involving exp and E in SymPy's integration routines. The goal is to fix or improve the coercion logic so that the provided code does not throw the error. While the exact technical solution is not specified, the problem is well-scoped and actionable for someone familiar with the codebase and the referenced issue.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14038": {
        "explanation": "The issue provides a clear code example, the current (incorrect) output, and the expected mathematical result (sinc(a)). It is clear that the infinite product should not evaluate to 0, and instead should return sinc(a). The mathematical context is well-known, and the bug is well-demonstrated. The required fix is to ensure that product(1 - a**2 / (n*pi)**2, [n, 1, oo]) returns the correct symbolic result. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14070": {
        "explanation": "The issue clearly describes the expected behavior: logcombine(log(3) - log(2)) should return log(3/2), but currently it does not. It also references a previous version where this worked, and provides a concrete example. The function, input, and expected output are all specified, making it clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14082": {
        "explanation": "The issue provides a clear description of the problem: integrating 1/(x**2 + y**2) with respect to x returns 0, but the expected result is atan(x/y)/y. The code to reproduce the issue is included, and the expected output is specified. There is also a reference to a similar issue (#8246), which may provide additional context. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14085": {
        "explanation": "The issue clearly describes a specific problem: sympify (or S) does not accept unicode Greek letters (e.g., '\u03b1') as valid input, even though SymPy can pretty-print symbols with such names. The expected behavior is that sympify(u\"\u03b1\") should return a Symbol('\u03b1'), analogous to how sympify('x') returns Symbol('x'). The error trace and the rationale are provided, and the desired outcome is clear. There is enough information to attempt a solution: update sympify/S to recognize and parse unicode Greek letters as valid symbol names.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14180": {
        "explanation": "The issue clearly describes a specific problem: the conversion of 'ln(10)' to LaTeX is currently producing 'log{\\left(10\\right)}', but it should produce '\\ln{\\left(10\\right)}' instead. The expected behavior is unambiguous, and the required change is well-defined. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14207": {
        "explanation": "The issue provides a clear code example that reproduces the problem, specifies the expected output, and explains the context (code generation, need for correct parenthesis to avoid type promotion). It also mentions that the problem affects both the string printer and code generation printers. The desired behavior is unambiguous: multiplication expressions like -2*u/(a*a) should be printed with necessary parentheses to avoid ambiguity. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14248": {
        "explanation": "The issue clearly describes the problem: when subtracting MatrixSymbols, the output is printed as a sum with (-1) coefficients instead of using the subtraction operator. It provides concrete code examples, expected behavior, and actual output. The context is sufficient to understand what needs to be fixed (the printing of differences involving MatrixSymbols), and the desired outcome is clear (print as a-b, not a+(-1)*b).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14308": {
        "explanation": "The issue provides a concrete code example that demonstrates the problem: pretty-printing of vector expressions in SymPy's vector module is broken, resulting in duplicated or misaligned output. It also mentions a secondary issue about baseline alignment. While it does not specify the exact desired output, the example and the description of the problems (duplication and baseline misalignment) are clear enough for an experienced developer to investigate and attempt a fix. The expected behavior (correct, non-duplicated, and centered pretty-printing) is reasonably inferable from context and standard pretty-printing conventions in SymPy.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14317": {
        "explanation": "The issue clearly describes a discrepancy between the LaTeX printer and the str/pretty printers for Poly objects: the order of monomials is inconsistent. It provides a concrete example, showing the expected and actual outputs, and specifies that the LaTeX printer should use the same monomial order as the other printers (from highest to lowest degree). This is a well-specified behavioral bug with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14333": {
        "explanation": "The issue provides a clear description of the current behavior of SymPy's mod_inverse function, including how it handles negative inputs and moduli, and contrasts this with mathematical conventions and the behavior of other languages. It also provides concrete examples and references to expected results (e.g., WolframAlpha). The problem is well-motivated: SymPy does not find modular inverses for negative moduli, and the sign of the result does not match mathematical conventions. While the issue does not specify the exact API or all edge cases, it is clear that the goal is to (1) allow negative moduli, and (2) ensure the result has the same sign as the modulus, matching mathematical convention and Python's behavior. This is sufficient for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14396": {
        "explanation": "The issue clearly describes a specific problem: constructing a Poly with domain='RR[y,z]' raises an OptionError, and the error message could be improved. The code to reproduce the issue is provided, and the expected behavior is implied (Poly should accept 'RR[y,z]' as a domain, or at least provide a better error message). While the precise intended behavior for 'RR[y,z]' as a domain could be debated, the issue is clear enough to attempt a solution: either support this domain or improve the error message. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14531": {
        "explanation": "The issue provides concrete examples of the problem, showing that the 'sympy_integers' setting in StrPrinter is not respected in certain subexpressions, such as within Eq and Limit. It also mentions a related issue in code generation. The expected and actual outputs are clearly shown, making it clear what the correct behavior should be. However, while the examples are clear, the scope of the fix (e.g., which subexpressions or printers are affected) may require some interpretation. Still, there is a sensible interpretation of what is required: ensure that the StrPrinter settings are respected recursively in all subexpressions. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14564": {
        "explanation": "The issue provides a detailed description of ambiguous behaviors in the ConditionSet class, including code examples, expected vs. actual outputs, and specific questions about how ConditionSet should behave in certain cases. It also suggests possible fixes and improvements, such as raising errors at instantiation, adding a doit method, and clarifying substitution behavior. While there are some open-ended questions (e.g., should ConditionSet(x, x>5, Interval(1,3)) evaluate to False?), the issue is clear about the problematic behaviors and gives enough context for a developer to propose and implement concrete improvements. There is a sensible interpretation of what is required: clarify and fix the behavior of ConditionSet in the described scenarios.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14627": {
        "explanation": "The issue clearly describes the expected behavior: binomial(n, n) should simplify to 1, especially when n is a positive integer. It provides a code example and references a specific commit after which the behavior changed. While there is a question about whether this was intentional, the main request is clear: ensure that binomial(n, n) simplifies to 1 under appropriate assumptions. This is a well-specified issue for a software engineer familiar with symbolic computation libraries.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14699": {
        "explanation": "The issue clearly describes a missing feature: the inability to specify indices when creating a new ReferenceFrame via the orientnew method in sympy.physics.mechanics. It provides an example of the current behavior and the desired behavior, and also suggests a possible enhancement (a global setting for default indices). While the global setting suggestion is less concrete, the main request (supporting indices in orientnew) is clear and actionable. There is enough information to attempt a solution by adding an indices parameter to orientnew and ensuring it is passed to the new ReferenceFrame.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14711": {
        "explanation": "The issue provides a clear code example that triggers the error, the error message, and the context (using SymPy's vector module). The problem is that summing a list of vectors, where one element is '0 * N.x', raises a TypeError because '0 * N.x' is not recognized as a Vector. The expected behavior is implied: summing vectors with zero terms should not error. The fix would likely involve ensuring that '0 * N.x' is treated as a zero vector or is compatible with the sum operation. The issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14774": {
        "explanation": "The issue clearly describes the problem (latex printer does not support full inverse trig function names for acsc and asec), provides a concrete example of the current and expected output, and even suggests a specific code change to resolve the issue. The file and line number are given, and the desired behavior is unambiguous. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14817": {
        "explanation": "The issue provides a concrete error traceback and a code example that triggers the problem, specifically when pretty-printing a MatAdd involving a MatrixSymbol with a name containing a '*'. It also points out that the code is incorrectly using sympify on string arguments from MatrixSymbol, and questions the logic around handling negative arguments in MatAdd's pretty printer. However, while the error and the problematic code path are clear, the issue does not specify exactly what the correct behavior should be (e.g., how should MatrixSymbol names with special characters be handled, or what the expected pretty-printed output should look like). There is enough information to make a reasonable attempt at a fix (stop sympifying MatrixSymbol names, clarify negative argument handling), but some interpretation is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14821": {
        "explanation": "The issue clearly states that the code generation for the two-argument zeta function is incorrect: `octave_code(zeta(x,n))` should output `zeta(n, x)`. It provides a concrete example and a reference to the correct behavior. The required change is unambiguous: swap the arguments in the generated code. This is sufficient information for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14976": {
        "explanation": "The issue clearly describes a problem with how lambdify (with modules='mpmath') handles rational numbers, specifically that rationals like 232/3 are not wrapped or converted to mpmath types, leading to reduced precision. The code examples demonstrate the problem, and the expected behavior is implied: rationals should be converted to mpmath.mpf or similar for full precision. While it doesn't specify the exact implementation, the problem and the desired outcome are clear enough for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15011": {
        "explanation": "The issue provides a clear description of the problem, including the context (use of lambdify with MatrixSymbol names containing curly braces), the expected behavior (should work with dummify=True), and the actual behavior (SyntaxError is raised). It also includes minimal reproducible code snippets that demonstrate both the working and failing cases. This is sufficient information to investigate and attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15017": {
        "explanation": "The issue clearly describes the current and expected behavior for `__len__` on rank-0 arrays in `sympy.tensor.array.NDimArray`. It provides code examples showing the discrepancy, references the behavior in numpy as the desired standard, and specifies the context (SymPy version, Python version). The required change is to make `len(a)` return 1 for rank-0 arrays, matching the number of elements and numpy's behavior. This is a well-specified issue with a clear, actionable outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15085": {
        "explanation": "The issue provides a clear description of the problem: using sympy.lambdify with an expression involving sympy.Matrix results in a NameError. It includes a minimal working example (MWE) that reproduces the error, specifies the error message, and notes that this is a regression since at least sympy 1.2. The expected behavior is implied: lambdify should work with Matrix objects without raising a NameError. While it does not specify the exact desired output, it is clear that the solution should ensure lambdify-generated functions can handle Matrix objects as in the example. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15198": {
        "explanation": "The issue provides some context: a regression in code generation for Octave/Julia/JS code printers, specifically related to the handling of 'laguerre' and 'assoc_laguerre' functions after a particular commit. However, it is not clear what the expected behavior is for each function, what the current (broken) behavior is, or what the correct fix should be. The issue mentions that 'laguerre' is printed but 'assoc_laguerre' is supposed to error, but it is not explicit about what the code should do, nor does it provide a test case or a clear description of the desired output. There is also ambiguity about which code printers are affected and what the regression actually is. Therefore, there is room for ambiguity and it is unclear what a successful solution would look like without further clarification.",
        "label": "VAGUE"
    },
    "sympy__sympy-15222": {
        "explanation": "The issue provides a concrete code example, showing the current and expected behavior. It demonstrates that substituting 'a' with 'z' in a Subs object with (a,) as the variable and (a,) as the value does not yield the expected result. The expected result is clearly stated. While the issue does not provide a detailed description in words, the code and comments make the problem and the desired outcome clear. Therefore, it is possible to attempt a meaningful solution based on the information given.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15225": {
        "explanation": "The issue provides a clear code example, the observed output, and a description of the unexpected behavior: bool_map returns a mapping between two functions (Xor and its negation) that are not equivalent. The expectation is that bool_map should not return a mapping in this case, or should indicate non-equivalence. The issue is well-specified because it includes the context, the code to reproduce, the actual and expected behavior, and a clear statement of the flaw.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15231": {
        "explanation": "The issue provides a clear description of the problem: when using autowrap with a SymPy expression containing the Mod function twice, the generated Fortran code declares the Mod function twice, leading to a compilation error. The issue includes a minimal reproducible example, the error message, and the problematic generated code. The expected outcome is also clear: the code should not declare Mod twice, and autowrap should work for such expressions. This is sufficient information to attempt a fix in the code generation logic.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15241": {
        "explanation": "The issue provides a clear description of the desired behavior for the canonicalization of variables in the Derivative class. It specifies the current problem (ordering of variables, especially symbols not in the free symbols of the function) and gives a concrete example of the expected output. While it does not specify every edge case, the main requirement is clear and actionable for someone familiar with the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15286": {
        "explanation": "The issue description is somewhat scattered and mixes several topics: (1) the slow numerical evaluation of the circumference of an ellipse in SymPy, (2) a faster algorithm for this calculation, (3) a suggestion that the symbolic integrator should recognize and handle elliptical integrals more efficiently, and (4) a list of unrelated geometry API changes (new methods, pluralization, etc.). The main actionable item seems to be: 'Perhaps recognition of such integrals by integrate/Integral.evalf would be a good idea.' However, it is not clear exactly what is expected: Should the symbolic integrator automatically recognize the integral for the circumference and use a fast algorithm? Should it recognize all elliptic integrals? Should it only be for the ellipse circumference? There is no specification of the scope, the expected API, or test cases. The rest of the description is a changelog for a PR, not a clear issue description. Therefore, there is significant ambiguity about what a successful solution would look like.",
        "label": "VAGUE"
    },
    "sympy__sympy-15308": {
        "explanation": "The issue clearly describes the problem: when calling latex(trace(A**2)), the output is 'Trace(A**2)' instead of the expected LaTeX representation. It points out two specific problems: (1) 'Trace' is not recognized and thus not rendered in LaTeX, and (2) the inner expression A**2 is not rendered as LaTeX (should be 'A^2'). The expected behavior is implied: the output should be valid LaTeX for the trace of a matrix squared. While it doesn't specify the exact LaTeX output, it is clear what is wrong and what needs to be fixed, and a reasonable solution can be implemented based on this description.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15320": {
        "explanation": "The issue provides a set of code examples that demonstrate inconsistent behavior in the handling of generators by the RootOf function. However, it does not specify what the correct or expected behavior should be, nor does it define what the desired outcome is. The author notes the inconsistency and flags it as something to document, but does not provide acceptance criteria or a clear problem statement. As a result, while the issue is well-motivated and illustrated, it is ambiguous what a successful solution would look like without further clarification.",
        "label": "VAGUE"
    },
    "sympy__sympy-15345": {
        "explanation": "The issue clearly describes the problem: the function 'mathematica_code' produces output that is not valid Mathematica code when given a Max expression. The expected output and the actual output are both provided, and the difference is explained. The input code is also given. This is sufficient information to attempt a fix: ensure that 'mathematica_code' outputs 'Max[x,2]' instead of 'Max(2, x)'.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15346": {
        "explanation": "The issue provides a clear description of the observed behavior: symbolic expressions involving sin/cos with symbols simplify as expected, but when using sin/cos with Rational arguments, the simplification does not occur. The user provides code samples and expected vs. actual output, making it clear that the request is to improve or fix the simplification of trigonometric expressions with Rational arguments in SymPy. While the issue does not specify the exact implementation approach, it is clear what the desired outcome is, and a solution can be attempted based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15349": {
        "explanation": "The issue provides a concrete example of incorrect output from Quaternion.to_rotation_matrix(), including the code to reproduce the problem and a clear description of the expected mathematical property (one of the sin(x) terms should be negative in a rotation matrix). However, it does not specify what the correct output should be, nor does it provide a reference for the expected equations. Still, as an experienced engineer, I can infer from standard quaternion-to-rotation-matrix formulas what the correct output should be and how to fix the sign error. The question about the reference is not strictly necessary to resolve the bug. Therefore, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15446": {
        "explanation": "The issue clearly describes a specific problem: when substituting a matrix symbol with its negation in a matrix product, the LaTeX printer outputs ambiguous LaTeX (e.g., 'x -y' instead of 'x (-y)'). The code example demonstrates the problem, and the expected behavior (parentheses around the negated matrix symbol) is implied. The context and desired output are clear enough to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15523": {
        "explanation": "The issue provides detailed information about test failures when running the codebase under Python 3.7.1, including specific error messages, stack traces, and the files/tests involved. However, it does not specify what the expected behavior is under Python 3.7, nor does it provide guidance on whether the goal is to make the tests pass under 3.7, skip them, or something else. The issue is clear about what is failing, but not about what the desired outcome is. Still, a reasonable interpretation is to make the tests pass under Python 3.7.1, as the failures are unexpected. Thus, it is possible to make a meaningful attempt at a solution, but some assumptions are required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15555": {
        "explanation": "The issue clearly describes a bug: the function 'primepi' does not handle symbolic arguments and raises a TypeError when passed a SymPy Symbol. The expected behavior is implied: 'primepi' should be able to accept symbolic arguments (like 'x') and work with them, at least in the context of limit computations. The stack trace and code sample make it clear what the problem is and where it occurs. However, the issue does not specify exactly what the symbolic return value should be (e.g., should it return unevaluated, raise a more informative error, or return a symbolic expression?). Still, a reasonable interpretation is to make 'primepi' return a symbolic object when given a symbolic argument, similar to how other SymPy functions behave. Thus, the issue is reasonably specified, but not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15586": {
        "explanation": "The issue provides a clear code sample, describes the error encountered, and specifies the context (using sympy and numpy to invert a float matrix via lambdify). The expected behavior is implied: the user wants to be able to invert a float matrix using sympy's lambdify and numpy, but encounters a TypeError. The error message and links to further discussion are provided. While the issue could be more explicit about the desired output, it is clear enough to attempt a solution: make the code work as intended (invert a float matrix using sympy and numpy without error).",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15596": {
        "explanation": "The issue describes a specific behavior: calling degree((x-2)/(x**2+1), x) in SymPy returns 1, but the user is unsure if this is correct or if degree should handle rational functions differently (e.g., raise an error or return something else). However, the issue does not specify what the correct behavior should be\u2014should degree return the degree of the numerator, denominator, or something else for rational functions? Should it raise an error? The issue is more of a question or a prompt for discussion than a clear request for a code change or bug fix. Therefore, it is too vague to implement a solution without making assumptions about the intended behavior.",
        "label": "VAGUE"
    },
    "sympy__sympy-15599": {
        "explanation": "The issue provides a clear description of the problem (Mod(3*i, 2) should reduce to Mod(i, 2)), references a related StackOverflow post, and even supplies a suggested code diff and a new test case. The expected behavior is unambiguous, and the scope of the change is well-defined. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15625": {
        "explanation": "The issue provides a clear description of the problem (incorrect LaTeX output with nested math environments), steps to reproduce, the specific error message, the problematic LaTeX code, and a proposed solution (change wrapping from $$...$$ to $\\displaystyle ...$). It also references related issues and PRs, and includes before/after screenshots. The required change is well-defined and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15635": {
        "explanation": "The issue provides concrete examples of current and expected output for both `str` and `srepr` printers for several set-related classes in SymPy. It also gives clear guidelines for what the `str` and `srepr` printers should do, and suggests a general audit of the sets module's printing. While it leaves some room for interpretation (e.g., \"There are likely others\"), the main requirements and the direction for a solution are clear and actionable. Thus, it is possible to make a meaningful attempt at a solution, though some minor decisions may need to be made during implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15678": {
        "explanation": "The issue provides concrete examples of current failures in the idiff function, including specific error messages and input/output cases. It clearly states two desired improvements: support for Eq objects and support for function notation (f(x)) instead of just y. The examples illustrate both the current limitations and the expected behavior. While it does not specify the exact implementation details, the requirements and success criteria are clear enough for a developer familiar with the codebase to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15685": {
        "explanation": "The issue provides a clear description of the problem: the .scale_factor property of certain units (like ohm) is exposed to users and shows values (like 1000) that are confusing and inconsistent with SI expectations. The issue suggests that .scale_factor should be made private (i.e., not user-facing) and that the kilogram workaround should be internal. The expected behavior is well-described, and the code example demonstrates the problem. While there may be some implementation details to decide (e.g., whether to rename, hide, or otherwise restrict access to .scale_factor), the core requirement is clear: make .scale_factor private/internal and not user-facing. This is a well-specified issue for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15809": {
        "explanation": "The issue clearly describes the current behavior (raising ValueError when Min() or Max() are called with no arguments) and proposes a specific change: instead of raising an error, return 'oo' for Min() and '-oo' for Max(), with a mathematical justification and a reference. The required change is unambiguous and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15875": {
        "explanation": "The issue provides a clear description of the problem: the `is_zero` property is returning `False` for certain complex integer expressions when it should return `None` if it cannot decide. It gives a concrete example, explains the expected behavior, and references related issues for context. The release note also specifies the scope of the fix. While some implementation details are left to the developer, the requirements for a successful solution are clear.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15948": {
        "explanation": "The issue provides a detailed description of the observed bug, including code snippets, output, and a clear explanation of the unexpected behavior when substituting values in SymPy matrix expressions. It also references a specific bug (incorrect use of Add and Mul instead of MatAdd and MatMul in matrix expressions) and describes the intended fix (modifying Mul.flatten to return the correct types for matrices). The expected behavior is clear, and the code context is sufficient to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15970": {
        "explanation": "The issue clearly states that the LaTeX representation for lists, tuples, and dicts should use '\\ ' instead of '\\quad'. The scope is limited to these data structures and the change is specific. The linked Twitter discussion provides context for the reasoning, but the technical requirement is unambiguous. It is clear what needs to be changed and where to look in the codebase (the LaTeX rendering logic for these types).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15971": {
        "explanation": "The issue requests adding pretty printing functionality for the 'lerchphi' function, specifically in 'pretty.py'. It references fixing the 'lerchphi' part of issue #6013. While it is clear that the goal is to implement pretty printing for this function, the issue does not specify the desired output format, examples, or edge cases. However, given standard conventions in similar codebases (like SymPy), it is reasonable to infer what is required: adding a pretty-print representation for the lerchphi function consistent with other special functions. Thus, while not exhaustively specified, there is a sensible and conventional interpretation of what is required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15976": {
        "explanation": "The issue provides a clear description of the problem: variables with names ending in numbers (e.g., 'x2') are rendered invisibly in MathML output, while single-letter variables work as expected. It includes a minimal reproducible example, the environment details, and screenshots of the output. The expected behavior is implied (that 'x2' should be visible, just like 'x'), and the bug is well-demonstrated. This is sufficient information to investigate and attempt a fix in the MathML printer code.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16003": {
        "explanation": "The issue provides a clear description of the current problem (incorrect MathML rendering of multiple derivatives), a concrete example (with images and code), and a target for improvement (matching the LaTeX rendering, grouping variables, correct powers, etc.). It specifies which method to improve (_print_Derivative), and references related discussions and code for further guidance. While there are some minor open questions (e.g., when to put the function on a separate line), the main requirements are clear and actionable. Thus, a meaningful attempt at a solution is possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16056": {
        "explanation": "The issue provides a clear description of the problem: inconsistent use of \\mathrm and \\text in LaTeX printing routines, and the desire to standardize and make this configurable. It outlines the pros and cons of each approach, gives a rough count of where each is used, and suggests a possible solution (a helper function). However, it leaves some details open, such as the exact configuration mechanism, the default choice, and whether all instances should be replaced or just new code should use the helper. Still, a sensible interpretation is possible: refactor to use a helper function, make the choice configurable, and document the default. Thus, it is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16088": {
        "explanation": "The issue provides a clear description of the current and desired behavior: currently, 'simplify' on an Integral does not pull out constant (independent) terms, but it does for Sum. The issue includes concrete before-and-after code examples, and the release note clarifies the intended change. There is a clear, actionable requirement: make 'simplify' on Integral pull out independent factors, matching the behavior of Sum. No significant ambiguities remain.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16221": {
        "explanation": "The issue states that the current printers for Wolfram Mathematica do not support matrices and arrays, and requests that support be added. However, it does not specify what 'support' means: Should the output be in a specific format (e.g., pretty-printed, plain text, LaTeX)? Should it handle all dimensions and types of arrays? Are there size or formatting constraints? Without these details, there is significant ambiguity about what constitutes a successful solution.",
        "label": "VAGUE"
    },
    "sympy__sympy-16281": {
        "explanation": "The issue provides concrete examples of the current pretty-printing output for Product, points out specific aspects that could be improved (extra empty line, width/fatness of the symbol, possible extension of the top bar), and even provides alternative sketches of what improved output might look like. However, it does not specify exactly what the new output should be, nor does it set clear acceptance criteria. There is some room for interpretation in how to address the concerns, but the direction is clear and actionable. Thus, while not fully prescriptive, it is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16331": {
        "explanation": "The issue provides a clear description of the problem: attempting to use ufuncify with a two-argument function and the Cython backend results in a Cython compilation error due to a variable redeclaration. The error trace and the relevant code snippets are included, and the expected behavior (works with one argument or a different backend) is specified. While the issue does not prescribe a specific solution, it is clear what needs to be fixed: ufuncify should work with two arguments and the Cython backend without causing a variable redeclaration error. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16334": {
        "explanation": "The issue provides a clear description of the problem: the expression (q**p).is_positive should return False when q is a symbol constrained to be zero and p is a real, nonzero symbol. The expected mathematical behavior is explained, and a code snippet is provided to reproduce the issue. The desired outcome is also explicitly stated. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16437": {
        "explanation": "The issue provides clear input/output examples of the current (incorrect) and expected (correct) behavior for the Float constructor when parsing strings with underscores. It specifies what is wrong (misplacement of the decimal point) and what the correct output should be. Although it mentions a possible upstream bug, the task for this codebase is clear: ensure that Float('1_234.345_678', 24) and similar cases parse as 1234.345678... rather than the current incorrect results. The requirements for a successful solution are well-defined by the examples.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16449": {
        "explanation": "The issue clearly requests the addition of a function to compute the cumulative distribution function (CDF) for the Maxwell distribution, and provides a reference link to the relevant Wikipedia page. While it does not specify the exact function signature or the file in which to place the function, it is clear what mathematical function needs to be implemented and where to find the formula. This is a common type of request in scientific or statistical libraries, and the requirements are sufficiently clear for an experienced developer to proceed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16450": {
        "explanation": "The issue provides a clear description of the problem: the 'posify' function removes the 'finite' assumption from a symbol, as demonstrated by the code example. The expected behavior is also stated: 'posify' should preserve the 'finite' assumption, and possibly other similar assumptions (integer, rational, prime, even, odd). While the main requirement (preserve 'finite') is explicit, the mention of 'possibly other assumptions' introduces some ambiguity about the full scope of the fix. However, a sensible interpretation is to at least ensure 'finite' is preserved, and optionally consider the others if feasible. Thus, the issue is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16474": {
        "explanation": "The issue specifies that LaTeX, MathML, and pretty printers should be added for 'HadamardPower', and also suggests that 'HadamardProduct' may be extended to support the division symbol. While it does not provide detailed requirements or examples of the expected output, it is clear what classes/functions need to be extended and what features to implement. The task is standard for mathematical expression libraries, and a developer familiar with the codebase would know where to add these printers. The division symbol extension for HadamardProduct is a bit less clear, but the main checklist items are actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16503": {
        "explanation": "The issue clearly describes a formatting problem with the pretty-printing of a mathematical sum expression. It provides a concrete example, shows the current (undesired) output, and specifies the desired alignment between the summand ('x') and the '+ 3'. While the reporter is unsure about the exact vertical alignment (whether to move 'x' down or '+ 3' up), the core requirement is clear: the two should be aligned. This leaves some implementation detail to the developer, but the goal is unambiguous.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16527": {
        "explanation": "The issue provides a clear description of the problem: the collect_const() function in SymPy does not collect numeric rationals as expected. It includes a code example showing the current behavior and specifies the expected output. The context (SymPy 1.3, collect_const, rationals) is clear, and the desired change in behavior is explicitly stated. This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16597": {
        "explanation": "The issue provides a concrete code example, the observed behavior, and the user's expectation: that an object marked as 'even' should also be considered 'finite'. However, it does not specify what the correct behavior should be (should is_finite return True if is_even is True? Should there be an error? Should the documentation be updated?). There is also uncertainty expressed by the reporter ('I'm not sure what the right answer is here'), and no acceptance criteria or clear definition of 'success' for a fix. Thus, while a reasonable interpretation is possible (make is_finite True if is_even is True), there is some ambiguity.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16601": {
        "explanation": "The issue provides a clear code example that demonstrates the problem: when a Parabola is constructed with symbolic parameters, accessing the vertex property raises a TypeError due to an attempt to evaluate the truth value of a symbolic relational expression. The traceback pinpoints the relevant lines in the codebase. The expected behavior is implied: the vertex should be computable (or at least not error) even when the parabola is defined symbolically. This is a well-specified bug report with a reproducible example and a clear indication of what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16632": {
        "explanation": "The issue describes a problem with parsing and simplifying expressions involving powers using SymPy. The user provides code samples, explains the observed behavior, and lists attempted solutions. The core request is: 'how can I get zero after expr1 - expr2?' However, the issue is more of a support question than a clear feature request or bug report. There is no explicit requirement for a code change, new feature, or bug fix. The desired outcome is for expr1 - expr2 to simplify to zero, but it's not clear if the solution should be a documentation update, a code fix, or a usage example. Thus, while there is a sensible interpretation (find a way to make expr1 - expr2 simplify to zero), there are some blanks to fill in regarding the expected deliverable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16637": {
        "explanation": "The issue provides a clear description of the problem: Plane.intersection(Segment3D) returns intersection points that are not actually on the segment, but rather on the infinite line containing the segment. It includes a reproducible code example, the expected behavior (should return an empty list if the intersection is not within the segment), and even a workaround. The desired fix is for Plane.intersection to only return intersection points that are actually on both the plane and the segment. The issue also references the relevant part of the code (intersection method in Plane class) and the type check that may need updating. This is sufficient information for a developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16766": {
        "explanation": "The issue clearly describes the current limitation (PythonCodePrinter does not support Indexed), provides a reproducible example, shows the current output, and even suggests a concrete implementation for the missing method. The expected behavior is clear: PythonCodePrinter should handle Indexed expressions without warnings, and the suggested code provides a direct path to a solution. This is well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16781": {
        "explanation": "The issue clearly describes a specific problem: the dot diagram for the expression x**2 is displaying the operands in the wrong order, showing Integer(2) as the left child and Symbol('x') as the right child, when it should be the other way around. The expected output is also provided. The issue further specifies that the fix should be applied both to the dot printer and the tutorial. This provides enough detail to attempt a solution: investigate the dot printer's output for Pow nodes, correct the operand order, and update the tutorial diagram accordingly.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16792": {
        "explanation": "The issue provides a clear description of the bug, a minimal reproducible example, the expected and actual behavior, and even points to the likely cause in the code generation step. It is clear what needs to be fixed: the autowrap code generation should handle array arguments correctly even if they do not appear in the expression. The requirements for a successful solution are well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16840": {
        "explanation": "The issue describes a problem where the expression S(2)//S.Half gives a ZeroDivisionError, while in Python, 2//0.5 evaluates to 4. It is clear that the user expects S(2)//S.Half to behave similarly to 2//0.5 in Python, i.e., to return 4 instead of raising an error. The context suggests this is about a symbolic math library (likely SymPy), and the desired behavior is well-specified: fix the implementation so that S(2)//S.Half returns 4. The input and expected output are both provided, making the issue well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16862": {
        "explanation": "The issue provides a clear description of a bug: using args[0] on the result of linsolve can cause an IndexError when linsolve returns an empty set. It points to the exact line in the code and references a failing test. The expected behavior is implied: the code should not raise an IndexError when linsolve returns an empty set. While there is a secondary question about type-casting, the main bug is well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16886": {
        "explanation": "The issue clearly identifies the file (simpy.crypto.crypto), the specific mapping that is incorrect (\"----\": \"1\"), and the correct mapping that should be used (\".----\": \"1\"). The required change is unambiguous and localized. This is sufficient information to make a precise and meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16943": {
        "explanation": "The issue provides a clear example of the unexpected behavior: calling fps(x**2) returns x**2 (a Pow object) instead of a formal power series object, which does not support the expected interface (e.g., indexing). The user also references the documentation, indicating that the current behavior is not as documented or expected. The goal is to ensure that fps always returns an object supporting the formal power series interface, even for polynomials. This is a well-specified behavioral bug with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16963": {
        "explanation": "The issue description contains two distinct topics: (1) sympify not converting Python dict to sympy's Dict, and (2) a discussion about decimal precision and evaluation in N and sympify. The first part is well-motivated, with a clear example and a specific request: sympify should convert dict to Dict. The second part is more of a background discussion and not a direct request for code change. For the first part, the requirements are clear enough to attempt a solution: modify sympify so that it converts dict to Dict. The second part can be ignored for the purposes of this issue. Therefore, the issue is reasonably specified, though not perfectly focused.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16988": {
        "explanation": "The issue provides concrete examples of current and desired behavior for the Intersection operation, specifically regarding how duplicates should be handled. It suggests that duplicates should be removed at the outset and provides a specific expected output for a given input. The suggestion about ordering for canonical processing is also clear. While there is a minor ambiguity about whether the output should always be a Piecewise or sometimes remain unevaluated, the main requirement (removal of duplicates in Intersection) is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17010": {
        "explanation": "The issue provides a clear description of the problem: certain uses of could_extract_minus_sign in sympy/functions/special/polynomials.py (specifically in chebyshevu) can lead to infinite recursion. It gives a concrete code example and explains the recursion scenario. The desired outcome is also described: a more robust way to canonicalize the sign, ideally by removing the minus sign from the highest order term. While it leaves some implementation details open (e.g., the exact method to achieve this), the problem and the goal are clear, and a sensible solution can be attempted based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17038": {
        "explanation": "The issue provides a clear description of the problem (incorrect results from isqrt for certain inputs), gives concrete examples, and analyzes the root cause in the code. It also discusses platform-specific considerations and suggests a possible solution (using int(_sqrt(n)), but verifying correctness before returning). While there is some room for interpretation in the exact implementation details, the requirements and context are well-explained, making it clear what needs to be fixed and why. A meaningful attempt at a solution can be made based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17067": {
        "explanation": "The issue provides a concrete minimal working example (MWE) in code, the expected output, the actual (incorrect) output, and a clear description of why the result is wrong. It also references external verification (Mathematica) and explains the mathematical reasoning. The problem is well-scoped: 'simplify' returns an incorrect result for a specific trigonometric expression. This is sufficient for a developer to reproduce, investigate, and attempt a fix or improvement to the 'simplify' function's handling of such expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17115": {
        "explanation": "The issue provides a concrete code example that triggers an error, the full traceback, and a clear description of the context (SymPy's Piecewise with a specific condition involving And, Le, Ge, and S.Integers.contains). It also references relevant code and suggests a possible cause. However, the issue is somewhat ambiguous about the desired outcome: is the goal to make this code work without error, to change how Piecewise handles such conditions, or to provide an alternative approach? There is enough information to attempt a fix (e.g., making Piecewise handle such conditions gracefully), but some interpretation is required regarding the intended behavior. Thus, it is reasonably specified, but not perfectly clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17139": {
        "explanation": "The issue provides a clear, reproducible error with a code snippet, stack trace, and the context (calling simplify on cos(x)**I in SymPy). The error is traced to a specific line in fu.py where a comparison is made that is invalid for complex numbers. The expected behavior is implied: simplify(cos(x)**I) should not raise an exception. While the exact desired output is not specified, it is clear that the bug is the exception and the fix is to prevent this error. This is a common type of bug report in mathematical libraries and is actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17150": {
        "explanation": "The issue provides a concrete example of incorrect behavior, points to the relevant code location, explains the problematic logic, and even suggests two possible fixes. It also notes a possible deeper issue, but the immediate bug and its context are clear. This is sufficient for a meaningful attempt at a solution, as the expected output and the problematic code are both specified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17176": {
        "explanation": "The issue provides concrete code examples, expected and actual outputs, and points to a specific flaw in the _finger fingerprint routine as well as a problem with bool_map's handling of Xor and its negation. It also references a related issue for context. While it does not specify the exact desired output, it is clear that the current behavior is incorrect and that the goal is to fix the fingerprinting and bool_map logic for Xor/XNR with more than two arguments. The problem is well-localized and reproducible, making it possible to attempt a meaningful solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17194": {
        "explanation": "The issue provides concrete examples of the current (erroneous) output and the expected correct output for specific function calls. The scope is clear: update the MathML printer so that the output for acoth, acsc, and asec uses the 'arc' prefix (arccoth, arccsc, arcsec) instead of the current output. No further clarification is needed to implement and test the fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17223": {
        "explanation": "The issue provides a clear description of the problem: the `match` function in SymPy does not respect the non-commutativity of matrix multiplication when matching expressions. It includes a concrete code example demonstrating the undesired behavior, and specifies the expected behavior (that the match should fail when the matrix order is reversed). The scope and expected outcome are clear, making it possible to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17239": {
        "explanation": "The issue provides a clear code example, the observed output, and a specific diagnosis: several code printers (glsl, javascript, julia, mathematica, octave, rust) are not printing equality expressions (Eq(x, 1)) in the expected target language syntax, but instead outputting 'Eq(x, 1)'. It also points out that C and Fortran printers work because they override '_print_Relational', suggesting the solution is to implement or fix '_print_Relational' in the affected printers. The expected behavior is clear: these printers should output the appropriate equality syntax for their language, not the generic 'Eq(x, 1)'. This is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17271": {
        "explanation": "The issue provides a clear description of the problem: calling frac(zoo) raises a TypeError, and the user does not want an exception to be raised. The stack trace and the relevant code lines are provided, and the user is trying to cover those lines in a test. However, the issue does not specify what the correct behavior of frac(zoo) should be (e.g., should it return zoo, nan, 0, or something else?). The user only states that an exception is not desired, but is unsure what the correct output should be. Therefore, while the problem is clear, the expected solution is not fully specified, but a reasonable interpretation (e.g., returning zoo or nan) could be made based on typical mathematical conventions and the context of the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17273": {
        "explanation": "The issue clearly states that the value for hbar in sympy.physics.units does not match the CODATA recommended value, and provides the correct value to use. The required action is to update the value of hbar in the codebase to match the provided CODATA value. This is a straightforward and well-specified task.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17288": {
        "explanation": "The issue clearly describes a specific problem: when displaying higher-order derivatives with superscripted symbols in LaTeX, the output is incorrect (e.g., x^{*}^{2} instead of x^{*2}). It provides a concrete code example and the actual vs. expected LaTeX output. The root cause is also suggested. This is enough information to attempt a fix in the code that generates the LaTeX for derivatives with superscripted symbols.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17313": {
        "explanation": "The issue provides some mathematical expressions involving 'ceiling' and 'floor' functions and their expected behavior for positive ('pos') and negative ('neg') values. However, it does not specify what 'pos' and 'neg' are (are they variables, types, or specific values?), nor does it indicate where in the codebase these expressions are relevant, what language or library is being used, or what the current incorrect behavior is. There is no context about whether this is a bug, a feature request, or a test case. As such, it is unclear what exactly needs to be changed or implemented, making the issue too vague for a meaningful solution attempt.",
        "label": "VAGUE"
    },
    "sympy__sympy-17318": {
        "explanation": "The issue provides a clear description of the bug (IndexError in sqrtdenest when given a specific input), the expected behavior (if an expression cannot be denested, it should be returned unchanged), and an example of both the current (incorrect) and desired (correct) outputs. It also references the relevant code paths and the specific error encountered. This is sufficient information to attempt a meaningful fix: ensure sqrtdenest returns the input unchanged instead of raising an IndexError when denesting is not possible.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17340": {
        "explanation": "The issue description mentions that 'Permutation' has a 'list' subtype in its args and that it was changed to 'Tuple'. However, it does not specify what needs to be done: Is the task to update the codebase to use 'Tuple' instead of 'list'? Is it to review or test the change? There is no clear instruction or acceptance criteria, and the context is minimal. Therefore, it is ambiguous what a successful solution would look like.",
        "label": "VAGUE"
    },
    "sympy__sympy-17394": {
        "explanation": "The issue provides a clear description of the problem: when using lambdify with 'scipy', the lowergamma and uppergamma functions should be mapped to scipy.special.gammainc and gammaincc, respectively, and the scaling (regularization) should be handled appropriately. It also specifies that with 'numpy' as the module, an error should be raised for unsupported functions. The references to related issues (#16533, #16535, #15134) provide additional context. While some implementation details (e.g., exact scaling) may require consulting the referenced issues or documentation, the core requirements are clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17512": {
        "explanation": "The issue provides a clear description of the current and desired behaviors for both Range and ImageSet. It specifies that Range(range(10)) should not be allowed, explains why, and gives the expected error behavior. It also points out that ImageSet should sympify its arguments and raise if the second argument is not a Set after sympification. The examples and reasoning are detailed, and the required changes are well-motivated and actionable. There is little ambiguity about what needs to be changed and what the expected outcomes are.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17630": {
        "explanation": "The issue provides a clear description of the problem, including a minimal reproducible example, the expected and actual behavior, and the traceback of the exception. It identifies that multiplying BlockMatrix objects containing ZeroMatrix blocks works once, but fails on repeated multiplication due to ZeroMatrix blocks being replaced by scalar Zero objects, which lack the required attributes. The root cause is also suggested. The task is to ensure that block matrix multiplication preserves ZeroMatrix blocks instead of converting them to scalar Zero, thus preventing the AttributeError. This is a well-specified bug report with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17653": {
        "explanation": "The issue provides a clear description of the problem: a change in SymPy's IndexedBase constructor causes subclasses of Symbol (specifically TypedSymbol from pystencils) to be upcast to Symbol, losing subclass-specific information. The issue includes reproduction steps, a code snippet highlighting the problematic line, and a clear statement of the undesired behavior. However, it does not explicitly specify what the desired fix should be (e.g., whether to remove the upcasting, or to handle subclasses differently), nor does it discuss potential side effects or compatibility concerns. Still, the intent is clear enough to attempt a solution: prevent the loss of subclass information when passing a Symbol subclass as the label. Some interpretation is required, but the problem and context are well described.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17655": {
        "explanation": "The issue provides a clear description of the problem, including code snippets that reproduce the bug, the exception traceback, and a statement of the expected behavior (that both expressions should yield the same result). The context (SymPy geometry.Point and multiplication/addition with numbers) is explicit, and the desired outcome is unambiguous. This is sufficient information to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17696": {
        "explanation": "The issue provides a clear code example, specifies the current and expected behavior (that refine(sign(x), Q.positive(x) & Q.nonzero(x)) and refine(sign(x), Q.positive(x + 1)) should return 1), and references a similar PR for guidance. The context and desired outcome are well described, making it clear what needs to be fixed and how success can be measured.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17770": {
        "explanation": "The issue provides some context: it is about correcting the _eval_is_real and _eval_is_finite functions in subclasses of HyperbolicFunction, specifically for cosh, sinh, and tanh. However, it does not specify what is currently wrong with these functions, what the expected behavior is, or what cases are missing. The statement that the functions are 'sort of incomplete' and that 'any improvements are welcome' is vague and open-ended. There is no clear definition of what constitutes a successful solution, nor are there concrete examples or acceptance criteria. Therefore, the issue is too ambiguous to proceed with confidence.",
        "label": "VAGUE"
    },
    "sympy__sympy-17809": {
        "explanation": "The issue provides a clear description of the desired behavior: (1) cosh(x).is_positive should return True if x is real, and (2) abs(cosh(x)) should simplify to cosh(x) for real x. It also provides examples of current and expected behavior, and references a specific issue to be fixed. The requirements are concrete and testable, and the scope is clear. There is enough information to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17821": {
        "explanation": "The issue provides a clear primary task: implement `S.Catalan.rewrite(Sum)`. This is a concrete code change. However, the rest of the issue discusses a broader idea about generating LaTeX equations in documentation programmatically, but does not specify any concrete requirements or scope for this part. It asks for a demonstration of the idea, but does not specify which equations, where, or how to integrate this into the documentation. The mention of LaTeX printer optimizations is also speculative and not actionable. Therefore, while the first part is well-specified, the second part is vague. Overall, there is a sensible interpretation for a PR: implement the Catalan sum rewrite and demonstrate auto-generated LaTeX in doctests for this case. However, the broader LaTeX generation is not well-defined. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17845": {
        "explanation": "The issue describes a discrepancy between the string representation of Interval and FiniteSet objects and the principle that str(object) should be valid code to recreate the object. It provides examples of the current behavior and notes a potential downside of changing it. However, it does not specify what the new behavior should be (e.g., should str(Interval(0,1)) return 'Interval(0, 1)' or something else?), nor does it specify whether the change should apply to __str__, __repr__, or both. There is also no mention of how to handle the 'ugly' output or whether that is acceptable. Thus, while the problem is identified, the requirements for a solution are not fully clear.",
        "label": "VAGUE"
    },
    "sympy__sympy-18033": {
        "explanation": "The issue provides a clear set of tasks: remove the print_cyclic flag from the Permutation object, move the flag to the printers, ensure doctests use the flag via init_printing, follow the deprecation policy for the public API, and adjust the str and pretty printers as described. There are references to relevant discussions and policies. While some implementation details (e.g., exact flag name in printers) are left to the implementer, the requirements and rationale are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18057": {
        "explanation": "The issue provides a clear description of the problematic behavior: Sympy's __eq__ method, when comparing to an object with a custom __repr__, passes the repr string to eval, which can cause errors or unsafe behavior. The issue includes concrete repro code, the observed error, and a related edge case. The expected outcome is clear: Sympy should not eval arbitrary reprs in equality checks, and the comparison should not result in errors or unsafe behavior. While the exact implementation details are left to the developer, the problem and the desired change are well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18062": {
        "explanation": "The issue provides a clear description of the problem: when using imageset with a Lambda that includes a complex number (1 + I*n), the constant part (1) is dropped from the output, which is incorrect. The expected behavior is implied: the output should include the constant part. The code example and the observed incorrect output are provided, making it clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18087": {
        "explanation": "The issue provides a clear description of the problem: the 'trigsimp' function in SymPy incorrectly simplifies 'cos(x) + sqrt(sin(x)**2)' to 'cos(x) + sin(x)' for general complex x, when this is only correct for real x. The expected behavior is implied: the simplification should not assume 'sqrt(sin(x)**2)' is always 'sin(x)' unless x is real. The issue is reproducible and the incorrect behavior is specified, making it clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18109": {
        "explanation": "The issue provides a clear code example, describes the unexpected behavior (pretty printing 'oo - oo' gives 'nan' when evaluation is disabled), and implicitly asks for an explanation or fix. The expected behavior is implied: pretty printing should display 'oo - oo' or something similar, not 'nan', when evaluation is disabled. While it doesn't specify the exact desired output, the context and code make the intent clear enough for a meaningful solution attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18116": {
        "explanation": "The issue mentions a conceptual problem: that Relational is a Boolean, but Expr methods are being called on them in some places. However, it does not specify what the desired outcome is, what changes should be made, or which methods or files are involved. There is no clear acceptance criteria or actionable steps. The reference to an old issue may provide more context, but based solely on this description, it is too vague to proceed with a solution.",
        "label": "VAGUE"
    },
    "sympy__sympy-18130": {
        "explanation": "The issue provides a concrete code example, the expected mathematical context (intersection of an ImageSet with the integers), and the observed incorrect output (EmptySet). It also hints at the underlying problem (a diophantine bug). While it does not explicitly state the expected output, it is clear from mathematical reasoning that the intersection should not be empty, as n**2-1 is always an integer for integer n. Therefore, the issue is well-specified for a developer familiar with the codebase to investigate and fix the bug.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18168": {
        "explanation": "The issue raises a valid question about the correctness and initialization of the `is_closed` and `is_open` properties for `S.Rationals` in SymPy. It provides current observed behavior, suggests what the values should be (both False), and also raises the alternative of raising a NotImplementedError if undefined. However, it is not entirely clear whether the reporter wants the properties to be set to False, or to raise an exception, or if they are simply asking for clarification. There is ambiguity about the desired behavior, and no clear acceptance criteria for a successful solution. Therefore, while the issue is grounded in a real problem, it is too vague to proceed without making assumptions.",
        "label": "VAGUE"
    },
    "sympy__sympy-18189": {
        "explanation": "The issue provides a clear description of the problem: the 'diophantine' function returns different (and incomplete) results depending on the order of the 'syms' argument when 'permute=True'. It includes concrete input/output examples that demonstrate the bug. The expected behavior is implied: the results should be complete and independent of the order of 'syms' when 'permute=True'. This is sufficient information to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18191": {
        "explanation": "The issue provides a concrete example (sqrt(1 / tan(1 + I))) that triggers a recursion error, includes the full traceback, and points to a specific location in the codebase where the problem occurs. It also suggests that an 'additional check' is needed at a particular line. While it does not specify exactly what the check should be, the context (preventing recursion errors in Pow._eval_power) and the example make it clear that the solution should prevent infinite recursion for such cases. Thus, a sensible interpretation and solution are possible.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18198": {
        "explanation": "The issue is primarily a suggestion for a refactor and API improvement, outlining a proposed new structure for handling global parameters. It describes the current state, the motivation for change, and provides a clear example of the desired API (including dict-like access, property access, and context manager usage). However, it is not a direct request for implementation, and the author even mentions having already implemented part of the suggestion. There is some ambiguity about whether the task is to implement the full suggestion (including context manager support), to review the existing partial implementation, or just to discuss the idea. Therefore, while there is a sensible interpretation (implement the full described API), there are some blanks to fill in regarding the scope and expectations.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18199": {
        "explanation": "The issue clearly describes a specific bug in the nthroot_mod function: when a \u2261 0 mod p, the function should return 0 as a root, but currently does not. It provides a concrete example (nthroot_mod(17*17, 5, 17)) and explains the mathematical reasoning. The required fix is to ensure that 0 is included as a root when a % p == 0. This is a well-specified issue with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18200": {
        "explanation": "The issue provides a clear, reproducible error (an AttributeError) with a specific code example, the context (intersection of ImageSet with S.Integers), and points to the relevant area of the code (diophantine-related intersection code). It also references related issues for further context. The expected behavior is implied: the intersection should not raise an AttributeError and should return the correct mathematical set. While it does not specify the exact expected output, it is clear that the bug is the exception and the goal is to fix it. This is enough for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18211": {
        "explanation": "The issue clearly describes the current behavior (solveset raises NotImplementedError) and the expected behavior (solveset should return a ConditionSet in this case). It provides a concrete example, the input that triggers the problem, and the expected output. The context is specific to the SymPy library and the solveset/as_set functionality. This is sufficient information for an experienced developer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18256": {
        "explanation": "The issue provides a clear description of the current behavior (SymPy outputs parenthesized superscripted symbols when raised to a power) and the desired behavior (output should use nested superscripts without parentheses, e.g., {x^{i}}^{2} instead of \\left(x^{i}\\right)^{2}). It also distinguishes this from the case of 'powered superscription' (x^{i^{2}}), and provides concrete examples and references. The only minor ambiguity is whether this should apply to all superscripted symbols or only certain cases, but the intent is clear enough for a meaningful implementation attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18273": {
        "explanation": "The issue provides a clear, reproducible example: constructing an expression with two CRootOf terms and passing it to cse, which results in an IndexError. The traceback is included, showing exactly where the error occurs in the code. The expected behavior is implied: cse should not raise an IndexError in this case. While the issue does not specify what the correct output should be, it is clear that the error is unintended and should be fixed. This is sufficient for a meaningful attempt at a solution, as the goal is to prevent the IndexError when using cse with RootOf expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18351": {
        "explanation": "The issue provides a list of specific matrix expressions that should be supported by the NumPy printer, which gives a clear direction for what needs to be implemented. However, it does not specify the expected output or behavior for each of these expressions, nor does it provide examples or test cases. While a developer familiar with the codebase and the context (e.g., the referenced issue #17013) could make reasonable assumptions about what is required, there is still some ambiguity about the exact requirements for each matrix type. Therefore, the issue is not fully well-specified, but it is reasonably specified for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18477": {
        "explanation": "The issue provides a clear description of the desired feature: exposing the `min_fixed` and `max_fixed` settings for Float in the printers, similar to how `strip_zeros` is currently exposed. It also suggests unifying the behavior of Float printing across different printers (e.g., LaTeX and string printers). While it doesn't specify every implementation detail, the intent and scope are clear enough for an experienced engineer to proceed with a meaningful solution, such as updating printer interfaces and ensuring consistent Float formatting options.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18478": {
        "explanation": "The issue describes an inconsistency in the behavior of the `subs` method when substituting infinity (`oo`) for `x` in two similar expressions. It provides concrete examples and the observed outputs, and implicitly requests an explanation or fix for the inconsistent evaluation. However, it does not specify what the correct or expected behavior should be, nor does it state whether the goal is to make both cases return `oo`, both return `AccumBounds`, or something else. While a reasonable interpretation is to make the behavior consistent, the lack of a clearly stated expected outcome leaves some ambiguity.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18587": {
        "explanation": "The issue clearly describes the unexpected behavior: when creating a Permutation object with a list longer than the specified 'size', the 'size' argument is ignored and no exception is raised. The user provides a concrete code example and suggests that an exception should be raised in this case. The expected behavior is clear (raise an exception if the list length exceeds 'size'), and the current behavior is demonstrated. This is sufficient information to attempt a solution: modify the constructor to raise an exception if the input list's length is greater than the 'size' argument.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18605": {
        "explanation": "The issue clearly describes the unexpected behavior (Idx accepts non-integer bounds when given as a tuple), provides a code example that reproduces the problem, and specifies the expected behavior (both lower and upper bounds should be checked for being integers). It also contrasts this with the correct behavior when a single bound is given. The required change is unambiguous: add integer checks for both bounds when a tuple is provided. This is well-specified for a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18630": {
        "explanation": "The issue provides a clear error traceback, a minimal reproducible example, and identifies the root cause as an AttributeError related to 'TupleArg' not having a 'compute_leading_term' method. It also points out that the error occurs when computing the nseries of a hypergeometric function. While it does not specify the exact fix, it is clear that the solution should address the missing method or handle 'TupleArg' appropriately in the nseries computation. This is a common type of bug report in open source math libraries, and the information provided is sufficient for an experienced developer to investigate and attempt a fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18650": {
        "explanation": "The issue provides a clear input (sqrt(8)**Rational(2, 3)), the current output, and the expected output (2). It is clear that the user expects the expression to simplify to 2, and the context is likely a symbolic mathematics library (e.g., SymPy). The task is to ensure that such expressions are fully simplified. The requirements and the expected behavior are explicit.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18667": {
        "explanation": "The issue description is not a request for a change or a bug report, but rather a statement that a new feature (Schur_Number) has been added, along with a brief explanation of what the Schur number is. There is no clear problem to solve, nor any actionable request. It reads more like a pull request description or a changelog entry. Without a specific task, bug, or feature request, it is impossible to determine what needs to be done.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "sympy__sympy-18698": {
        "explanation": "The issue provides a clear example of inconsistent output between 'sqf' and 'sqf_list' functions, specifically regarding the ordering and grouping of factors with the same multiplicity. It specifies what the correct output should be for a given input and contrasts it with the current (incorrect) output. While it does not specify the exact internal cause or the precise function to change, it is clear what the expected behavior is and what needs to be fixed: the output should group factors of the same multiplicity together, not list them separately. This is enough information for a developer familiar with the codebase to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18728": {
        "explanation": "The issue provides a detailed description of the problem with the current implementation of Pow.is_zero and Pow.is_positive, including specific examples of incorrect and correct behavior, and references to the relevant issue (#9532). It also discusses the desired changes and the reasoning behind them. While there are some notes about possible future improvements and related assumption system changes, the core requirement\u2014fixing is_zero and is_positive for Pow so that they do not assume finiteness unless justified\u2014is clear and actionable. The expected behavior is well-illustrated with code examples, making it possible to attempt a meaningful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18763": {
        "explanation": "The issue provides a clear example of the current behavior (LaTeX output for a specific SymPy expression), the undesired output, and the desired output, including both the LaTeX string and a rendered image. The problem is well-scoped: the LaTeX printer for Subs expressions should parenthesize the argument when multiplied by a coefficient. The expected change is unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18765": {
        "explanation": "The issue clearly describes the current behavior of MatAdd and MatMul compared to Add and Mul, provides code examples, and specifies the desired change: introduce an 'evaluate' keyword argument to MatAdd (and presumably MatMul) so that when evaluate=True, expressions like MatAdd(A, A) are canonicalized to 2*A, matching the behavior of Add. The expected behavior is well-illustrated, and the scope of the change is clear.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18810": {
        "explanation": "The issue provides a clear description of the bug: the function generate_derangements is incorrectly assuming that the input permutation is sorted, which leads to incorrect results when the input is not sorted. It provides a concrete example of the failure and the expected behavior. The solution is to ensure that generate_derangements works correctly for any permutation, not just sorted ones. This is a well-specified bug report with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18835": {
        "explanation": "The issue clearly describes the current behavior of the 'uniq' function when its input list is modified during iteration, and contrasts it with Python's built-in behavior for dicts/sets (raising a RuntimeError). It specifies the desired behavior: 'uniq' should raise a similar RuntimeError if the input list is modified during iteration. It also suggests a possible implementation approach (recording the size at the start and checking it during iteration). The requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18903": {
        "explanation": "The issue provides a concrete example of unexpected behavior: nested floor (and ceiling) expressions are not being fully evaluated/simplified, as shown by the difference in output between two similar expressions. The expectation is that the simplification logic should handle nested floor/ceiling calls so that expressions like floor(floor(x) + 1) + x simplify in the same way as x + floor(floor(x)) + 1. While the issue does not specify the exact desired output for all cases, the intent is clear from the examples. The same logic should apply to ceiling. This is enough information to attempt a solution by improving the simplification rules for nested floor/ceiling.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18908": {
        "explanation": "The issue provides a general direction (add more SciPy special functions to the SciPyPrinter), but it does not specify which functions are missing, how they should be added, or what the acceptance criteria are. There is no guidance on prioritization, testing, or expected behavior. While a motivated engineer could make progress by comparing the current implementation to the linked documentation and adding missing functions, there is significant ambiguity about scope and requirements.",
        "label": "VAGUE"
    },
    "sympy__sympy-18922": {
        "explanation": "The issue describes a specific problem: when plotting a constant (presumably y=3), the x-axis is incorrectly rendered at y=3 instead of at y=0, and a screenshot is provided. However, the issue does not specify what code or plotting function was used, what the expected behavior is in more detail, or what the input data was. While the screenshot helps, there is ambiguity about the context and the steps to reproduce. A reasonable interpretation is possible (fix the plotting so that a constant function plots a horizontal line at y=3 and the x-axis remains at y=0), but some assumptions are required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18961": {
        "explanation": "The issue provides a clear and concrete request: add a 'bits' argument to the 'digits' function, similar to the existing 'ibin' function, so that the output is padded with zeros to reach the specified bit length. It includes a code diff showing exactly where and how the change should be made, and it also requests that tests be added. The expected behavior is well-defined, and the code context is provided. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19007": {
        "explanation": "The issue provides a clear code example, expected vs. actual output, and a specific description of the problem: when accessing an element of a BlockMatrix, the result is incorrectly simplified to an element of a submatrix (A) even when the index could refer to another block (B). The desired behavior is that C[i, 0] should not be simplified in this ambiguous case. This is a well-specified bug report with a clear reproduction and expected behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19016": {
        "explanation": "The issue description clearly identifies two specific problems: (1) the lack of an is_finite_set property for the Range class, and (2) sup and inf not working for symbolic/infinite ranges. For the first, the desired behavior and even a code suggestion are provided, making it straightforward to implement. For the second, the problem is demonstrated with an example and traceback, and while the exact solution is not specified, the expected outcome (handling symbolic/infinite endpoints gracefully) is implied. There is enough information to make a meaningful attempt at both fixes, though the second may require some interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19040": {
        "explanation": "The issue provides a clear description of the problem: when factoring the expression (x-1)*(y-1) with extension=[I], the factor y-1 is dropped, which is unexpected. The issue includes a reproducible code example, the expected output, and the actual (incorrect) output. The context is clear (SymPy's factor function), and the bug is well-demonstrated. There is a reference to a related issue, but the current issue is self-contained and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19091": {
        "explanation": "The issue provides a clear description of the underlying bug: contractions are performed before metric application, leading to incorrect tensor contraction results. It points to the relevant code, explains the mathematical reasoning, and references a related issue. However, the issue also states that a significant refactor is needed and suggests that the current design makes a fix difficult or impossible without broader architectural changes. It does not specify the exact scope or requirements for the refactor, nor does it define acceptance criteria for a solution. The mention of ongoing work and the suggestion for discussion further indicate that the path forward is not fully defined. Thus, while the problem is well-identified, the solution space is ambiguous and open-ended.",
        "label": "VAGUE"
    },
    "sympy__sympy-19093": {
        "explanation": "The issue provides a clear regression: an operation that previously worked (adding two BlockDiagMatrix objects containing mutable matrices) now raises a TypeError. The expected behavior is implied by the fact that it 'used to work' in version 0.7.3, and the error message is provided. The code to reproduce the issue is included. While the issue does not specify the exact expected output, it is reasonable to infer that the operation should succeed without error, as it did in the past. Therefore, there is a sensible interpretation of what is required: restore the ability to add such BlockDiagMatrix objects without error.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19201": {
        "explanation": "The issue provides a clear description of the current and proposed behavior for printing symbolic matrix slices, including a detailed table comparing the current output with the desired output for various slice cases. It specifies that the changes should affect the str/pretty/latex printers and highlights a specific problem with missing brackets in composite expressions. While there is a minor ambiguity in the handling of certain edge cases (e.g., 'A[, ] (???)'), the overall intent and direction for the improvement are clear, and a sensible implementation can be derived from the description.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19254": {
        "explanation": "The issue provides a clear direction: replace the current Mignotte bound implementation in `dup_zz_mignotte_bound` with the Knuth-Cohen bound, and similarly update `dmp_zz_mignotte_bound` for multivariate polynomials. However, it does not specify the exact algorithm for the Knuth-Cohen bound, nor does it provide references or details about the expected interface or test cases. While the intent is clear, some research or interpretation is required to determine the precise implementation of the Knuth-Cohen bound and how it should be integrated. Therefore, the issue is not fully well-specified, but it is reasonably specified for an experienced developer familiar with the domain.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19346": {
        "explanation": "The issue clearly describes the current and expected behavior of the `srepr` function for different container types (list, tuple, dict, set). It provides concrete examples of both the correct and incorrect outputs, making it clear that the goal is to have `srepr` print dict and set elements in the same detailed, symbolic representation as it does for lists and tuples. While it asks if the behavior is intended, it also suggests a fix if it is not, and the examples make the required change clear. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19487": {
        "explanation": "The issue provides a clear description of the mathematical relationship between sign and abs, and gives a concrete example of the desired rewrite behavior. However, it explicitly states uncertainty about how to handle the zero case, presenting two possible approaches but not specifying which to implement. This leaves some ambiguity in the requirements, but a sensible interpretation would be to implement the rewrite for nonzero arguments and either skip or raise for zero, possibly with a warning or documentation note. Thus, the issue is reasonably specified but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19495": {
        "explanation": "The issue provides a detailed sequence of code examples, showing both expected and unexpected behaviors when using subs with ConditionSet and ImageSet in SymPy. The user points out a specific case (Out[75]) where substituting y in a ConditionSet containing an ImageSet gives a strange result, and contrasts this with the correct behavior in other cases. However, the issue does not specify what the correct output should be for the problematic case, nor does it clearly state what the desired behavior is. While the context and the problematic behavior are well illustrated, there is ambiguity about what the 'fix' should be\u2014should subs behave like it does for FiniteSet, or is some other behavior desired? Thus, while a sensible interpretation is possible (make subs on ConditionSet+ImageSet behave like the other cases), there is some room for interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19601": {
        "explanation": "The issue is not a request for a code change, bug fix, or feature, but rather a question about the release schedule of a new version (1.6.1) of SymPy. There is no actionable engineering task or clear deliverable specified. As such, it is impossible to determine what code changes, if any, would resolve this issue.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "sympy__sympy-19637": {
        "explanation": "The issue provides a specific error message ('kern' referenced before assignment), the context (using kernS from sympy.core.sympify), and a code snippet that reproduces the problem. It is clear that the goal is to fix the UnboundLocalError in the kernS function. While the issue does not specify the intended behavior of kernS beyond what is implied by the code, it is reasonable to assume that the function should parse the input string without raising this error. Therefore, there is a sensible interpretation of what is required: fix the UnboundLocalError so that kernS works as expected.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19783": {
        "explanation": "The issue provides a clear description of the undesired behavior (Dagger(A) * IdentityOperator() does not simplify to Dagger(A)), shows the expected behavior by analogy (A * IdentityOperator() simplifies to A), and provides a minimal reproducible code example. The context (SymPy quantum module) and the expected outcome are both clear, making it straightforward to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19885": {
        "explanation": "The issue provides a clear description of the problem: 'linsolve' returns a wrong answer for a specific system of linear equations, while 'solve' returns the correct answer. The user has provided both a code snippet and a test file (test3.zip), as well as a screenshot of the result. The equations and variables are explicitly given, and the expected behavior is clear (linsolve should return the same/correct solution as solve). This is sufficient information to reproduce the issue, investigate the discrepancy, and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19954": {
        "explanation": "The issue provides a clear description of the problem: calling sylow_subgroup(p=2) on a DihedralGroup in sympy raises an IndexError. The user provides the exact code to reproduce the error, the full traceback, and the environment details. The expected behavior is implied (sylow_subgroup should not raise an IndexError), and the bug is reproducible. This is sufficient information for a developer to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20049": {
        "explanation": "The issue provides a clear description of the current behavior, the expected behavior, and a reproducible code example. It specifies that the method Point.vel() should attempt to calculate the velocity from the position if it has not been explicitly set, similar to how angular velocity is inferred from orientation. The desired output is also shown. There is a small ambiguity about edge cases (e.g., what if the position is not differentiable), but the main requirement is clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20139": {
        "explanation": "The issue description specifies that MatrixSymbol should use Str instead of Symbol for its name (first argument). It also includes a release note indicating that MatrixSymbol will store Str in its first argument. While it does not provide step-by-step instructions, the intent is clear: refactor MatrixSymbol so that its name is stored as a string rather than a Symbol. The scope and expected outcome are clear enough for an experienced developer to proceed, though some implementation details (e.g., how to handle backward compatibility) are left to the implementer\u2019s judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20154": {
        "explanation": "The issue describes a specific problem: the partitions() iterator in sympy.utilities.iterables reuses output dictionaries, which can lead to confusing and potentially buggy behavior. The issue suggests that copying the dictionary before yielding would be preferable, even if it incurs a small performance cost. While the issue does not explicitly state 'change the code to copy the dictionary before yielding,' it is clear from the context and the reasoning provided that this is the intended solution. The function and file are specified, the undesired behavior is described, and the desired change is implied. There is some room for interpretation (e.g., whether to make this change unconditionally or behind a flag), but a sensible, actionable solution is clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20169": {
        "explanation": "The issue provides a clear direction: the current implementation of the lens maker's formula only supports thin lenses, and the request is to extend support to plano-concave, plano-convex, and thick lenses. While it does not specify the exact equations or API changes required, it is clear what the expected outcome is: the code should be able to handle these additional lens types. As an experienced engineer, I can research the appropriate formulas and update the implementation accordingly. Thus, the issue is reasonably specified, though some details (like input/output format or edge cases) are left to the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20264": {
        "explanation": "The issue provides a clear description of the bug: a RecursionError occurs when converting certain SymPy expressions (specifically Pow(Rational(1, n), -1, evaluate=False) for certain values of n) to LaTeX using the latex() function. The user provides the exact code to reproduce the error, the version of SymPy used, and a detailed traceback. The expected behavior is implied by the successful outputs for similar cases, and the problematic cases are clearly identified. While the issue does not specify the exact desired output for the problematic cases, it is clear that the RecursionError is the bug to be fixed, and the solution should ensure that latex() returns a valid LaTeX string for these inputs without error. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20428": {
        "explanation": "The issue provides a clear description of the bug: after calling clear_denoms() on a certain Poly, the resulting Poly prints as zero but behaves inconsistently (e.g., Poly.is_zero is False, as_expr() is 0, and some methods raise errors). The root cause is identified as an unstripped leading zero in the DMP representation. The issue includes concrete examples, expected vs. actual behavior, and even points to the relevant internal representation. The fix is implied: ensure that the DMP representation of a zero polynomial is properly stripped (i.e., DMP([]) instead of DMP([0])). This is a well-specified bug report with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20438": {
        "explanation": "The issue provides a concrete example where the `is_subset` method gives incorrect results for certain set objects in SymPy. It includes code snippets, expected and actual behavior, and a traceback for a related error. However, it does not specify what the correct behavior should be in all cases, nor does it outline the expected output for all the methods shown. Still, the main problem (incorrect `is_subset` results between ProductSet and FiniteSet of tuples) is clear, and a developer familiar with the codebase could attempt a fix based on the provided example. Some interpretation is needed, but the core issue is understandable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20442": {
        "explanation": "The issue provides a clear description of the unexpected behavior in the convert_to function when attempting to convert a compound unit (joule*second) to a non-matching unit (joule). It includes a concrete example, the actual output, and the expected behavior (either the original expression, an expression in base units, or an error). It also provides related cases that work as expected, helping to clarify the scope of the problem. While it does not specify exactly which of the three expected behaviors should be implemented, it is clear that the current behavior is incorrect and that a fix should prevent nonsensical conversions. This is enough to make a meaningful attempt at a solution, though some judgment is required in choosing the best expected behavior.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20565": {
        "explanation": "The issue clearly states that the `Rationals` set in SymPy should consider floating point numbers (like 0.5) as contained within it, and provides a concrete example where `sympy.Rationals.contains(0.5)` currently returns False but should return True. The expected behavior is unambiguous, and the codebase context (SymPy) is clear. The task is to modify the `contains` method or related logic so that floats representing rational numbers are recognized as being in `Rationals`.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20590": {
        "explanation": "The issue provides a clear description of the observed behavioral change between two versions of the software (SymPy): in 1.6.2, Symbol instances did not have a __dict__, but in 1.7 they do. The user suspects this is a bug related to __slots__ and possibly a change in a parent class. However, the issue does not specify what the expected behavior should be (should Symbol instances never have a __dict__? Is this a breaking change? Is there a performance concern?), nor does it specify what the correct fix would be. There is enough information to investigate the codebase and likely identify the cause, but some interpretation is required regarding the intended behavior and the desired outcome.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20639": {
        "explanation": "The issue provides a clear description of the problem: the pretty printer (likely in SymPy or a similar symbolic math library) renders the expression pi**(1/E) incorrectly, showing a square root instead of the correct exponentiation. The user provides the actual and expected outputs, and notes that LaTeX and str representations are correct. The context (recent master, version info) is also given. This is sufficient to attempt a fix in the pretty printer for this case.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20741": {
        "explanation": "The issue provides a clear description of the problem: matrix expressions involving MatrixSymbol and ZeroMatrix are not being simplified as expected. It includes concrete code examples showing both the current (incorrect) and expected (simplified) outputs, and references a related issue for further context. While it does not specify the exact form of the 'correct' simplified output, the intent is clear: expressions like -(-A + B) - A + B should be simplified to ZeroMatrix or an equivalent canonical form. This is enough information for an experienced developer to attempt a solution, likely by improving the simplification logic for matrix expressions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20801": {
        "explanation": "The issue provides a clear description of the observed inconsistent behavior when comparing S(0.0) and S.false in SymPy, including code examples and the expected behavior. It also references related issues for context. The desired outcome is explicitly stated: S(0.0) == S.false should return True, matching the behavior of S.false == S(0.0). This is sufficient information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20916": {
        "explanation": "The issue provides a clear example of the desired output ('Good') and the current, incorrect output ('Bad'). It specifies that the problem is with the pretty-printing (pprint) of unicode, specifically that subscripts are not being applied to Greek letters (e.g., '\u03c90' instead of '\u03c9\u2080'). The expected behavior is unambiguous: subscripts should be formatted on Greek letters as they are on Latin letters. This is sufficient information to attempt a solution, as the scope and the nature of the bug are clear.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21101": {
        "explanation": "The issue provides a specific code example (`latex(Mul(6, S.Half, evaluate=False))`) and describes the incorrect output (`6 1/2`) with a clear concern: the lack of `numbersep` (presumably a spacing or formatting issue in the LaTeX output). The context is clear: the LaTeX representation of an unevaluated multiplication is not formatted as expected. While it does not specify the exact desired output, it is clear that the output should include proper separation (likely `6 \\times \\frac{1}{2}` or similar). This is enough information for a developer familiar with the codebase to attempt a fix or at least improve the output formatting. Thus, the issue is reasonably specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21171": {
        "explanation": "The issue provides a clear error message, a reproducible code snippet, and the context in which the error occurs (using SymPy's Beam module in a Jupyter notebook). The stack trace shows that the error is due to the _print_SingularityFunction method not accepting an 'exp' keyword argument, which is passed during LaTeX printing of a Pow object whose base is a SingularityFunction. The expected behavior is that the shear_force() method should display correctly without this error. The solution would involve updating the _print_SingularityFunction method to accept and handle the 'exp' argument appropriately. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21208": {
        "explanation": "The issue describes a discrepancy in the results of differentiating a SymPy matrix as a whole versus differentiating its elements individually or via applyfunc. However, the issue is primarily a question about whether this is a bug or intended behavior, rather than a request for a specific change or fix. There is no clear instruction on what the expected behavior should be, nor is there a concrete task to implement. As such, it is unclear what a successful solution would look like without further clarification from the issue author or maintainers.",
        "label": "VAGUE"
    },
    "sympy__sympy-21260": {
        "explanation": "The issue provides a clear description of the observed behavior, a reproducible code snippet, and a discussion of what is expected versus what is happening. However, it does not specify what the desired solution is: Should symbols created in different processes be considered equal if they have the same name and assumptions? Should there be a canonicalization mechanism? Is this a documentation issue, a bug, or a feature request? The issue is well-motivated and detailed, but the exact requirements for a successful solution are not fully specified, leaving some room for interpretation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21271": {
        "explanation": "The issue provides a clear description of the problem: a specific doctest in sympy/physics/vector/frame.py is failing, and the exact expected and actual outputs are shown. The context (test was disabled to unblock CI, but the underlying issue remains) is given, and the relevant code location is specified. The task is to fix the doctest failure, which is a well-defined goal. No further clarification is needed to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21286": {
        "explanation": "The issue provides concrete examples of the current (non-canonical) behavior of symbolic Ranges versus numeric Ranges, and suggests that symbolic Ranges should be made more canonical, similar to the numeric case. However, it does not specify exactly what the canonical form should be for symbolic Ranges, nor does it define the expected output for all cases. There is an implicit suggestion (e.g., that all symbolic Ranges in the second example could be canonicalized to Range(i, i + 1, 5)), but the rules for this are not fully spelled out. A reasonable interpretation can be made, but some design decisions would need to be filled in by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21313": {
        "explanation": "The issue provides a clear description of the problem: imageset canonicalization is being incorrectly applied to Floats, leading to unexpected and incorrect results. It gives concrete examples, explains the root cause (canonicalization of b to b%c), and specifies the desired behavior (canonicalization should not be done for Floats, only for Rationals). The issue also provides a workaround and test cases that can be used to verify the solution. The requirements for a successful solution are clear: prevent imageset from canonicalizing when dealing with Floats. This is well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21370": {
        "explanation": "The issue provides a concrete example of an input to the minpoly function that fails with a NotImplementedError, including the full traceback and the problematic expression. It is clear that the user expects minpoly to handle this complicated algebraic number, or at least not to fail with NotImplementedError due to 'multiple candidates for the minimal polynomial.' However, the issue does not specify what the expected output should be, nor does it discuss the mathematical context or constraints for resolving the ambiguity in minimal polynomial selection. While the failure is clear and reproducible, the desired behavior (e.g., how to choose among multiple candidates, or whether to return all, or raise a more informative error) is not fully specified. Still, a reasonable interpretation is to improve minpoly so it can handle such cases or provide a better error message. Thus, the issue is reasonably specified, but not fully well-specified.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21379": {
        "explanation": "The issue provides a clear minimal working example (MWE) that triggers the bug, including the exact code, the error message, and the context in which the error occurs. It also describes the conditions under which the error appears or disappears, and the environment (Sympy version). While the root cause is not known, the issue is well-specified for a developer to reproduce and investigate the bug, and to attempt a fix or at least add a regression test.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21436": {
        "explanation": "The issue provides a clear description of the current behavior, the undesired limitation (overwriting of orientation relationships when calling orient()), and the desired behavior (allowing orientation relationships to be established in any order, as long as the graph remains acyclic). It also provides concrete code examples, both of the current behavior and the expected use case. The requirement to prevent loops is also specified. While some implementation details (e.g., how to check for graph consistency) are left to the developer, the functional requirements are clear and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21476": {
        "explanation": "The issue provides a clear minimal working example (MWE) that reproduces the bug, including the code, the error traceback, and a detailed debugging session. The root cause is also hypothesized: a type mismatch between keys in a dictionary (CoordSystem vs. sympy.Str). The expected behavior is that indirect coordinate system transformations should work without raising a KeyError. The information is sufficient to attempt a fix: ensure consistent key types in the relevant data structures. Thus, the issue is well-specified for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21527": {
        "explanation": "The issue provides a clear code example demonstrating inconsistent behavior in sympy.linsolve when solving similar systems of two equations. The expected behavior is implied: all three systems should return the same solution, but they do not. The issue is well-specified because it includes reproducible code, observed outputs, and a clear question about the inconsistency. A meaningful attempt at a solution would involve investigating why linsolve behaves inconsistently and fixing it so that similar systems yield consistent results.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21567": {
        "explanation": "The issue clearly describes the current behavior of the `expand_trig` function for `csch` and `sech`, and provides reference behavior for similar functions (`csc`, `sec`). It also gives explicit examples of the current and desired output, making it clear that the goal is to implement expansion for `csch(x + y)` and `sech(x + y)` in a manner analogous to `csc(x + y)` and `sec(x + y)`. This is sufficient information for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21586": {
        "explanation": "The issue provides a detailed description of the observed behavior, including code snippets, expected and actual outputs, and a clear demonstration of the inconsistency when using parse_expr with local_dict and implicit multiplication. However, it does not explicitly state what the desired or correct behavior should be, nor does it specify what a successful solution would look like (e.g., should both expressions always yield the same result regardless of local_dict?). The intent is implied, but not fully specified. Therefore, while a reasonable interpretation can be made (that parse_expr should treat both expressions equivalently even with local_dict), there is some ambiguity.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21596": {
        "explanation": "The issue provides a clear description of the bug, including the code to reproduce it, the current (incorrect) output, and the expected (correct) output. It specifies the context (imageset, intersection with Reals, membership testing) and the precise behavior that is wrong. This is sufficient information for a developer to investigate and attempt a fix, as the expected behavior is unambiguously described.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21612": {
        "explanation": "The issue provides a clear description of the problem: parsing a specific LaTeX fraction expression yields an incorrect result due to missing brackets in the denominator. It includes the problematic input, the actual output, and the expected output, as well as steps to reproduce the issue and the environment in which it was tested. This information is sufficient to understand the bug and attempt a fix in the LaTeX parser.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21614": {
        "explanation": "The issue provides a clear description of the problem: the `kind` attribute for `Derivative(A, x)` returns `UndefinedKind`, but the expectation is that it should behave similarly to `Integral(A, x)`, which returns `MatrixKind(NumberKind)`. The code examples are concrete, and the expected vs. actual behavior is well illustrated. The only minor ambiguity is whether the reporter expects the exact same kind as the integral, but the context and comparison make the intent clear enough for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21627": {
        "explanation": "The issue provides a clear bug report: evaluating is_zero on a specific sympy expression causes a RecursionError. The code to reproduce the bug is given, and the expected behavior is implied (is_zero should not cause a recursion error). The scope is clear: fix the RecursionError when checking is_zero for this expression. There is little ambiguity about what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21769": {
        "explanation": "The issue provides a clear description of the problem (LaTeX rendering of CG with powers), includes code to reproduce, shows the current output, and suggests a concrete fix (wrapping the LaTeX string in braces). It also references the relevant code location and related issues. The expected behavior is clear: the LaTeX representation should render correctly in Jupyter when CG is raised to a power. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21847": {
        "explanation": "The issue provides a clear description of the problem: the `itermonomials` function in SymPy does not return all expected monomials when the `min_degrees` argument is used. It includes a concrete code example, the actual output, the expected output, and references the relevant documentation. The expected behavior is unambiguous (all monomials of total degree 3 should be returned), and the failure is clearly demonstrated. This is sufficient information to attempt a fix or at least to reproduce and investigate the bug.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21849": {
        "explanation": "The issue provides a clear description of the problem: attempting to display an expression involving LambertW raised to a power in JupyterLab (using SymPy) results in a TypeError. The error traceback is included, showing that the problem is in the LaTeX printer's handling of LambertW within a power. The example code is provided, and the expected behavior (successful display) is implied. The root cause is also suggested by the error message. This is sufficient information to attempt a fix in the codebase, likely in the SymPy LaTeX printer for LambertW.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21930": {
        "explanation": "The issue provides a clear description of the problem: LaTeX output in the 'secondquant' module does not render double superscripts with the 'dagger' command correctly in Jupyter Notebook. It includes a minimal reproducible example, the expected output, and a suggested fix (adding curly brackets around the superscripted expression). This is sufficient information to attempt a solution, as it identifies the module, the specific function, the nature of the rendering bug, and a likely fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21932": {
        "explanation": "The issue provides a clear description of the problem: using imageset with a lambda and a Range object throws a specific exception. It includes a concrete code example and the error message encountered. While it does not specify the exact expected behavior, it is reasonable to infer that the intent is for imageset to work with Range objects without error. The context and desired outcome are clear enough for a developer familiar with the codebase to attempt a fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21952": {
        "explanation": "The issue provides a concrete example of unexpected behavior in the code (the .is_even property returning None instead of True for a specific symbolic expression). It also suggests a possible direction for a solution (improving the logic for determining evenness/oddness of symbolic integers, especially those involving denominators). Furthermore, the issue includes a proposed change to a specific function (_eval_is_odd) and describes the intended behavior. While there is some discussion about broader architectural changes (like a dictionary of prime divisors), the core actionable request\u2014fixing the .is_even property for cases like n**2/2 when n is even\u2014is clear and testable. Thus, a meaningful attempt at a solution can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-22005": {
        "explanation": "The issue provides a clear description of the problem: the function solve_poly_system currently does not correctly detect and handle cases where the system has infinitely many solutions (i.e., is not zero-dimensional). It gives a concrete example (solve_poly_system((y - 1,), x, y)) that is not handled correctly, and shows the expected behavior (raising NotImplementedError). The diff provided shows a proposed code change and corresponding test updates, making it clear what is required for a successful solution: ensure that solve_poly_system raises NotImplementedError for non-zero-dimensional systems. The requirements and expected behavior are unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22080": {
        "explanation": "The issue provides a clear description of the bug: when using lambdify with modules=[], the multiplication outside Mod is incorrectly moved inside Mod. It gives a concrete example, including code and the unexpected output, and contrasts it with the expected behavior. The context (Sympy, lambdify, Mod, modules=[]) is clear, and the problem is reproducible. The expected behavior is implied: the output of lambdify with modules=[] should match the mathematical structure of the original expression, not move the multiplier inside Mod. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22098": {
        "explanation": "The issue provides a clear description of the unexpected behavior: when using sympy.parse_expr with evaluate=False, expressions under sqrt are still being partially evaluated, whereas the user expects them to remain unevaluated (e.g., sqrt(2*2) should not become 2). The issue includes specific code examples demonstrating both the current behavior and the expected behavior, as well as evidence that the underlying sympy functions do support evaluate=False correctly. This makes it clear what needs to be fixed: ensure that parse_expr with evaluate=False does not evaluate expressions under sqrt (and possibly other functions) when it shouldn't. The requirements for a successful solution are well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22236": {
        "explanation": "The issue provides a clear example of the current behavior and the undesired output, specifically pointing out that the second version of the unevaluated Mul should use brackets to clarify the expression. The context (SymPy's printing of unevaluated Mul objects) is clear, and the expected improvement (use of brackets for clarity) is well-motivated by the example. While it does not specify the exact bracket style, it is clear what the problem is and what a successful solution would look like: the output should use brackets to avoid ambiguity. This is enough for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22383": {
        "explanation": "The issue clearly states the objective: to drop testing for Python 3.6 and PyPy 3.6 in the CI configuration. It references a policy (NEP 29) and a related issue (#21884) for context. The required change is to update the CI configuration so that these Python versions are no longer tested. There is no ambiguity about what needs to be done, and the scope is well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22456": {
        "explanation": "The issue describes a specific invariance that should hold for the `String` class in `codegen.ast`, namely that reconstructing an instance using `expr.func(*expr.args)` should yield an object equal to the original, as is expected for any `Basic` subclass. It also points out that currently, only the keyword-argument-based invariance holds. The required change is clear: modify the `String` class so that the positional argument invariance holds. The context and expected behavior are well-defined, making it clear what needs to be changed and how to verify correctness.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22706": {
        "explanation": "The issue provides a clear error message (IndexError: list index out of range) and the exact code snippet that triggers the error. It also specifies the context (StrPrinter for UnevaluatedMul) and the line of code where the error occurs. This information is sufficient to reproduce the bug and understand what needs to be fixed: prevent the IndexError when printing certain unevaluated Mul expressions. The expected behavior is implied: the print statement should not raise an error. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22714": {
        "explanation": "The issue provides a clear description of the problem, including the code that triggers the error, the error message, and the context (using 'with evaluate(False)'). It also specifies that the code works without the context manager, and with the 'evaluate=False' argument directly. The expected behavior is implied: the code should not crash when used with 'with evaluate(False)'. This is sufficient information to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22740": {
        "explanation": "The issue provides a clear description of the problem: lambdifying Heaviside(Min(x, y)) in sympy with numpy backend fails due to nested select statements, which are generated from ITE (if-then-else) constructs in the Piecewise representation. The issue also describes a workaround (using simplify or simplify_logic to remove ITEs), and suggests a direction for a fix: detect ITEs in Piecewise and rewrite them as And/Or or simplify them. The expected behavior is clear (lambdify should work without error), and the root cause is identified. There is enough information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22773": {
        "explanation": "The issue clearly describes the problem: the LaTeX output for a determinant is incorrect (| (A) | instead of |A|). It provides a concrete example of the current and desired output, and even supplies a code snippet that fixes the issue. The context (modifying LatexPrinter, replacing _print_Determinant = _print_Abs with a custom method) is clear. While the reporter expresses uncertainty about the best solution, the requirements for a successful fix are unambiguous: correct the LaTeX output for determinants. This is sufficient for an experienced engineer to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22914": {
        "explanation": "The issue clearly describes the current limitation (PythonCodePrinter does not support Min and Max), provides a reproducible example, and even suggests the exact methods to implement. The expected behavior is unambiguous: Min and Max should be printed as Python's min() and max() functions. This is sufficient for a meaningful and straightforward solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22969": {
        "explanation": "The issue provides a clear description of a suspected bug: the beam waist (w_0) calculation in the BeamParameter class does not account for changes in refractive index, leading to unphysical results when the Rayleigh length changes but the wavelength does not. The user points out the relevant code and the physical principle being violated. However, the issue does not specify exactly how the refractive index should be incorporated into the calculation, nor does it provide a reference formula or expected behavior. While the physical context is clear to someone with optics knowledge, there is some ambiguity in the exact requirements for the fix (e.g., should the wavelength be replaced by the wavelength in medium, or should z_r be redefined?). Thus, a sensible interpretation is possible, but some blanks must be filled in by the implementer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-23021": {
        "explanation": "The issue provides a clear description of the current problem (decompogen fails with Min/Max, resulting in a recursion error), a concrete example (decompogen(Min(5, x), x)), and a desired outcome ([Min, 5, x]). It also specifies that the goal is to add Min/Max support to decompogen. The before/after behavior is well-documented, and the scope of the fix is clear. Therefore, the issue is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23262": {
        "explanation": "The issue is well-described: it provides a minimal working example, specifies the expected and actual outputs, and clearly identifies the problem (the Python code printer in SymPy 1.10 does not generate a trailing comma for single-element tuples, causing a semantic change). The desired behavior is unambiguous: the generated code should return (1,) for a single-element tuple, matching previous versions. This is sufficient information to attempt a fix in the code printer.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23296": {
        "explanation": "The issue provides a clear description of the problem: the function _collect_factor_and_dimension incorrectly handles dimensionless numbers in exponents, leading to a ValueError. It includes a minimal reproducible example, the observed error, and a comparison with another function (check_dimensions) that behaves correctly. The expected behavior is implied: exp(u/v) should be recognized as dimensionless if u and v have the same dimension. The scope of the fix is clear: correct the dimensional evaluation in _collect_factor_and_dimension for mathematical functions like exp. This is sufficient for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23413": {
        "explanation": "The issue provides a clear description of the bug, including the input, the expected output, and the actual output. It also explains the context (using flips and transposes to achieve a row-style Hermite Normal Form) and the suspected cause (matrix being falsely identified as rank-deficient). This is sufficient information for a developer to reproduce the issue and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23534": {
        "explanation": "The issue provides a clear description of the problem, including the Sympy version, a code snippet that reproduces the issue, a link to the relevant documentation, and a clear statement of the expected vs. actual behavior. The goal is to ensure that using an extra layer of parentheses with `symbols` and `cls=Function` produces `UndefinedFunction` objects instead of `Symbol` objects. This is a well-specified bug report with a clear path to a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23729": {
        "explanation": "The issue clearly describes a concrete problem: the julia_code printer in SymPy generates Julia code that is syntactically invalid due to missing spaces between numbers and broadcasting operators (e.g., '2.*'). It provides a specific example, explains the root cause, and suggests a robust solution: emit code with spaces around binary operators by default. The expected output format is also illustrated. While there is some leeway in exactly how to implement the spacing (e.g., whether to always add spaces or only in ambiguous cases), the core requirement is clear and actionable. Thus, a meaningful attempt at a solution can be made.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-23808": {
        "explanation": "The issue provides a clear description of the problem: a RecursionError occurs when calling sum_of_squares with relatively small inputs. It includes a reproducible code snippet, the exact error message, and the context (inputs used, expected behavior vs. actual behavior). While it does not specify the desired fix, it is clear that the expectation is for the function to handle such inputs without a recursion overflow. This is sufficient for a developer to investigate and attempt a solution, such as refactoring the recursion or optimizing the algorithm.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23824": {
        "explanation": "The issue provides a clear description of the bug, including the expected and actual behavior, a minimal reproducible example, and a precise explanation of the root cause. It specifies the function involved, the nature of the bug (reversal of order), and even points to the relevant code logic that needs to be fixed. This is sufficient for an experienced developer to attempt a solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23950": {
        "explanation": "The issue clearly describes the problem: the Contains.as_set method currently returns a Contains object, which is not a set and leads to downstream errors because it lacks the as_relational method. The issue provides a concrete example of the incorrect behavior and the resulting error. The expected behavior is implied: Contains.as_set should return a set (presumably the set described by the Contains object), not a Contains object. This is enough information to attempt a meaningful fix, such as updating Contains.as_set to return the appropriate set object.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24066": {
        "explanation": "The issue provides a clear description of the problem: SI._collect_factor_and_dimension() fails to recognize that the exponent in exp(expr) is dimensionless, resulting in a ValueError. It includes a reproducible code snippet, the expected behavior (the dimension should be Dimension(1)), and the actual error. The function and context are specified, and the desired outcome is clear. This is sufficient information for a developer to attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24102": {
        "explanation": "The issue clearly describes the problem: the new `parse_mathematica` function in `sympy.parsing.mathematica` cannot parse Greek characters (e.g., '\u03bb'), whereas the old `mathematica` function could. It provides a concrete example of expected and actual behavior, including code snippets and the error message. The goal is to make `parse_mathematica('\u03bb')` behave like `mathematica('\u03bb')`, i.e., successfully parse and return the Greek character. This is a well-specified bug report with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24213": {
        "explanation": "The issue provides a clear code example that demonstrates the problem: when adding two quantities with equivalent dimensions (velocity and acceleration*time), the function _collect_factor_and_dimension raises a ValueError, even though the dimensions are equivalent. The expected behavior is implied: the function should recognize that these dimensions are equivalent and not raise an error. The context, the function involved, and the undesired behavior are all specified, making it clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24325": {
        "explanation": "The issue clearly describes a numerical error in the conversion between coulomb and statcoulomb in the SymPy units system. It provides the current (incorrect) output, the expected output, and even specifies the exact changes needed in the test cases. The context, the functions involved, and the expected numerical values are all provided. This makes it clear what needs to be fixed and how to verify the solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24353": {
        "explanation": "The issue provides a clear error message, the command that triggers the error, and the relevant code paths. It is clear that running 'bin/py.bench' fails due to an ImportError related to 'py.__.test.item', and the version of the 'py' library is specified. The goal is to fix 'py.bench' so that it works without this ImportError. While the exact intended behavior of 'py.bench' is not described, the immediate problem (the ImportError) is well-specified and can be addressed directly.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24370": {
        "explanation": "The issue provides a clear, reproducible code example, the exact error message, and a comparison to expected behavior (it works with a plain Python int). However, it does not explicitly state what the desired behavior should be: should sympy.Integer(1024)//s0 return a symbolic floor division expression, or should it raise a different error? The intent is implied (consistency with Python int), but not fully specified. Still, a reasonable interpretation is to make the behavior consistent with Python int, i.e., return a symbolic floor division. Thus, the issue is reasonably specified, but not perfectly so.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-24443": {
        "explanation": "The issue provides a clear description of the bug: the `_check_homomorphism` function is broken for `PermutationGroup` objects, specifically due to how the internal `_image()` function checks for inverted generators. The error is reproducible with a code snippet, and the problematic code location is linked. The expected behavior is implied: the homomorphism should be constructed without error if the images define a valid homomorphism. While the suggestion to 'greatly simplify' is open-ended, the core bug and its cause are well-specified, making it possible to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24455": {
        "explanation": "The issue provides a clear description of the bug: the is_cyclic property of PermutationGroup returns True for a group isomorphic to S3 constructed from generators, when it should return False. The issue includes a reproducible code example, an analysis of the relevant code section, and a discussion of the mathematical background. It also suggests a brute-force fix and raises the question of whether the current method can be patched or improved. While the issue leaves open the possibility of a more efficient fix, it is clear that a correct solution must at least ensure is_cyclic returns False for S3 constructed as in the example. Thus, the requirements for a successful solution are clear, even if there is room for improvement beyond brute force.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-24539": {
        "explanation": "The issue provides a clear description of the problem: the PolyElement.as_expr() method is supposed to allow the user to specify which symbols to use in the output expression, but currently it either errors if the wrong number of symbols is provided or ignores the provided symbols and always uses the default ones from self.ring.symbols. The issue includes a code example demonstrating the undesired behavior. The expected behavior is implied: as_expr(U, V, W) should return an expression using U, V, W as the variables instead of x, y, z. The location of the relevant code is also provided. This is sufficient information to attempt a meaningful fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24562": {
        "explanation": "The issue provides a clear description of the observed bug: the behavior of Rational('0.5', '100') in sympy 1.11.1 returns 1/100100, which is unexpected, while Rational(0.5, 100) returns 1/200, which is correct. It also notes that this worked as expected in sympy 1.8. The environment (Python 3.11, sympy 1.11.1) is specified. The expected and actual behaviors are both described, and the regression is noted. This is sufficient information to investigate and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24661": {
        "explanation": "The issue clearly describes the problem: the 'evaluate=False' parameter is being ignored by 'parse_expr' when parsing relational expressions, resulting in immediate evaluation instead of returning an unevaluated relational object. It provides a concrete example of the current (incorrect) behavior and the expected (correct) behavior. The references to related issues (#22305 and #22098) may provide additional context, but the core requirement is clear from the description and examples. Therefore, it is well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24723": {
        "explanation": "The issue provides a clear description of the problem: the probability density function (PDF) returned by the MatrixNormal distribution in Sympy has an incorrect normalization constant. The issue includes a reference to the correct formula (Wikipedia), a code example showing the current (incorrect) output, and a clear statement of what the correct output should be. The difference is explicitly identified (the normalization constant in the denominator). This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24909": {
        "explanation": "The issue provides a clear description of the bug, including code to reproduce it, the expected behavior, and the actual behavior. It specifies the environment (SymPy version, OS, Python version) and gives a concrete example. The desired outcome is also stated: 'milli*W should evaluate to milli watts / mW'. This is sufficient information to attempt a fix in the codebase.",
        "label": "WELL_SPECIFIED"
    }
}