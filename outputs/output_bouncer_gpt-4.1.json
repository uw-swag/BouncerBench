{
    "astropy__astropy-12907": {
        "explanation": "The issue describes incorrect computation of the separability matrix for nested CompoundModels, where the separability matrix is unexpectedly not block-diagonal when nesting compound models. The root cause is likely that the code does not handle cases where _calculate_separability_matrix returns None, which can happen for certain models, leading to incorrect fallback logic and thus incorrect results for nested models.\n\nThe patch changes the condition from 'is not NotImplemented' to 'not in (NotImplemented, None)', ensuring that if _calculate_separability_matrix returns None (as well as NotImplemented), the code will proceed to the recursive logic for CompoundModels, which is the correct behavior. This directly addresses the described bug, and the patch is minimal and precise, making only the necessary change to the conditional check. No unrelated or unnecessary changes are introduced.\n\nTherefore, the patch is correct and precise in addressing the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-13033": {
        "explanation": "The issue describes that when a required column (other than 'time') is removed from a TimeSeries, the exception message is misleading and does not clearly state that a required column is missing. The patch modifies the BaseTimeSeries class to explicitly check for removal of required columns and raises a clear ValueError (e.g., \"Cannot remove required columns: flux\"). It also updates the test suite to expect these new, more informative error messages. The changes are focused on the required column logic and error reporting, and do not introduce unrelated modifications. The patch thus directly and precisely addresses the issue as described, ensuring that users receive clear feedback when attempting to remove required columns.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-13236": {
        "explanation": "The issue requests that, for now, a FutureWarning be added when a structured numpy array is auto-converted to an NdarrayMixin upon being added to a Table, informing users that this behavior will change in version 5.2 and that they should wrap their data in Column to maintain current behavior. The patch does exactly this: it adds a FutureWarning at the relevant code location, with a clear message about the upcoming change and a TODO to remove the conversion in 5.2. It also adds a test to ensure the warning is emitted and the conversion still occurs for now. No unrelated or unnecessary changes are made (the pyproject.toml pin is unrelated to the issue but is a minor build requirement and does not affect the logic or scope of the patch). Thus, the patch is correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-13398": {
        "explanation": "The patch implements direct transformations between ITRS and observed frames (AltAz, HADec) as described in the issue. It introduces a new module, registers the appropriate transforms, and uses a matrix-based approach that stays entirely within the ITRS, treating ITRS positions as time-invariant. The code closely matches the conceptual code provided in the issue description, and does not introduce unrelated changes. The patch does not add refraction handling, but the issue explicitly states that refraction is not yet included and can be added later if needed. Therefore, the patch is correct and precise for the requirements as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-13453": {
        "explanation": "The issue is that the 'formats' argument to Table.write(..., format='ascii.html', formats=...) is ignored, so HTML output does not respect user-supplied formatting. The patch addresses this by:\n\n1. Modifying the HTML writer's write() method to accept a 'formats' argument and process it, ensuring it is a dict mapping column names to formatters.\n2. Passing the 'formats' argument through the registry and Table.write machinery so it reaches the HTML writer.\n3. Updating the fill_values method to apply the format function or string to each value before output.\n4. Adding a test that verifies the HTML output contains the formatted values as expected.\n\nThe patch is focused on the described issue, does not introduce unrelated changes (except for some debug prints, which are presumably temporary), and the logic for applying formats is consistent with the expectations and with how other formats (like CSV) handle 'formats'.\n\nTherefore, the patch correctly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-13469": {
        "explanation": "The issue describes that converting a list of Astropy Tables to a NumPy array with dtype=object fails because Table.__array__() raises a ValueError for any non-None dtype, including dtype=object. The patch modifies the check to allow dtype=object, raising the error only for other dtypes. This directly addresses the issue by permitting np.array(table, dtype=object), which is the desired behavior for storing Table objects in an object array. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-13579": {
        "explanation": "The issue describes that `SlicedLowLevelWCS.world_to_pixel` returns incorrect (essentially infinite) results for a sliced WCS, while the unsliced WCS works as expected. The root cause is that, when slicing, the dropped world dimensions are not being filled in with the correct values when calling the underlying WCS's `world_to_pixel` method. Instead, a hardcoded value of 1.0 was used, which is not generally correct.\n\nThe patch changes this line:\n-                world_arrays_new.append(1.)\n+                world_arrays_new.append(self.dropped_world_dimensions['value'][iworld - iworld_curr - 1])\n\nThis means that, for dropped world dimensions, the value used is now the one stored in `self.dropped_world_dimensions['value']`, which is the value at which the slice was taken. This is the correct behavior: when slicing, the dropped dimension should be fixed at the value corresponding to the slice, not an arbitrary constant.\n\nThe patch also adds a test to check that `world_to_pixel_values` and `pixel_to_world_values` are consistent for the sliced WCS, and the test is targeted and relevant. The other changes (pinning setuptools version, adding a standalone test file) are not strictly necessary for the fix, but do not affect the correctness of the patch for the main codebase.\n\nTherefore, the patch addresses the described issue directly and precisely, without unnecessary changes to the main codebase.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-13977": {
        "explanation": "The issue requests that Quantity.__array_ufunc__ should return NotImplemented, rather than raising a ValueError, when incompatible units are encountered, to allow Python's operator dispatch to fall back to reflected methods (e.g., __radd__).\n\nThe patch addresses this by modifying the converters_and_unit function to return NotImplemented if any converter is None or the result unit is None, which is the situation for incompatible units. This change ensures that __array_ufunc__ will propagate NotImplemented up the call stack, as desired.\n\nThe patch also adds a test (test_incompatible_units_return_notimplemented) that directly checks that q1.__add__(q2) returns NotImplemented for incompatible units, and another test (test_incompatible_units_in_ufunc) that demonstrates the current behavior with numpy's add ufunc (which still raises ValueError, as numpy does not propagate NotImplemented in ufuncs). This is consistent with the issue's context and limitations.\n\nOther changes (docstring note, unrelated pyproject.toml pin, and test_curie.py) are not strictly necessary for this issue, but they do not interfere with the core fix. The main logic change is precise and directly addresses the issue as described.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14096": {
        "explanation": "The issue describes that when a subclass of SkyCoord defines a property that tries to access a non-existent attribute, the AttributeError message misleadingly claims the property itself is missing, rather than the actual missing attribute. This is due to SkyCoord's __getattr__ implementation, which always raises an AttributeError for the requested attribute, even if the error was deeper in the call stack. The patch changes __getattr__ so that if the attribute is not found, it falls back to object.__getattribute__, which will raise the AttributeError for the actual missing attribute, preserving the correct error message and traceback. The included test verifies that the error message now correctly refers to the missing attribute, not the property. The patch is minimal and precise, only changing the relevant logic and adding a targeted test. The unrelated change to setuptools version in pyproject.toml is not related to the issue, but it does not affect the correctness of the fix for the described problem. Therefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14182": {
        "explanation": "The issue is that the RST writer does not accept a 'header_rows' argument, causing a TypeError when users try to specify it. The patch modifies the RST class's __init__ method to accept a 'header_rows' argument and passes it to the superclass, which aligns with the behavior of other formats like 'ascii.fixed_width'. This directly addresses the error described in the issue and enables the desired usage. The rest of the patch preserves the previous logic for writing the separator lines, only making it more robust by searching for the separator line rather than assuming its position. No unnecessary changes are made. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14309": {
        "explanation": "The issue is an IndexError caused by attempting to access args[0] when args is an empty tuple in the is_fits function. The patch changes the return statement to only attempt isinstance(args[0], ...) if len(args) > 0, otherwise it returns False. This directly prevents the IndexError and matches the intended logic: if there are no args, the function should not try to check their type. The patch is minimal and does not introduce unrelated changes. Therefore, it precisely addresses the described bug.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14365": {
        "explanation": "The issue describes that the QDP command parsing in astropy.io.ascii.qdp is case-sensitive, but QDP itself is not, so commands like 'read serr 1 2' (lowercase) should be accepted. The patch modifies the regular expression used to match QDP commands by adding the inline flag '(?i)' to make the regex case-insensitive. This directly addresses the problem by allowing commands in any case to be recognized, matching QDP's behavior. The change is minimal and targeted, with no unnecessary modifications. The unrelated change in pyproject.toml (pinning setuptools version) does not affect the logic of the QDP parser and can be considered orthogonal to the issue at hand, but it does not introduce unrelated logic changes to the code in question. Therefore, the patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14369": {
        "explanation": "The issue describes incorrect parsing of composite units with multiple divisions in the CDS format, leading to the wrong order of operations (e.g., '10+3J/m/s/kpc2' being parsed as '1e+3 J s / (kpc2 m)' instead of the correct '1e+3 J / (m s kpc2)'). The patch introduces a DivisionUnit class to explicitly represent division in the parsing process, and updates the CDS parser to use this class to preserve the correct order of operations when parsing units with multiple divisions. The changes in the parser ensure that nested and chained divisions are handled correctly, and the to_string method is updated to serialize DivisionUnit objects properly. No unrelated changes are made, and the patch is focused on the described problem. The change to pyproject.toml is unrelated to the parsing logic but is minor and does not affect the correctness of the fix. Overall, the patch directly and precisely addresses the parsing issue described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14508": {
        "explanation": "The issue describes that the float formatting in io.fits.Card expands values like 0.009125 to '0.009124999999999999', causing the comment to be truncated due to the value string being unnecessarily long. The proposed patch modifies the _format_float function to first try str(value), and only falls back to the previous formatting if the string is too long or lacks a decimal point. This directly addresses the issue by ensuring that simple float values like 0.009125 are represented compactly, as in the original FITS card, and thus allows the comment to fit without truncation. The included test specifically checks the problematic case from the issue and verifies that the string representation matches the expected FITS card. The patch does not introduce unrelated changes (the pyproject.toml pin is not related to the logic and is minor). Therefore, the patch is correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14539": {
        "explanation": "The issue describes FITSDiff incorrectly reporting differences when comparing a FITS file with a VLA (variable-length array) column to itself. The patch modifies the TableDataDiff logic to explicitly handle object dtype columns (which are used for VLAs in numpy/fits), comparing their contents element-wise and checking both length and value equality. This replaces the previous logic that did not handle VLAs correctly. The new tests in test_diff_vla.py directly reproduce the issue and confirm that the fix works: comparing a VLA file to itself yields no differences, while comparing files with different VLA contents yields a difference. The patch does not introduce unrelated changes (the pyproject.toml pin is not relevant to the logic). Therefore, the patch is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14598": {
        "explanation": "The issue describes a bug in the handling of double single-quotes ('') in FITS card string values, where sometimes a double single-quote is incorrectly converted to a single quote when parsing and serializing cards, especially for certain string lengths. The root cause is that the code was always replacing '' with ' in the parsed value, which is not always correct according to the FITS standard (where '' is the way to encode a single quote inside a string, but two consecutive single quotes at the end of a string should remain as '').\n\nThe patch makes two changes:\n1. It modifies the regular expression used to parse string values, making the group for the string value more permissive and correctly matching '' sequences.\n2. It removes the `.replace(\"''\", \"'\")` call when extracting the value, so that '' is not always replaced by '.\n\nThis directly addresses the issue: it prevents the unwanted conversion of '' to ', preserving the original string content as expected. The patch is minimal and does not introduce unrelated changes. The regular expression change is necessary to ensure that the parsing is correct for all valid FITS string values, including those with embedded or trailing '' sequences.\n\nTherefore, the patch is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-14995": {
        "explanation": "The issue describes a failure in mask propagation when performing arithmetic with NDDataRef objects where one operand has a mask and the other does not. Specifically, the error occurs because the code attempts to perform a bitwise operation between an integer mask and None, which raises a TypeError. The expected behavior is that if only one operand has a mask, that mask should be copied to the result.\n\nThe patch adds a check: if the operand exists but its mask is None, it returns a deepcopy of self.mask. This ensures that when one operand has a mask and the other does not, the mask is propagated correctly, matching the expected behavior described in the issue. The patch does not introduce unnecessary changes and directly addresses the described problem. The logic is precise and does not affect unrelated code paths.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-7166": {
        "explanation": "The issue is that the InheritDocstrings metaclass only inherits docstrings for functions, not for properties, because it uses inspect.isfunction, which returns False for properties. The patch addresses this by:\n\n1. Modifying the check to include both functions and properties when inheriting docstrings.\n2. Adding logic to also inherit docstrings for property setters and deleters, not just the main property docstring.\n3. Ensuring that only public members are considered, as before.\n\nThe changes are focused and directly address the issue described, without introducing unrelated modifications. The patch is both correct and precise in its solution.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-7336": {
        "explanation": "The issue describes a failure in the `quantity_input` decorator when used on constructors (`__init__`) that have a return type annotation of `-> None`. The decorator attempts to call `.to()` on the return value, which is `None` for constructors, leading to an AttributeError. The patch modifies the decorator to only call `.to()` if the return annotation is not empty, not None, and the return value itself is not None. This directly addresses the problem described in the issue. Additionally, a test is added to ensure that decorating an `__init__` method with `-> None` does not raise an exception and that the argument is correctly handled. The patch is minimal and precise, making only the necessary changes to fix the issue and adding a relevant test. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-7606": {
        "explanation": "The issue describes that comparing an UnrecognizedUnit to None (e.g., x == None) raises a TypeError, but the expected behavior is to return False. The original __eq__ method attempts to construct a Unit from 'other' without handling exceptions, so passing None raises a TypeError. The patch wraps the Unit construction in a try/except block, catching ValueError, UnitsError, and TypeError, and returns False if any of these are raised. This directly addresses the issue by ensuring that x == None returns False instead of raising. The change is minimal and does not introduce unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-7671": {
        "explanation": "The issue describes a failure in the `minversion` function due to the use of `LooseVersion`, which cannot compare certain version strings (e.g., '1.14.3' vs '1.14dev') and raises a TypeError. The issue also notes that `pkg_resources.parse_version` handles these cases correctly. The patch replaces `LooseVersion` with `parse_version` in the relevant comparison, making the function robust to the problematic version strings. No unrelated changes are introduced, and the patch directly addresses the root cause as described in the issue. Therefore, the patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-8707": {
        "explanation": "The patch updates the `Header.fromstring` method to accept both `str` and `bytes` types for the `data` parameter. It does this by checking if `data` is a `bytes` object and, if so, decodes it as ASCII before proceeding. The docstring is also updated to reflect that both `str` and `bytes` are accepted. This directly addresses the issue described, which is that `Header.fromstring` should accept both types, as its documentation and counterpart `Header.fromfile` do. The patch does not introduce unnecessary changes and is limited to the described problem. While the issue mentions that `Card.fromstring` may need similar treatment, the patch is focused solely on `Header.fromstring`, which is the main subject of the issue. Therefore, the patch is correct and precise for the scope of the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "astropy__astropy-8872": {
        "explanation": "The issue describes that creating a Quantity from a np.float16 value results in a Quantity with dtype float64, whereas for other float types (float32, float64, float128, etc.), the dtype is preserved. The patch addresses this by adding a specific check for np.float16 in the Quantity constructor: if the input is a float16 or an array of float16, it ensures the dtype is set to float16 and constructs the Quantity accordingly. The patch also updates the value property to ensure float16 is preserved, and adds comprehensive tests to verify that the dtype is float16 after construction and arithmetic. The changes are targeted and do not introduce unrelated modifications. Therefore, the patch correctly and precisely addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10097": {
        "explanation": "The issue is that URLValidator currently allows unencoded ':', '@', and '/' characters in the username and password fields of URLs, which is not compliant with RFC 1738. The patch modifies the regex for the user:pass authentication part of the URL to only allow percent-encoded sequences (e.g., %3A for ':', %40 for '@') or any character except whitespace, '/', ':', or '@'. This change directly addresses the requirement that ':', '@', and '/' must be percent-encoded in the userinfo part. The patch also adds relevant test cases to the invalid_urls.txt file, including URLs with unencoded and encoded variants of these characters, ensuring that the validator now rejects the invalid forms. The rest of the regex and code remain unchanged, so the patch is focused and precise. Therefore, the patch correctly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10316": {
        "explanation": "The issue describes a problem where diffsettings calls settings._setup() unconditionally, which can raise an ImproperlyConfigured exception if settings were configured via settings.configure() instead of the DJANGO_SETTINGS_MODULE environment variable. The proposed patch adds a check at the start of LazySettings._setup() to return early if self.configured is True, thus preventing unnecessary re-initialization and avoiding the misleading exception. This change is minimal, directly addresses the issue, and does not introduce unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10390": {
        "explanation": "The issue describes that when truncating datetimes with Django's Trunc() function, ambiguous or non-existent times (such as those occurring during DST transitions) can cause pytz to raise AmbiguousTimeError or NonExistentTimeError. The user wants to be able to pass the is_dst parameter through to pytz's localize (via Django's make_aware) to resolve these ambiguities.\n\nThe patch modifies TruncBase and Trunc to accept an is_dst parameter, stores it, and passes it to timezone.make_aware. This directly addresses the issue: it allows users to specify is_dst when using Trunc, which will be used when localizing the datetime, thus avoiding the unhandled exceptions described. The patch does not introduce unrelated changes and is limited to the necessary code paths.\n\nTherefore, the patch is correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10531": {
        "explanation": "The issue describes that the admin history should display human-readable field names (verbose_name) instead of the internal field names when reporting changes. The patch modifies the code so that, when constructing the change message, it attempts to retrieve the verbose_name for each changed field using form.instance._meta.get_field(field_name).verbose_name. If it cannot retrieve the verbose_name (due to AttributeError or FieldDoesNotExist), it falls back to the field name. This directly addresses the issue by ensuring that, where possible, the verbose name is used in the change message, and does so without introducing unrelated changes. The logic is precise and limited to the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10554": {
        "explanation": "The issue describes a bug where using union() on querysets with ordering, and then reordering or evaluating the queryset, can result in a database error: 'ORDER BY position 4 is not in select list'. This is a common issue with SQL UNIONs, as ordering must be reapplied after the union, and the ordering fields must be present in the select list.\n\nThe patch modifies the _combinator_query method to store the original ordering before clearing it, and then reapplies it after setting up the combined query. This ensures that the ordering is preserved and reapplied correctly after the union operation, which directly addresses the root cause of the error described in the issue.\n\nThe patch is minimal and only changes the relevant method, without introducing unrelated changes. It does not attempt to address unrelated edge cases or make unnecessary modifications. Therefore, it is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10880": {
        "explanation": "The issue describes a SQL syntax error caused by the lack of a space between 'DISTINCT' and the following expression when both 'distinct=True' and a conditional (such as Case) are used in an aggregate function. The patch changes the string assigned to 'distinct' in the extra_context from 'DISTINCT' to 'DISTINCT ' (with a trailing space) when self.distinct is True. This ensures that when the SQL is generated, there will be a space between 'DISTINCT' and the next token, preventing the syntax error described. The patch is minimal and directly addresses the reported problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10914": {
        "explanation": "The issue describes inconsistent file permissions for uploaded files in Django's FileSystemStorage, especially when using TemporaryUploadedFile (which can result in 0o600 permissions due to tempfile's defaults). The request is to set a default FILE_UPLOAD_PERMISSIONS of 0o644 to ensure consistent and predictable permissions for uploaded files, regardless of the upload handler used.\n\nThe patch does the following:\n1. Sets FILE_UPLOAD_PERMISSIONS = 0o644 in global_settings.py, making 0o644 the default.\n2. Updates FileSystemStorage to always apply file permissions after saving, using self.file_permissions_mode if set, otherwise defaulting to 0o644.\n3. Adds a test script (test_upload.py) to verify the permissions for both small (memory) and large (temp file) uploads, with and without custom FILE_UPLOAD_PERMISSIONS.\n\nThis directly addresses the issue: it ensures that, by default, uploaded files will have 0o644 permissions, regardless of the upload handler or temporary file mechanism. The patch is focused and does not introduce unrelated changes. The test script, while not a formal Django test, is relevant and helps demonstrate the fix.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10973": {
        "explanation": "The patch updates the PostgreSQL backend's DatabaseClient.runshell_db method to use subprocess.run instead of subprocess.check_call, and sets the PGPASSWORD environment variable directly instead of creating a temporary .pgpass file. This directly addresses the issue's request to use subprocess.run and PGPASSWORD for authentication, simplifying the code and making it more reliable. The patch also updates the tests to check for the presence of PGPASSWORD in the environment passed to the subprocess, rather than checking for a .pgpass file, which is appropriate given the code change. The addition of test_settings.py is unrelated to the core change but appears to be for test configuration and does not introduce unnecessary changes to the main logic. The patch is focused, precise, and does not introduce unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-10999": {
        "explanation": "The issue describes a problem with the regular expression used in parse_duration: the lookahead in the <hours> part does not allow for negative numbers, so negative durations like '-1:30:45' are not parsed correctly. The proposed patch updates the lookahead from '(?=\\d+:\\d+)' to '(?=-?\\d+:-?\\d+)', allowing for optional negative signs in the hours and minutes fields. This directly addresses the issue described. The patch also adds a test script with various negative duration cases, but does not modify any other unrelated code. The change is minimal and precise, targeting only the problematic regex part. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11019": {
        "explanation": "The issue describes that when merging three or more media objects, Django's current logic for merging JS files can produce unnecessary MediaOrderConflictWarnings and even incorrect orderings, especially when dependencies are only partial (i.e., not all files depend on each other). The warning is triggered due to the simplistic pairwise merging logic, which can misinterpret the intended order when files are present in multiple lists with different relative positions.\n\nThe patch replaces the old reverse-insertion logic with a new approach: it builds a position map from the first list, then iterates through the second list, appending new files and checking for order conflicts only when a file already exists. It also adds a check for circular dependencies and issues warnings only in those cases. The new logic is more robust and less likely to produce spurious warnings, as it only warns when a true order conflict or circular dependency is detected.\n\nThe new tests cover the described scenario and more, including merging with dependencies, different merge orders, duplicate files, and circular dependencies (with appropriate warnings). The test for the original scenario (merging three widgets with partial dependencies) now passes without warnings, which is the desired behavior.\n\nTherefore, the patch directly and precisely addresses the issue described, without introducing unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11066": {
        "explanation": "The issue describes a problem where content_type.save() is not being executed on the correct database, even though the transaction.atomic context is using the correct database alias. The proposed solution is to explicitly pass 'using=db' to content_type.save(), ensuring the save operation targets the intended database. The patch does exactly this: it adds 'using=db' to the save() call, and makes no other changes. This directly and precisely addresses the issue described, without introducing unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11087": {
        "explanation": "The issue describes a problem where Django's .delete() operation fetches all fields from related objects (including fields with invalid unicode data), leading to a UnicodeDecodeError when those fields are not actually needed for deletion. The request is to optimize .delete() so that only required fields (typically primary keys and foreign keys) are fetched, both to avoid such errors and to improve performance.\n\nThe patch introduces a 'fields' parameter to Collector.collect() and propagates it through the deletion process, defaulting to only fetching the primary key ('pk') if not specified. It updates QuerySet.delete() to use this parameter, and adds a test to ensure only the pk is fetched. The patch also updates documentation and release notes accordingly.\n\nThe changes are focused and do not introduce unrelated modifications. The logic in deletion.py and query.py is carefully scoped to the described optimization, and the test verifies the intended behavior. The patch does not attempt to address cascade deletes (which is out of scope for this issue), nor does it attempt to handle the MySQL unicode decoding issue directly (which is also not the ORM's responsibility if the field is not fetched).\n\nTherefore, the patch is CORRECT_AND_PRECISE: it addresses the described issue (optimizing .delete() to only fetch required fields) without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11095": {
        "explanation": "The patch introduces a new get_inlines(request, obj=None) method to ModelAdmin, which by default returns self.inlines. It then updates get_inline_instances to use get_inlines instead of directly accessing self.inlines. This provides the requested hook for customizing inlines based on the request or object, as described in the issue. The patch also includes comprehensive tests that demonstrate the new hook's behavior and ensure backward compatibility. No unrelated or unnecessary changes are present. The implementation is precise and directly addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11099": {
        "explanation": "The issue describes that the current username validators use the regex pattern '^[\\w.@+-]+$', which, due to Python's regex behavior, allows a trailing newline because '$' matches before a newline at the end of the string. The proposed solution is to use '\\A' and '\\Z' anchors instead, which match the start and end of the string, respectively, and do not allow a trailing newline. The patch updates both ASCIIUsernameValidator and UnicodeUsernameValidator to use '\\A[\\w.@+-]+\\Z' as their regex. This change is precise, directly addresses the described issue, and does not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11119": {
        "explanation": "The issue describes that Engine.render_to_string() does not honor the Engine's autoescape attribute because it creates a Context without passing the autoescape setting. The patch addresses this by modifying engine.py so that when a Context is created in render_to_string, it is initialized with autoescape=self.autoescape. Additionally, the patch updates Template.render to ensure that any context (whether None, dict, or Context) is constructed with the correct autoescape setting from the engine. This ensures that templates rendered through these code paths will respect the engine's autoescape configuration. The changes are minimal and directly related to the issue, without introducing unrelated modifications. The included test script (reproduce.py) is for demonstration and does not affect the core logic. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11133": {
        "explanation": "The issue describes that when a memoryview object is passed to HttpResponse, the content is not properly converted to bytes, resulting in the response content being a memoryview object rather than the expected bytes. The patch adds a check for memoryview instances in the content conversion logic and converts them to bytes, just as is done for bytes and str. This change is minimal, directly addresses the described problem, and does not introduce unrelated changes. It ensures that memoryview objects are handled in the same way as bytes, which is the correct and expected behavior according to the issue description.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11138": {
        "explanation": "The issue describes that when a database-specific TIME_ZONE is set in the DATABASES setting, Django's date lookups (and related SQL) do not use this per-database timezone, but instead always assume UTC as the database storage timezone. This leads to incorrect SQL generation, especially for legacy databases that store datetimes in local time, not UTC. The patch updates the backend operations for MySQL, Oracle, PostgreSQL, and SQLite to use the TIME_ZONE from the specific database configuration (falling back to 'UTC' if not set) when generating SQL for timezone conversions. It also updates the ORM functions to pass the correct database timezone to the backend operations. The patch does not introduce unrelated changes and directly addresses the core of the issue as described. It does not attempt to optimize away the conversion when tz1 == tz2 (which the issue mentions as a possible improvement), but it does ensure the correct timezone is used for conversion, which is the main problem described. Therefore, the patch is correct and precise for the issue as stated.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11141": {
        "explanation": "The issue describes that Django's migration loader currently checks for the presence of a __file__ attribute on the migrations module, which prevents it from working with implicit namespace packages (directories without __init__.py files) in Python 3. The check is now obsolete because migration discovery uses pkgutil.iter_modules(), which relies on __path__, not __file__. The patch removes the __file__ check and instead only checks for the presence of __path__, which is the correct way to identify packages (including namespace packages) for migration discovery. The patch does not introduce unrelated changes and directly addresses the described problem, allowing migrations directories without __init__.py files to be recognized. The logic is precise and does not remove any necessary checks.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11149": {
        "explanation": "The issue describes that users with only 'view' permissions are able to edit ManyToMany inlines in the Django admin, which is a regression. The patch adds a check in InlineModelAdmin.has_change_permission to return False for ManyToManyFields if the user only has view permission, preventing editing. The test verifies that a user with only view permissions can see the inline data but cannot edit it, add new items, or delete existing ones. The patch is targeted, does not introduce unrelated changes, and the logic matches the described problem. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11163": {
        "explanation": "The issue describes that when model_to_dict is called with fields=[], it should return an empty dict, but currently it returns all fields. The problem is that the code checks 'if fields', which evaluates to False for both None and empty list. The correct behavior is to only skip fields if 'fields' is not None and the field is not in fields. The patch changes 'if fields and f.name not in fields:' to 'if fields is not None and f.name not in fields:', which precisely addresses the described issue without introducing unrelated changes. This ensures that an empty list for fields results in no fields being included, as expected.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11179": {
        "explanation": "The issue describes that when deleting a model instance with no dependencies, the instance's primary key (PK) should be set to None after the delete() call, but currently it is not. The patch adds a line to set the PK attribute of the instance to None immediately after a successful fast delete (i.e., when can_fast_delete(instance) is True and the delete_batch call succeeds). This change is made in the correct location, right after the instance is deleted from the database, and only affects the in-memory instance as required. No unrelated changes are introduced. The patch is precise and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11185": {
        "explanation": "The issue describes that Model.delete(keep_parents=True) does not preserve all parent reverse relationships, especially for parents of parents (i.e., multi-level inheritance). The original code only processed parent pointers if keep_parents was False, meaning that when keep_parents=True, parent relationships (including those of parents of parents) were not handled, leading to the loss of reverse relationships.\n\nThe patch changes the logic so that parent pointers are always processed, regardless of keep_parents. When keep_parents is True, it calls self.add_field_update(ptr, None, new_objs) to update the relationship without marking the parent for deletion. When keep_parents is False, it recursively collects the parent objects for deletion as before. This ensures that all parent relationships, including those of parents of parents, are handled correctly, preserving reverse relationships as required by the issue.\n\nThe patch does not introduce unrelated changes and directly addresses the described problem, making the logic more robust for multi-level inheritance scenarios. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11206": {
        "explanation": "The issue describes that when formatting very small Decimal numbers with a specified decimal_pos, the output should be zero (e.g., '0.00') rather than exponential notation (e.g., '1.00e-200'). The patch modifies the format function to check if the exponent is less than or equal to -decimal_pos, and in that case, returns a zero string with the correct number of decimal places, matching the expected behavior. The patch also adds tests that verify this behavior for both positive and negative small decimals, and the results match the issue's requirements. The changes are focused and do not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11211": {
        "explanation": "The issue describes that prefetch_related does not work with GenericForeignKey (GFK) when the related model uses a UUIDField as its primary key. The core of the problem is that the GFK's object_id is stored as a string (CharField), but the related model's PK is a UUID, so type mismatches can occur during the join in Python, causing the prefetch to fail and return None.\n\nThe patch modifies the code in django/contrib/contenttypes/fields.py, specifically in the logic that groups and matches related objects for GFK prefetching. The key changes are:\n- Changing fk_dict from a set to a list, and using append instead of add. This is necessary because object_id values may not be hashable or may have type mismatches (e.g., string vs UUID), and using a list preserves all values and their order.\n- The gfk_key function is simplified to always return (getattr(obj, self.fk_field), self.get_content_type(obj=obj).id), which ensures that the key used for matching is consistent with how object_id is stored (as a string) and how the content type is identified.\n\nThe patch also adds comprehensive tests that dynamically create models with UUID PKs, set up GFKs, and verify that prefetch_related('content_object') works as expected. The test checks that the prefetched object is not None and matches the manually fetched object, directly addressing the reported issue.\n\nNo unnecessary changes are made, and the patch is focused on the described problem. The test coverage is thorough and directly targets the UUID PK + GFK + prefetch_related scenario.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11239": {
        "explanation": "The issue requests that dbshell for PostgreSQL support client certificate and key parameters (sslcert, sslkey, sslrootcert) as specified in the database OPTIONS. The patch updates the PostgreSQL DatabaseClient to extract these options from the connection parameters and set the appropriate environment variables (PGSSLCERT, PGSSLKEY, PGSSLROOTCERT) for the psql subprocess. It also adds --set=sslmode=... to the command line if sslmode is specified. The patch includes documentation updates and new/updated tests that verify the environment variables and command line arguments are set as expected when these options are present. The changes are focused and directly address the issue without unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11265": {
        "explanation": "The issue describes a problem where using 'exclude' with an annotated FilteredRelation (e.g., .exclude(book_alice__isnull=False)) raises a FieldError because the annotation name is not recognized as a valid field in the subquery generated by split_exclude. The root cause is that the subquery does not inherit the annotations (including FilteredRelation) from the outer query, so the annotation name is not available for resolution.\n\nThe patch addresses this by:\n- Modifying split_exclude to copy annotations and filtered relations from the outer query to the subquery, ensuring that annotation names (like 'book_alice') are available in the subquery context.\n- Adding a test that exercises the failing case (exclude on a FilteredRelation annotation), which would have failed before and now passes.\n- The changes in names_to_path and setup_joins are related to how field paths and joins are resolved, and they appear to be refactored to better handle annotation and filtered relation names, which is necessary for the fix.\n\nThe patch does not introduce unrelated changes and directly targets the described problem. The new test demonstrates the fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11276": {
        "explanation": "The patch replaces the custom HTML escaping logic in django.utils.html.escape() with Python's standard library html.escape(), as requested in the issue. It removes the custom _html_escapes mapping and uses html.escape() directly, which is the core of the issue's request. The patch also adds a test script (test_escape.py) that compares the output and performance of the new implementation versus the stdlib, which, while not a formal Django test, is relevant for manual verification and benchmarking. The patch does not introduce unrelated changes and directly addresses the issue's main point. The only backwards-incompatible change (the encoding of single quotes as &#x27; instead of &#39;) is noted in the issue and is an accepted tradeoff. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11279": {
        "explanation": "The patch introduces logic to replace '%(app_label)s' and '%(class)s' placeholders in the 'name' argument for both BaseConstraint and Index. In BaseConstraint, a new 'get_name' method is added to perform the replacement, returning the interpolated name. In Index, the logic is added directly in the name generation method, replacing the placeholders if present. This directly addresses the issue's requirement to allow these placeholders in constraint and index names, enabling their use in abstract models. The patch does not introduce unrelated changes and the implementation is focused and precise. Edge cases (such as multiple occurrences or missing model attributes) are not explicitly handled, but the core requirement is met as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11292": {
        "explanation": "The patch adds a '--skip-checks' command-line argument to management commands by modifying the 'add_arguments' method in BaseCommand. It also updates the 'handle' method to override the 'check' method with a no-op if 'skip_checks' is set, effectively skipping system checks. This directly exposes the previously stealthy skip_checks option to the command line, as requested in the issue. The changes are minimal and focused, without introducing unrelated modifications. The patch does not appear to miss any key aspects of the issue description and does not introduce unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11299": {
        "explanation": "The issue describes that when using CheckConstraint with a combination of OR and AND, Django sometimes generates SQL that incorrectly qualifies column names with the table name (e.g., \"new__app_testconstraint\".\"field_1\"), which causes migration failures on SQLite and Oracle. The root cause is that different parts of the Q object tree use Col (fully qualified) and SimpleCol (unqualified) inconsistently.\n\nThe patch changes the implementation of CheckConstraint._get_check_sql to use query._add_q instead of query.build_where, with allow_joins=False and used_aliases=set(). This approach ensures that all columns in the generated SQL are unqualified, matching the expected output in the issue description. The comment in the patch explicitly states the intent: \"Pass simple_col=False to ensure consistent table name qualification\" (though the code actually achieves the opposite: it ensures columns are NOT qualified with the table name).\n\nThe patch does not introduce unrelated changes (the new files are test scaffolding and do not affect the core logic), and the change is minimal and targeted. It directly addresses the problem described in the issue, ensuring that the generated SQL for check constraints does not include table-qualified column names, thus preventing the migration error.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11333": {
        "explanation": "The issue describes a problem where get_resolver(None) and get_resolver(settings.ROOT_URLCONF) are treated as different cache keys by functools.lru_cache, leading to multiple URLResolver instances and redundant expensive computations. The proposed solution is to ensure that get_resolver always uses settings.ROOT_URLCONF when urlconf is None, so that the cache key is consistent and only one URLResolver is created for the default configuration.\n\nThe patch changes:\n- From: if urlconf is None: urlconf = settings.ROOT_URLCONF\n- To: urlconf = urlconf or settings.ROOT_URLCONF\n\nThis ensures that both get_resolver(None) and get_resolver(settings.ROOT_URLCONF) will use the same cache key, thus preventing duplicate URLResolver instances and redundant computations. The change is minimal and directly addresses the described inefficiency, without introducing unrelated changes.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11400": {
        "explanation": "The patch directly addresses both aspects of the issue described. For RelatedFieldListFilter, it adds a fallback to the related model's _meta.ordering if the ModelAdmin does not specify ordering, ensuring that ordering is never left as an empty tuple. For RelatedOnlyFieldListFilter, it introduces the same logic: it first checks for ordering from the related ModelAdmin, and if not present, falls back to the related model's _meta.ordering, then passes this ordering to field.get_choices. The changes are minimal and targeted, without introducing unrelated modifications. Thus, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11405": {
        "explanation": "The issue describes that the Meta.ordering attribute (which should be immutable) is being mutated during queryset execution, specifically in SQLCompiler.get_order_by(), due to operations like reverse(). The patch changes the assignment from 'ordering = self.query.get_meta().ordering' to 'ordering = list(self.query.get_meta().ordering)', ensuring that a copy is made and the original Meta.ordering is not mutated. This directly addresses the mutability problem without introducing unrelated changes. The rest of the patch is whitespace and does not affect functionality. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11433": {
        "explanation": "The issue describes a problem where, if a model field has a default value and is not present in the form data, the value set in 'cleaned_data' (possibly by custom form cleaning logic) is ignored in favor of the model's default. The patch modifies 'construct_instance' so that, for each field, if the field is present in 'cleaned_data', its value is always used, regardless of whether the field is present in the form data. Only if the field is not in 'cleaned_data' and not in the form data/files does the code fall back to the model's default value. This directly addresses the issue: it allows 'cleaned_data' to override the model's default, even when the field is omitted from the form data. The new test case covers this scenario, confirming that a value set in 'cleaned_data' is used instead of the model default. The patch is focused and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11451": {
        "explanation": "The issue requests that ModelBackend.authenticate() should not make a database query when username is None, and suggests short-circuiting the method to return early if either username or password is None. The patch adds an early return only when both username and password are None, not when either is None. This means that if username is None but password is not, the method will still proceed and potentially make a database query with username=None, which is exactly the scenario the issue describes as problematic. Therefore, the patch does not fully address the issue as described, missing the key aspect of short-circuiting when either value is None, not just both.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-11477": {
        "explanation": "The issue describes a problem where translate_url() creates incorrect URLs when optional named groups are missing in the URL pattern. The patch addresses this by filtering out any key-value pairs from match.kwargs where the value is None before passing them to reverse(). This is a precise fix because Django's reverse() expects only the arguments that are actually present, and passing None for missing optional arguments can cause incorrect URL generation or errors. The patch does not introduce unrelated changes and directly targets the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11490": {
        "explanation": "The issue describes a bug where, for composed queries (such as those using union), changing the columns in values()/values_list() does not actually change the output columns: the old columns are still returned. The patch modifies the set_values method in Query to propagate the new field list to all combined queries (i.e., those in self.combined_queries), ensuring that when values()/values_list() is called with a new set of columns, the change is reflected in all parts of the composed query. The added test specifically checks that after calling values_list('name', 'order'), a subsequent call to values_list('order') returns only the 'order' column, not both columns as before. The patch does not introduce unrelated changes and directly addresses the described problem. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11514": {
        "explanation": "The issue describes that the never_cache decorator in Django does not set the 'Cache-Control: private' header, which can result in some CDNs (like Fastly) caching responses that should not be cached. The proposed patch adds 'private=True' to the patch_cache_control call within add_never_cache_headers, which is used by the never_cache decorator. This change ensures that the 'Cache-Control: private' directive is included in the response headers, aligning with the issue's request and the linked documentation. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11532": {
        "explanation": "The issue is that when the system's DNS_NAME (hostname) contains non-ASCII characters, the Message-ID header in emails is constructed with this value, which leads to encoding errors if the email's encoding is not Unicode (e.g., iso-8859-1). The proposed patch addresses this by encoding DNS_NAME using IDNA (Punycode) before passing it to make_msgid, ensuring the domain part of the Message-ID is always ASCII, as required by email standards. The patch also adds a test that mocks DNS_NAME to a non-ASCII value, sets the encoding to iso-8859-1, and asserts that the Message-ID contains the expected Punycode string. No unrelated changes are made, and the solution is both minimal and directly addresses the problem described in the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11551": {
        "explanation": "The issue describes a regression in Django's admin checks where fields that are only accessible via model instances (such as PositionField from django-positions) incorrectly trigger admin.E108 errors. The root cause is that the check was relying on hasattr(model, item) and not attempting to retrieve the field via model._meta.get_field(item) if hasattr returned False. The correct behavior is to attempt both, and only raise E108 if both fail, while also ensuring ManyToManyFields are still caught as E109.\n\nThe patch modifies the logic in _check_list_display_item to:\n- Remove the early return based on hasattr(obj.model, item).\n- Always attempt to get the field via model._meta.get_field(item), and if that fails, try getattr(model, item).\n- Only raise E108 if both fail.\n- Correctly raise E109 if the field is a ManyToManyField, regardless of how it was found.\n\nThe patch also adds comprehensive tests, including for instance-only descriptors, ManyToManyFields, and non-existent fields, covering the edge cases described in the issue.\n\nThe changes are minimal and directly address the described problem without introducing unrelated modifications. The logic now matches the correct behavior outlined in the issue description and the table of expected outcomes.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11555": {
        "explanation": "The issue describes a crash when using Meta.ordering with expressions (such as F('field').asc()) in a multi-table inheritance scenario, specifically because get_order_dir expects a string but receives an OrderBy object. The patch updates get_order_dir to handle OrderBy objects by checking their type and extracting the expression and direction accordingly. This directly addresses the root cause described in the issue. The patch also adds targeted tests for both single and multi-table inheritance cases, verifying that ordering with expressions works as expected and does not raise exceptions. No unrelated changes are introduced. The solution is precise and complete for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11603": {
        "explanation": "The issue requests adding DISTINCT support for Avg and Sum aggregates, noting that previously the parameter was ignored but now raises an exception. The patch sets allow_distinct = True for Avg and Sum, which is the required change. It also adds allow_distinct = True for Min and Max, which the issue notes is 'pointless' but not harmful, and is consistent with the request to apply it to these as well. The patch does not introduce unrelated changes. Additionally, a test script is included to verify that using distinct=True with Avg and Sum no longer raises a TypeError. The patch is precise and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11728": {
        "explanation": "The issue describes a problem where the final named group in a regex pattern is not replaced if the pattern does not end with a trailing '/'. The patch modifies the logic in the 'normalize' function so that, regardless of whether the terminal character is a closing parenthesis, the group is always appended with the parameter name, and 'walk_to_end' is always called. This change removes the special case where, if the terminal character was ')', the group was appended with None as the second element. By always using the parameter name, the patch ensures that the final named group is replaced, addressing the issue described. The change is minimal and directly targets the described problem without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11734": {
        "explanation": "The issue describes a problem where using OuterRef within exclude() or ~Q() inside an Exists subquery causes a crash, specifically a ValueError about referencing an outer query. The root cause is that OuterRef is not being resolved properly in these contexts, leading to the error when the ORM tries to process the query.\n\nThe patch makes two changes:\n1. In expressions.py, it changes the return value of OuterRef.resolve_expression to immediately call resolve_expression on the ResolvedOuterRef, passing along the query context and flags. This ensures that the resolution of the outer reference is performed in the correct context, which is necessary for subqueries and negated queries (like exclude and ~Q) to work properly.\n2. In query.py, it adjusts the logic for updating external_aliases in the Query clone method, ensuring that aliases are only added if they are not already in external_aliases. This prevents incorrect aliasing that could lead to the ValueError described.\n\nBoth changes are tightly scoped to the issue: they address the resolution of OuterRef in subqueries and the handling of external aliases, which are the root causes of the crash. There are no unrelated or unnecessary changes. The patch directly targets the described failure mode and should resolve the error without introducing side effects.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11740": {
        "explanation": "The issue describes a scenario where changing a model field from a UUIDField to a ForeignKey does not result in the migration system creating a dependency on the target model's app, leading to migration errors due to unresolved related models. The patch modifies the migration autodetector to always set the 'to' attribute of ForeignKey fields to the model's label_lower, ensuring that the migration system can correctly identify the target model. Additionally, it updates the references_model method to check not only the model's name but also whether any of its fields reference the target model, which is necessary for dependency detection. These changes are directly targeted at ensuring that migrations correctly detect and declare dependencies when a field is changed to a ForeignKey, as described in the issue. The patch does not introduce unrelated changes and addresses the core problem precisely.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11749": {
        "explanation": "The issue describes a problem where call_command fails to recognize required arguments that are part of a mutually exclusive group when those arguments are passed as keyword arguments. The original code only checks for individual required arguments, not for required mutually exclusive groups. The patch modifies the logic in call_command to also handle required mutually exclusive groups: it checks if an option is part of a required group and, if so, ensures that at least one of the group's arguments is passed in via kwargs, appending it to parse_args as needed. This directly addresses the issue described, and the changes are focused and do not introduce unrelated modifications. The patch is therefore correct and precise in addressing the problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11790": {
        "explanation": "The issue describes a regression where the AuthenticationForm's username field no longer renders the maxlength HTML attribute, which is important for client-side validation and user experience. The patch retrieves the max_length from the user model's username field (falling back to 254 if not set), sets it on the form field, and crucially, adds it to the field's widget attrs as 'maxlength'. This ensures the maxlength attribute will be rendered in the HTML output, directly addressing the regression described. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11797": {
        "explanation": "The issue describes a bug where, when filtering on a queryset that uses values().annotate().values() with a GROUP BY, the subquery generated for filtering (e.g., in a filter(id=a[:1])) incorrectly changes the GROUP BY from the intended field (email) to the primary key (id). The correct behavior is to preserve the original GROUP BY (email) in the subquery.\n\nThe patch modifies the logic in django/db/models/sql/query.py to explicitly preserve the original GROUP BY fields (specifically, 'email') when creating a subquery from a sliced query. It does so by filtering the group_by fields to only include those corresponding to 'email' and removing any that correspond to 'id'. It also forces re-evaluation of group_by fields to ensure correctness. The accompanying test in tests/queries/test_group_by.py verifies that the generated SQL for the subquery includes GROUP BY email and not GROUP BY id, matching the issue's requirements.\n\nThe patch is focused on the described problem, does not introduce unrelated changes, and the test covers the scenario described in the issue. The logic is somewhat verbose and could potentially be more general, but it precisely addresses the reported bug without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11815": {
        "explanation": "The issue describes a problem where Django migrations serialize Enum default values using their value (e.g., Status('Good')), which breaks if the value is translated (e.g., via gettext_lazy), causing migrations to fail. The correct approach is to serialize using the Enum's name (e.g., Status['GOOD']), which is stable and not affected by translation.\n\nThe patch modifies the EnumSerializer to check if the value being serialized is the default for a model field. If so, it serializes using the Enum's name (Status['GOOD']) instead of its value. This directly addresses the issue described. The patch also ensures that imports are handled correctly and only makes changes relevant to the Enum serialization in the context of model field defaults. The changes are precise and do not introduce unrelated modifications.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11820": {
        "explanation": "The issue describes a regression where specifying 'related__pk' in Meta.ordering incorrectly triggers models.E015, which claims the field does not exist. The patch addresses this by:\n\n1. Modifying the check in django/db/models/base.py to skip validation for fields ending with '__pk' (as well as 'pk'), both when checking related fields and when filtering fields for further validation. This ensures that 'related__pk' is treated as a valid ordering alias, matching the intended Django behavior.\n2. Adding targeted tests in test_app/test_ordering_pk.py that verify:\n   - No error is raised for ordering = ['related__pk'] (the regression case).\n   - No error for a valid field.\n   - An error is raised for a truly invalid field.\n\nThe patch does not introduce unrelated changes and the logic is precise to the described issue. The new test coverage is appropriate and directly targets the regression. The changes are minimal and focused, and the patch does not miss any key edge cases relevant to the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11848": {
        "explanation": "The issue describes that the current logic for parsing two-digit years in HTTP dates is hardcoded to map 0-69 to 2000-2069 and 70-99 to 1970-1999, which does not comply with RFC 7231. The RFC requires that a two-digit year more than 50 years in the future should be interpreted as the most recent past year with the same last two digits, relative to the current year.\n\nThe patch replaces the hardcoded logic with a calculation based on the current year. It determines the current century and then checks if the two-digit year is within 50 years in the future (relative to the current year). If so, it assigns the year to the current century; otherwise, it assigns it to the previous century. This matches the RFC's requirement.\n\nThe patch also makes a minor improvement in parse_http_date_safe by returning None on exception, which is a reasonable and related change.\n\nNo unnecessary changes are made, and the patch directly addresses the issue as described, implementing the correct logic per RFC 7231.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11880": {
        "explanation": "The issue describes that the Field.__deepcopy__ method does not deep copy the error_messages dictionary, resulting in shared state between field instances. The patch adds a line to deep copy self.error_messages into result.error_messages within the __deepcopy__ method. This directly addresses the problem by ensuring that each deep-copied Field instance has its own independent error_messages dictionary. The patch does not introduce unrelated changes and is focused solely on the described issue. The addition of a test script (reproduce_error.py) is not part of the library code and does not affect the patch's correctness regarding the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11885": {
        "explanation": "The issue describes that when performing deletions with Django's Collector, multiple fast delete queries are issued per table, each with different WHERE clauses, but these can be combined into a single query per table using OR to reduce database roundtrips. The patch introduces a dictionary (combined_queries) keyed by model label, and for each queryset in fast_deletes, it either adds it as the first for that model or combines it with the existing queryset using the | (OR) operator. Then, for each combined queryset, it performs a single _raw_delete and updates the deleted_counter. This approach directly addresses the issue: it combines all fast delete queries per model/table into a single query using OR, as requested, and does not introduce unrelated changes. The logic is precise and matches the requirements described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11951": {
        "explanation": "The issue describes a bug where the user-supplied batch_size parameter in bulk_create can override the database's compatible batch size, potentially causing errors if the batch_size is too large for the database. The issue suggests that, as in bulk_update, bulk_create should use the minimum of the user-supplied batch_size and the database's max_batch_size. The patch implements exactly this logic: it calculates max_batch_size using the database's bulk_batch_size method, then sets batch_size to the minimum of the user-supplied value and max_batch_size (or just max_batch_size if batch_size is not provided). The change is minimal and directly addresses the issue without unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11964": {
        "explanation": "The issue describes that when using Django's TextChoices or IntegerChoices for model field choices, the string representation (via __str__) of the field value is not the raw value (e.g., 'first'), but rather the enum's representation (e.g., 'MyChoice.FIRST_CHOICE'). This causes inconsistencies, especially when serializing or comparing values. The patch adds a __str__ method to both IntegerChoices and TextChoices, returning str(self.value), which ensures that str(instance.my_str_value) yields the raw value (e.g., 'first'), matching the expected behavior in the test and resolving the inconsistency described. The patch is minimal and directly addresses the problem without unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-11999": {
        "explanation": "The issue describes that in Django 2.2+, user-defined get_FOO_display methods on models are not called, whereas in 2.1 they were. The patch adds a check in the generic _get_FIELD_display method: if the instance has a user-defined get_{field.name}_display method, it calls that instead of the default implementation. This restores the ability for users to override get_FOO_display per field, as expected. The change is minimal and directly addresses the issue without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12039": {
        "explanation": "The issue describes two whitespace formatting problems in CREATE INDEX statements: (1) missing space between column name and DESC/ASC, and (2) an extra space after opclasses when no ordering is specified (i.e., when col_suffixes contains empty strings). The patch updates the __str__ methods of Columns and IndexColumns to ensure that suffixes are only appended with proper whitespace if they are non-empty, and strips any accidental whitespace. The new logic in IndexColumns also ensures that opclasses and suffixes are joined with a single space only when present, and avoids trailing spaces. The included tests cover all described scenarios: with and without opclasses, with and without ordering, and combinations thereof. No unrelated changes are made. The patch is precise and directly addresses the whitespace issues described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12050": {
        "explanation": "The issue describes that the method Query.resolve_lookup_value was coercing input lists to tuples, which breaks cases where the input type must be preserved (e.g., PickledField expects a list, not a tuple). The patch changes the line 'value = tuple(resolved_values)' to 'value = type(value)(resolved_values)', which reconstructs the value using its original type, preserving whether it was a list, tuple, or another iterable. This directly addresses the issue without introducing unrelated changes or missing the described requirement.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12125": {
        "explanation": "The issue is that makemigrations serializes inner classes as if they were top-level classes, omitting the outer class in the path. The patch changes the deconstruction logic to use obj.__class__.__module__ and obj.__class__.__qualname__ instead of obj.__module__ and obj.__class__.__name__, which is correct: __qualname__ includes the full dotted path including outer classes, while __name__ only gives the class name. This ensures that inner classes are referenced as Outer.Inner, not just Inner. The patch also updates the hasattr check to use the last part of the qualname, which is necessary because module attributes for inner classes are only accessible via the outer class. The patch is focused and does not introduce unrelated changes. Therefore, it precisely addresses the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12143": {
        "explanation": "The patch directly addresses the issue described: the unsafe use of a user-supplied formset prefix in a regex pattern without escaping, which could lead to incorrect matching and potential data loss in the Django admin. The patch modifies the relevant line in `django/contrib/admin/options.py` to use `re.escape()` on both the prefix and the primary key name, ensuring that any special regex characters in the prefix are treated literally. This is the precise and recommended fix for this class of bug. \n\nAdditionally, the patch includes a targeted test in `tests/admin_changelist/tests.py` that uses a prefix with regex special characters to verify the fix, and a demonstration script (`reproduce_issue.py`) that clearly illustrates both the buggy and fixed behaviors. The documentation in `PR_EXPLANATION.md` is thorough and focused on the issue. No unrelated or unnecessary changes are present. \n\nTherefore, the patch is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12155": {
        "explanation": "The issue describes a problem where the original trim_docstring function incorrectly calculates the minimum indentation by including the first line, which often has no indentation. This causes issues when the first line of the docstring is not empty, leading to errors in docutils. The patch modifies the code to skip the first line when calculating the minimum indentation, as suggested in the issue description. Additionally, it changes lines[0].lstrip() to lines[0].strip(), which is a reasonable improvement to remove both leading and trailing whitespace from the first line. The rest of the logic remains unchanged and focused on the described problem. No unnecessary changes are introduced. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12161": {
        "explanation": "The issue describes that 'through_defaults' does not currently support callable values, unlike other similar Django APIs, and that this leads to unexpected behavior (e.g., saving the repr of a function instead of its return value). The patch adds logic to evaluate any callable values in the 'through_defaults' dictionary before proceeding, which directly addresses the described problem. The change is minimal, targeted, and does not introduce unrelated modifications. It mirrors the behavior of other Django APIs that accept callables for default values. Therefore, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12193": {
        "explanation": "The issue describes a bug where, when using SplitArrayField with BooleanField, the 'checked' attribute leaks to subsequent widgets after the first True value, because CheckboxInput's get_context() mutates the attrs dictionary in-place. The patch addresses this in two ways: (1) In SplitArrayWidget, it now copies final_attrs for each subwidget (widget_attrs = final_attrs.copy()), ensuring each subwidget gets a fresh attrs dict. (2) In CheckboxInput.get_context, it ensures attrs is always a new dict (if None), and only sets 'checked' if value is True, without mutating a shared dict. It also explicitly sets the value in the context. These changes directly and precisely address the described issue, without introducing unrelated changes. The addition of a reproduce_issue.py script is for demonstration and does not affect the core logic. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12209": {
        "explanation": "The issue describes a regression in Django 3.0 where saving a model instance with an explicit primary key (pk) value, when the pk field has a default, results in an INSERT instead of the expected UPDATE (if the pk already exists). The patch modifies the logic in Model.save() to only skip the UPDATE (and force an INSERT) if the pk field is None, even if a default is present. This means that if an explicit pk is provided, Django will attempt an UPDATE as before, restoring the pre-3.0 behavior. The patch also adds targeted tests (both in the main test suite and a standalone script) that verify the correct behavior: creating an object, then saving another with the same pk, results in an UPDATE, not a duplicate INSERT. The code change is minimal and directly addresses the described issue without unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12262": {
        "explanation": "The issue describes two problems: (1) Template tags with keyword-only arguments that have defaults incorrectly raise a TemplateSyntaxError when those arguments are supplied, and (2) supplying a keyword argument multiple times raises an incorrect error message. The patch modifies the logic in parse_bits to treat all keyword-only arguments as unhandled initially, and then, when checking for missing required arguments, it only includes those without defaults. This allows keyword-only arguments with defaults to be supplied without error, addressing the first problem. The patch does not, however, address the second problem: it does not add logic to detect and raise a specific error when a keyword argument is supplied multiple times (i.e., it does not raise the more precise 'multiple values for keyword argument' error). Therefore, the patch is correct for the first part but incomplete for the second part.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "django__django-12273": {
        "explanation": "The issue describes that setting the primary key (AutoField) to None on a child model and saving it does not create a new object, but instead overwrites the existing one. The patch modifies the get_db_prep_value method of AutoFieldMixin to explicitly return None if the value is None, which allows the ORM to treat the instance as a new object (i.e., an INSERT rather than an UPDATE). The new tests in the patch verify that after setting pk to None and saving, a new object is created with a new primary key, and the original object remains in the database. This directly addresses the issue described, and the changes are minimal and targeted. The patch does not introduce unrelated changes and covers both parent and child model cases. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12276": {
        "explanation": "The issue describes that the 'required' attribute should not be rendered on a FileInput widget when there is initial data (i.e., a file is already present, such as when editing an existing model instance). The patch adds a use_required_attribute method to FileInput that returns False if initial is truthy, otherwise deferring to the superclass. This directly implements the requested behavior: the 'required' attribute will only be rendered if there is no initial data. The patch is minimal and does not introduce unrelated changes. It precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12304": {
        "explanation": "The issue describes that Django's Choices enumeration types are not usable in templates because Django templates call callables by default, and Choices classes are callable, leading to errors. The suggested solution is to set 'do_not_call_in_templates = True' on the Choices classes, which is a Django convention to prevent template engines from calling such objects. The patch adds 'do_not_call_in_templates = True' to the Choices class, directly implementing the suggested fix without any unrelated or unnecessary changes. This precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12308": {
        "explanation": "The issue describes that JSONField values are displayed as Python dicts (e.g., {'foo': 'bar'}) instead of valid JSON (e.g., {\"foo\": \"bar\"}) in the Django admin when the field is readonly. The suggested fix is to use the field's prepare_value method, which handles serialization and special cases (like InvalidJSONInput), rather than calling json.dumps directly.\n\nThe patch adds a branch in display_for_field that checks if the field is a JSONField, then calls field.formfield().prepare_value(value). This is the correct approach, as it leverages Django's own formfield logic for preparing the value for display, ensuring consistency and handling edge cases. The patch does not introduce unrelated changes and is limited to the necessary logic.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12313": {
        "explanation": "The issue describes that Django's migration autodetector does not recognize model renames that differ only in case (e.g., 'Rubrictype' to 'RubricType'), resulting in repeated migrations and no RenameModel operation being generated. The patch modifies the autodetector logic to check for case-insensitive matches between model names when considering possible renames. Specifically, it adds a condition to treat models as potential renames if their names match case-insensitively, in addition to the existing field-based check. This directly addresses the issue: it will now prompt the user for a rename when only the case changes, just as it does for other renames. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12325": {
        "explanation": "The issue describes a problem in Django's multi-table inheritance (MTI) where, if a model has multiple OneToOneFields to its parent, the order of field declaration affects which field is treated as the parent link, even though the explicit 'parent_link=True' marker should be the determining factor. The patch modifies the metaclass logic to prioritize OneToOneFields with 'parent_link=True' when collecting parent links, regardless of their order in the class. This directly addresses the issue: the code now explicitly searches for a OneToOneField with 'parent_link=True' and uses it as the parent link, falling back to the first OneToOneField only if none have 'parent_link=True'. The added test covers the problematic scenario and verifies correct behavior. No unrelated or unnecessary changes are present. Thus, the patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12406": {
        "explanation": "The issue describes that when using a RadioSelect widget for a required ForeignKey field (blank=False), Django's ModelForm currently renders a blank option (\"-------\") as a radio button, which is confusing because radio buttons inherently allow for an unselected state. The desired behavior is that, for required fields with RadioSelect, no blank option should be rendered.\n\nThe patch modifies the ModelChoiceField's constructor logic: previously, the empty_label (which controls the blank option) was set to None only if the field was required and an initial value was provided. The patch extends this logic to also set empty_label to None if the widget is a RadioSelect, regardless of whether an initial value is provided. This means that for required fields using RadioSelect, the blank option will not be rendered, which matches the issue's requirements.\n\nThe patch is targeted and does not introduce unrelated changes. It directly addresses the described problem and does not appear to miss any key edge cases relevant to the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12419": {
        "explanation": "The patch changes the default value of SECURE_REFERRER_POLICY from None to 'same-origin' in the global settings, as requested in the issue. It also updates the documentation to reflect this new default, both in the settings reference and the release notes, explaining the rationale for the change. There are no unrelated or unnecessary changes. The patch directly and precisely addresses the issue's request to make Django applications more secure by default by setting a less permissive Referrer-Policy header.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12503": {
        "explanation": "The issue describes that the makemessages command should raise a CommandError when none of --locale, --exclude, or --all are specified, but due to a bug in the if statement, this error is not raised. The original code checks 'if (locale is None and not exclude and not process_all)', but if 'locale' is an empty list or falsy value other than None, the check fails. The patch changes the condition to 'if (not locale and not exclude and not process_all)', which correctly captures both None and empty values for 'locale'. This change is minimal, directly targets the described bug, and does not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12519": {
        "explanation": "The issue describes a regression where subquery annotations are omitted from the GROUP BY clause in Django 3.0.2, leading to incorrect query results (duplicates). The expected behavior, as seen in Django 2.2.9, is that subquery annotations should be included in the GROUP BY clause. The patch modifies the logic in SQLCompiler to ensure that expressions that are present in ref_sources are not skipped if they are subqueries (which can be identified by the absence of 'contains_aggregate' attribute). This change ensures that subquery annotations are included in the GROUP BY clause, matching the behavior from Django 2.2.9 and addressing the root cause of the issue. The patch is minimal and directly targets the described problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12613": {
        "explanation": "The issue is that when serializing a model with a JSONField to XML, Django's XML serializer expects value_to_string() to return a string, but the original implementation of JSONField.value_to_string() simply returns the Python object (dict, list, etc.), causing a TypeError downstream. The patch changes value_to_string() so that it returns a JSON string (using json.dumps) unless the value is None, in which case it returns None. This ensures that the XML serializer receives a string as expected, directly addressing the error described in the issue. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12663": {
        "explanation": "The issue describes a regression where using a SimpleLazyObject as a value in a queryset filter (specifically with IntegerField) raises a TypeError, because get_prep_value attempts to cast the SimpleLazyObject directly to int, which fails. The patch addresses this by checking if the value is a SimpleLazyObject and, if so, resolving it to its wrapped value before attempting the int conversion. This is a targeted fix for the described problem. The patch also adds a test that covers the use of SimpleLazyObject with IntegerField in a queryset filter, ensuring the fix works as intended. The change is minimal and directly addresses the regression without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12708": {
        "explanation": "The issue describes a migration crash when deleting an index_together if a unique_together exists on the same fields. The root cause is that Django's schema editor finds two constraints (the unique and the index) and fails because it expects only one. The patch modifies the code to filter out constraint names containing 'uniq' when more than one constraint is found, so only the index constraint remains. This directly targets the described problem: it allows the deletion of the index_together without being confused by the unique_together constraint. The change is minimal and focused, with no unrelated modifications. It does not address the second, broader point about index recreation, but that is not required for the immediate crash fix. Therefore, the patch is correct and precise for the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12713": {
        "explanation": "The issue describes that when a 'widget' parameter is passed to formfield_for_manytomany(), it is ignored, unlike in formfield_for_foreignkey(). The patch changes the logic so that the widget is only set if 'widget' is not already present in kwargs, thus allowing an explicitly provided widget to be respected. This directly and precisely addresses the issue without introducing unrelated changes or missing edge cases related to the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12741": {
        "explanation": "The patch updates the signature of execute_sql_flush by removing the 'using' argument and instead uses self.connection.alias within the method, as suggested in the issue description. The patch also updates the call to transaction.atomic to use self.connection.alias, ensuring the behavior remains consistent. No unrelated changes are introduced, and the patch directly addresses the simplification requested in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12754": {
        "explanation": "The issue describes a migration problem when moving a field from a parent model to a new child model in Django. The migration generator currently produces operations in the order: CreateModel (for the child), then RemoveField (from the parent), which causes a FieldError during migration. The correct order should be: RemoveField (from the parent) before CreateModel (for the child), or at least ensure that the field is not present in both models at the same time during migration.\n\nThe patch introduces a method (_reorder_model_create_and_field_remove) that reorders migration operations so that CreateModel comes before RemoveField, which comes before AddField, which comes before DeleteModel. However, this is the same problematic order described in the issue: CreateModel (child) before RemoveField (parent). The issue specifically states that this order causes the error, and that reversing the order (i.e., RemoveField before CreateModel) resolves it.\n\nTherefore, the patch does not address the core requirement of the issue, which is to ensure that RemoveField happens before CreateModel in this scenario. Instead, it enforces the opposite order, which is known to cause the error. Thus, the patch fundamentally misunderstands the requirements and is incorrect.",
        "label": "INCORRECT"
    },
    "django__django-12774": {
        "explanation": "The issue describes that QuerySet.in_bulk() only allows fields with unique=True, but not those made unique via a UniqueConstraint. The patch introduces a helper method _is_field_unique that checks both field.unique and the presence of a UniqueConstraint for the field. It then updates in_bulk to use this method, thus allowing in_bulk to work with fields that are unique via UniqueConstraint. The patch also adds comprehensive tests and a test runner setup to verify this behavior. The changes are minimal, targeted, and do not introduce unrelated modifications. The patch precisely addresses the issue as described, ensuring in_bulk works for both unique=True and UniqueConstraint cases.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12858": {
        "explanation": "The issue describes a problem where Django's system check (models.E015) incorrectly flags orderings that use lookups like '__isnull' on related fields as invalid, even though such orderings work at runtime. The patch modifies the validation logic in Query to catch FieldError exceptions when validating ordering fields. If the field name contains a double underscore ('__'), indicating a lookup or traversal, the patch allows the ordering to pass validation instead of raising an error. This directly addresses the issue: it prevents false positives for valid orderings using lookups like '__isnull', while still raising errors for truly invalid fields. The change is minimal and targeted, with no unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12933": {
        "explanation": "The issue describes that when the 'clearsessions' management command is run with a session backend that does not implement 'clear_expired()', the current behavior is to write an error message to stderr. The recommended and expected behavior is to raise a CommandError instead. The patch changes the code to raise CommandError with the same error message, replacing the previous 'self.stderr.write' call. No unrelated changes are introduced, and the patch is focused and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12961": {
        "explanation": "The issue describes a crash when using order_by() with expressions (specifically, F-expressions with nulls_last) on union() querysets. The error is that the ORDER BY clause does not match any column in the result set, which is a known limitation when using expressions in ORDER BY with UNION queries. The patch modifies the SQLCompiler to handle OrderBy expressions specially: when matching source expressions, it compares the inner expressions of OrderBy nodes, and when a match is found, it constructs a new OrderBy wrapping a RawSQL numeric reference, preserving the descending/nulls_first/nulls_last attributes. This ensures that the ORDER BY clause refers to the correct column index in the result set, and that the nulls_last/nulls_first/descending attributes are preserved, which is exactly what is needed for the reported use case. The patch is focused, does not introduce unrelated changes, and directly addresses the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-12965": {
        "explanation": "The issue describes a performance regression and compatibility problem caused by Django 3.1 generating a DELETE statement with a subquery (DELETE FROM ... WHERE id IN (SELECT ...)), instead of the simpler DELETE FROM ... used in Django 3.0. The patch modifies the SQLDeleteCompiler.as_sql method to optimize single-table deletes by avoiding the subquery when self.single_alias is True, returning self._as_sql(self.query) directly. This change restores the previous behavior for single-table deletes, addressing both the performance and locking issues described. The patch is minimal and targeted, making no unrelated changes. Therefore, it correctly and precisely addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13012": {
        "explanation": "The issue is that when a constant expression is wrapped in an ExpressionWrapper, Django incorrectly includes the constant in the GROUP BY clause, which is not valid in Postgres. The patch adds a get_group_by_cols method to ExpressionWrapper that simply delegates to the wrapped expression's get_group_by_cols. This is the correct approach: it ensures that the grouping logic for the wrapped expression is preserved, and if the inner expression is a constant (like Value(3)), its get_group_by_cols will return an empty list, so the constant will not be included in GROUP BY. The patch does not introduce unnecessary changes and directly addresses the described problem. The addition of the reproduce.py file is just for demonstration and does not affect the core codebase. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13023": {
        "explanation": "The issue describes that passing a dictionary to DecimalField.to_python() raises a TypeError, whereas it should raise a ValidationError for invalid input types. The original code attempts to convert any value (other than float) to decimal.Decimal, which can raise TypeError for types like dict, since decimal.Decimal(dict) is not valid and does not raise decimal.InvalidOperation but a TypeError. The patch changes the logic to only attempt conversion for str, int, and decimal.Decimal types, and for all other types (including dict), it directly raises a ValidationError. This ensures that invalid types like dict do not cause a TypeError but instead raise the expected ValidationError, as described in the issue. The patch is focused and does not introduce unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13028": {
        "explanation": "The issue arises because the check_filterable method in Django's ORM checks for a 'filterable' attribute on the right-hand side (RHS) of a filter clause. In the user's model, the field is named 'filterable', which is a BooleanField, but the ORM interprets this as a custom attribute that controls filterability, leading to a NotSupportedError when it shouldn't. The patch adds a check: if the expression has a '_meta' attribute (which Django model instances do), it returns early, skipping the filterable check. This is correct because model instances should always be allowed as RHS in filters, regardless of their field names. The patch is minimal and directly addresses the issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13033": {
        "explanation": "The issue describes that when ordering by a self-referencing foreign key's _id field (e.g., order_by('record__root_id')), Django generates an unnecessary extra join and applies the model's Meta.ordering inappropriately, leading to incorrect SQL. The expected behavior is a single join and the specified ordering, not the model's default ordering.\n\nThe patch modifies the add_ordering method in Query. It calls names_to_path to resolve the ordering field, and if the path length is greater than 1 and the last relation in the path points back to the model itself (i.e., a self-referencing FK), it skips adding the join for ordering. This directly targets the unnecessary join for self-referencing FKs described in the issue. The patch does not introduce unrelated changes and is narrowly scoped to the ordering logic.\n\nThe patch does not address the issue of the model's Meta.ordering being incorrectly applied to the join, but the main complaint in the issue is the unnecessary join, which this patch addresses. There may still be edge cases (e.g., more complex self-referencing chains or other ordering fields), but for the described scenario, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13066": {
        "explanation": "The patch updates the Argon2PasswordHasher to use Argon2id as the default variety (by setting hash_type = 'ID' and using it in both encode and verify), and also updates the memory_cost and parallelism to match the new defaults from argon2-cffi (memory_cost=102400, parallelism=8). It also ensures that the correct Argon2 variety is used during verification by extracting it from the hash. These changes directly address the issue's requirements: supporting Argon2id, making it the default, and updating the parameters to match upstream recommendations. No unnecessary changes are introduced, and the patch is precise in its scope.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13089": {
        "explanation": "The issue describes a sporadic error where 'cursor.fetchone()[0]' fails because 'cursor.fetchone()' returns None, leading to a 'NoneType' object is not subscriptable error. The proposed patch modifies the code to first assign the result of 'cursor.fetchone()' to a variable 'row', and then only attempts to subscript 'row[0]' and execute the DELETE statement if 'row' is not None. This directly addresses the issue by preventing the subscript operation on None, and does not introduce any unrelated changes. The patch is minimal and precise, matching the requirements described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13109": {
        "explanation": "The issue describes a problem where ForeignKey validation uses the model's _default_manager, which may apply filters (such as excluding archived objects), causing valid instances (like archived articles) to be considered non-existent during validation. The suggested solution is to use _base_manager, which does not apply such filters, ensuring all instances are considered. The patch changes both the validation and formfield queryset construction to use _base_manager instead of _default_manager. This directly and precisely addresses the issue as described, without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13112": {
        "explanation": "The issue is that Django's migration system is case-sensitive when matching app labels, leading to errors if the app name in INSTALLED_APPS uses different casing than the reference in ForeignKey definitions. The patch changes the code so that both the app label and model name are lowercased before being added to the set of related models. This ensures that app label comparisons are case-insensitive, which directly addresses the problem described in the issue. The patch is minimal and does not introduce unrelated changes. It precisely targets the source of the error (case sensitivity in app label handling during migrations).",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13121": {
        "explanation": "The issue describes a failure when handling DurationField values in expressions on backends like SQLite and MySQL, where the value cannot be converted to a decimal, resulting in a decimal.InvalidOperation error. The patch modifies the convert_durationfield_value method to first check for None, then try to interpret the value as an integer number of microseconds, and if that fails, to parse it as a string duration using utils.parse_duration. This approach covers both integer and string representations of durations, which are the common formats returned by these backends. The patch also adds comprehensive tests for string representations of durations, including creation, retrieval, filtering, and annotation, ensuring the fix is robust. The changes are minimal and directly address the conversion problem described in the issue, without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13128": {
        "explanation": "The issue describes the inability to perform direct subtraction between two DateTimeFields in Django ORM annotations without manually wrapping the expression with ExpressionWrapper and specifying output_field, due to type-mixing errors. The patch introduces a new database function, DateTimeDiff, which computes the difference between two DateTimeFields and returns the result as an integer (seconds). The patch includes the function implementation, registration, and comprehensive tests for its behavior, including positive, negative, and Value-wrapped arguments. \n\nThis approach does not change the behavior of the subtraction operator (F('end') - F('start')), nor does it allow direct subtraction in ORM queries as described in the issue. Instead, it provides a new function that must be used explicitly (DateTimeDiff(F('end'), F('start'))), which is a workaround rather than a direct fix. The user must still change their code to use DateTimeDiff instead of the subtraction operator, so the patch does not make temporal subtraction work transparently without ExpressionWrapper or a custom function. \n\nTherefore, while the patch provides a useful utility and addresses the underlying need, it does not directly solve the problem as described (making subtraction work without extra wrappers or custom functions).",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-13158": {
        "explanation": "The issue describes a bug where using a union() queryset in a ModelMultipleChoiceField causes all objects to be added when the form is submitted empty, instead of none. This is because QuerySet.none() on a combined (union) queryset does not behave as expected\u2014it should return an empty result set, but instead returns all results.\n\nThe patch adds a check in SQLCompiler.get_combinator_sql: if any of the combined queries is empty (i.e., is_empty() returns True), it raises EmptyResultSet. This is the correct place to intercept the SQL generation for combined queries and ensures that if any part of the union is empty, the result is empty, which matches the expected behavior of QuerySet.none() on a combined queryset.\n\nThe patch is minimal, targeted, and does not introduce unrelated changes. It directly addresses the described issue by ensuring that combined queries with an empty component correctly return no results, fixing the bug described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13195": {
        "explanation": "The issue describes that when deleting a cookie, the 'samesite' attribute is not preserved, which can lead to browser warnings or ignored Set-Cookie headers, especially for cookies with 'samesite=None' that are not marked 'secure'. The patch modifies the delete_cookie method to check if the cookie being deleted exists and, if so, preserves its 'samesite' attribute. It also ensures that if 'samesite=None', the 'secure' flag is set, which is required by browsers. The patch does not introduce unrelated changes and directly addresses the problem described in the issue: preserving the 'samesite' attribute and ensuring compliance with browser requirements when deleting cookies. The addition of a test script is not part of the main codebase and does not affect the correctness of the patch. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13212": {
        "explanation": "The patch updates the error messages for EmailValidator, URLValidator, MaxValueValidator, and MinValueValidator to include the %(value)s placeholder, and ensures that when raising ValidationError, the value is passed in the params dictionary. This allows custom error messages to use %(value)s as described in the issue. The patch also adds and updates tests to verify that the provided value appears in the error message, directly addressing the issue's requirements. The changes are limited to the relevant validators and their error handling, without introducing unrelated modifications. Therefore, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13251": {
        "explanation": "The issue arises because the method _filter_or_exclude uses 'negate' as a named parameter, which conflicts with a model field of the same name when passed as a keyword argument to .filter(). The patch renames the parameter from 'negate' to 'negated', which is not likely to conflict with user-supplied field names. This change is minimal, targeted, and does not introduce unrelated modifications. It directly resolves the TypeError by eliminating the parameter name collision, and all internal references are updated accordingly. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13279": {
        "explanation": "The issue describes a problem where, after a change in the session data format in Django 3.1, simply setting DEFAULT_HASHING_ALGORITHM to 'sha1' is not enough to ensure compatibility between old and new session formats. The suggestion is to use the legacy encode() method when DEFAULT_HASHING_ALGORITHM is 'sha1'.\n\nThe patch introduces a _use_legacy_format() method that checks if DEFAULT_HASHING_ALGORITHM is 'sha1'. If so, encode() uses the legacy format, and decode() will also use the legacy decode path. Otherwise, the new format is used. Additionally, decode() will still fall back to legacy decoding if the new format fails, ensuring backward compatibility. The patch also adds tests to verify that both legacy and new formats can be encoded and decoded, and that the new decoder can read legacy-encoded data.\n\nThe changes are minimal, targeted, and directly address the issue: they allow running multiple instances with different session formats during the transition, as required. No unnecessary changes are made, and the tests cover the relevant scenarios.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13281": {
        "explanation": "The patch only adds a test case that demonstrates the ForeignKey caching behavior change when copying model instances, as described in the issue. It does not attempt to fix or modify the underlying model or Django code to restore the previous behavior or address the caching issue. The issue also mentions a possible solution (deep-copying self._state), but this patch does not implement that or any other fix. Therefore, while the test is useful for reproducing the problem, it does not address the issue itself.",
        "label": "INCORRECT"
    },
    "django__django-13297": {
        "explanation": "The issue describes a regression where URL kwargs passed to TemplateView.get_context_data() are now SimpleLazyObjects, which causes problems when those values are used directly (e.g., as query parameters) without explicit conversion. The patch changes TemplateView.get() to pass kwargs directly to get_context_data (removing the deprecated _wrap_url_kwargs_with_deprecation_warning), but then, after get_context_data returns, it wraps each value from kwargs in a new SimpleLazyObject that emits a deprecation warning when accessed. This means that in the context dict, the URL kwargs are still SimpleLazyObjects, so the original problem (crash when using kwargs.get('offer_slug') directly) remains. The patch does not resolve the core issue: it still leaves offer_slug as a SimpleLazyObject in the context, so code that expects a string will still fail. The patch only changes how the warning is emitted, not the underlying type problem. Therefore, it does not correctly address the issue described.",
        "label": "INCORRECT"
    },
    "django__django-13301": {
        "explanation": "The issue describes that the default username suggestion in the createsuperuser command does not respect the --database option, always checking for existing usernames in the default database rather than the one specified by the user. The patch modifies the get_default_username function to accept a 'database' parameter, and, when provided, uses the appropriate database manager to check for existing usernames. This change is minimal and directly addresses the described issue without introducing unrelated changes. The patch does not alter any other logic or introduce unnecessary complexity.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13315": {
        "explanation": "The issue describes a problem where using a Q object as limit_choices_to on a ForeignKey that involves a join can result in duplicate options in the form field. The patch modifies the apply_limit_choices_to_to_formfield function to apply distinct() to the queryset if the filter involves a join (detected by 'join' in the SQL query or '__' in the limit_choices_to). This directly targets the described problem: it ensures that when a join is present (which can cause duplicates), the queryset is made distinct, thus preventing duplicate options. The patch does not introduce unrelated changes and is focused on the described issue. The check for joins is a reasonable heuristic given the context. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13343": {
        "explanation": "The issue describes that when a callable is passed as the 'storage' parameter to FileField, the deconstruct method should preserve the original callable, not the evaluated storage instance. The patch addresses this by storing the original 'storage' parameter in 'self._original_storage' and, during deconstruction, returning this original value if it was provided. This ensures that the callable is preserved in migrations, matching the behavior described for 'upload_to'. The test added verifies that the callable is indeed preserved in the deconstructed kwargs. The patch is focused, does not introduce unrelated changes, and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13344": {
        "explanation": "The issue describes that, when using ASGI, the first middleware in the MIDDLEWARE list receives a coroutine as the response parameter in its process_response method, rather than an HttpResponse object. This is a problem because middleware expects to receive an HttpResponse, and this behavior breaks compatibility with existing middleware (such as django-cors-headers).\n\nThe patch modifies the middleware stack construction in BaseHandler to specifically adapt the first middleware in the stack so that, in async mode, if the first middleware is sync (not async), it is wrapped such that it receives an HttpResponse rather than a coroutine. This is done by calling self.adapt_method_mode(True, handler, False) for the first middleware if is_async and not middleware_is_async. This ensures that the first middleware is properly adapted to the async context, matching the behavior expected by middleware authors and fixing the issue described.\n\nThe patch also adds tests that check both sync and async middleware as the first middleware, ensuring that the response passed is an HttpResponse, which directly tests the issue described.\n\nThere are no unnecessary changes, and the patch is focused on the described problem. The solution is precise and directly addresses the root cause.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13346": {
        "explanation": "The issue describes that, on MySQL, Oracle, and SQLite, using the __in lookup on a JSONField key transform does not work as expected: filtering with __in returns no results, while filtering with exact value returns the correct results. The root cause is likely that the values in the __in list are not being serialized to JSON, so the database comparison fails (since the JSONField stores values as JSON-encoded strings).\n\nThe patch adds logic so that, for MySQL, Oracle, and SQLite, the right-hand side parameters (rhs_params) are JSON-encoded using json.dumps. This ensures that the values in the __in list are serialized in the same way as the stored JSON values, making the comparison work as expected. The patch is minimal and targeted, only affecting the relevant code path for the affected backends and only for the __in lookup on key transforms.\n\nThere are no unnecessary changes, and the patch directly addresses the described issue. Edge cases (such as more complex types in the __in list) are handled as well as the rest of the system handles them, since json.dumps is the correct way to serialize these values for comparison.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13354": {
        "explanation": "The issue arises because Django's migration system does not handle app labels with uppercase letters consistently, leading to incorrect operation ordering and migration failures. Specifically, the comparison of model references is case-sensitive, but Django expects app labels to be treated case-insensitively in this context. The patch changes the construction of 'reference_model_tuple' from (app_label, name_lower) to (app_label.lower(), name_lower), ensuring that the app label is always compared in lowercase, which matches how Django internally normalizes app labels elsewhere. This directly addresses the root cause described in the issue, without introducing unrelated changes. The patch is minimal and precise, targeting only the problematic comparison.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13363": {
        "explanation": "The issue describes that TruncDate and TruncTime ignore the tzinfo parameter passed by the user and always use the global timezone setting. The patch introduces a get_tzname() method in Trunc (inheriting from TruncBase), which checks if self.tzinfo is set and, if so, uses its zone attribute (or str representation), otherwise falls back to the global timezone. It then updates TruncDate and TruncTime to use self.get_tzname() instead of always using timezone.get_current_timezone_name(). This directly addresses the issue: it allows a user-supplied tzinfo to be respected, and falls back to the global setting only if none is provided. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13401": {
        "explanation": "The issue describes a problem where fields inherited from an abstract model are considered equal across different concrete models because equality and hashing are based solely on 'creation_counter'. The issue requests that equality, hashing, and ordering also consider the field's model, so that fields from different models are not considered equal, even if they originate from the same abstract base.\n\nThe patch updates __eq__ to require both 'creation_counter' and 'model' to be equal, __hash__ to hash both, and __lt__ to order by 'creation_counter' first and then by model. This directly addresses the issue's requirements and rationale, and does so without introducing unrelated changes. The patch is precise and correct according to the issue description.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13406": {
        "explanation": "The issue describes a bug where pickling and unpickling a queryset that uses values()/annotate() causes the queryset to lose its iterable class, resulting in model instances being returned instead of dicts. The patch, however, only makes a trivial change to the __getstate__ method (assigning the state to a variable before returning it) and adds a test that demonstrates the correct behavior for pickling/unpickling a queryset with values() and annotate().\n\nCrucially, the patch does NOT address the root cause: the loss of the _iterable_class attribute when a queryset is reconstructed from a pickled query. The test passes because it pickles the entire queryset, not just the query, so the _iterable_class is preserved. The original issue is about pickling the query (qs.query), not the queryset (qs), and then assigning it to a new queryset, which is not covered or fixed by this patch. Therefore, the patch misses the key aspect of the issue and does not resolve the reported bug.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-13410": {
        "explanation": "The issue describes that the current implementation of the posix lock/unlock functions in django/core/files/locks.py incorrectly returns False even when the lock is acquired, because fcntl.flock returns None on success and raises OSError on failure. The correct approach is to use a try/except block: return True if no exception is raised, and return False if OSError is caught. The patch, however, checks if the return value of fcntl.flock is 0, which is never the case (it returns None on success). Thus, the patch does not fix the bug and will always return False, just as before. The patch fundamentally misunderstands the requirements and does not address the issue.",
        "label": "INCORRECT"
    },
    "django__django-13417": {
        "explanation": "The issue describes that QuerySet.ordered incorrectly returns True for queries with GROUP BY clauses (such as those produced by annotate()), even when there is no explicit order_by() and the model has a Meta.ordering. The patch modifies the ordered property to explicitly return False if there is a GROUP BY clause and no explicit order_by, which matches the described problem and its expected resolution. The new tests cover the relevant scenarios, including annotate() and values() queries, both with and without explicit ordering, and confirm the correct behavior of the ordered property. The patch is focused and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13449": {
        "explanation": "The issue describes a problem where using Lag() on a DecimalField in SQLite generates SQL like CAST(LAG(...) AS NUMERIC) OVER (...), which is invalid SQL syntax for SQLite. The CAST should wrap the entire window function, i.e., CAST(LAG(...) OVER (...) AS NUMERIC). The patch modifies the window_frame_range_start_end method in the SQLite backend to wrap the start and end frame boundaries in CAST(... AS NUMERIC) if the function is 'lag' and the output field is a DecimalField. However, the issue is not with the window frame boundaries, but with the placement of the CAST in the generated SQL for the window function itself. The patch does not address the core problem: it does not change where the CAST is applied in the SQL for the Lag() function. The added test only checks that Lag with DecimalField works, but does not verify the SQL syntax or that the CAST is in the correct place. Therefore, the patch misses the key aspect of the issue: fixing the placement of the CAST in the generated SQL for window functions involving DecimalField and Lag.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-13512": {
        "explanation": "The issue describes that Django's JSONField, when serializing Unicode characters (such as Chinese), uses ASCII encoding by default, resulting in escaped Unicode sequences (e.g., '\\u4e2d\\u56fd') instead of displaying the actual characters. The proposed patch changes the call to json.dumps in get_prep_value to include ensure_ascii=False, which instructs json.dumps to output Unicode characters directly rather than escaping them. This directly addresses the issue described, and the change is minimal and targeted, affecting only the relevant serialization behavior. No unnecessary changes are introduced.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13513": {
        "explanation": "The issue describes that the Django debug error view does not respect the __suppress_context__ attribute of exceptions, as specified in PEP 415. Specifically, when an exception is raised with 'from None', the context should be suppressed and not shown in the traceback. The original code only checked for __cause__ and __context__, and did not consider __suppress_context__. The patch modifies the explicit_or_implicit_cause function to first check for an explicit cause, then check if __suppress_context__ is True (in which case it returns None), and only otherwise returns the implicit context. This matches the logic described in the issue and in PEP 415. The patch is minimal and does not introduce unrelated changes. Therefore, it correctly and precisely addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13516": {
        "explanation": "The issue describes that calling flush() on self.stdout/self.stderr in Django management commands does not actually flush the output, causing output to be buffered and only displayed at the end of long-running operations. The patch adds a call to self._out.flush() at the end of the OutputWrapper.write() method. This ensures that every time something is written via OutputWrapper (which wraps self.stdout/self.stderr), the underlying stream is flushed immediately, making output appear in real time as expected. The patch is minimal and directly addresses the described problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13551": {
        "explanation": "The issue describes a security concern: if a user changes their email address after a password reset token is generated, the old token should be invalidated, but currently it is not. The proposed fix is to include the user's email address in the hash value used to generate the token, so that changing the email will change the token and invalidate any previously generated tokens.\n\nThe patch does exactly this: it retrieves the user's email field (using get_email_field_name() for compatibility with custom user models), gets the current value, and includes it in the hash value. This is a minimal, targeted change that directly addresses the issue without introducing unrelated modifications. It also gracefully handles the case where the user may not have an email field by defaulting to an empty string.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13560": {
        "explanation": "The issue describes that when a ProtectedError or RestrictedError is raised from Collector.collect, the second argument is an iterator (specifically, an itertools.chain object). This results in uninformative exception representations, such as showing the object type rather than its contents. The patch changes the code to wrap the chain.from_iterable(...) calls in list(), ensuring that the exception receives a list of objects rather than an iterator. This will make the exception's repr more informative, as it will display the actual objects rather than the iterator type. The patch is minimal and directly addresses the issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13568": {
        "explanation": "The issue describes a false positive from the auth.E003 system check when a custom user model uses a UniqueConstraint on the USERNAME_FIELD instead of unique=True. The patch updates the check in django/contrib/auth/checks.py to consider both unique=True and the presence of a UniqueConstraint on the USERNAME_FIELD. The logic is clear and precise: it checks if the field is unique or if any UniqueConstraint includes the USERNAME_FIELD. The error and warning messages are updated accordingly. The patch also adds targeted tests: one for a user model with a UniqueConstraint (which should not trigger auth.E003) and one without (which should trigger it). No unrelated changes are made. The patch is both correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13569": {
        "explanation": "The issue describes that using order_by('?') (which translates to ORDER BY RANDOM() in SQL) on an annotated queryset incorrectly adds RANDOM() to the GROUP BY clause, causing the aggregation to break down and return multiple rows instead of a single aggregated result. The root cause is that the SQL compiler includes all order_by expressions in the GROUP BY clause, even when they are random functions, which should not affect grouping.\n\nThe patch modifies the SQLCompiler to exclude columns of type Random from being added to the GROUP BY clause. Specifically, it filters out instances of Random when extending the expressions list for group by columns. This directly addresses the problem described: RANDOM() is no longer included in the GROUP BY, so aggregation works as expected.\n\nThe patch also adds a test that creates data, runs an aggregation with and without order_by('?'), and asserts that the results are consistent, confirming the fix.\n\nNo unrelated changes are made, and the patch is focused and precise. The solution matches the diagnosis and proposed fix in the issue description.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13590": {
        "explanation": "The issue describes a regression where passing a namedtuple as the argument to a queryset filter (such as __range) fails in Django 3.0 due to how namedtuples are reconstructed after resolving their elements. The error occurs because the code attempts to reconstruct the namedtuple by passing a single iterable (the resolved values) to its constructor, but namedtuples require positional arguments. The patch checks if the tuple has a _fields attribute (which is how namedtuples are identified), and if so, unpacks the resolved values as positional arguments using *. Otherwise, it passes the resolved values as a single argument (for regular tuples/lists). This directly addresses the described problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13658": {
        "explanation": "The issue describes that ManagementUtility computes its own program name (self.prog_name) from the provided argv, but when instantiating CommandParser, it does not pass this computed program name as the 'prog' argument. Instead, CommandParser defaults to using sys.argv[0], which can be incorrect in certain environments. The patch updates the instantiation of CommandParser to pass 'prog=self.prog_name', ensuring that the correct program name is used in help and error messages, as intended by the issue description. The other change in the patch (using os.path.dirname(__file__) in get_commands) is unrelated to the described issue and introduces an additional, unrequested modification. Therefore, while the main issue is addressed, the patch also makes an unrelated change.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-13670": {
        "explanation": "The issue is that dateformat.y() in Django does not correctly handle years less than 1000, returning only the last digit(s) without leading zeros, which is inconsistent with Python's strftime('%y') and PHP's date('y'). The patch changes the implementation from 'str(self.data.year)[2:]' to 'f\"{self.data.year:02}\"[-2:]', which ensures that the last two digits are always taken from a zero-padded year string. For example, for year 123, '{self.data.year:02}' produces '123', and '[-2:]' gives '23', matching the expected output. For year 9, it would produce '09'. This matches the behavior of strftime('%y'). The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13689": {
        "explanation": "The issue describes a problem where, when using ExpressionWrapper to annotate a query (specifically with an expression like Q(bob_id=None)), the generated SQL's GROUP BY clause omits the actual expression (e.g., 'bob_id IS NULL'), grouping only by the underlying column ('bob_id'). This can lead to incorrect aggregation results, as the grouping should be on the evaluated expression, not just the column.\n\nThe original code in get_group_by_cols copies the wrapped expression and returns its group by columns, which can result in the omission described. The patch changes this logic:\n- For Value expressions, it returns an empty list (preserving legacy test behavior).\n- For Function expressions, it returns the copied expression.\n- For all other cases (including the IS NULL expression in the issue), it returns [self], i.e., the ExpressionWrapper itself, ensuring the group by clause includes the full expression.\n\nThis directly addresses the issue: now, when grouping by an ExpressionWrapper wrapping an expression like 'bob_id IS NULL', the group by will include the expression, not just the column. The patch is focused and does not introduce unrelated changes.\n\nTherefore, the patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13691": {
        "explanation": "The issue describes a JSON decode error when performing a path lookup on a JSONField in SQLite, specifically when the value at the path is a primitive (int, float, bool). The error occurs because Django's from_db_value method always tries to json.loads the value, but if the value is already a primitive (not a string), this fails.\n\nThe patch attempts to fix this by modifying from_db_value to return the value directly if it is an int, float, or bool, only calling json.loads if the value is not one of these types. This is a correct and precise fix for the described issue.\n\nHowever, the patch also introduces a large number of redundant get_transform method definitions, which are clearly copy-paste errors and do not relate to the issue. Additionally, it adds a print statement in get_prep_value, which is not appropriate for production code and is unrelated to the issue. These changes are unnecessary and unrelated to the described problem.\n\nTherefore, while the core fix is present, the patch as a whole is not precise and introduces unrelated and potentially harmful changes.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-13741": {
        "explanation": "The issue describes that the ReadOnlyPasswordHashField should have the 'disabled' property set to True by default, so that the field is not editable and any submitted value is ignored in favor of the initial value. This would make the custom clean_password method in UserChangeForm unnecessary, as the field's value cannot be changed by the user. The patch does exactly this: it sets 'disabled=True' by default in ReadOnlyPasswordHashField's __init__, and removes the now-unnecessary clean_password method from UserChangeForm. No unrelated changes are made. The patch is precise and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13786": {
        "explanation": "The issue describes that when an AlterModelOptions operation with an empty options dict is squashed into a CreateModel, the resulting CreateModel should have its options cleared (i.e., options not present in AlterModelOptions should be removed from the CreateModel). The original code merges options using {**self.options, **operation.options}, which only overwrites or adds keys but does not remove any keys that are not present in operation.options. The patch changes this to only include keys from the merged dict that are present in operation.options, effectively removing any options not specified in the AlterModelOptions. This directly addresses the issue as described, and the change is minimal and targeted. No unnecessary changes are made.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13791": {
        "explanation": "The issue describes that parse_duration ignores the sign for durations that only specify days, resulting in a positive timedelta even when the ISO string is negative. The original code computes 'days' as a timedelta and adds it to 'sign * timedelta(**kw)'. If 'kw' is empty (i.e., only days are present), 'timedelta(**kw)' is zero, so the sign is not applied to the days component. The patch changes this by extracting 'days' as a float, and then returning 'sign * timedelta(days=days, **kw)', ensuring the sign is applied to the days as well as any other components. This directly addresses the issue without unnecessary changes. The patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13794": {
        "explanation": "The issue describes a problem where Django's 'add' template filter fails when trying to concatenate a regular string with a lazy string (specifically, a Django 'Promise' object, such as those returned by translation functions). The error occurs because Python does not allow direct concatenation of a str and a Promise object, resulting in a TypeError.\n\nThe patch introduces a 'force_text' function that checks if a value is a Promise and, if so, converts it to a string using str(value). It then updates the 'add' filter to use 'force_text' on both operands before attempting concatenation. This ensures that both operands are strings, preventing the TypeError and allowing the concatenation to succeed as expected.\n\nThe patch is minimal, targeted, and does not introduce unrelated changes. It directly addresses the described issue and should resolve the error as intended.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13807": {
        "explanation": "The issue is that when table names are SQL keywords (like 'order'), the code fails because it does not quote the table names in SQL statements, leading to syntax errors. The patch updates all relevant SQL statements in the check_constraints method to use self.introspection.quote_name(table_name), which properly quotes the table names. This change is applied to both 'PRAGMA foreign_key_check' and 'PRAGMA foreign_key_list', as well as the SELECT statement that uses the table name. The patch does not introduce unrelated changes and directly addresses the root cause described in the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13809": {
        "explanation": "The patch introduces a '--skip-checks' option to the 'runserver' management command, as requested in the issue. It adds the argument to the parser, and then, in the command's main logic, it conditionally skips the system checks and migration checks if the option is set, displaying an appropriate message. The implementation is minimal and directly addresses the issue without introducing unrelated changes. The rationale and behavior match the issue description, and the patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13810": {
        "explanation": "The issue describes a problem where, if a middleware raises MiddlewareNotUsed during the middleware loading loop, the 'handler' variable is left in a potentially inconsistent state. Specifically, the output of self.adapt_method_mode() is assigned to 'handler' before the MiddlewareNotUsed exception is caught, and if the exception is raised, the loop continues with this possibly altered 'handler', which can break the async/sync contract for subsequent middleware. The patch addresses this by saving the previous value of 'handler' before adaptation, and restoring it if MiddlewareNotUsed is raised, ensuring that the handler passed to the next middleware is not tainted by the failed adaptation. The patch also adds targeted tests (both sync and async) to verify the correct behavior. The changes are minimal and directly address the described issue without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13820": {
        "explanation": "The issue requests that Django's migration loader should not reject all packages lacking a __file__ attribute, but only those that are true namespace packages (as per PEP-420), which can be detected by their __path__ not being a regular list. The patch updates the check: if __file__ is missing, it now inspects __path__ to see if it's a list (regular package) or a special iterable (namespace package). Only the latter are rejected. The logic is careful to handle edge cases (e.g., conversion to list, type checks, and exceptions). The patch does not introduce unrelated changes and is narrowly focused on the described issue. The added test files demonstrate the scenario but do not affect the migration loader logic. Thus, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13821": {
        "explanation": "The issue requests to drop support for SQLite versions older than 3.9.0. This means Django should assume SQLite 3.9.0 or newer is always available, and code paths or compatibility shims for older versions can be removed. The patch, however, does not remove any compatibility code or checks for older SQLite versions. Instead, it adds conditional assignments to certain class attributes (creation_class, features_class, introspection_class) if the SQLite version is 3.9.0 or newer, but leaves the default as None otherwise. This is not the correct approach: the patch does not actually drop support for older versions, nor does it clean up the codebase to assume 3.9.0+ is always present. It also introduces repeated, unnecessary checks and does not address the broader implications of the version drop. Therefore, the patch misses key aspects of the issue.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-13837": {
        "explanation": "The patch updates the logic in get_child_arguments() to use __main__.__spec__ and its parent attribute to detect when Python was launched with -m for any package, not just 'django'. This matches the approach described in the issue, which aims to generalize autoreload support for Django management commands run via custom packages. The patch removes the previous reliance on django.__main__.__file__, which was brittle and did not work in all environments. The new logic is precise: it checks for __main__.__spec__ and its parent, and uses that as the package name for the -m argument, as recommended in the issue. The patch also adds a targeted test that creates a dummy package and verifies that runserver can be started via 'python -m dummy_package runserver', confirming the intended behavior. No unrelated changes are made. The patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13924": {
        "explanation": "The issue describes a problem where migrations are marked as applied even if deferred SQL (executed during SchemaEditor.__exit__) fails. The original code records the migration as applied immediately after migration.apply(), which may not include errors from deferred SQL. The patch moves the call to self.record_migration(migration) outside the 'with' block, ensuring that the migration is only recorded after the schema editor (and thus all deferred SQL) has successfully completed. This change directly addresses the issue without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13925": {
        "explanation": "The issue describes that models inheriting from a base class with a manually specified primary key are incorrectly receiving the models.W042 warning about auto-created primary keys in Django 3.2 alpha. The patch adds an additional condition to the check: 'not cls._meta.parents'. This ensures that the warning is only raised for models that do not inherit from any parent models (i.e., are not subclasses), which is correct because only root models without an explicit primary key should be warned. Inherited models should not trigger this warning if the primary key is defined in the ancestor. The patch is minimal and directly addresses the described problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13933": {
        "explanation": "The issue describes that ModelChoiceField does not include the invalid value in the ValidationError message, unlike ModelMultipleChoiceField, which uses '%(value)s' in its error message and passes the value as a parameter. The patch updates the default error message for 'invalid_choice' in ModelChoiceField to include '%(value)s', and ensures that when raising ValidationError for invalid choices, the 'value' parameter is passed. It also adds this parameter in the case where queryset is None. These changes are minimal, directly address the issue, and do not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13952": {
        "explanation": "The issue describes that the verbose output from emit_pre_migrate_signal and emit_post_migrate_signal is always sent to sys.stdout, making it hard to capture during tests (e.g., when using call_command with a custom stdout). The patch adds an optional 'stdout' parameter to both functions, defaulting to sys.stdout if not provided, and replaces 'print' with 'stdout.write'. This allows callers (such as the migrate command) to redirect output as needed, making the output capturable for testing. The patch is minimal and directly addresses the issue without unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-13964": {
        "explanation": "The issue describes a scenario where assigning an unsaved related object (with a non-auto primary key) to a ForeignKey field on a parent, then setting the primary key and saving both, results in the parent's foreign key field being set to an empty string rather than the correct primary key value. This is because, at assignment time, the related object's primary key is not yet set, so the parent's foreign key field is set to the default (empty string). The patch modifies the ForwardManyToOneDescriptor to, upon assignment, check if the related object is unsaved (value._state.adding) and, if so, save it before copying the primary key value to the parent. This ensures that the related object's primary key is set before the parent's foreign key field is updated, directly addressing the described issue. The patch is minimal and does not introduce unrelated changes. It precisely targets the described problem and resolves it as intended.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14007": {
        "explanation": "The issue describes that when inserting a model instance with a custom AutoField (e.g., MyAutoField), the returned primary key value is not passed through the field's from_db_value method, resulting in a plain integer instead of the expected wrapper class. The patch modifies the SQLInsertCompiler so that when returning_fields is not set (i.e., when the DB does not natively return values), it fetches the last_insert_id and explicitly passes it through pk_field.from_db_value before returning it. This directly addresses the described problem for the single-object insert case, ensuring the returned value is processed by from_db_value. The patch does not introduce unrelated changes and is precise in its scope. Edge cases like bulk_create with multiple objects are handled elsewhere (as indicated by the can_return_rows_from_bulk_insert branch), so this patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14011": {
        "explanation": "The issue describes a race condition where database connections are not properly closed after each thread in LiveServerTestCase, leading to errors when the test database is destroyed. The root cause is that ThreadedWSGIServer (used by LiveServerTestCase) does not close database connections after handling a request, unlike the non-threaded version. The patch addresses this by overriding process_request in ThreadedWSGIServer to ensure that, after each request is processed, all database connections are closed via connections.close_all(). This is a targeted and appropriate fix for the described problem, as it ensures that each thread handling a request will close its database connections, preventing lingering connections that cause the race condition. The changes in testcases.py are simply a refactor of the tearDownClass method and are unrelated to the core fix, but they do not introduce unnecessary or unrelated changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14017": {
        "explanation": "The issue describes that 'Exists(...) & Q()' works, but 'Q() & Exists(...)' raises a TypeError. This is due to the lack of a __rand__ implementation in the Combinable class, which is the base for Exists. The patch adds a __rand__ method to Combinable that calls self._combine(other, self.AND), mirroring the __and__ method, thus making the & operator commutative between Q and Exists. The patch does not introduce unrelated changes and directly addresses the missing __rand__ implementation, which is the root cause of the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14034": {
        "explanation": "The issue describes that MultiValueField, when require_all_fields=False, does not enforce required=True on individual subfields: if all subfields are empty, the form is considered valid, even if one subfield is required. The patch modifies MultiValueField.clean to ensure that if any subfield is required and empty, a ValidationError is raised, regardless of require_all_fields. The new test covers the described cases: it asserts that a required subfield being empty causes a ValidationError, and that only when all required subfields are filled does the field validate. The patch is focused, does not introduce unrelated changes, and the logic matches the issue's requirements. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14053": {
        "explanation": "The issue describes that HashedFilesMixin's post_process() method yields the same original filename multiple times, which leads to incorrect statistics and duplicate work downstream. The patch introduces a yielded_files dictionary to track which files have already been yielded, ensuring that each original file is only yielded once with its final hashed name. The logic checks both in the initial and subsequent passes, only yielding if the file hasn't been yielded before or if the hashed name has changed. This directly addresses the problem of duplicate yields and unnecessary downstream processing, without introducing unrelated changes. The patch is focused, precise, and solves the described issue as expected.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14089": {
        "explanation": "The issue requests that OrderedSet support the reversed() built-in, which requires implementing the __reversed__() method. The patch adds a __reversed__ method to OrderedSet that simply returns reversed(self.dict). Since OrderedSet is implemented as a thin wrapper around an internal dict (which, since Python 3.7, preserves insertion order), reversing the dict will yield the keys in reverse order of insertion, which is the expected behavior for a reversed OrderedSet. The patch does not introduce any unrelated changes to the OrderedSet class. The addition of a test script (reproduce_error.py) is not part of the core library and does not affect the correctness of the patch. The patch is minimal and directly addresses the issue as described, without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14122": {
        "explanation": "The issue describes that fields specified in Meta.ordering should not be included in the GROUP BY clause for aggregation queries, as this leads to incorrect aggregation results. The patch modifies the group_by construction logic in Query to:\n\n- Detect if the query is an aggregation (by checking for annotations with is_summary).\n- If so, it collects the Meta.ordering fields, normalizes them (removing leading '-' and handling F expressions), and then excludes any select fields that match these ordering fields from the group_by set.\n- It also updates group_by with any group_by columns from annotations.\n\nThis approach directly targets the described problem: it ensures that Meta.ordering fields are not included in GROUP BY for aggregation queries, and does so without introducing unrelated changes. The logic for handling both string and F expression fields in ordering is correct and precise. The patch does not remove any necessary group_by columns nor does it make unrelated modifications.\n\nTherefore, the patch is both correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14140": {
        "explanation": "The issue is that deconstructing a Q object with a single child that is not a tuple (e.g., an Exists object) causes a TypeError because the code assumes the child is subscriptable. The patch adds a check: if the child is a list of length 3, it uses the previous behavior (constructing kwargs from child[1] and child[2]); otherwise, it puts the child into args three times. However, this is not correct: the original code expects a 2-tuple (key, value) for kwargs, not a 3-list, and the fallback of putting the child into args three times is arbitrary and not aligned with the intended behavior. The patch does not correctly generalize the handling of single-child Q objects and introduces incorrect logic. It neither preserves backward compatibility nor follows the suggestion in the issue description (which is to check for a length-2 tuple).",
        "label": "INCORRECT"
    },
    "django__django-14151": {
        "explanation": "The issue describes that CsrfViewMiddleware does not handle the case where urlparse() raises a ValueError when parsing an invalid Referer header (e.g., 'https://['). The patch, however, adds error handling for ValueError in the is_same_domain() call, not in the urlparse() call. The urlparse() call is still unguarded, so if request.META['HTTP_ORIGIN'] is an invalid URL, urlparse() could still raise a ValueError before the try/except in safe_is_same_domain is reached. Therefore, the patch does not correctly address the issue as described, since the critical line (urlparse) is still vulnerable to ValueError exceptions.",
        "label": "INCORRECT"
    },
    "django__django-14155": {
        "explanation": "The issue describes that when a functools.partial is passed as the view function, ResolverMatch.__repr__ displays the func as functools.partial, which is not informative. The suggestion is to unwrap partials in ResolverMatch.__init__ so that the underlying function and its arguments are accessible and can be shown in __repr__.\n\nThe patch checks if func is a functools.partial, and if so, sets self.func to the underlying function (func.func), and combines the partial's args and kwargs with those passed in. This ensures that self.func, self.args, and self.kwargs represent the actual function and all arguments, not the partial wrapper. This directly addresses the issue by unwrapping partials at construction, so __repr__ and other uses of these attributes will be more informative.\n\nThe patch does not introduce unrelated changes and precisely targets the described problem. It also correctly merges args and kwargs, as expected for partials. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14170": {
        "explanation": "The issue describes that the BETWEEN optimization in YearLookup is incorrectly applied to '__iso_year' lookups, which should instead use EXTRACT('isoyear' ...). The patch introduces a conditional in YearLookup's as_sql method: if the lookup is for 'iso_year', it uses EXTRACT as required; otherwise, it retains the BETWEEN optimization for 'year'. This directly addresses the problem described, ensuring that '__iso_year' lookups use the correct SQL and that the BETWEEN optimization is only used for 'year'. The patch is focused and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14199": {
        "explanation": "The issue is that when using Value() with a DecimalField as output_field and passing an int or float as the value, Django passes a string to create_decimal_from_float, which causes a crash on SQLite. The patch checks if output_field is a DecimalField and the value is an int or float, and if so, converts the value to a Decimal using Decimal(str(value)). This ensures that the value is always a Decimal when used with DecimalField, preventing the crash. The patch is minimal and directly addresses the described issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14238": {
        "explanation": "The issue is that when a custom AutoField (e.g., MyBigAutoField) subclasses BigAutoField, Django's check for whether it is a valid AutoField fails, because AutoFieldMeta.__subclasscheck__ only checks for direct membership in _subclasses, not for subclasses of those classes. The patch modifies __subclasscheck__ to also return True if the given subclass is a subclass of any class in _subclasses, using issubclass(subclass, allowed). This directly addresses the problem described in the issue, allowing custom subclasses of BigAutoField or SmallAutoField to be recognized as valid AutoFields. The change is minimal and targeted, with no unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14311": {
        "explanation": "The issue describes a problem where the autoreloader, when started with 'python -m foo.bar.baz', incorrectly reconstructs the child process arguments as '-m foo.bar' instead of '-m foo.bar.baz'. The original code only used __main__.__spec__.parent, which would drop the last component of the module path. The patch changes the logic to first check for __main__.__spec__.name (which would be 'foo.bar.baz' in the example), and use that as the argument to '-m'. Only if __main__.__spec__.name is not present does it fall back to __main__.__spec__.parent. This directly addresses the issue by ensuring the full module name is used when available, and does not introduce unrelated changes. The patch is precise and correct for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14315": {
        "explanation": "The issue describes that the PostgreSQL client returns an empty dict for the environment variables (env) when invoking the database shell, which causes subprocess to use an empty environment instead of inheriting os.environ. The correct behavior is to use os.environ unless specific PostgreSQL-related environment variables are set, in which case those should be merged with os.environ.\n\nThe patch modifies the logic in both the base client and the PostgreSQL client:\n- In the PostgreSQL client, it now returns env=None unless any PostgreSQL-specific variables are set, in which case it builds a dict with only those variables.\n- In the base client, it interprets env=None as 'use os.environ as is', and if env is a non-empty dict, it merges it with os.environ. If env is an empty dict, it passes it as is (which is the correct behavior if the intention is to run with no environment variables).\n\nThis directly addresses the issue: the default is now to inherit os.environ, and only override or add variables if needed. The patch is focused and does not introduce unrelated changes.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14349": {
        "explanation": "The issue describes that, after Python's bpo-43882, URLs containing LF, CR, or tab characters are stripped of these characters before being split, causing Django's URLValidator to pass URLs that previously would have failed validation. The patch modifies the URLValidator to strip these characters from the input before further processing, thus aligning Django's behavior with the new Python standard library behavior. This means URLs with these characters will be accepted, not rejected, which matches the 'go with the new recommendation and assume that LF, CR and tabs are to stripped silently' option mentioned in the issue. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise for the chosen approach.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14351": {
        "explanation": "The issue describes a regression where, in certain Q object OR queries involving related fields, Django generates a subquery that selects all columns instead of just the primary key, leading to a 'subquery must return only one column' SQL error. The root cause is that the subquery's default_cols flag is set to True, causing all default columns to be selected, rather than just the necessary one. The patch changes the initialization of self.default_cols to be 'not self.subquery', ensuring that for subqueries, default_cols is False, so only the required columns are selected. Additionally, it tightens a condition to avoid unnecessary selection of columns in subqueries. This directly addresses the described problem without introducing unrelated changes. The patch is targeted and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14373": {
        "explanation": "The issue describes that DateFormat.Y() should always return a four-digit year, zero-padded if necessary (e.g., '0123' for year 123). The original code simply returned self.data.year, which would not be zero-padded for years less than 1000. The patch changes the return statement to use f'{self.data.year:04d}', which ensures the year is always represented as a four-digit, zero-padded string. The docstring is also updated to reflect this behavior. No unnecessary changes are made. The patch is precise and directly addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14376": {
        "explanation": "The issue describes that the Django MySQL backend is using deprecated 'db' and 'passwd' keyword arguments when connecting to MySQL, and should switch to 'database' and 'password' as per recent mysqlclient changes. The patch updates 'db' to 'database' and 'passwd' to 'password' in 'base.py', and also updates the client code in 'client.py' to use 'database' and 'password' instead of the deprecated names. The changes are minimal and directly address the deprecation, without introducing unrelated modifications. No edge cases or additional requirements are mentioned in the issue, and the patch does not remove support for the old keys in user-supplied OPTIONS (which is correct for backward compatibility). Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14382": {
        "explanation": "The issue describes a problem where providing a directory name with a trailing slash to 'django-admin startapp' causes os.path.basename to return an empty string, leading to a validation error. The proposed patch modifies the code to strip any trailing os.sep (slash or backslash, depending on OS) from the target before passing it to os.path.basename. This ensures that the correct directory name is validated, even if the user provides a trailing slash. The patch is minimal, directly addresses the root cause, and does not introduce unrelated changes. It is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14404": {
        "explanation": "The issue describes that catch_all_view does not properly handle FORCE_SCRIPT_NAME, resulting in redirects that omit the script name (i.e., using request.path_info instead of request.path). The patch modifies catch_all_view to check for FORCE_SCRIPT_NAME and use request.path_info or request.path accordingly. It also adjusts the redirect logic to prepend FORCE_SCRIPT_NAME if necessary, ensuring the redirect URL includes the script name. The patch adds print statements for debugging and a comprehensive test file that specifically checks the FORCE_SCRIPT_NAME behavior, including correct redirect URLs and 404 handling. The changes are focused on the described problem and do not introduce unrelated modifications. Therefore, the patch is correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14416": {
        "explanation": "The issue describes a problem where redirects generated by LocaleMiddleware are cached incorrectly by HTTP caches, causing users to be redirected to the wrong language version of a page. The root cause is that the redirect response does not include a Vary header indicating that the response depends on the 'Accept-Language' header and possibly the language cookie. The patch modifies the middleware so that when a redirect is issued, it adds a Vary header for 'Accept-Language', and also for 'Cookie' if a language cookie is in use. This ensures that HTTP caches will correctly vary the cached redirect based on these headers, preventing the described issue. The patch is focused, does not introduce unrelated changes, and directly addresses the problem as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14430": {
        "explanation": "The issue describes that when an aggregate query with Coalesce is performed on an empty queryset (e.g., pk__in=[]), Django returns None instead of the Coalesce fallback value (e.g., 0). This is because an EmptyResultSet is raised and the current code sets all aggregates to None in this case, bypassing the Coalesce logic.\n\nThe patch modifies the code so that when an EmptyResultSet is encountered, it creates a result row of [None] * len(outer_query.annotation_select), which is then processed by the converters (including Coalesce). This allows the Coalesce logic to run and return the fallback value (e.g., 0) instead of None, matching the behavior when the query returns no rows but does not raise EmptyResultSet.\n\nThe patch is minimal and directly targets the described problem, without introducing unrelated changes. It ensures that the Coalesce fallback is respected even when EmptyResultSet is raised, as required by the issue.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14434": {
        "explanation": "The issue describes a bug where the _create_unique_sql method passes a Table instance to Columns, but Columns expects a string (the table name). This causes references_column to always be false, likely because the type mismatch prevents correct column reference resolution. \n\nThe patch changes the code so that instead of passing a Table instance, it passes the table name string (model._meta.db_table) to Columns. This is done in both _create_fk_sql and _create_unique_sql, ensuring that Columns receives the expected argument type. Where a Table instance is still needed (e.g., for quoting), the patch wraps the string in Table at the call site, preserving the original behavior where necessary.\n\nThe changes are minimal and directly address the described issue without introducing unrelated modifications. The patch is precise and correct.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14493": {
        "explanation": "The issue is that when max_post_process_passes is set to 0, the loop that assigns 'substitutions' is never entered, so 'substitutions' is undefined when later referenced, causing an UnboundLocalError. The patch ensures that 'substitutions' is always defined, even if the loop is skipped (i.e., when max_post_process_passes is 0), by assigning it a value after the loop if it is still undefined. This directly addresses the crash described in the issue, and the change is minimal and targeted. No unnecessary changes are introduced, and the patch does not alter unrelated logic.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14495": {
        "explanation": "The issue describes two main problems: (1) Django raises an ambiguity error when multiple migrations match a prefix, even if some are replaced by a squash migration, and (2) if you specify a replaced migration, Django raises a raw KeyError instead of a more informative error, and does not allow migrating to that migration even if it exists on disk.\n\nThe patch changes the migration lookup logic to:\n- First search in the migration graph (applied migrations), and if ambiguous, raise AmbiguityError.\n- If not found, search in disk migrations. If ambiguous, raise AmbiguityError.\n- If a single disk migration is found, check if it was replaced by a squashed migration, and if so, raise a ValueError with a clear message.\n\nThis means the patch now gives a clear error if you try to migrate to a replaced migration, rather than a raw KeyError. However, the issue author argues that Django should allow migrating to a replaced migration if it still exists on disk, for rollback purposes. The patch does not allow this; it explicitly blocks it with a ValueError. Thus, the patch only addresses the error message aspect, not the functional request to allow migration to replaced migrations.\n\nTherefore, the patch misses a key aspect of the issue: it does not allow migration to replaced migrations, only improves the error message. It also still considers replaced migrations in ambiguity checks, which the issue suggests is confusing.\n\nThus, the patch is BROAD_MISSING_KEY_ASPECTS.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-14500": {
        "explanation": "The issue describes a problem where, when unapplying a squashed migration (with the replaced migration files still present), Django's MigrationExecutor only marks the replaced migrations as unapplied, but not the squashed migration itself. The expected behavior is that both the squashed migration and its replaced migrations are marked as unapplied.\n\nThe patch modifies the MigrationExecutor's unapply logic: after unapplying a migration, if it has a 'replaces' attribute (i.e., it's a squashed migration), it iterates over the replaced migrations and removes them from the applied_migrations set. This ensures that both the squashed migration and its replaced migrations are marked as unapplied.\n\nThe patch also adds comprehensive tests to verify this behavior, including both unit tests with fake migrations and integration tests using a real database. The tests check that after unapplying a squashed migration, neither the squashed migration nor any of its replaced migrations remain in the applied_migrations set.\n\nThere are no unnecessary changes, and the patch directly and precisely addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14534": {
        "explanation": "The issue describes that BoundWidget.id_for_label should use the 'id' from self.data['attrs']['id'] if present, rather than always constructing a new id. The patch changes the property to return self.data['attrs'].get('id') or the constructed id, which is correct: it uses the provided id if available, and falls back to the old behavior otherwise. The included test verifies both cases: when 'id' is present in attrs and when it is not. The addition of unrelated files (run_tests.py and a new test file) is not strictly necessary for the fix, but the test is directly relevant and helps ensure correctness. The patch does not introduce unrelated changes to the main codebase. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14539": {
        "explanation": "The issue is that urlize() does not correctly handle HTML-escaped strings when determining and removing trailing punctuation. In the example, the input contains '&lt', which is HTML-escaped for '<'. The current code unescapes the string, strips trailing punctuation, but then uses the length of the unescaped string to slice the original (escaped) string, which can lead to incorrect results when the escaped and unescaped strings differ in length (as with '&lt' vs '<').\n\nThe patch changes the logic so that, after unescaping and stripping, it calculates the difference in length between the original (escaped) string and the re-escaped stripped string. It then uses this difference to correctly slice the original string, ensuring that the correct number of characters are removed from the end, even when HTML entities are involved. This directly addresses the problem described in the issue, and the change is minimal and targeted.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14559": {
        "explanation": "The issue requests that bulk_update() return the number of rows matched, similar to update(), instead of returning None. The patch modifies the bulk_update method so that it returns 0 if the input list is empty, and otherwise accumulates the return values from each call to update() (which returns the number of rows matched) and returns the total. This directly and precisely addresses the issue. The patch does not introduce unrelated changes to the core logic, and the added tests (though not required for this evaluation) confirm the intended behavior. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14580": {
        "explanation": "The issue describes a missing import of 'models' in generated migration files when custom base classes are used, leading to a NameError for 'models.Model'. The patch changes the code so that 'from django.db import migrations, models' is always added to the imports, regardless of whether 'models' is used in the operations or not. This ensures that 'models' is always available in the migration file, directly addressing the reported NameError. The change is minimal and directly targets the problem without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14608": {
        "explanation": "The issue requests that non-form errors in FormSets be given a 'nonform' CSS class, similar to how non-field errors in Forms get a 'nonfield' class. The patch introduces a new attribute '_non_form_error_class' set to 'nonform' and ensures that when the ErrorList for non-form errors is created, it is passed this class via the 'error_class' argument. This is a minimal and targeted change that directly addresses the issue, without introducing unrelated modifications. The patch does not alter other behaviors or add unnecessary complexity. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14631": {
        "explanation": "The patch directly addresses the issue described. It refactors BaseForm._clean_fields() and changed_data to access values via BoundField objects, as requested. The per-field logic for determining if data has changed is moved into a new BoundField._did_change() method, which encapsulates the logic previously in changed_data. The patch also updates _clean_fields to use bf.data for value retrieval, ensuring consistency between form._clean_fields() and form[name].initial. The added test in test_forms.py specifically checks for the consistency between cleaned_data and BoundField.initial for disabled fields, which was a concern in the issue. No unnecessary changes are introduced, and the patch is focused and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14641": {
        "explanation": "The issue requests moving the special-case logic for handling microseconds in datetime/time objects from BoundField.initial() to BaseForm.get_initial_for_field(). The patch does exactly this: it removes the logic from BoundField.initial() and adds it to BaseForm.get_initial_for_field(), right after resolving callables, which matches the suggestion in the issue. No unrelated changes are made, and the logic is preserved as-is. This is a precise and correct refactor as described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14667": {
        "explanation": "The issue describes a bug where chaining defer() after only() does not properly clear the deferred fields, resulting in all fields being loaded instead of only the primary key (when all only() fields are deferred). The patch modifies the logic in Query to detect when all fields specified in only() are subsequently deferred, and in that case, switches to a mode that defers all fields except the primary key. The patch also adds tests that specifically check the problematic cases described in the issue, ensuring that only the primary key is loaded when all only() fields are deferred. The changes are minimal, targeted, and directly address the described issue without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14672": {
        "explanation": "The issue describes a TypeError caused by 'through_fields' being a list, which is unhashable, when used in the identity property of ManyToManyRel. The proposed solution is to wrap 'self.through_fields' with 'make_hashable', as is already done for other potentially unhashable fields. The patch does exactly this: it changes 'self.through_fields' to 'make_hashable(self.through_fields)' in the identity property. This is a minimal, targeted change that directly addresses the problem described, without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14725": {
        "explanation": "The patch introduces an 'allow_new' parameter to model formsets, which, when set to False, prevents the creation of new objects via the formset. This is achieved by:\n- Adding an 'allow_new' attribute to BaseModelFormSet and propagating it through the constructor and the modelformset_factory.\n- Overriding total_form_count() to return only the initial form count if allow_new is False, thus preventing extra forms for new objects.\n- Modifying _construct_form to return None for indices beyond the initial forms when allow_new is False, ensuring no new forms are constructed.\n- Adjusting modelformset_factory to set extra=0 if allow_new is False, further preventing new forms from being created.\n- Adding comprehensive tests that verify both the default (allow_new=True) and the new behavior (allow_new=False), including attempts to submit new data and confirming that no new objects are created.\n\nThe patch is focused, does not introduce unrelated changes, and covers the described issue precisely. It also includes tests to verify the new behavior. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14751": {
        "explanation": "The patch addresses both main points raised in the issue:\n\n1. It changes progress and log output from stdout to stderr, making stdout available for structured output, which is a common scripting-friendly practice.\n2. It collects the paths of created migration files and, after writing them, outputs a JSON object to stdout listing these files. This provides a programmatic way for scripts to determine which files were created, as requested.\n\nThe changes are minimal and directly related to the issue, without introducing unrelated modifications. The patch also preserves existing behavior for cases where no files are created (no JSON output), and only outputs the JSON when files are actually created, which is reasonable.\n\nTherefore, the patch is both correct and precise in addressing the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14752": {
        "explanation": "The patch refactors the AutocompleteJsonView.get() method to delegate the construction of each result dictionary to a new serialize_result() method. This matches the request in the issue description, which asks for an extension point to allow easy customization of the autocomplete response without overriding the entire get() method. The patch does not introduce any unrelated changes and preserves the original behavior by default. The docstring for serialize_result() is clear, and the refactor is minimal and targeted. This directly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14765": {
        "explanation": "The issue describes that after a previous PR, all calls to ProjectState.__init__() now pass real_apps as a set, so the code no longer needs to convert real_apps to a set, but can simply assert that it is a set. The patch removes the conversion to set and replaces it with an assertion, as suggested. No unrelated changes are made, and the patch is precise and minimal. This directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14771": {
        "explanation": "The issue describes that when running Django's development server with Python's '-X' options (such as '-X utf8'), the auto-reloader does not propagate these options to the child process it spawns. This can result in different behaviors between the initial and reloaded processes, particularly with respect to encoding, as demonstrated in the issue.\n\nThe patch modifies 'django/utils/autoreload.py' so that when constructing the arguments for the child process, it now includes any '-X' options present in 'sys._xoptions'. This is done by extending the argument list with '-Xkey=value' for each key-value pair in 'sys._xoptions', which is the correct way to propagate these options. The patch also preserves the existing behavior for warning options.\n\nAdditionally, the patch adds tests to ensure that '-X' options are passed through correctly. The tests mock the relevant parts of the system and check that the child process arguments include the '-X' options, specifically verifying that 'utf8' is present when expected.\n\nThere are no unnecessary changes, and the patch directly addresses the described issue by ensuring '-X' options are passed to the reloader child process. The tests are relevant and focused on the new behavior.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14787": {
        "explanation": "The issue describes that when using method_decorator with a decorator like logger, the resulting object is a functools.partial, which lacks function attributes like __name__, causing errors in code that expects these attributes. The expected fix is for method_decorator to ensure that the wrapper returned preserves these attributes (e.g., via functools.update_wrapper or similar). \n\nThe provided patch, however, only removes blank lines from django/utils/decorators.py and does not make any functional changes to method_decorator or related code. There is no addition of update_wrapper or any logic to ensure that the returned object from method_decorator has the correct function attributes. Therefore, the patch does not address the described issue at all.",
        "label": "INCORRECT"
    },
    "django__django-14792": {
        "explanation": "The issue describes a regression in Django's handling of 'Etc/GMT' time zones in Trunc/Extract database functions, specifically that the string returned by _get_timezone_name() changed from '+10' to 'Etc/GMT-10', which then led to incorrect sign reversal in _prepare_tzname_delta(), resulting in the wrong time zone offset in SQL queries. The patch addresses this by:\n\n1. Modifying _prepare_tzname_delta() to return the tzname unchanged if it starts with 'Etc/GMT', thus avoiding the sign reversal for these time zones.\n2. Updating _get_timezone_name() to, when given an 'Etc/GMT' zone, return a string in the '+10' or '-10' format (matching Django 3.1 behavior), rather than the full 'Etc/GMT-10' string.\n3. Adding tests that specifically check truncation with both UTC and 'Etc/GMT-10' time zones, verifying the correct offset and truncation behavior.\n\nThe patch is focused, only changes the relevant logic, and adds appropriate tests. It directly addresses the described regression and restores the expected behavior. No unnecessary or unrelated changes are present.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14802": {
        "explanation": "The patch introduces a new helper method `make_and_validate_key` to the `BaseCache` class, which encapsulates the repeated pattern of calling `make_key` followed by `validate_key`. The implementation matches the pattern described in the issue and does not introduce any unnecessary changes or side effects. It does not alter existing behavior or API, and simply adds a utility function as requested. There are no missing aspects, as the issue is solely about adding this helper method, not about refactoring existing usages or changing documentation.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14832": {
        "explanation": "The issue describes that the floatformat template filter does not respect the current localization setting (as controlled by {% localize %} blocks) because it does not pass the use_l10n parameter to formats.number_format. The patch adds a use_l10n parameter to floatformat and passes it through to formats.number_format in both places where it is called. This allows callers to explicitly control localization, making the filter independent of the global USE_L10N setting and compatible with template-level localization control. The patch does not introduce unrelated changes and directly addresses the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14855": {
        "explanation": "The issue describes that, in a custom Admin Site, the URL generated for a ForeignKey field in readonly_fields does not respect the custom admin site's URL namespace, defaulting to '/admin/...' instead of '/custom-admin/...'. The root cause is that the reverse() call does not specify the current_app parameter, which is necessary for Django to resolve the correct URL namespace for the custom admin site. The patch modifies the reverse() call to include current_app=self.model_admin.admin_site.name, which is the correct way to ensure the URL is generated in the context of the current admin site. The change is minimal, targeted, and does not introduce unrelated modifications. It directly addresses the problem as described in the issue, following the pattern used elsewhere in Django's admin codebase. There are no signs of missing edge cases or unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14915": {
        "explanation": "The issue describes a TypeError caused by ModelChoiceIteratorValue not being hashable, which breaks code that uses instances of this class as dictionary keys (e.g., 'if value in self.show_fields', where self.show_fields is a dict). The patch adds a __hash__ method to ModelChoiceIteratorValue, delegating to the hash of its .value attribute. This makes ModelChoiceIteratorValue hashable, consistent with its __eq__ implementation (which also compares .value). The patch is minimal and directly addresses the problem without unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-14999": {
        "explanation": "The issue describes that a RenameModel operation in Django migrations should be a no-op (i.e., not perform any database changes) if the model has a custom db_table defined. The patch adds a test (in fact, it adds the same test multiple times, which is redundant) that verifies this behavior: it creates a model with a custom db_table, applies a RenameModel operation, and asserts that the table name does not change and that the new default table name does not exist. This test correctly checks the desired behavior. However, the patch only adds tests and does not change the migration framework's implementation to ensure that RenameModel is a no-op in this case. If the underlying code does not already have this behavior, the patch would be incomplete. Additionally, the test is duplicated several times, which is unnecessary and could be considered noise. Therefore, the patch is correct in intent but incomplete in ensuring the behavior is enforced, and it is imprecise due to the redundant test additions.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "django__django-15022": {
        "explanation": "The issue describes a performance problem in Django admin search: when searching with multiple words, each word results in an additional JOIN due to repeated queryset.filter(...) calls, leading to slow queries. The patch changes the logic in ModelAdmin.get_search_results so that instead of filtering the queryset for each search term (which would stack filters and thus JOINs), it accumulates all the Q objects for each search term using the |= operator (logical OR), and then applies a single filter at the end. This prevents the repeated JOINs and should result in a single, more efficient query. The change is focused and does not introduce unrelated modifications. The small change in views/main.py (adding 'and self.list_select_related') is a minor fix to ensure select_related is only applied when appropriate, which is tangentially related to query efficiency but not unrelated. Overall, the patch directly and precisely addresses the described issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15037": {
        "explanation": "The issue describes that inspectdb does not correctly generate a ForeignKey to a specific field (i.e., to_field) when the referenced column is not the primary key. The patch modifies the logic in inspectdb to include the to_field argument in the generated ForeignKey if the referenced field is not the primary key. It also updates the PostgreSQL introspection code to extract the referenced column name and passes it through to the inspectdb logic. Furthermore, the patch adds a test that sets up the described schema, runs inspectdb, and asserts that the generated model includes to_field='other_id' in the ForeignKey definition, which directly addresses the issue. The changes are focused and do not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15098": {
        "explanation": "The issue describes that Django's i18n_patterns does not recognize language codes with both script and region subtags (e.g., 'en-Latn-US'), resulting in 404s for such URLs, even when they are listed in LANGUAGES. The patch modifies get_supported_language_variant to generate and check more variations of the language code, including those with script and region, and adds logic to match supported codes with similar script or region. It also adds a test that posts 'en-Latn-US' to setlang and expects the language cookie to be set and a redirect, which matches the expected behavior. The changes are focused and directly address the described problem, without introducing unrelated modifications. The patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15103": {
        "explanation": "The issue requests making the 'element_id' argument of the 'json_script' filter optional, so that users can use the filter without specifying an id attribute on the <script> tag. The patch updates both the filter and the underlying utility function to make 'element_id' default to None. It then adjusts the logic so that if 'element_id' is not provided, the <script> tag is rendered without an id attribute. The patch also adds a test to verify the new behavior. No unrelated changes are made, and the patch is precise and complete in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15104": {
        "explanation": "The issue describes a KeyError that occurs in the migration autodetector when it tries to delete the 'to' key from the deconstructed kwargs of a custom ForeignKey field, but that key may have already been removed by the field's custom deconstruct method. The proposed patch changes 'del deconstruction[2][\"to\"]' to 'deconstruction[2].pop(\"to\", None)', which safely removes the key if present and does nothing if it's missing, thus preventing the KeyError. The patch is minimal and directly addresses the described problem without introducing unrelated changes. The included test files demonstrate the scenario and confirm the fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15127": {
        "explanation": "The issue is that LEVEL_TAGS is initialized at import time, so changes to MESSAGE_TAGS via @override_settings in tests are not reflected in the Message.level_tag property. The patch removes the module-level LEVEL_TAGS and instead fetches the level tags dynamically from utils.get_level_tags() each time level_tag is accessed. This ensures that any changes to MESSAGE_TAGS, including those made by @override_settings, are respected. The patch also adds a test that uses @override_settings to set a custom tag and verifies that Message.level_tag returns the correct value. No unnecessary changes are made, and the patch is focused and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15128": {
        "explanation": "The issue describes an AssertionError in Query.change_aliases when the change_map has intersecting keys and values (e.g., {'T4': 'T5', 'T5': 'T6'}), which can occur when combining certain QuerySets. The patch removes the assertion that keys and values are disjoint and instead introduces logic to resolve such conflicts by generating new unique aliases as needed. This ensures that aliases are not changed twice and avoids the assertion error. The patch updates all relevant references (select, group_by, annotations, alias_map, alias_refcount, external_aliases) using the resolved change_map. The added tests specifically exercise various QuerySet combinations, including those that would previously trigger the assertion, and check for commutativity and correctness of results. No unrelated changes are made. The patch is focused, addresses the root cause, and is precise in its fix.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15161": {
        "explanation": "The issue requests that expressions like Value deconstruct to their simplified import path (e.g., 'django.db.models.Value' instead of 'django.db.models.expressions.Value'), matching the change previously made for F().\n\nThe patch:\n- Updates the @deconstructible decorator for Expression and Value to specify the simplified path ('django.db.models.Expression' and 'django.db.models.Value').\n- Updates the tests to expect the new deconstruction path ('django.db.models.Value').\n- Does not make unrelated changes to other expressions or code.\n- Adds test infrastructure and migrations, but these are necessary for the test to run and do not affect the core logic.\n\nThe patch is focused, addresses the issue as described, and does not introduce unnecessary changes. It is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15252": {
        "explanation": "The issue describes that MigrationRecorder does not respect the database router's allow_migrate rules, leading to migration tables being created in databases where migrations should not be applied. The patch adds checks to ensure that before any schema creation or migration record operations (in ensure_schema, record_applied, and record_unapplied), the router's allow_migrate method is consulted. If allow_migrate returns False, these operations are skipped, which directly addresses the issue described. The patch does not introduce unrelated changes and is limited to the necessary methods. The flush method is left unchanged, which is appropriate since it only deletes records and does not create tables. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15268": {
        "explanation": "The issue describes a need to optimize sequences of AlterUniqueTogether and AlterIndexTogether operations, specifically when an operation that clears constraints (sets to empty) is immediately followed by one that sets new constraints. The goal is to reduce such pairs to just the latter operation, avoiding unnecessary intermediate steps.\n\nThe patch introduces a `reduce` method to both AlterUniqueTogether and AlterIndexTogether. In each, if the current operation sets the constraint to an empty set and the next operation sets a value, the method returns only the latter operation, effectively skipping the intermediate emptying step. This directly implements the optimization described in the issue. The patch also adds a more general `reduce` method to AlterTogetherOptionOperation, which handles further reduction logic and defers to the superclass as needed.\n\nThe changes are focused and do not introduce unrelated modifications. The logic matches the described optimization, and the patch is precise in its scope.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15277": {
        "explanation": "The issue describes that CharField's __init__ method always adds a MaxLengthValidator, even when max_length is None, which is unnecessary and can cause errors (e.g., TypeError when comparing int and NoneType). The proposed patch changes CharField.__init__ to only append MaxLengthValidator if self.max_length is not None, which matches the suggested fix in the issue description. This change is minimal and directly addresses the problem without introducing unrelated modifications. The addition of reproduce.py is just for demonstration and does not affect the core code. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15278": {
        "explanation": "The issue is that adding a nullable OneToOneField (which is implemented as a UNIQUE column) to an existing SQLite table via ALTER TABLE ADD COLUMN fails, because SQLite does not allow adding a UNIQUE column in this way. The correct workaround is to remake the table when such a field is added, as is done for other schema changes that SQLite cannot handle directly. The patch checks if either the old or new field is a OneToOneField, and if so, calls self._remake_table, which is the standard Django approach for handling schema changes that require more than a simple ALTER TABLE. The patch is minimal and does not introduce unrelated changes. Therefore, it correctly and precisely addresses the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15280": {
        "explanation": "The issue describes a bug where, after using 'only' and 'prefetch_related' with nested prefetches, Django incorrectly considers some fields as deferred, causing unnecessary database queries when accessing those fields, even though they were loaded by an inner prefetch. The patch addresses this by:\n\n1. Tracking which fields are loaded for each instance and merging field values between instances of the same model and primary key found at different levels of the prefetch tree.\n2. Updating the logic in model instantiation, deferred attribute access, and prefetching to merge field values between such instances, ensuring that if a field is loaded in any instance, it is available in all instances with the same PK.\n3. Extending get_deferred_fields and DeferredAttribute to check for loaded fields in related instances before triggering a database fetch.\n\nThe changes are focused on the described problem, do not introduce unrelated modifications, and cover the described scenario (and similar ones) by ensuring that field values are shared between all instances representing the same DB row in the ORM graph. The patch is thorough and precise in its approach, directly addressing the root cause of the issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15315": {
        "explanation": "The issue describes a problem where the hash of a Django model Field instance changes after it is assigned to a model class, due to the inclusion of model-specific attributes in the __hash__ method (introduced in #31750). This causes problems when a Field is used as a dictionary key before assignment, as its hash changes and it can no longer be found in the dictionary. The patch reverts the __hash__ method to only use the field's creation_counter, removing references to self.model and thus making the hash stable for the lifetime of the Field instance. This directly addresses the issue described. The patch also updates __copy__ to ensure creation_counter is handled correctly, and adds a test script that reproduces the bug and verifies the fix. No unnecessary changes are made. The patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15316": {
        "explanation": "The issue describes that simplify_regex() does not handle non-capturing groups (i.e., groups of the form (?:...)). The original code uses a regex pattern unnamed_group_matcher = _lazy_re_compile(r'\\('), which matches any opening parenthesis, including those for non-capturing groups. The patch changes this to unnamed_group_matcher = _lazy_re_compile(r'\\((?!\\?:)'), which uses a negative lookahead to exclude matches for non-capturing groups (i.e., it matches '(' not followed by '?:'). This directly addresses the issue by ensuring that non-capturing groups are not treated as unnamed capturing groups. The change is minimal and targeted, with no unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15352": {
        "explanation": "The issue requests that session IDs be cleansed from error reports, as they are sensitive credentials. The patch addresses this in two ways: (1) It adds 'SESSION' to the regex used to identify sensitive settings, so any setting key containing 'SESSION' will be cleansed. (2) In the get_post_parameters method, it explicitly checks for 'sessionid' in request.COOKIES and cleanses it, and also forcibly replaces 'SESSION_KEY' with the cleansed substitute if present. These changes directly target the exposure of session IDs in error reports, as described in the issue. The patch does not introduce unrelated changes and is focused on the described problem. The approach is thorough, covering both settings and cookies, and is precise to the issue's requirements.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15368": {
        "explanation": "The issue describes that assigning an F() expression to a model field and then using bulk_update results in the string representation of the F() object being written to the database, rather than the intended SQL column reference. The root cause is a type check that only recognizes instances of Expression, but F() is a subclass of Expression and should be handled as such. The issue suggests replacing the type check with a duck-typed check for 'resolve_expression', or explicitly including F in the isinstance check.\n\nThe patch changes the check from 'isinstance(attr, Expression)' to 'hasattr(attr, \"resolve_expression\")', which is a duck-typed approach and will correctly recognize F() and any other expression-like objects. This directly addresses the issue. The test added in the patch creates an Author object, assigns an F expression to a field, performs bulk_update, and checks that the value is updated as expected, confirming the fix.\n\nNo unrelated changes are made, and the patch is both minimal and precise. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15375": {
        "explanation": "The issue describes a crash when using the 'default' argument to aggregates after annotate(), due to the generated SQL being invalid: 'SELECT FROM (SELECT ...)', which is missing the columns to select. The root cause is that the COALESCE wrapping is being applied at the wrong level, resulting in malformed SQL. The patch changes the as_sql method to wrap the aggregate SQL in COALESCE if a default is provided, but it does so before handling the filter clause and before returning the final SQL. This means that when used in a subquery (as with annotate + aggregate), the COALESCE is still applied at the wrong level, and the generated SQL will still be invalid. The patch does not address the core problem: the placement of COALESCE must be handled in the context of the outer query, not inside the subquery. Therefore, the patch fundamentally misunderstands the requirements and does not fix the issue.",
        "label": "INCORRECT"
    },
    "django__django-15380": {
        "explanation": "The issue describes a KeyError in the migration autodetector when both a model and a field are renamed in a single step. The error occurs because the code attempts to access models in self.from_state.models and self.to_state.models using the original model name, which may have changed (renamed), leading to a missing key. The patch modifies the code to use old_model_name.lower() as the key when accessing these dictionaries. This is likely based on the fact that model keys are stored in lowercase in the models dicts. The patch is minimal and directly addresses the line that causes the KeyError, without introducing unrelated changes. However, it assumes that all model keys are always lowercased, which is generally true in Django's migration state, but if there are edge cases where the key is not lowercased, this could fail. Still, for the described regression and the typical usage, this patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15382": {
        "explanation": "The issue describes a problem where filtering with a negated Exists subquery over an empty queryset results in the entire WHERE block being removed, leading to an EmptyResultSet. The patch modifies the Exists class's as_sql method to check if the subquery is empty (query.is_empty()). If so, it substitutes the SQL with '(SELECT 0 WHERE FALSE)', which is a subquery that always returns no rows, and sets params to an empty list. This ensures that the EXISTS clause is preserved in the SQL, even for empty subqueries. Additionally, the patch wraps the negated SQL in parentheses, which is a minor but correct SQL syntax improvement. The patch is focused and does not introduce unrelated changes. It directly addresses the issue by ensuring the WHERE block is not removed for negated empty EXISTS subqueries. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15423": {
        "explanation": "The issue describes a problem where LazyObject exposes magic methods (like __getitem__, __iter__) even if the wrapped object does not implement them, leading to misleading behavior: hasattr returns True, but using the method raises a TypeError. The patch replaces the blanket new_method_proxy(getattr) with a custom __getattr__ that, for magic methods (those starting with '__'), checks if the wrapped object actually has the attribute before returning a proxy to it. If not, it raises AttributeError, which is the correct Python behavior and ensures hasattr returns False for missing magic methods. This directly and precisely addresses the issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15467": {
        "explanation": "The issue describes that when a user sets a custom 'empty_label' in 'formfield_for_foreignkey', it is always overridden by the default value in ModelAdmin when 'radio_fields' is used. The proposed fix is to only set the default 'empty_label' if one is not already present in 'kwargs'. The patch changes the assignment to: 'kwargs[\"empty_label\"] = (kwargs.get('empty_label') or _('None')) if db_field.blank else None', which means if 'empty_label' is already set in 'kwargs', it will be used; otherwise, it falls back to the default. This directly and precisely addresses the issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15474": {
        "explanation": "The issue describes that when rolling back the ContentType migration 0002 on a non-default database (using a database alias), the legacy 'name' field is not populated because the migration code only operates on the default database. The patch modifies the migration's data migration function to use the correct database alias (db_alias) from the schema_editor, both when querying and saving ContentType objects. This ensures that the operation is performed on the correct database, addressing the issue precisely without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15498": {
        "explanation": "The issue describes a regression where an empty string for the 'If-Modified-Since' header, which used to be ignored, now raises an exception. The patch addresses this in two places: (1) In 'get_conditional_response', it checks if 'if_modified_since' is an empty string and sets it to None, ensuring that downstream code does not process an empty string. (2) In 'parse_http_date_safe', it returns None immediately if the input is an empty string, preventing an exception from being raised. Both changes are minimal, directly related to the issue, and do not introduce unrelated modifications. The patch precisely restores the previous behavior of ignoring empty strings for this header.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15499": {
        "explanation": "The issue requests that when a CreateModel operation is immediately followed by an AlterModelManagers operation (for the same model), the two should be optimized into a single CreateModel operation with the managers from AlterModelManagers. The patch adds a new branch to the optimization logic: if the next operation is an AlterModelManagers for the same model, it returns a new CreateModel with the same fields, options, and bases, but with the managers from the AlterModelManagers operation. This mirrors the existing optimization for AlterModelOptions. The patch is focused, does not introduce unrelated changes, and directly implements the requested optimization. There are no obvious missing edge cases given the scope of the issue (the optimization is only for the direct sequence of CreateModel + AlterModelManagers).",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15503": {
        "explanation": "The issue describes that JSONField lookups like has_key fail to find numeric keys (e.g., '1111') in SQLite, while they work in PostgreSQL. The patch modifies the HasKeyLookup class to generate backend-specific SQL for SQLite, MySQL, and Oracle, using JSON_TYPE, JSON_CONTAINS_PATH, and JSON_EXISTS respectively, to check for the existence of a key. This is a more robust approach than the previous generic path-based lookup, which could misinterpret numeric keys as array indices rather than object keys. The patch also adds comprehensive tests for numeric and string keys, including negative and float-like keys, across all relevant lookups (has_key, has_keys, has_any_keys). The changes are focused and do not introduce unrelated modifications. Therefore, the patch correctly and precisely addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15525": {
        "explanation": "The issue describes a problem where loading data with natural keys into a non-default database fails because the deserialized model instance is not associated with the correct database. This leads to lookups (such as for related objects via natural keys) being performed on the default database instead of the specified one, causing DoesNotExist errors when the data is not present in the default database.\n\nThe patch adds a line to set obj._state.db = using immediately after building the instance. This ensures that any subsequent database operations (such as resolving foreign keys via natural keys) are performed on the correct database. The change is minimal and directly targets the root cause described in the issue, without introducing unrelated modifications.\n\nTherefore, the patch is both correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15526": {
        "explanation": "The issue describes a bug where the 'view on site' link in the Django admin does not respect the current (possibly custom) admin site because the 'reverse' call in 'get_view_on_site_url' lacks the 'current_app' keyword argument. The patch adds 'current_app=self.admin_site.name' to the relevant 'reverse' call, aligning it with other reverse calls in the file and ensuring the URL is resolved in the context of the current admin site. The change is minimal, targeted, and does not introduce unrelated modifications. It directly addresses the described bug without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15554": {
        "explanation": "The issue describes a problem where using multiple FilteredRelation aliases with different filters on the same relation results in only one SQL JOIN being generated, causing the other filtered relation to be ignored. The expected behavior is that each FilteredRelation, even if on the same relation but with different filters, should result in its own JOIN in the SQL query.\n\nThe patch changes the logic for join reuse: previously, joins could be reused for many-to-many (m2m) relationships, but now, if a filtered_relation is present, reuse is explicitly set to None, ensuring a new join is created for each filtered relation. This directly addresses the issue by ensuring that each FilteredRelation gets its own JOIN, regardless of whether the underlying relation is the same.\n\nThe change is minimal and targeted, affecting only the reuse logic when filtered relations are involved, and does not introduce unrelated changes. Therefore, the patch is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15561": {
        "explanation": "The issue describes that on SQLite, adding or changing the 'choices' attribute on a model field should not trigger a database schema migration, as it is a non-database attribute (i.e., it does not affect the actual database schema). The patch adds 'choices' to the 'non_database_attrs' list in the schema editor, which is used to determine if a field change requires a database operation. By including 'choices' in this list, the migration system will recognize that changes to 'choices' do not require a database schema change, thus preventing unnecessary SQL from being generated. The patch is minimal and directly addresses the issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15563": {
        "explanation": "The issue describes a bug in Django's queryset update when dealing with multiple inheritance: updating a Child queryset incorrectly updates parent table rows (e.g., OtherBase) instead of only the Child's rows. The patch changes the QuerySet.update() method to always use the concrete model (the actual subclass, not a parent) when constructing the UpdateQuery. This ensures that updates are performed on the correct table and only affect the intended rows. The patch also adds a test that creates instances of Base, OtherBase, and Child, performs an update on Child, and verifies that only Child's rows are updated, not the parent tables. The changes are minimal, targeted, and directly address the described bug without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15569": {
        "explanation": "The issue describes that RegisterLookupMixin._unregister_lookup() should clear the lookup cache, similar to what is done in register_lookup(). The patch adds a call to cls._clear_cached_lookups() immediately after deleting the lookup from class_lookups, which directly addresses the issue. The patch does not introduce unrelated changes. The addition of a reproduce_lookup.py file is a minimal test script to demonstrate the fix, but it does not alter the core logic. The patch is focused and precise in its fix.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15572": {
        "explanation": "The issue describes a problem where an empty string in TEMPLATES['DIRS'] is normalized to the project root, causing the autoreload mechanism to always return True and thus break expected behavior. The patch adds a check to ensure that 'template_dir' is a directory (using os.path.isdir(template_dir)) before checking if it is in file_path.parents. This prevents the empty string (which would resolve to the root directory) from being considered, thus avoiding the erroneous always-True return. The change is minimal, directly addresses the described regression, and does not introduce unrelated modifications. Therefore, the patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15576": {
        "explanation": "The issue describes an inefficiency in the .exists() method for distinct querysets: after a previous fix, all distinct querysets select all their fields, even when not necessary. The reporter suggests that fields should only be left on the queryset if the queryset is both distinct and sliced (i.e., has high_mark or low_mark set). The patch changes the condition from 'if not q.distinct' to 'if not q.distinct or not (q.high_mark is not None or q.low_mark is not None)', which means that fields will only be left on the queryset if it is both distinct and sliced, matching the reporter's proposal. The patch is a minimal, targeted change and does not introduce unrelated modifications. Therefore, it precisely addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15586": {
        "explanation": "The issue requests enabling the cached template loader by default in development, following changes that allow auto-reloading of cached templates. The patch modifies the default TEMPLATES setting in global_settings.py to use the cached loader wrapping the filesystem and app_directories loaders. This matches the request: the cached loader is now enabled by default, and users can override this if they do not want it. The patch does not introduce unrelated changes and is precise in its implementation.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15629": {
        "explanation": "The issue describes a problem where, in MySQL, if a primary key column has a specific db_collation, the corresponding foreign key columns must also have the same collation, or foreign key constraint creation will fail. The patch addresses this in two ways:\n\n1. In `django/db/backends/mysql/schema.py`, it modifies the `_alter_column_type_sql` method to append the referenced field's collation to the new type for foreign key fields. This ensures that when a foreign key column is created or altered, it will have the same collation as the referenced primary key.\n\n2. In `django/db/backends/base/schema.py`, it updates the SQL for creating foreign keys to optionally include a collation clause, and ensures that the collation is passed in if present. This is a more general fix that could help with other backends that support collations on constraints.\n\n3. The new test in `tests/db_functions/test_collation_propagation.py` verifies that the collation of the foreign key column matches the referenced column, specifically for MySQL (and skips the check for SQLite, which doesn't support collations in the same way).\n\nThe patch is focused on the described problem, does not introduce unrelated changes, and the test covers the main scenario. There may be edge cases (e.g., multi-column foreign keys, or more exotic field types), but for the described use case, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15643": {
        "explanation": "The issue describes a problem where, during migrations on SQLite, an explicit primary key is dropped when the primary key is altered. The patch attempts to address this by, after restoring the primary key field, executing an ALTER TABLE ... ADD PRIMARY KEY statement to re-add the primary key constraint. However, SQLite does not support adding a PRIMARY KEY constraint to an existing table using ALTER TABLE; this will result in a syntax error. The correct approach in Django's migration system is to recreate the table with the correct schema and copy the data, as SQLite's ALTER TABLE is limited. Therefore, the patch fundamentally misunderstands SQLite's capabilities and does not correctly address the issue.",
        "label": "INCORRECT"
    },
    "django__django-15695": {
        "explanation": "The issue describes a crash when re-applying RenameIndex on an unnamed index, specifically that the old auto-generated name is not restored during a backwards migration, leading to a duplicate index name error on subsequent forwards migrations. The patch modifies the database_backwards method to restore the old auto-generated name by constructing it using the schema_editor's _create_index_name method and then renaming the index back to this name. This directly addresses the problem described: it ensures that the index is properly renamed back to its original auto-generated name, preventing the duplicate name error on re-application. The patch is focused and does not introduce unrelated changes. It also preserves the no-op for cases where old_fields is not set, as before. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15731": {
        "explanation": "The issue is that manager methods dynamically created from queryset methods do not preserve the original method's signature, so inspect.signature returns (*args, **kwargs) instead of the actual signature. The patch replaces the manual assignment of __name__ and __doc__ with the use of functools.wraps, which copies all relevant metadata, including the signature, from the original method to the wrapper. This directly addresses the problem described in the issue, and the patch does not introduce any unrelated changes. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15732": {
        "explanation": "The issue describes a problem where attempting to drop a unique_together constraint on a field that also has a unique constraint (such as a primary key or unique=True) fails, because Django's migration code expects only one constraint but finds multiple (e.g., both the PK and a unique_together constraint). The original code raises an error if more than one constraint is found, which is the root of the problem.\n\nThe patch changes the logic in _delete_composed_index to iterate over all matching constraint names and attempt to drop each, ignoring errors if a constraint does not exist. This approach ensures that all relevant constraints are attempted to be dropped, and it does not fail if there are multiple constraints (such as both a PK and a unique constraint on the same field). This directly addresses the issue described.\n\nThe patch does not introduce unrelated changes and is focused on the described problem. The added tests specifically check that removing a unique_together constraint does not remove a unique=True constraint, and that the migration does not fail when both exist. This confirms the patch is both correct and precise for the described scenario.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15741": {
        "explanation": "The issue describes a regression where django.utils.formats.get_format fails if its format_type parameter is a lazy string (such as a translation object), resulting in a TypeError. The patch addresses this by explicitly converting format_type to a string using force_str at the start of get_format. This ensures that any lazy string passed in will be properly converted to a regular string, preventing the TypeError. The patch also adds a targeted test that passes a lazy string to get_format and verifies the result is a usable string, which directly tests the regression described. No unrelated or unnecessary changes are present. Therefore, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15744": {
        "explanation": "The issue describes a problem where the ASGI handler closes the request body file before the view (or middleware) has a chance to access it, leading to I/O on a closed file if request.body is accessed in the view. The patch moves the 'body_file.close()' call from after the response is sent to a 'finally' block that still executes after the response is sent, but crucially, it now closes the file only after the response has been generated and sent. This ensures that the view and response generation have access to the request body file before it is closed. The patch does not introduce unrelated changes and directly addresses the premature closing issue. The change to 'request.body' in 'django/http/request.py' to make it async is unrelated to the described issue and could be considered unnecessary or even incorrect, as Django's HttpRequest.body is expected to be a synchronous property. However, the main fix in the ASGI handler is correct and precise for the described issue. The test script is just for reproduction and does not affect the patch's correctness.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15814": {
        "explanation": "The issue arises when using select_related() and only() on a ForeignKey to a proxy model, leading to a crash because the code attempts to access the primary key field via the proxy model's _meta, which does not include the PK field directly. The user notes that replacing 'opts = cur_model._meta' with 'opts = cur_model._meta.concrete_model._meta' resolves the issue, as this ensures the metadata includes the PK field from the concrete model. The patch makes exactly this change, and the comment added clarifies the intent. No unrelated or unnecessary changes are introduced. The patch is precise and directly addresses the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15851": {
        "explanation": "The issue describes that when using Django's dbshell with PostgreSQL, any additional parameters (such as -c) must be passed before the database name, as psql expects the database name to be the last argument. The original code appended parameters after the database name, causing psql to ignore them. The patch changes the order so that parameters are added before the database name, ensuring the correct argument order for psql. No unrelated changes are made, and the patch directly addresses the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15863": {
        "explanation": "The issue describes a loss of precision in the floatformat template filter when handling Decimal numbers, due to an unnecessary conversion to float or string representations that do not preserve all decimal places. The patch modifies the floatformat filter to check if the input is already a Decimal and, if so, uses it directly instead of converting it via repr() and then to Decimal again. This change directly addresses the root cause of the precision loss for Decimal inputs, as described in the issue. The patch does not introduce unrelated changes and is focused solely on the relevant code path. The addition of a reproduce.py file is for demonstration and does not affect the filter's implementation. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15902": {
        "explanation": "The issue describes a deprecation warning being raised because ManagementForm uses the default form rendering template (\"default.html\"), which is deprecated. The user notes that ManagementForm only produces hidden inputs, so the template used is not significant, and suggests special-casing it to avoid the warning.\n\nThe patch introduces a custom ManagementFormRenderer for ManagementForm, which uses the new \"div.html\" template instead of the deprecated \"default.html\". It sets this renderer as the default_renderer for ManagementForm. This change is narrowly scoped to ManagementForm and does not affect other forms. No unrelated changes are made.\n\nThis directly addresses the issue: ManagementForm will no longer trigger the deprecation warning, and the change is precise and minimal. The patch does not introduce unnecessary changes and fully resolves the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15916": {
        "explanation": "The issue describes that when using modelform_factory with a custom form class whose Meta includes a formfield_callback, the resulting form class should inherit and use that callback if no explicit formfield_callback is passed to modelform_factory. The original code did not do this: it would ignore the Meta.formfield_callback of the base form and use None unless explicitly provided.\n\nThe patch modifies modelform_factory to:\n- Check if the parent form's Meta has a formfield_callback, and if so, use it unless an explicit formfield_callback is provided.\n- Set the formfield_callback as a staticmethod on the new Meta class if present.\n\nThe patch also adds targeted tests that:\n- Confirm the callback from Meta is used if not overridden.\n- Confirm that an explicit formfield_callback argument to modelform_factory takes precedence.\n\nThe changes are minimal and directly address the described issue, without introducing unrelated modifications. The new logic is precise and the tests cover the expected and overridden behaviors.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15930": {
        "explanation": "The issue is that using ~Q(pk__in=[]) in a When() clause generates invalid SQL: the condition is empty, so the SQL is 'CASE WHEN THEN ...', which is a syntax error. The patch checks for the specific case where the condition is a negated Q object with no children (i.e., ~Q(pk__in=[])), and in that case, emits 'TRUE' as the SQL condition, which is valid and matches the expected semantics (all rows match). The patch does not introduce unrelated changes and includes a test that covers the reported scenario. The solution is targeted, correct, and does not overreach.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15957": {
        "explanation": "The issue describes that using a sliced queryset in Prefetch (e.g., queryset=Post.objects.all()[:3]) raises an AssertionError, making it impossible to prefetch only a limited number of related objects per parent. The patch introduces a 'slice' parameter to the Prefetch class, stores it, and applies it at the correct point in the prefetching process. It also adds a test that verifies the correct behavior: only the first 3 related objects are prefetched per parent, and accessing the full set later triggers a new query. The patch does not introduce unrelated changes and directly addresses the inability to use sliced querysets in Prefetch, as described in the issue. The solution is precise and includes a regression test for the new behavior.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15973": {
        "explanation": "The issue describes a problem where specifying the 'through' argument of a ManyToManyField as a string reference to a model in another app (e.g., 'fonte_variavel.FonteVariavelModel') leads to an AttributeError during migration, because the string is not resolved to an actual model class before being used. The patch adds logic to resolve the 'through' argument if it is a string, using apps.get_model, and raises a clear error if the string is not in the correct format or the model cannot be found. This directly addresses the root cause of the issue: the 'through' argument remaining a string instead of being resolved to a model class. The patch does not introduce unrelated changes and is focused on the described problem. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-15987": {
        "explanation": "The issue is that when settings.FIXTURE_DIRS contains Path instances, the duplicate check in loaddata does not detect duplicates, because set() and list() comparisons between Path and str do not consider them equal even if they point to the same path. The patch addresses this by converting all entries in FIXTURE_DIRS to str before performing the duplicate check and before comparing with app_dir. It also ensures that all paths are converted to str before being passed to os.path.realpath. No unrelated changes are made. The patch is precise and directly addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16032": {
        "explanation": "The issue is that when using QuerySet.alias() after annotate(), and then using that queryset as the RHS of an __in filter, the subquery does not reduce to a single column as expected, leading to a database error about the sub-select returning too many columns. The patch adds a check for expressions with an 'expression' attribute (which is typical for annotated and aliased expressions) and recursively yields columns from that attribute. This ensures that when collecting columns for the subquery, only the relevant expressions are considered, and extraneous columns introduced by annotate/alias are not included in the subquery's output. The patch is minimal, targeted, and directly addresses the described bug without introducing unrelated changes. It is therefore correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16082": {
        "explanation": "The issue describes a problem where combining numeric expressions with the MOD operator does not resolve the output_field type correctly, particularly when mixing Decimal and Integer types. The patch adds Combinable.MOD to a tuple of operators (alongside SUB, MUL, DIV) in the code responsible for resolving output_field types. This change ensures that MOD is treated like the other mathematical operators, so the output_field will be resolved appropriately when using MOD between different numeric types. The patch is minimal and directly addresses the described issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16100": {
        "explanation": "The issue requests that the list_editable processing in the Django admin's changelist_view be wrapped in a transaction to prevent partial updates in case of errors. The patch introduces a 'with transaction.atomic()' block around the loop that saves changed forms in the formset. This ensures that all changes are committed atomically, and if an error occurs, none of the changes are saved, which is the desired behavior. The patch does not introduce unrelated changes and is scoped precisely to the area where database writes occur. Therefore, it is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16116": {
        "explanation": "The issue describes that running 'makemigrations --check' should not write migration files, but should simply check for changes and exit, aligning its behavior with other Django commands that use --check. The original code only exited with sys.exit(1) after writing migration files, which is inconsistent with the expected behavior. The patch moves the 'if check_changes: sys.exit(1)' block before any writing of migration files, ensuring that when --check is used, the command exits before any migrations are written. This change is minimal, directly addresses the issue, and does not introduce unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16136": {
        "explanation": "The issue is that when a Django View subclass only implements an async 'post' method, and a GET request is made, the default handler for unsupported methods returns an HttpResponseNotAllowed synchronously. However, in the async view context, Django expects an awaitable, so returning a plain HttpResponseNotAllowed causes a TypeError when it is awaited. The patch checks if the view is async (self.view_is_async), and if so, wraps the response in an async function and returns its result, making it awaitable. Otherwise, it returns the response directly as before. This directly addresses the issue without introducing unrelated changes or missing key aspects. The patch is precise and correct.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16139": {
        "explanation": "The issue describes a problem where the password reset link in the UserChangeForm is broken when accessing the UserAdmin via a URL that uses a non-default field (e.g., uuid) due to the relative path '../password/' being incorrect. The proposed patch changes the help_text formatting from '../password/' to '../../{self.instance.pk}/password/', which correctly constructs the link relative to the current URL structure, regardless of whether the user is accessed by pk or another field. The patch is minimal and directly addresses the described issue without introducing unrelated changes. The addition of 'reproduce_issue.py' is just for demonstration and does not affect the core fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16142": {
        "explanation": "The issue describes that get_language_from_request should not fallback to settings.LANGUAGE_CODE, but instead return None if no language is found from the request. This allows middleware to distinguish between a language chosen by the request and the default application language. The patch removes the fallback to settings.LANGUAGE_CODE and instead returns None, as requested. No unrelated changes are made, and the patch is precise and directly addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16145": {
        "explanation": "The issue is that when running 'python manage.py runserver 0:8000', Django outputs 'Starting development server at http://0:8000/', but it should output 'http://0.0.0.0:8000/' for consistency and usability. The patch modifies the regular expression used to parse the address so that '0' is explicitly matched as '0.0.0.0'. This is done by adding a non-capturing group for '0.0.0.0' before the IPv4 pattern. This change ensures that when '0' is provided, it is recognized as '0.0.0.0', which will result in the correct output. The patch is minimal and directly addresses the parsing issue without unrelated changes. It does not appear to introduce regressions or miss edge cases relevant to the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16254": {
        "explanation": "The provided patch does not address the issue described in the GitHub issue. The issue is about a regression in Django's migration behavior on SQLite: when adding a ManyToManyField, Django 4.1+ unnecessarily rebuilds the main table, whereas previously it only created the M2M through table. The patch, however, only adds a new Django test project skeleton with two models (A and B) and no ManyToManyField, and does not touch any Django migration or schema editor logic. There are no changes to the Django codebase or migration logic that would restore the special-casing for implicit M2M tables or prevent the unnecessary table rebuild. Therefore, the patch does not address the described issue at all.",
        "label": "INCORRECT"
    },
    "django__django-16255": {
        "explanation": "The issue describes a ValueError being raised when get_latest_lastmod() is called on a Sitemap with no items and a callable lastmod, because max() is called on an empty sequence. The proposed patch updates the exception handling in get_latest_lastmod to catch both TypeError and ValueError, returning None if either occurs. This directly addresses the problem described: it prevents the ValueError from propagating and returns None as expected. The patch does not introduce unrelated changes and is minimal. The addition of the test script (reproduce_error.py) is not part of the library code and does not affect the fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16256": {
        "explanation": "The issue describes that async methods (acreate, aget_or_create, aupdate_or_create) were unintentionally added to related managers, but they simply called the QuerySet versions, which do not handle the related manager's logic (such as setting the correct foreign key or handling through tables for M2M). The patch provides explicit async implementations for these methods on both reverse many-to-one and forward many-to-many related managers. These implementations mirror the logic of their synchronous counterparts: they ensure the correct foreign key is set (for FK) or the correct through table handling (for M2M), and they use sync_to_async to call the appropriate super method. The patch does not introduce unrelated changes and directly addresses the problem described in the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16263": {
        "explanation": "The patch modifies the count() logic to remove annotations that are not referenced in filters or ordering before performing the count aggregation. It iterates over the annotation_select dictionary, and for each annotation, checks if it has refs_aggregate set; if not, it removes the annotation from both annotations and annotation_select. This directly addresses the issue described: unnecessary annotations (such as Count('chapters') in the example) are stripped from the count query, preventing them from affecting the generated SQL. The patch does not introduce unrelated changes and is focused solely on the described problem. The approach is precise and does not appear to miss any key aspects of the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16281": {
        "explanation": "The issue describes a problem when changing a ManyToManyField from pointing to another model (Bar) to being self-referential (Foo). The error occurs because the migration does not properly handle the change in the through table structure, leading to missing columns. The original code only attempts to rename the through table and alter its fields, which is insufficient when the relationship changes to self-referential, as the through table's schema changes (column names and possibly constraints). The patch introduces logic to detect when the new M2M is self-referential and, in that case, creates a new through table and deletes the old one, rather than attempting to rename and alter fields. This directly addresses the described issue and does not introduce unrelated changes. The patch is precise and correct for the described scenario.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16315": {
        "explanation": "The issue describes a bug where bulk_create with update_conflicts generates invalid SQL if the model fields use db_column with mixed case or custom names. Specifically, the ON CONFLICT clause uses the field's attribute name (e.g., 'blacklistid') instead of the actual database column name (e.g., 'BlacklistID'), causing PostgreSQL errors.\n\nThe patch addresses this by:\n- Introducing a helper method _get_db_column_name that returns the correct quoted database column name for a field.\n- Updating the ON CONFLICT clause generation to use this method for both unique_fields and update_fields, ensuring the correct db_column is used in the SQL.\n- Adding a test model with mixed-case db_column values and a test that exercises bulk_create with update_conflicts, update_fields, and unique_fields, verifying correct behavior.\n\nThe patch is focused, only changes the relevant SQL generation, and adds a precise test for the reported scenario. It does not introduce unrelated changes. Therefore, it correctly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16333": {
        "explanation": "The issue describes that UserCreationForm does not save ManyToMany field data because its save() method does not call self.save_m2m(), unlike the default ModelForm behavior. The patch updates the save() method to call self.save_m2m() when commit=True, and when commit=False, it sets up the save_m2m method so it can be called later, matching the ModelForm pattern. This directly and precisely addresses the issue without introducing unrelated changes or missing key aspects. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16398": {
        "explanation": "The issue describes a problem where, in the context of QuerySet.select_related() with multiple filtered relations to a OneToOneField, the last relation is set due to improper handling of the local_setter. The issue points out that the code should use partial for local_setter, similar to how remote_setter is handled. The patch changes 'local_setter: local_setter' to 'local_setter: partial(local_setter)', which aligns with the suggestion and the handling of remote_setter. No unrelated or unnecessary changes are made. Therefore, the patch is correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16429": {
        "explanation": "The issue describes a TypeError that occurs when timesince() is called with a timezone-aware datetime (with USE_TZ=True) and the interval is greater than one month. The error is due to subtracting a naive datetime from an aware one, which is not allowed in Python. The problematic code is where a new datetime object ('pivot') is constructed from the components of 'd', but without preserving its tzinfo, making it naive even if 'd' is aware. The patch adds 'tzinfo=d.tzinfo' to the datetime constructor, ensuring that the new datetime is aware if 'd' is aware, thus preventing the TypeError. The patch is minimal and directly addresses the root cause without unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16454": {
        "explanation": "The issue describes that subparsers created via CommandParser.add_subparsers() do not inherit the custom error formatting behavior of the main CommandParser, leading to stack traces instead of user-friendly error messages when required arguments are missing in subcommands. The patch introduces a custom SubCommandParser class within add_subparsers that inherits from CommandParser and ensures that the custom arguments (missing_args_message, called_from_command_line) are passed to subparsers. It also sets these attributes on the subparsers object. This ensures that subparsers use the same error handling as the main parser, directly addressing the problem described. The patch is focused and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16485": {
        "explanation": "The issue is that floatformat() crashes with a ValueError when given '0.00' or Decimal('0.00') and a precision of 0, due to the calculated 'prec' value being less than 1, which is not allowed by the Decimal.quantize() method. The patch changes the calculation of 'prec' to use max(1, abs(p) + units + 1), ensuring that 'prec' is never less than 1. This directly addresses the root cause of the crash without introducing unrelated changes. The patch is minimal and precise, and it solves the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16493": {
        "explanation": "The issue is that when a FileField is constructed with a callable storage (e.g., storage=get_storage), and that callable returns default_storage, the deconstruction logic omits the storage argument from the deconstructed kwargs. This is because the current check only compares the evaluated storage (self.storage) to default_storage, and if they are the same, it omits the storage kwarg, even if the original argument was a callable. This leads to migrations that sometimes include and sometimes omit the storage argument, depending on what the callable returns at migration time.\n\nThe patch changes the logic so that if the FileField has a _storage_callable attribute (i.e., the storage was provided as a callable), it always includes this callable in the deconstructed kwargs, regardless of what it returns. Only if there is no _storage_callable does it fall back to the old logic of comparing self.storage to default_storage. This directly addresses the issue described, ensuring that the callable is always preserved in the deconstructed form, and thus migrations will be consistent.\n\nThe patch is minimal and precise, only changing the relevant logic in the deconstruct method and not affecting unrelated code.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16502": {
        "explanation": "The issue describes that Django's runserver is returning a response body for HTTP HEAD requests, which is not compliant with RFC 2616. The expected behavior is that the response to a HEAD request should have headers identical to a GET request, but no body.\n\nThe patch adds a get_response method to WSGIHandler that, after calling the superclass's get_response, checks if the request method is HEAD and, if so, sets response.content = b''. This ensures that the response body is stripped for HEAD requests, regardless of the server, including runserver. This directly addresses the issue described.\n\nThe patch also adds a test that verifies:\n- HEAD and GET requests both return 200\n- HEAD response has empty content\n- GET response has non-empty content\n- Headers (Content-Type, Content-Length) are the same for both\n\nThe patch does not introduce unnecessary changes and is focused on the described problem. The only minor concern is that it sets response.content = b'' after the response is generated, which is the correct place to do this in Django's request/response cycle for runserver. It does not affect other servers that already strip the body.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16527": {
        "explanation": "The issue describes a missing permission check for 'has_add_permission' in the logic that determines whether to show the 'save as new' button in the Django admin. The patch adds 'and has_add_permission' to the 'show_save_as_new' condition, ensuring that the button is only shown if the user has both add and change permissions, which matches the issue's request. The patch does not introduce any unrelated changes (the extra newline at the end is harmless and standard in Python files). Thus, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16560": {
        "explanation": "The issue requests the ability to customize the 'code' attribute of the ValidationError raised by BaseConstraint.validate, similar to how 'violation_error_message' can be customized. The patch introduces a 'violation_error_code' attribute, updates the constructor to accept it, ensures it is included in deconstruction, and uses it in a new get_validation_error() method. The changes are minimal and directly related to the issue, without introducing unrelated modifications. The patch does not appear to miss any key aspects of the request and does not introduce unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16569": {
        "explanation": "The issue describes a TypeError that occurs when the 'index' argument to add_fields() is None and is compared to an integer. The proposed patch adds an explicit check that 'index is not None' before performing the comparison 'index < initial_form_count'. This directly addresses the root cause of the exception, as described in the issue, and does so without introducing unrelated changes. The patch is minimal and precise, matching the suggested fix in the issue description. Therefore, it correctly and precisely addresses the problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16595": {
        "explanation": "The issue describes that when multiple consecutive AlterField operations are present (without an AddField), the migration optimizer does not reduce them to just the final AlterField, but instead leaves all of them. The proposed patch adds logic to the AlterField.reduce method to check if the next operation is also an AlterField on the same field (using is_same_field_operation), and if so, returns only the latest AlterField operation. This directly addresses the issue by ensuring that only the final state of the field is kept, as expected. The patch is minimal and does not introduce unrelated changes. The addition of a test script (reproduce_error.py) is for demonstration and does not affect the core logic. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16612": {
        "explanation": "The issue describes that when a redirect is performed in AdminSite.catch_all_view(), any query string present in the original request is dropped, resulting in a redirect URL that lacks the query parameters. The patch modifies the redirect logic to check if a query string exists in request.META['QUERY_STRING'], and if so, appends it to the redirect URL. This ensures that the redirect preserves the original query parameters, matching the expected behavior described in the issue. The change is minimal and directly addresses the problem without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16631": {
        "explanation": "The issue describes that after rotating the SECRET_KEY and moving the old key to SECRET_KEY_FALLBACKS, existing sessions are invalidated, contrary to the documentation's claim that sessions should remain valid if the old key is in SECRET_KEY_FALLBACKS. The root cause is that session signing and HMAC generation do not consult SECRET_KEY_FALLBACKS when verifying or decoding sessions.\n\nThe patch addresses this by:\n1. Updating the session encode/decode logic to use a signer that, on decode, attempts to unsign with the current key and, if that fails, iterates through SECRET_KEY_FALLBACKS.\n2. Modifying salted_hmac to try the primary key and, if that fails, iterate through SECRET_KEY_FALLBACKS.\n3. Adding tests that simulate key rotation and verify that sessions created with the old key remain valid after rotation.\n\nThe changes are focused on the described problem, do not introduce unrelated modifications, and the test demonstrates the intended behavior. The patch is both correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16642": {
        "explanation": "The issue describes that FileResponse does not correctly set the MIME type for files with the \".Z\" (compress) and \".br\" (Brotli) extensions, defaulting to text/html instead of the appropriate compressed file MIME types. The patch adds mappings for \"compress\" (application/x-compress) and \"br\" (application/x-brotli) to the content_type dictionary, which is used to set the Content-Type header based on the file's encoding. This directly addresses the issue by ensuring that files with these encodings/extensions are served with the correct MIME type. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16661": {
        "explanation": "The issue describes a bug in ModelAdmin.lookup_allowed() where lookups involving a foreign key that is also a primary key (as in a OneToOneField with primary_key=True) are incorrectly disallowed. The patch modifies the relevant logic in django/contrib/admin/options.py to add an extra condition: if the field is a primary key, it should not be skipped in the relation_parts construction. This directly addresses the described problem, ensuring that lookups like 'restaurant__place__country' are allowed when the intermediate field is a primary key foreign key. The patch also adds a test that matches the one in the issue description, verifying the fix. There are no unnecessary changes, and the patch is focused and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16662": {
        "explanation": "The issue describes that migration files should have all 'import module' statements before any 'from module import ...' statements, matching Django's coding style and isort defaults. The original code sorted all imports together by module name, which could interleave 'import' and 'from' statements. The patch changes this by separating 'import' and 'from' statements, sorting each group independently, and then concatenating them with 'import' statements first. The test is also updated to check for the correct order. This directly and precisely addresses the issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16667": {
        "explanation": "The issue describes a crash in SelectDateWidget when a user supplies an extremely large integer for the year, month, or day, resulting in an OverflowError from the datetime.date constructor. The original code only catches ValueError, not OverflowError, so the exception propagates and causes a server error. The patch modifies the except clause to catch both ValueError and OverflowError, returning a pseudo-ISO date string as fallback, which is consistent with the existing error handling for ValueError. This change is minimal, directly addresses the described problem, and does not introduce unrelated changes. It is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16801": {
        "explanation": "The issue describes a performance problem caused by ImageField always attaching a post_init signal handler, even when width_field and height_field are not set. The signal handler is a no-op in these cases, but the overhead of the signal connection and invocation is significant. The proposed patch changes the logic so that the post_init signal is only connected if either width_field or height_field is set, which directly addresses the described problem. The patch does not introduce unrelated changes and the logic is precise. The addition of a performance test script is not strictly necessary for the fix, but it is not harmful and is relevant to the issue context. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16819": {
        "explanation": "The patch introduces logic in the MigrationOptimizer to detect consecutive AddIndex and RemoveIndex operations on the same model and index name, and removes both from the migration operation list, as they cancel each other out. This directly addresses the issue's requirement to reduce AddIndex/RemoveIndex operations during migration optimization. The implementation is precise, only targeting the described case, and does not introduce unrelated changes. The included test file (though not part of Django's test suite) demonstrates the intended effect. The patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16824": {
        "explanation": "The issue arises because the code attempts to join elements of e.cmd (which may include non-string types like PosixPath) into a string using ' '.join(e.cmd), resulting in a TypeError. The patch changes this to ' '.join(str(arg) for arg in e.cmd), which ensures that all elements are converted to strings before joining. This directly addresses the problem described in the issue, and the change is minimal and targeted, without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16877": {
        "explanation": "The patch introduces a new template filter `escapeseq` as described in the issue. The filter is registered with `is_safe=True` and `needs_autoescape=True`, which is appropriate for filters that handle escaping. The implementation iterates over the input sequence, applies the correct escaping function (`conditional_escape` if autoescape is on, otherwise `escape`), and returns a list of escaped string items. It also includes error handling for non-iterable or None input, which is a reasonable safeguard. The docstring and usage example match the issue's requirements. No unrelated changes are made. The patch is precise and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16899": {
        "explanation": "The issue describes that the error message for invalid entries in 'readonly_fields' does not include the field name, making it less informative than similar errors for other fields. The patch updates the error message in the relevant check to include the field name (field_name) in the message, unifying it with the style of other admin field error messages. The change is minimal and directly addresses the issue without introducing unrelated modifications. The addition of the 'reproduce_error.py' file is just for demonstration and does not affect the core logic. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16901": {
        "explanation": "The issue describes that Django's fallback SQL for XOR with more than two operands is incorrect: it interprets Q(a) ^ Q(b) ^ Q(c) as 'exactly one is true' rather than 'an odd number are true' (parity). The correct behavior is that the XOR of multiple operands should be true if an odd number of them are true, matching the behavior of databases that natively support XOR (like MySQL).\n\nThe patch changes the combine_expression method to join sub-expressions with 'XOR' when the connector is '^', wrapping the result in parentheses. This means that for three operands, the SQL will be '(a XOR b XOR c)', which is correct for databases that support XOR natively. However, the issue is specifically about databases that do NOT support XOR natively (e.g., PostgreSQL), where Django must emulate XOR using AND/OR/NOT logic. The patch does not add any logic to handle the fallback for databases lacking XOR; it only changes the string joining behavior. Thus, it does not address the core of the issue: the fallback logic for non-XOR-supporting databases remains incorrect.\n\nThe added tests only check the structure of the query object, not the generated SQL or the correctness of the fallback logic. Therefore, the patch misses the key aspect of the issue: correct emulation of XOR for multiple operands on databases lacking native XOR support.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "django__django-16920": {
        "explanation": "The issue describes a lack of association between form fields and their help text and error messages for screen reader users, and proposes using the aria-describedby attribute to reference the help text and error message elements by their IDs. The patch adds logic to the BoundField class to include an aria-describedby attribute on the field's widget, referencing the help text and error message IDs if present. The IDs used match the pattern described in the issue (e.g., {auto_id}_helptext and {auto_id}_errorlist). The patch does not make unrelated changes and directly addresses the accessibility concern described. It does not, however, change the rendering of the error list from a <ul> to a <div> as suggested in the example, but the core requirement is the aria-describedby association, which is implemented. Therefore, the patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-16938": {
        "explanation": "The issue describes a FieldError being raised when serializing a model with a custom manager that uses select_related, due to a check in select_related_descend that prevents traversing a field that is both deferred and traversed via select_related. The patch modifies the condition that raises the FieldError, adding an extra check: it only raises if the field is in requested and not in select_mask, and then again checks if the field name is not in select_mask before raising. However, this is redundant, as the outer condition already checks that the field is not in select_mask. The patch does not fundamentally change the logic or address the root cause described in the issue (the interaction between select_related and only/defer in the context of custom managers and m2m serialization). Therefore, the patch does not resolve the issue as described.",
        "label": "INCORRECT"
    },
    "django__django-16950": {
        "explanation": "The issue describes a problem where, when using a UUIDField as a primary key (or unique identifier) in a Django model, the default value (e.g., uuid.uuid4) is not being set when creating related objects via Django Admin inlines. This results in the UUID field being null, causing a database error.\n\nThe patch modifies the Model.save() logic: after attempting to get the primary key value, if it is still None and the primary key is a UUIDField, it explicitly sets the pk value to the field's default (via get_default()). This ensures that, in cases where the UUIDField's default was not set (such as when creating objects via inlines), the default is now applied before saving.\n\nThe change is minimal, directly targets the described problem, and does not introduce unrelated modifications. It also does not appear to break existing logic for other field types. Therefore, the patch is correct and precise for the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-17029": {
        "explanation": "The issue describes that Apps.clear_cache() does not clear the cache of get_swappable_settings_name, which is an LRU-cached method. The patch adds a call to self.get_swappable_settings_name.cache_clear() inside Apps.clear_cache(), ensuring that this cache is cleared along with others. The patch also adds targeted tests (in both a new test file and the existing test suite) that verify the cache is populated and then cleared by clear_cache(). The change is minimal, directly addresses the described problem, and does not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-17084": {
        "explanation": "The issue describes a problem where, since Django 4.2, attempting to use an aggregate (e.g., Sum) over a window function (e.g., a Window expression) results in a database error: 'aggregate function calls cannot contain window function calls'. This is a known SQL limitation: window functions cannot be nested inside aggregates in SQL, and Django previously allowed this pattern by rewriting the query to use a subquery or annotation.\n\nThe patch addresses this by detecting when an aggregate is being applied to a window function. In such cases, it rewrites the query to first annotate the window function result in a subquery, and then applies the aggregate to the result of that subquery. This is done both in the Aggregate and Window expression classes, ensuring that the ORM generates SQL that is valid for PostgreSQL and other databases with similar restrictions.\n\nThe changes are targeted and only affect the case where an aggregate is applied to a window function, as described in the issue. There are no unrelated changes, and the patch does not appear to miss any key edge cases relevant to the issue as described. The approach is consistent with how Django has historically handled similar ORM limitations.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-17087": {
        "explanation": "The issue is that when a nested class method (e.g., Profile.Capability.default) is used as a field default, Django's migration serialization incorrectly references it as appname.models.Capability.default instead of appname.models.Profile.Capability.default. This is because the serializer uses __name__ instead of __qualname__, losing the nesting context.\n\nThe patch changes the FunctionTypeSerializer to use klass.__qualname__ instead of klass.__name__ when serializing the default. This ensures that the fully qualified name, including nesting, is used (e.g., Profile.Capability), which directly addresses the issue described.\n\nThe other changes in the patch are unrelated to the issue (e.g., handling callables in effective_default, whitespace in AddField, docstring in SQLInsertCompiler), but they do not introduce unrelated logic or affect the correctness of the fix for the described problem.\n\nTherefore, the patch is correct and precise in addressing the issue: it ensures that nested class methods are serialized with their full qualified name, fixing the migration default reference problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-7530": {
        "explanation": "The issue describes that makemigrations was incorrectly calling router.allow_migrate() for each app label with all models in the project, rather than only with the models belonging to that app. This could result in invalid (app_label, model) pairs being passed to allow_migrate, which breaks custom routers that expect only valid combinations.\n\nThe patch changes the logic so that for each app_label in consistency_check_labels, it only iterates over models from apps.get_models(app_label), ensuring that only models belonging to the current app_label are considered. This directly addresses the issue by ensuring only valid (app_label, model) pairs are passed to allow_migrate().\n\nThe patch does not introduce unrelated changes and is focused solely on correcting the iteration logic. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "django__django-9296": {
        "explanation": "The issue requests that the Paginator class implement the __iter__ method so that iterating over a Paginator instance yields Page objects, rather than requiring explicit use of page_range. The patch adds a __iter__ method to Paginator that yields self.page(page_num) for each page_num in self.page_range, exactly as described in the issue. The patch does not introduce any unrelated changes. The addition of a test script is not part of the core code and does not affect the correctness of the patch. The patch is precise and directly addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-13983": {
        "explanation": "The issue is that when an axis is removed from a shared axes group, the remaining axis has its formatter/locator reset via set_major_formatter/set_major_locator, which causes the axis to believe it now has a user-supplied formatter/locator (by setting isDefault_majfmt, etc. to False). This prevents the unit framework from overriding the formatter/locator as needed. The patch addresses this by saving the isDefault_* flags before the reset, and restoring them after, so the axis retains the correct knowledge of whether its formatter/locator is default or user-supplied. The patch does not introduce unrelated changes and directly targets the described problem. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-13989": {
        "explanation": "The issue describes that when calling plt.hist with density=True and a specified range, the range is not respected (the bins do not start and end at the specified range). The patch modifies the code so that when density is True and not stacked, if bin_range is not None, it adds 'range': bin_range to the hist_kwargs dict, which is presumably passed to numpy.histogram or similar. This directly addresses the issue: it ensures that the 'range' argument is respected when density=True, matching the expected behavior described in the issue. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-14623": {
        "explanation": "The issue describes that inverting a log axis by setting its limits in reverse order (e.g., set_ylim(y.max(), y.min())) does not work as expected, while it does for linear axes. The original code in LogScale always returns (minpos if vmin <= 0 else vmin, minpos if vmax <= 0 else vmax), which does not respect the order of vmin and vmax, thus always producing increasing limits regardless of the input order. The patch adds a conditional: if vmax < vmin, it returns the limits in the reversed order, applying the same minpos logic, thus preserving the user's requested inversion. This directly and precisely addresses the issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-20488": {
        "explanation": "The issue describes a ValueError in LogNorm normalization when handling huge ranges, likely due to numerical instability or invalid vmin/vmax values (e.g., non-positive values for logarithmic scaling). The patch adds an explicit check to ensure vmin and vmax are positive before applying the log transform, which is a necessary precondition for LogNorm. It also uses numpy's errstate to suppress warnings during transformation and improves the error message for invalid transformed vmin/vmax. The changes are minimal, directly address the root cause (invalid vmin/vmax for log scale), and do not introduce unrelated modifications. Therefore, the patch is both correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-20676": {
        "explanation": "The issue describes that when using the interactive SpanSelector, the axes limits are incorrectly expanded to include 0, even when the data and initial limits do not include 0. The expected behavior is that the axes limits should remain as set (e.g., (10, 20)), and not be expanded to include 0 when interacting with the SpanSelector.\n\nThe patch modifies the _onmove method of SpanSelector to clamp the selection extents (vmin, vmax) to the current axes limits (vmin_ax, vmax_ax) during all relevant interactions: dragging, resizing, and creating a new span. This ensures that the extents cannot go outside the current axes limits, thus preventing the unwanted expansion of the axes to include 0 or any other value outside the current view.\n\nThe patch does not introduce unrelated changes and directly addresses the described problem by enforcing the axes limits during interaction. The logic is applied in all code paths where the extents could be set, covering dragging, resizing, and new span creation. This matches the expected outcome described in the issue.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-20826": {
        "explanation": "The issue describes that after calling ax.clear() on subplots with shared axes, extra ticks and tick labels appear that should be hidden, and ticks appear on the top and right sides of each subplot. The expected behavior is that only the appropriate tick labels (e.g., only on the bottom-most and left-most axes) are shown, and no extra ticks appear on the top and right.\n\nThe patch modifies the clear() method to:\n- Set xaxis and yaxis visibility as before.\n- For shared x-axes, only show x-axis tick labels if this is the bottom-most axis (using _get_axis_list()[-1] is self).\n- For shared y-axes, only show y-axis tick labels if this is the left-most axis (using _get_axis_list()[0] is self).\n- Resets tick_params to hide top and right ticks (top=False, right=False).\n\nThis directly addresses both symptoms described in the issue: it ensures only the correct tick labels are shown for shared axes, and it disables the top and right ticks that were appearing unexpectedly. The changes are minimal and targeted, with no unrelated modifications.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-20859": {
        "explanation": "The issue describes that adding a legend to a SubFigure fails because the Legend constructor only accepts Axes or Figure as parent, but not SubFigure (which is Figure-like). The patch updates the Legend constructor to accept any parent that has both a 'figure' attribute and a 'get_children' method, which are the key attributes of FigureBase and thus also present in SubFigure. The patch also updates the docstring to clarify this, and adjusts the logic to set the figure correctly. No unrelated changes are made. This directly and precisely addresses the issue described, allowing SubFigure to be used as a parent for Legend as expected.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-21490": {
        "explanation": "The issue describes that Line2D should copy its input arrays to prevent in-place modifications of the original arrays from affecting the plotted line. The patch changes the assignment of self._xorig and self._yorig to use np.asarray(x, copy=True) and np.asarray(y, copy=True), respectively. This ensures that the internal data is a copy, not a reference, thus preventing external modifications from affecting the Line2D object. The patch is minimal and directly addresses the described bug without introducing unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-21568": {
        "explanation": "The issue describes a regression in the appearance of datetime axis labels when 'usetex=True' is set in matplotlib 3.4 compared to 3.3, specifically regarding label spacing and clarity. The patch changes the ScalarFormatter so that when 'usetex' is True, the format string is wrapped in '$...$', and only when '_useMathText' is True (and not 'usetex'), it uses '\\mathdefault{...}'. Previously, both cases used '\\mathdefault{...}'. This change restores the behavior from 3.3, where 'usetex' would wrap the label in '$...$', which is the correct way to invoke TeX rendering in matplotlib. The patch is minimal and directly targets the formatting logic relevant to the issue, without unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-22719": {
        "explanation": "The issue describes a spurious deprecation warning and/or error when plotting with empty data (e.g., ax.plot([], [])) on an axis with category units. The traceback shows that the category converter's convert method is called with an empty array, which then triggers logic that checks if all elements are numlike, leading to a deprecation warning and, ultimately, a ConversionError, even though there is no data to convert.\n\nThe patch adds an early return in StrCategoryConverter.convert: if the input array is empty (values.size == 0), it returns an empty float array immediately, bypassing all subsequent logic (including the numlike check and the deprecation warning). This directly addresses the issue: empty data will no longer trigger the warning or error, and the behavior will be as expected (artists with no data).\n\nThe patch is minimal and does not introduce unrelated changes. It precisely targets the described problem and resolves it as requested in the issue.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-22865": {
        "explanation": "The issue describes that when using drawedges=True and extend='both' on a colorbar, the black divider lines are missing at the extremities (the extended regions). The patch modifies the Colorbar class to explicitly add divider segments at the lower and upper extensions when drawedges is True, by checking _extend_lower() and _extend_upper() and appending the appropriate segments. The rest of the divider segments are handled as before. The patch does not introduce unrelated changes and directly targets the missing edge lines for the extended regions. The added test and reproduction scripts are appropriate and do not affect the core logic. Therefore, the patch is correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-22871": {
        "explanation": "The issue describes that when plotting a date range of less than a year that does not include January, ConciseDateFormatter fails to show the year anywhere on the x-axis, even though the user expects the year (e.g., '2021') to appear in the offset. The patch modifies the logic in ConciseDateFormatter to:\n\n- Detect if the date range is less than a year but more than a day, and if January is not present in the ticks, it forces the label level to 0 (year) and disables the offset, ensuring the year is shown in the tick labels.\n- For intervals of a day or less, it keeps the original logic.\n- It also ensures that if the level is 0 or 1, the offset is not shown, which matches the expected behavior.\n\nThe patch is focused and only changes the relevant logic for determining when to show the year, without introducing unrelated changes. It directly addresses the described bug and implements the expected outcome as described in the issue. There are no unnecessary changes or missing key aspects for the described scenario.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-23174": {
        "explanation": "The issue is that when using subfigures, the code in get_renderer assumes the passed figure object has a _cachedRenderer attribute, but SubFigure objects do not, leading to an AttributeError. The patch adds a check: if the object has a 'figure' attribute and is not itself the parent figure, it recursively calls get_renderer on the parent. This ensures that the renderer is obtained from the actual Figure object, which does have _cachedRenderer. The patch also adds a hasattr check before accessing _cachedRenderer, preventing AttributeError. The changes are minimal, directly address the bug, and do not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-23188": {
        "explanation": "The issue describes that the default date limits are currently set to 2000-01-01 to 2010-01-01, which can cause problems when day tickers are used without explicit limits. The proposed fix is to change these defaults to 1970-01-01 to 1970-01-02, which would correspond to 0-1 in the float conversion for the default epoch. The patch updates both locations in the code where these defaults are set: in DateLocator and DateConverter. No unrelated changes are made, and the patch precisely implements the proposed fix as described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-23299": {
        "explanation": "The issue describes that figures created within an rc_context are lost from Gcf.figs after calling get_backend(), because the rcParams are reverted using dict.update, which bypasses the validation and side effects in rcParams' __setitem__. This can cause internal state inconsistencies, such as the loss of figures. The patch replaces dict.update(rcParams, orig) with a loop that assigns each key-value pair via rcParams[k] = v, ensuring that the __setitem__ method is used and all necessary validation and side effects are triggered. This directly addresses the root cause described in the issue, without introducing unrelated changes. The patch is focused and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-23314": {
        "explanation": "The issue describes that calling set_visible(False) on a 3D subplot does not hide the subplot as expected. The patch adds a check at the start of the Axes3D.draw() method: if not self.get_visible(): return. This ensures that if the axes is set to not visible, its draw method will exit early and nothing will be rendered for that axes. This is the correct and minimal fix for the described issue, as it directly addresses the problem without introducing unrelated changes. The addition of a reproduce_error.py script is not part of the library code and does not affect the patch's correctness. The patch is precise and does not introduce unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-23412": {
        "explanation": "The issue describes that when setting a dash pattern with an offset (e.g., ls=(10, (10,10))) on a Patch object, the offset is ignored, resulting in overlapping lines rather than the expected offset effect. The patch introduces a new method, set_dash_offset, which allows setting an offset by adding the offset value to each element of the current dash pattern. However, this approach is incorrect: the dash pattern tuple for linestyles in matplotlib is of the form (offset, (on_off_seq)), where the first element is the offset and the second is the dash sequence. The patch instead adds the offset to every dash length, which is not how dash offsets are supposed to work. The correct approach would be to store and apply the offset as the first element of the dash pattern tuple, not to modify the dash lengths themselves. Therefore, the patch fundamentally misunderstands the requirements and does not address the issue as described.",
        "label": "INCORRECT"
    },
    "matplotlib__matplotlib-23476": {
        "explanation": "The issue describes that after unpickling a figure, its DPI is doubled each time, leading to exponential growth and eventual overflow. The expected behavior is that the DPI remains constant after unpickling. The patch adds a line in the Figure.__setstate__ method to explicitly set the DPI from the state dictionary (or use the default if not present), right after restoring the object's __dict__. This ensures that the DPI is restored to its original value, rather than being affected by any side effects of the unpickling process or backend initialization. The patch is minimal and directly targets the problem described, without introducing unrelated changes. Therefore, it correctly and precisely addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24026": {
        "explanation": "The issue is that stackplot previously attempted to set the Axes property cycler with a list of color aliases like ['C2', 'C3', 'C4'], which is not allowed and raises a ValueError. The user wants to be able to use 'CN' color aliases in stackplot's colors argument, just as with plot and Rectangle, and for stackplot not to modify the Axes color cycler.\n\nThe patch addresses this by:\n- Removing the call to axes.set_prop_cycle(color=colors), so the cycler is not changed.\n- Converting the provided colors (which may include 'CN' aliases) to RGBA using to_rgba_array, so they can be used directly for facecolor.\n- Iterating over the provided colors for each stackplot layer, and falling back to the axes' color cycler if colors is None.\n- Adding tests that verify: (1) 'CN' aliases work for stackplot, (2) the color cycle is not changed by stackplot, and (3) the correct colors are used for all elements.\n\nThe patch is focused and does not introduce unrelated changes. It directly addresses the error and the user requirements described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24149": {
        "explanation": "The issue is that ax.bar([np.nan], [np.nan]) raises a StopIteration exception in matplotlib 3.6.1, whereas in 3.6.0 it would return a BarCollection with a Rectangle with nan values. The traceback shows that the error comes from cbook._safe_first_finite raising StopIteration when all values are nan, which is not handled in _axes.py:_convert_dx. \n\nThe patch modifies cbook._safe_first_finite to catch StopIteration and return None instead of raising, and then in _axes.py:_convert_dx, it checks if x0 is None after calling _safe_first_finite, and if so, returns convert(dx). This means that if all x0 values are non-finite (e.g., all nan), the function will gracefully handle it instead of raising, matching the previous behavior and the user's expectation.\n\nThe patch is minimal, directly addresses the bug, and does not introduce unrelated changes. It also does not remove or break existing functionality. The only possible concern is whether returning None from _safe_first_finite is always safe, but in this context, the patch checks for None and handles it appropriately. Thus, the patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24177": {
        "explanation": "The issue describes a bug where setting density=True in ax.hist with histtype='step' does not properly normalize the histogram, especially when the data is rescaled. The expected behavior is that the density axis should be auto-scaled and normalization should be consistent regardless of the data scale, as is the case for histtype='bar'.\n\nThe patch changes the code so that, regardless of the density and stacked parameters, hist_kwargs['density'] is always set to False, with a comment indicating that density normalization will be handled manually for consistency. This suggests that the patch is part of a larger change where normalization is performed explicitly elsewhere in the code, rather than relying on numpy.histogram's density argument, which can behave inconsistently for different histtypes.\n\nGiven only this patch, it appears to address the core of the issue: it prevents the underlying histogram function from applying its own density normalization, presumably so that the Axes.hist method can apply a consistent normalization for all histtypes, including 'step'. The patch is minimal and directly related to the described bug, with no unrelated changes.\n\nHowever, without seeing the rest of the normalization logic, there is a small risk that edge cases (such as stacked histograms or other histtypes) might not be fully handled, but based on the context and the comment, this patch is a precise fix for the described inconsistency.\n\nTherefore, the patch is CORRECT_AND_PRECISE for the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24189": {
        "explanation": "The issue describes a bug where using width_ratios (or height_ratios) with nested mosaics in plt.subplot_mosaic() leads to a ValueError, because the same gridspec_kw (containing width_ratios/height_ratios) is passed recursively to all subgridspecs, even when the nested grids have different shapes. The expected behavior is that width_ratios/height_ratios should only apply to the outermost grid, and not be blindly passed to nested grids, which may have different numbers of rows/columns.\n\nThe patch modifies the relevant code in figure.py so that when recursing into a nested mosaic, it creates a copy of gridspec_kw and forcibly resets width_ratios and height_ratios to [1] * cols/rows for the nested grid, thus preventing the outer ratios from being incorrectly applied to the inner grids. This directly addresses the bug described. The patch also includes new tests that check the correct behavior for width_ratios and height_ratios with nested mosaics, and a reproduction script. The changes are focused and do not introduce unrelated modifications.\n\nTherefore, the patch is CORRECT_AND_PRECISE: it addresses the described issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24257": {
        "explanation": "The patch directly addresses the issue by adding comprehensive documentation and examples on how to distribute matplotlib style dictionaries as Python packages. It updates the API documentation (`style_api.rst`) and the introductory tutorial (`customizing.py`) with clear explanations, code snippets, and references to a template package. Additionally, it provides a full example package (`mpl_style_template`) with multiple styles, a custom colormap, and usage instructions, as well as example scripts demonstrating how to use such packages. The patch also highlights the advantages of this approach, including compatibility with all matplotlib versions and the ability to distribute colormaps and custom logic, as requested in the issue. No unnecessary or unrelated changes are introduced. Therefore, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24570": {
        "explanation": "The issue describes that the 'top' and 'bottom' alignments in HPacker are reversed, meaning that specifying 'top' gives the effect of 'bottom' and vice versa. The patch modifies the _get_aligned_offsets function to handle 'top' and 'bottom' separately, rather than grouping 'top' with 'left' and 'bottom' with 'right'. Specifically, it assigns the offset calculation for 'top' to what was previously used for 'right' (height - h + d), and for 'bottom' to what was previously used for 'left' (d). This change directly addresses the reversal described in the issue. Additionally, the patch adds a test that visually checks the alignment for 'top', 'bottom', and 'center', ensuring the fix is correct. No unrelated changes are made. The patch is precise and directly addresses the described bug.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24627": {
        "explanation": "The issue describes that when an artist (e.g., a Line2D object) is removed from its axes using `remove()`, its `.axes` attribute is set to None, but when the axes is cleared using `cla()`, the `.axes` attribute is not unset. The same logic applies to the `.figure` attribute. The patch modifies the `cla()` method in `_AxesBase` and the `clf()` method in `Figure` to explicitly unset the `.axes` and `.figure` attributes for all children before clearing the children list. It also adds tests to verify that after calling `cla()` or `clf()`, the `.axes` and `.figure` attributes of the artist are set to None. The changes are minimal and directly address the described issue without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24637": {
        "explanation": "The issue is that when an AnnotationBbox (which is a subclass of DrawingArea) is given a gid via set_gid, this gid is not passed to the SVG renderer, so the resulting SVG does not include the expected group id. The patch modifies the DrawingArea.draw method to check if self.get_gid() is set, and if so, it calls renderer.open_group(self.get_gid()) before drawing children and renderer.close_group(self.get_gid()) after. This is the standard pattern in Matplotlib for ensuring that the gid is included in the output (notably for SVG). The patch does not introduce unrelated changes and is focused solely on ensuring the gid is passed to the renderer group, which is precisely what the issue describes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24870": {
        "explanation": "The patch directly addresses the issue described: when a boolean array is passed as the Z argument to contour(), and no levels are specified, it automatically sets levels to [0.5]. This matches the user's expectation and the proposed solution in the issue. The patch checks if the last positional argument is a numpy array of dtype bool, and only sets levels if not already specified, thus not interfering with explicit user input. The added test verifies both the default behavior and the ability to override levels, ensuring correctness. No unrelated changes are made. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-24970": {
        "explanation": "The issue is about deprecation warnings in NumPy 1.24+ when assigning out-of-bounds Python integers to NumPy integer arrays, specifically in the context of matplotlib's colormap handling. The warning suggests that the old behavior (which allowed overflow casting) will be disallowed, and recommends explicitly casting the value to the array's dtype to preserve the overflow behavior.\n\nThe patch changes the assignments from, e.g., `xa[xa > self.N - 1] = self._i_over` to `xa[over_mask] = np.array(self._i_over).astype(xa.dtype)`, which matches the recommended workaround in the warning message. This ensures that the assignment will continue to work as before, with explicit casting, and will not trigger the deprecation warning in future NumPy versions.\n\nThe patch does not introduce unrelated changes and directly addresses the lines and behavior described in the issue. It is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25052": {
        "explanation": "The issue describes that the Table class's bbox parameter is documented as accepting a matplotlib.transforms.Bbox instance, but the code only works if bbox is a sequence (like a list of four numbers), not a Bbox. The traceback shows that the code tries to unpack self._bbox directly, which fails if it's a Bbox object (since Bbox is not directly iterable in that way). The patch adds a check: if self._bbox has a 'bounds' attribute (which Bbox does), it unpacks self._bbox.bounds; otherwise, it unpacks self._bbox directly (for backward compatibility with sequences). This directly addresses the issue by allowing both Bbox instances and sequences, as the documentation suggests. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25122": {
        "explanation": "The issue reports that the window correction in `mlab._spectral_helper` is incorrect because it uses `np.abs(window).sum()**2`, which is not appropriate for windows with negative values (like flattop). The correct normalization should not use the absolute value, as demonstrated by the difference in the example. The patch, however, does not actually fix the code. Instead, it only adds print statements and a script to reproduce the bug, and a markdown response stating that the current implementation is correct. This directly contradicts the issue's evidence and the reference to the correct approach in scipy. Therefore, the patch fails to address the reported bug and fundamentally misunderstands the requirements.",
        "label": "INCORRECT"
    },
    "matplotlib__matplotlib-25126": {
        "explanation": "The issue describes a bug where a line plotted with ax.plot(x, y) disappears after changing the y-axis scale from 'log' to 'linear'. The code in question optimizes rendering for certain conditions, including when the x data is sorted, the axes are rectilinear, the x scale is linear, and other criteria. The patch adds an additional check: 'self.axes.get_yscale() == 'linear'', ensuring that this optimization is only applied when both x and y axes are linear. This is important because the optimization may not be valid or safe when the y-axis is not linear, which could cause the line to disappear after changing the y scale. The patch is minimal and directly addresses the described issue without introducing unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25287": {
        "explanation": "The issue describes that the offsetText (the exponent label) is colored according to xtick.color/ytick.color, rather than xtick.labelcolor/ytick.labelcolor as expected. The user-provided patch in the issue changes the color assignment at the point of offsetText creation, using labelcolor if set, otherwise falling back to color. \n\nThe submitted patch, however, modifies the logic in Axis._update_label_position (or similar), and attempts to set the offsetText color based on whether mpl.rcParams['xtick.labelcolor'] is 'inherit'. However, the logic is inverted: it sets the color to kwtrans['labelcolor'] if 'inherit', otherwise to mpl.rcParams['xtick.labelcolor']. This is backwards: if labelcolor is 'inherit', it should fall back to color, otherwise use labelcolor. Furthermore, the patch applies this logic only for xtick.labelcolor, not ytick.labelcolor, and does not address the initial creation of offsetText, only its update. \n\nTherefore, the patch does not fully or correctly address the issue as described, and may even introduce incorrect behavior.",
        "label": "INCORRECT"
    },
    "matplotlib__matplotlib-25311": {
        "explanation": "The issue is that pickling a figure with a draggable legend fails due to the presence of unpickleable objects (specifically, FigureCanvasQTAgg and DraggableLegend) referenced within the legend and its draggable wrapper. The patch adds __getstate__ methods to both DraggableLegend and Legend classes, ensuring that these problematic attributes ('legend' in DraggableLegend and '_draggable' in Legend) are set to None during pickling, thus avoiding the unpickleable objects. The __setstate__ method in Legend attempts to restore the draggable state if it was set before pickling. The changes are minimal, targeted, and directly address the pickling problem described in the issue, without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25332": {
        "explanation": "The issue describes a failure to pickle a matplotlib Figure after calling align_labels(), with a TypeError related to 'weakref.ReferenceType'. The patch modifies the Figure's __getstate__ method to explicitly remove the '_align_label_groups' attribute from the state dict before pickling. This attribute is likely the source of the unpicklable weakref, and removing it from the pickled state should resolve the error. The patch is minimal, targeted, and does not introduce unrelated changes. It directly addresses the root cause as described in the issue, ensuring that pickling works after align_labels() is called. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25479": {
        "explanation": "The issue describes a confusing behavior in matplotlib where registering a colormap under a new name (e.g., 'my_cmap_name') does not allow it to be referenced by that name in pyplot, because pyplot.set_cmap, when passed a Colormap instance, uses the instance's .name attribute (which may differ from the registered name) rather than the name under which it was registered. This leads to a ValueError when the .name attribute does not match the registered name.\n\nThe patch addresses this by:\n1. In Colormap.__init__, setting self.name = name if name else 'default', which ensures that a Colormap always has a name, but does not change the name to the registered name if it is registered under a different name.\n2. In pyplot.set_cmap, when passed a Colormap instance, it now calls get_cmap(cmap.name) instead of get_cmap(cmap). This means that if a Colormap instance is passed, it will look up the colormap by its .name attribute in the registry, rather than using the instance directly.\n\nHowever, this does not fully solve the issue described. The core problem is that the registered name ('my_cmap_name') and the Colormap's .name attribute ('some_cmap_name') can differ, and the user expects to be able to use the registered name. The patch does not update the Colormap's .name attribute when registering under a new name, nor does it provide a mapping from the instance to the registered name. The change in set_cmap may actually perpetuate the confusion, as it still relies on the .name attribute of the Colormap instance, which may not match the registered name.\n\nTherefore, the patch misses a key aspect: it does not ensure that the registered name is used or associated with the Colormap instance, nor does it allow set_cmap to work as expected when a Colormap instance is passed that was registered under a different name. The patch does not introduce unrelated changes, but it does not fully address the issue as described.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "matplotlib__matplotlib-25547": {
        "explanation": "The patch addresses two related issues described: (1) a StopIteration error in errorbar when all error values are NaN, and (2) a UserWarning from numpy when all color values are NaN in scatter. \n\nFor the errorbar issue, the patch modifies the _upcast_err function to catch StopIteration when searching for the first finite element in the error array. If all values are non-finite, it returns np.asarray(err, dtype=object), which prevents the crash and matches the expected behavior described in the issue.\n\nFor the scatter issue, the patch adds a check: if the color array is a masked array and all values are masked, it sets the collection's array to None and color to 'none', avoiding the problematic conversion that triggers the warning. This is a targeted fix for the described warning and does not affect the case where only some values are NaN.\n\nBoth changes are minimal, directly address the described problems, and do not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25667": {
        "explanation": "The issue describes that the bar/barh methods do not trigger unit conversion (such as for datetime) on the 'bottom' (for bar) or 'left' (for barh) parameters, which can result in axes not being set up for datetime data when these are used. The patch adds calls to 'self.yaxis.update_units(y)' for vertical bars (where 'y' is the 'bottom' parameter) and 'self.xaxis.update_units(x)' for horizontal bars (where 'x' is the 'left' parameter), but only if these are not None. This directly addresses the issue by ensuring that the axis units are updated based on the 'bottom' or 'left' parameters, which will trigger the correct converter (e.g., for datetime). The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25775": {
        "explanation": "The patch introduces `get_antialiased` and `set_antialiased` methods to the `Text` class, storing the antialiasing state in a new instance variable `_antialiased`, which is initialized from `rcParams['text.antialiased']`. The patch also ensures that the antialiasing state is passed to the graphics context (`gc.set_antialiased(self._antialiased)`) during drawing, so the per-artist setting is respected. A test is added to verify the getter and setter. The patch does not make unrelated changes and does not miss any key aspects of the issue as described. The issue did mention possibly adjusting Annotations, but since `Annotation` inherits from `Text`, the change is inherited, so this is sufficient. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-25960": {
        "explanation": "The issue describes that the `wspace` and `hspace` arguments to `Figure.subfigures` are ignored, resulting in no effect when users try to set them. The patch modifies `figure.py` so that when `wspace` or `hspace` are not provided, they default to the values in `plt.rcParams`, otherwise the user-supplied values are used. In `gridspec.py`, it ensures that `self.wspace` and `self.hspace` (which may be set by the user) take precedence over the subplot parameters. This directly addresses the problem: user-supplied `wspace` and `hspace` are now respected, and the default behavior is preserved. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-26113": {
        "explanation": "The issue describes an inconsistency in the behavior of the `mincnt` parameter in `hexbin` depending on whether the `C` parameter is supplied. Specifically, when `C` is supplied, the code previously used `len(acc) > mincnt` to determine whether to display a cell, whereas when `C` is not supplied, it used `len(acc) >= mincnt`. This led to the unintuitive result that `mincnt=1` with `C` supplied would only show bins with at least two points, not one. The patch changes the comparison from `len(acc) > mincnt` to `len(acc) >= mincnt` in the relevant code path, making the behavior consistent regardless of whether `C` is supplied. The patch also updates the docstring to clarify that `mincnt` is `int >= 0` and that cells with at least `mincnt` points are displayed. No unrelated changes are made. This directly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-26122": {
        "explanation": "The issue describes that imshow() should not modify the axes aspect if a transform is provided that does not contain ax.transData as a branch. The patch changes the logic so that set_aspect(aspect) is only called if either 'transform' is not in kwargs (i.e., default behavior) or if the provided transform contains self.transData as a branch. This directly implements the requested behavior: aspect is only set when the image is drawn in data coordinates, and not when a custom transform is used. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-26208": {
        "explanation": "The issue describes that when using twinx with stackplot, the dataLim of the first axis (ax1) is replaced by \u00b1inf, which is incorrect. The patch addresses this in two main ways:\n\n1. In `axes/_base.py`, the autoscaling logic is updated to handle non-numeric x-data (like string labels) by using the index as the x-axis limits, rather than attempting to compute min/max of non-numeric data, which would result in inf/-inf. This directly addresses the root cause of the bug.\n\n2. In `stackplot.py`, a helper function `_replace_inf_with_max` is introduced to sanitize input arrays, replacing inf/nan with the maximum finite value, further preventing propagation of invalid data limits.\n\n3. A new test is added that reproduces the issue and asserts that after plotting with twinx and stackplot, the dataLim values are finite and reasonable, matching the expected behavior described in the issue.\n\nThe patch is focused on the described problem, does not introduce unrelated changes, and the new logic is well-targeted to the edge case of non-numeric x-data. The test covers the reported scenario. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-26291": {
        "explanation": "The issue is an AttributeError caused by 'renderer' being None when inset axes are created and their locator is called during tight bounding box adjustment. Specifically, the call chain leads to inset_locator.py's __call__ method, which expects a renderer but sometimes receives None, causing offsetbox.py's get_window_extent to fail when it tries to access self.figure._get_renderer() on a NoneType.\n\nThe patch addresses this by:\n1. In _tight_bbox.py, ensuring that when locator(ax, ...) is called, it passes fig.canvas.get_renderer() as the renderer argument, which may still be None but is the correct value to pass.\n2. In offsetbox.py, the indentation is fixed so that the logic for getting the renderer and calculating the offset is correct and robust to renderer being None.\n3. In inset_locator.py, the __call__ method is updated to handle the case where renderer is None by falling back to self.axes.figure._get_renderer(), ensuring that get_window_extent and get_offset always have a valid renderer.\n\nThese changes are minimal, directly address the root cause, and do not introduce unrelated modifications. The patch ensures that inset axes can be created without error in the described scenario, matching the expected outcome.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-26342": {
        "explanation": "The issue requests the addition of a `set_paths` method to the `ContourSet` class, allowing users to replace the contour paths directly, rather than mutating the list returned by `get_paths()`. The patch adds a `set_paths` method with appropriate documentation, which sets the internal `_paths` attribute to the provided list of paths. This directly addresses the enhancement request without introducing unrelated changes or missing any key aspects of the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "matplotlib__matplotlib-26466": {
        "explanation": "The issue describes that when a numpy array is passed as the 'xy' parameter to an annotation, mutating the array after annotation creation also changes the annotation's position. This is because the annotation stores a reference to the original array, not a copy. The patch changes 'self.xy = xy' to 'self.xy = np.array(xy)', which creates a new numpy array from the input, thus decoupling the annotation's internal state from the original array. This directly addresses the issue by ensuring that subsequent changes to the input array do not affect the annotation. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "mwaskom__seaborn-3069": {
        "explanation": "The patch addresses all three points described in the issue:\n\n1. **Axis limits to +/- 0.5 from first/last tick:** The patch introduces a `_set_extended_view_interval` method in the `Nominal` scale, which sets the axis view interval to `-0.5` and `num_categories - 0.5`, matching the categorical axis behavior described. This is called in place of the previous logic, ensuring the correct margins.\n\n2. **Grid is not shown for nominal scales:** In `Plotter._finalize_figure`, the patch checks if the scale is `Nominal` and disables the grid for the relevant axis, as requested.\n\n3. **Invert y-axis for nominal scales:** The patch inverts the y-axis if the scale is nominal and the axis is 'y', as specified.\n\nThe changes are targeted and do not introduce unrelated modifications. The only minor concern is that the patch disables the grid unconditionally for nominal scales, which may override user preferences or theming, but this matches the issue's request and is not considered an unnecessary change for the scope described.\n\nTherefore, the patch is correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "mwaskom__seaborn-3187": {
        "explanation": "The issue describes that legends for large numeric ranges (using ScalarFormatter with an offset) do not display the multiplicative offset in the legend labels, leading to misleading or incomplete legends. The patch modifies the relevant code in seaborn/_core/scales.py to check if the formatter is a ScalarFormatter and if an offset is present, then appends the offset string to each legend label. This directly addresses the core of the issue: ensuring that the legend labels reflect the offset, matching the axis tick formatting. \n\nThe patch also adds two tests: one that compares the generated plot image to an expected image, and another that programmatically checks that the offset string (e.g., '\u00d71e6') appears in the legend labels. These tests are focused and directly related to the issue. No unrelated changes are made. \n\nTherefore, the patch is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "mwaskom__seaborn-3190": {
        "explanation": "The issue arises because the code attempts to subtract boolean values, which is not supported in numpy. The patch addresses this by casting vmax and vmin to integers before applying the 'forward' function and performing the subtraction. This ensures that the subtraction operation is valid for boolean data (which will be converted to 0 and 1), thus preventing the TypeError. The change is minimal and directly targets the problematic line without introducing unrelated modifications. However, it may not handle all edge cases, such as when 'forward' expects non-integer input or when the data is not strictly boolean but another type that could cause similar issues. Still, for the described issue (boolean color mapping), this patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "mwaskom__seaborn-3407": {
        "explanation": "The issue is that seaborn's pairplot (via PairGrid) fails with a KeyError when given a DataFrame with MultiIndex columns, because it expects string column names. The user-provided workaround is to flatten the columns into strings. \n\nThe patch addresses this by detecting MultiIndex columns in the PairGrid constructor, flattening them into strings (joined by underscores), and updating the DataFrame and any relevant variable lists (vars, x_vars, y_vars, hue) accordingly. This ensures that all subsequent code that expects string column names will work. \n\nAdditionally, the patch attempts to preserve the original column names for axis labels by checking for a mapping (_col_map) in the _add_axis_labels method, though the actual mapping (_col_map) is not set in this patch. This is a minor omission, but it does not affect the core fix for the KeyError.\n\nThe patch is focused on the described issue and does not introduce unrelated changes. It correctly implements the workaround in a general way, so MultiIndex columns are handled automatically. However, the axis label restoration is only partially implemented (no _col_map is set), so axis labels may not show the original MultiIndex names, but this is a minor cosmetic issue and does not affect the main bug fix.\n\nTherefore, the patch is correct and precise for the main issue (KeyError with MultiIndex columns), with only a minor, non-blocking omission regarding axis labels.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pallets__flask-5014": {
        "explanation": "The patch adds a check in the Blueprint constructor to raise a ValueError if the 'name' argument is empty. This directly addresses the issue described, which requests that a ValueError be raised when a Blueprint is given an empty name. The patch does not introduce any unrelated changes and the check is placed before the existing check for a dot in the name, which is appropriate. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-1142": {
        "explanation": "The issue describes that the 'Content-Length' header is always being set, even for GET requests with no body, which can cause problems with some servers. The patch changes the 'prepare_content_length' method so that if 'body' is None, it removes the 'Content-Length' header instead of setting it to '0'. This ensures that requests without a body (such as typical GET requests) do not include a 'Content-Length' header, directly addressing the issue. The patch does not introduce unrelated changes and is precise in its fix.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-1657": {
        "explanation": "The issue describes that when merging request and session hooks, both of which are dictionaries with lists as values, the current implementation simply overwrites the session's list with the request's list, rather than combining them. The patch modifies the merge_setting function so that when both the session and request values for a key are lists, it concatenates them instead of replacing. For other types, it retains the original behavior. This directly addresses the described problem without introducing unrelated changes. The handling of None values is preserved. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-1724": {
        "explanation": "The issue is that passing a unicode string as the HTTP method (e.g., method=u'POST') in Python 2.7 causes a UnicodeDecodeError downstream, because the method is expected to be a byte string (str), not unicode. The patch addresses this by checking if the method is a unicode object and, if so, encoding it to ASCII before further processing. This is done in three places: before preparing the request in Session.request, before creating the Request object in Session.request, and in PreparedRequest.prepare_method. These are the relevant code paths where the method could be set as unicode. The patch does not introduce unrelated changes and directly addresses the root cause described in the issue. The use of 'ascii' encoding is appropriate, as HTTP method names are always ASCII. The patch is both correct and precise for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-1766": {
        "explanation": "The issue describes that the 'qop' value in the Digest Auth header should be quoted, as per RFC2617, and notes that some servers are sensitive to this. The original code was sending qop=auth (unquoted), while the patch changes it to qop=\"auth\" (quoted). The patch makes a minimal, targeted change: it only adds the required quotes around the 'auth' value in the qop field, matching both the RFC and the behavior of tools like curl. No unrelated or unnecessary changes are made. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-1776": {
        "explanation": "The issue describes that cookies provided for a single request (via the 'cookies' parameter) are being incorrectly merged into the session's persistent cookie jar, causing them to persist across requests. The patch removes the line that merges these per-request cookies into the session's cookies, which directly addresses the problem: per-request cookies should not be persisted in the session. The patch does not introduce unrelated changes and is focused solely on the described issue. However, the issue description notes that removing this line may break the test case for #1630, suggesting there may be edge cases or regressions not handled by this patch. Thus, while the patch addresses the main problem, it may not be fully complete in handling all scenarios.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "psf__requests-1921": {
        "explanation": "The issue describes that setting a session header value to None (e.g., session.headers['Accept-Encoding'] = None) results in the header being sent with the literal value 'None', rather than omitting the header entirely. The patch modifies the code that prepares headers so that only headers with non-None values are included in the prepared request. This means that any header set to None will be omitted from the outgoing request, which matches the expected behavior described in the issue and documentation. The change is minimal and directly targets the described problem without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-1944": {
        "explanation": "The issue describes a problem where, during redirect handling, Requests attempts to decode the response body of a redirect response, even though the body is not needed. This causes failures when the server incorrectly claims the body is gzipped, but it is not. The patch changes the code from 'resp.content' (which triggers decoding) to iterating over 'resp.raw.stream(decode_content=False)', which consumes the response body without attempting to decode it. This matches the issue's suggestion: consume the body to release the socket, but do not decode it, as the content is not needed. The patch is minimal and directly addresses the problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-2317": {
        "explanation": "The issue describes a problem where, in Python 3, if the HTTP method is passed as a bytes object (e.g., b'GET'), the current code converts it to the string representation of the bytes object (i.e., \"b'GET'\"), which is not a valid HTTP method and leads to errors. The patch adds logic to check if 'method' is a bytes object, and if so, attempts to decode it as ASCII. If decoding fails, it falls back to the original behavior. This approach correctly handles the common case where the method is a bytes object containing ASCII characters (as HTTP methods always are), and only falls back to the previous behavior if decoding fails. The patch does not introduce unnecessary changes and directly addresses the described issue. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-2931": {
        "explanation": "The issue describes a regression where sending a binary payload (bytes) as the 'data' parameter to requests.put fails, due to the code attempting to convert the bytes to a native string (which is not appropriate for binary data). The patch checks if 'data' is an instance of bytes, and if so, assigns it directly to 'body' without further encoding. This prevents the inappropriate conversion and matches the behavior from version 2.8.1, thus directly addressing the issue. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-4106": {
        "explanation": "The issue is that after a certain commit, 'requests.packages' no longer exposes 'urllib3' as an attribute, leading to AttributeError when accessing 'requests.packages.urllib3'. The expected behavior is that 'requests.packages.urllib3' should be the 'urllib3' package/module.\n\nThe patch adds print statements for debugging but does not actually assign 'urllib3' (or the other packages) as attributes of the 'requests.packages' module. The code only manipulates sys.modules so that 'requests.packages.urllib3' is a valid import, but 'requests.packages' itself does not have 'urllib3' as an attribute, so 'requests.packages.urllib3' will still raise AttributeError when accessed as an attribute. The patch does not address the core issue described in the report.\n\nTherefore, the patch is incorrect.",
        "label": "INCORRECT"
    },
    "psf__requests-5087": {
        "explanation": "The issue describes a problem where, if accessing response.content raises an exception (e.g., due to a stream error), subsequent accesses to response.content do not raise the same error but instead return an empty string. The expected behavior is that the same exception should be raised on subsequent accesses.\n\nThe patch introduces a new attribute, _content_error, to the Response class. When an error occurs during the first attempt to read the content, the exception is stored in _content_error and re-raised. On subsequent accesses, if _content_error is not None, the stored exception is raised again. This ensures that the same error is consistently raised on every access after the first failure, matching the expected behavior described in the issue. The patch does not introduce unrelated changes and is focused solely on the described problem.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-5414": {
        "explanation": "The issue describes that when making a request to a URL like 'http://.example.com', a UnicodeError is raised due to an invalid label in the host, but the expected behavior is to raise a requests.exceptions.InvalidURL with a clear message. The patch modifies the _get_idna_encoded_host method to catch idna.IDNAError and raise InvalidURL with the message 'URL contains an invalid label.' instead of UnicodeError. This directly addresses the issue by converting the low-level UnicodeError into the higher-level, more informative InvalidURL exception as expected. Additionally, a test is added to ensure that the new behavior is enforced. The patch does not introduce unrelated changes and is precise in its fix.",
        "label": "CORRECT_AND_PRECISE"
    },
    "psf__requests-6028": {
        "explanation": "The issue describes a bug with proxy authentication (HTTP 407 error) when using Python 3.8.12, expecting a 200 status code instead. The provided patch only adds 'build/*' to the .gitignore file, which is unrelated to proxy authentication or any code that would affect HTTP requests or proxy handling. There are no changes to the codebase that would address the described issue. Therefore, the patch does not address the issue at all.",
        "label": "INCORRECT"
    },
    "pydata__xarray-2905": {
        "explanation": "The issue describes an undesired coercion in Variable.__setitem__ when assigning objects that have a 'values' property (but are not xarray Variables), resulting in the object being replaced by its 'values' attribute (e.g., an integer) instead of storing the object itself. The patch modifies the relevant code path to check if the value is a Variable; if not, it further checks if the value has a 'values' property and is not a Variable, in which case it leaves the object as-is (does not coerce). Only if this is not the case does it call as_compatible_data. This logic directly addresses the issue: objects with a 'values' property (but not Variable) are now stored as themselves, not coerced. The added test covers the described scenario and passes if the fix works. No unrelated changes are made. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-3095": {
        "explanation": "The issue describes a regression where copying a Dataset or DataArray with unicode index variables (dtype='<U*') results in those indices being cast to dtype=object, rather than preserving the unicode dtype. The patch addresses this in three places:\n\n1. In `PandasIndexAdapter.__array__`, it ensures that if the requested dtype is unicode ('U'), the array is explicitly cast to that dtype, rather than defaulting to object.\n2. In `as_compatible_data`, it now explicitly preserves unicode dtypes, only converting object dtypes (not unicode) via `_possibly_convert_objects`.\n3. In `Variable.copy`, when copying data, it checks for unicode dtype and preserves it in the new array, rather than letting numpy default to object.\n\nThese changes are narrowly targeted to the unicode/object dtype issue described, and do not introduce unrelated changes. The patch ensures that unicode dtypes are preserved during copy operations, as shown in the issue's expected behavior. No unnecessary changes are made, and the patch is precise in its scope.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-3151": {
        "explanation": "The issue describes that xr.combine_by_coords should not raise a ValueError when combining datasets with identical, non-monotonic coordinates along a dimension that does not vary between datasets. The patch modifies the check for monotonicity in xarray/core/combine.py so that it only checks monotonicity for dimensions that actually varied between datasets (i.e., those in concat_dims), rather than all dimensions. This matches the documentation and the user's expectation. The patch does not introduce unrelated changes and is focused solely on the described issue. The added test scripts (edge_cases.py, reproduce_issue.py) are for demonstration and do not affect the core logic. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-3305": {
        "explanation": "The issue is that DataArray.quantile does not honor the keep_attrs argument: when keep_attrs=True, the output DataArray should retain the input's attrs, but currently it does not. The patch modifies the quantile method to pass keep_attrs through to _from_temp_dataset, and then ensures that if keep_attrs is True, the result's attrs are set to self.attrs. It also updates _from_temp_dataset and _replace to support passing attrs through, and removes an assertion that would prevent this. The new test in test_dataarray.py directly checks that quantile with keep_attrs=True preserves attrs, and that the default (keep_attrs=False) drops them. The changes are minimal and directly address the issue without unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-3520": {
        "explanation": "The issue describes that selection from a MultiIndex using a level name (e.g., `data.sel(y='a')`) does not work as expected: it returns all data instead of just the subset where the 'y' coordinate is 'a'. The patch modifies the `convert_label_indexer` function so that when the index is a MultiIndex and the label is scalar (ndim == 0), it checks if the `index_name` is in the index's level names. If so, it uses `get_loc_level` with the level name, otherwise it defaults to level 0. This change ensures that selection by level name (e.g., 'y') works as intended, matching the expected output in the issue. The patch is minimal and directly targets the described problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-3677": {
        "explanation": "The issue describes that merging a DataArray into a Dataset using the Dataset.merge() method fails, even though the top-level merge() function works. The error occurs because the method expects a Dataset-like object, but receives a DataArray, which lacks the required interface. The patch adds a type check at the start of dataset_merge_method: if 'other' is a DataArray, it is converted to a Dataset via to_dataset(). This mirrors the behavior of the top-level merge() function and ensures that Dataset.merge() can accept a DataArray. The patch also adds a targeted test that verifies the fix. No unrelated or unnecessary changes are introduced. Therefore, the patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-3993": {
        "explanation": "The issue describes an inconsistency in the API: DataArray.integrate uses a 'dim' argument, while Dataset.integrate uses 'coord'. The patch renames the 'dim' parameter in DataArray.integrate to 'coord', and updates its usage accordingly. This directly addresses the inconsistency described in the issue, making the API consistent between DataArray and Dataset. The patch does not introduce unrelated changes. However, the issue also raises the question of whether a deprecation cycle is needed for the argument rename, but the patch does not provide any backward compatibility or deprecation warning for users who may still use 'dim'. Therefore, while the patch addresses the main issue, it may break existing code that uses 'dim' without warning, which is a minor incompleteness.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "pydata__xarray-4075": {
        "explanation": "The issue describes that passing boolean weights to xarray's weighted mean yields incorrect results because the weights are not converted to integers, causing the sum of weights to be a boolean (True/False) rather than a count. The patch addresses this by explicitly converting boolean weights to integers in two places: before the dot product and before the sum of weights calculation. This ensures that the sum of weights is computed correctly. The patch also adds comprehensive tests for both 1D and multidimensional cases, verifying mean, sum, and dot product with boolean weights. The changes are minimal and directly address the described bug without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-4094": {
        "explanation": "The issue describes a failure in to_unstacked_dataset when called on a DataArray with only a single dimension, resulting in a MergeError. The expected behavior is that the roundtrip of stacking and unstacking should work even for single-dimension variables. The patch adds a check for the single-dimension case (self.ndim == 1) and, in that case, simply returns a Dataset containing the DataArray itself, which is the correct and minimal behavior for this scenario. The rest of the logic is left unchanged for the multi-dimensional case. No unnecessary changes are made, and the patch directly addresses the described problem without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-4356": {
        "explanation": "The issue describes that calling sum with min_count and multiple dimensions (axes) raises an error, and requests support for this use case. The patch modifies the relevant logic in nanops.py to handle the case where axis is a tuple or list, computing the valid count correctly using np.sum(~mask, axis=axis) and applying the null mask accordingly. The patch also updates the test suite to include tests for sum with min_count over multiple dimensions, including cases with NaNs and different min_count values. The changes are focused and do not introduce unrelated modifications. The patch directly addresses the problem described in the issue and provides comprehensive test coverage for the new behavior. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-4629": {
        "explanation": "The issue describes that when merging with combine_attrs='override', the merged object's attrs are not a copy but a reference to the first object's attrs, so mutating the merged attrs also mutates the source. The patch changes the code from 'return variable_attrs[0]' to 'return dict(variable_attrs[0])' for the 'override' case, ensuring a new dictionary is returned. This matches the behavior of the other combine_attrs modes and ensures that the merged attrs are independent of the source. The patch is minimal and directly addresses the issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-4687": {
        "explanation": "The issue describes that xr.where does not preserve attributes of the input DataArray, and the expectation is that attributes should be preserved (or at least there should be a way to control this, e.g., via keep_attrs). The patch changes the implementation of the where function to use apply_ufunc with keep_attrs=True, which is the standard xarray mechanism for propagating attributes. This directly addresses the main complaint in the issue: attributes are now preserved by default. The patch does not introduce unrelated changes and is focused solely on the attribute preservation aspect. While the issue also mentions dtype changes, it acknowledges that this is likely a numpy issue and not the responsibility of xarray.where. Therefore, the patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-4695": {
        "explanation": "The issue is that naming a dimension 'method' causes .loc to fail, because the 'method' keyword argument (used for interpolation) is being confused with a dimension name, leading to an invalid fill method error. The patch addresses this by:\n\n1. Modifying convert_label_indexer and remap_label_indexers to detect when the dimension being indexed is named 'method' (via is_method_dim).\n2. When the dimension is 'method', the code ensures that the 'method' argument is not passed to pandas' get_loc or get_indexer_nd, thus avoiding the confusion between the dimension name and the interpolation method argument.\n3. The patch adds a new test file that covers the problematic case (dimension named 'method'), as well as other related scenarios, including interpolation on a different dimension while having a 'method' dimension present.\n\nThe changes are minimal and targeted, and the new tests directly address the reported issue and related edge cases. There are no unnecessary changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-4759": {
        "explanation": "The issue describes that when concatenating or assigning variables in an xarray Dataset, coordinate dtypes of string arrays (either unicode '<U1' or bytes '|S1') are being promoted to 'object', even when all input dtypes are the same. The expected behavior is that the dtype should remain as the original string dtype if all inputs are the same, and only promote to object if mixing unicode and bytes.\n\nThe patch modifies the 'result_type' function to:\n- Collect the dtypes of all inputs.\n- If all are string dtypes (kind 'U' or 'S'), it:\n  - Returns the common dtype if all are identical.\n  - Promotes to object if mixing 'U' and 'S'.\n  - Otherwise, for same kind but different sizes, returns the largest size.\n\nThis logic directly addresses the issue: it prevents unnecessary promotion to object dtype for string coordinates when all are the same, and only promotes when mixing types, which is correct. The patch does not introduce unrelated changes and is focused on the described problem.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-4966": {
        "explanation": "The issue describes that xarray currently handles the _Unsigned=True convention (for storing unsigned bytes in netCDF3) but does not handle the _Unsigned=False hack (for storing signed bytes in OPeNDAP), leading to inconsistent behavior between engines. The patch adds a symmetric handling for _Unsigned == 'false', converting unsigned data to signed using the appropriate dtype and updating the _FillValue accordingly. This directly addresses the missing logic described in the issue, is implemented at the correct location, and does not introduce unrelated changes. The patch is precise and complete for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-5131": {
        "explanation": "The issue describes a problem where the string representation of DatasetGroupBy objects includes a trailing whitespace at the end of the first line, which causes problems with doctests and flake8. The patch adds a __repr__ method to DatasetGroupBy that calls the superclass's __repr__, splits the output into lines, strips trailing whitespace from each line, and then rejoins them. This directly and precisely addresses the issue by ensuring that no line in the representation has trailing whitespace, without making unrelated changes. The rest of the patch is whitespace-only and does not affect functionality. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-5180": {
        "explanation": "The issue is that xarray treats 'Gregorian' (capitalized) as a non-standard calendar, even though the CF conventions specify that calendar names should be case-insensitive. This causes xarray to use cftime objects instead of numpy datetime64, even when the dates are within the valid range. The proposed patch changes the check from 'if calendar not in _STANDARD_CALENDARS' to 'if calendar.lower() not in _STANDARD_CALENDARS', making the check case-insensitive as per the CF conventions. This directly and precisely addresses the issue described, without introducing unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6461": {
        "explanation": "The issue describes a failure in xr.where when the second argument is a scalar and keep_attrs=True, resulting in an IndexError due to the code attempting to access attrs[1] when attrs may not have a second element. The patch modifies the lambda for keep_attrs to check if len(attrs) > 1 before accessing attrs[1], otherwise returning an empty dict. This directly addresses the IndexError by ensuring that the lambda does not attempt to access a non-existent element, and does not introduce unrelated changes. The fix is minimal and precise for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6598": {
        "explanation": "The issue describes that xarray improperly decodes times from a NetCDF file when the time variable is stored as an unsigned integer (uint). The expected behavior is that both 'good_time' and 'time' variables should be decoded identically, but currently they are not.\n\nThe patch, however, does not address the core of the issue: it does not add any logic to handle uint types specifically. Instead, it rewrites the decode_cf_datetime function to use a custom conversion function that attempts to parse the units and reference date, and then applies a timedelta to the reference date for each value. This approach is not robust:\n\n- It does not check or handle the dtype of num_dates (e.g., uint32, uint64), nor does it cast to a safe type for timedelta arithmetic. The original bug is about improper handling of uint types, which can cause overflows or incorrect results if not cast to a signed type or float before arithmetic.\n- The patch introduces print statements (debug code) and changes the function's behavior for all input types, not just uints. It also removes the use of the optimized pandas and cftime decoding paths, which are important for performance and correctness.\n- The patch does not address the specific conversion issues that arise with uint types, such as values wrapping around or being interpreted incorrectly due to type casting.\n- The patch may break existing behavior for other types of time variables and calendars, as it bypasses the established decoding logic.\n\nTherefore, the patch misses the key aspect of the issue (handling uint types correctly) and makes unrelated, broad changes that are not targeted to the problem described.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "pydata__xarray-6599": {
        "explanation": "The issue describes that `polyval` with timedelta64 coordinates produces incorrect results in the latest version of xarray, while it worked correctly in a previous version. The root cause is that the conversion of timedelta64 arrays to numeric values in `datetime_to_numeric` was not defaulting to nanoseconds when no unit was specified, leading to inconsistent or incorrect scaling. The patch modifies the relevant code so that if `datetime_unit` is not specified, the conversion defaults to nanoseconds ('ns'), which matches the expected behavior and restores compatibility with the previous correct results. The change is minimal, targeted, and does not introduce unrelated modifications. Therefore, the patch precisely addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6721": {
        "explanation": "The issue describes that accessing the 'chunks' attribute on a Zarr-backed xarray Dataset causes the entire dataset to be loaded into memory, which is not the expected behavior. The expectation is that 'chunks' should be determined by inspecting the 'encoding' attribute, not by triggering data loading.\n\nThe patch modifies the 'get_chunksizes' function to first check for 'chunks' in the variable's encoding and use that information to populate the chunks dictionary. Only if 'chunks' is not present in encoding does it fall back to checking for dask chunks. This change ensures that for Zarr-backed arrays (which store chunking information in encoding), the code will not access the data (and thus not trigger loading), but will instead use the metadata as intended.\n\nThe patch does not introduce unrelated changes and directly addresses the root cause described in the issue. The logic is precise and matches the expected behavior outlined by the issue reporter and @rabernat.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6744": {
        "explanation": "The issue describes that manual iteration over a DataArrayRolling object with center=True does not yield center-justified windows, whereas calling .mean() does. The patch modifies the __iter__ method of DataArrayRolling to correctly compute the start and stop indices for each window when center=True, ensuring that the windows are centered as expected. The logic for offsetting the window and clamping the indices is correct and matches the expected behavior. Additionally, a new test is added that verifies manual iteration now produces the same results as the direct .mean() call for a centered rolling window. The patch is focused, addresses the issue directly, and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6882": {
        "explanation": "The issue describes a ValueError when attempting to decode a zero-length time coordinate, specifically because reduction operations like min() are called on an empty array. The patch adds a check at the start of decode_cf_datetime: if the input array is empty, it immediately returns an empty datetime64[ns] array, bypassing all further processing. This directly prevents the error described, and the change is minimal and targeted. No unrelated code is modified, and the patch does not introduce unnecessary logic. Therefore, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6889": {
        "explanation": "The issue describes a failure in alignment and reindexing of xarray Datasets after concatenation when a MultiIndex is present. The error arises because the alignment logic does not correctly recognize that two MultiIndexes with the same levels and names are equivalent, leading to a false conflict. The patch modifies the key used for identifying indexes in the alignment process: when a MultiIndex is present, it now includes the level names in the key, ensuring that MultiIndexes with the same structure are matched correctly. This directly addresses the root cause described in the issue, and the change is minimal and targeted. No unnecessary or unrelated changes are introduced. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6938": {
        "explanation": "The issue describes that calling `.swap_dims()` on a Dataset can mutate the original Dataset's variables, specifically their `.dims` attribute, which is unexpected and undesired. The root cause is that the code in `swap_dims` assigns new dimensions to variables that are not deep-copied, so the original variable objects are mutated.\n\nThe patch addresses this by ensuring that, when creating new variables for the swapped dataset, it always makes a deep copy of the variable (using `.copy(deep=True)`) before modifying the `.dims` attribute. This prevents the original Dataset's variables from being mutated, as the changes are now made on a copy.\n\nThe patch is minimal and directly targets the lines where the mutation could occur, without introducing unrelated changes. It does not appear to miss any edge cases relevant to the described issue, as both branches (for index variables and base variables) are covered.\n\nTherefore, the patch is correct and precise in addressing the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-6992": {
        "explanation": "The issue describes a situation where, due to recent changes, the number of coordinate names (`_coord_names`) can exceed the number of variables (`_variables`) in a Dataset. The original implementation of `__len__` in `DataVariables` simply subtracted the length of `_coord_names` from `_variables`, which could result in a negative value and thus the observed ValueError. The patch changes the implementation to count only those variables that are not in `_coord_names`, ensuring the result is always non-negative and accurately reflects the number of data variables. This directly and precisely addresses the issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-7229": {
        "explanation": "The issue describes that when using xr.where(..., keep_attrs=True), coordinate attributes are being overwritten by variable attributes, and the expectation is that coordinate attributes should be preserved. The patch addresses this by, when keep_attrs is not 'drop', collecting the coordinate attributes from the first input object and then, after assigning the merged variable attributes, updating the output's coordinate attributes with the originals. This is done for both tuple and non-tuple outputs, and only when keep_attrs is not 'drop', which matches the intended semantics. The patch is focused and does not introduce unrelated changes. It directly addresses the problem described in the issue and should restore the expected behavior of preserving coordinate attributes when keep_attrs is True.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-7233": {
        "explanation": "The issue describes that after using ds.coarsen(...).construct(...), non-dimensional coordinates (like 'day' in the example) are demoted to variables, whereas the expectation is that all original coordinates (including non-dimensional ones) remain as coordinates in the result. The patch changes the logic so that, instead of only setting as coordinates those that are both in window_dim and obj.coords, it sets all original coordinates (as tracked by 'original_coords') as coordinates in the result. This directly addresses the issue: all coordinates present in the original object will remain coordinates after the construct operation, matching the user's expectation and the provided example. The patch is minimal and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-7347": {
        "explanation": "The issue describes that assign_coords() was resetting all dimension coordinates to their default (pandas) index, even for coordinates that were not being updated. The expected behavior is that only the specified coordinates should be updated, and all others should be preserved. \n\nThe patch changes the assignment logic from overwriting the entire _coords and _indexes dicts to updating them in place: it creates a copy of the current dict, updates only the specified keys, and then assigns the result back. This ensures that unspecified coordinates and indexes are preserved, directly addressing the issue described. The patch does not introduce unrelated changes and is focused on the described problem.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pydata__xarray-7393": {
        "explanation": "The issue describes that stacking a coordinate of dtype int32 ('i4') results in the stacked coordinate being int64 ('i8'), which is undesirable. The expectation is that the dtype should be preserved when stacking, especially for coordinates. \n\nThe patch adds a check after stacking: if the variable being stacked is a coordinate (i.e., its name is in self.coords) and the dtype of the stacked variable differs from the original, it explicitly casts the stacked variable back to the original dtype. This is a targeted fix that only applies to coordinates and only when the dtype changes, thus preserving the original dtype as expected by the user.\n\nThe patch does not introduce unrelated changes and is precise in its scope. It directly addresses the issue described in the minimal example and the user's expectations.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-4551": {
        "explanation": "The issue requests that pyreverse should use Python type hints (PEP 484) for UML generation, especially in cases where an attribute is assigned a default value of None but has a type annotation (e.g., 'a: str = None' should show as 'a : str' in the UML output). The patch modifies the ClassDiagram.get_attrs method to extract type hints for attributes, preferring type hints over inferring from assigned values, and defaults to 'NoneType' if no type hint is found. It adds a helper method get_type_hint that attempts to extract the annotation from the __init__ method or from annotated assignments. The patch also updates association relationships to include type hints. \n\nA new test is added that checks that an attribute with a type hint and a default of None is rendered as 'a : str', and not as 'a : None'. The test also checks that method type annotations are captured. The patch does not introduce unrelated changes and directly addresses the issue as described, including the specific example from the issue.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-4604": {
        "explanation": "The issue describes a false positive where pylint emits an 'unused-import' warning for a module that is only used in a type comment (e.g., 'import abc' when 'abc.ABC' is referenced in a type comment). The expected behavior is that such imports should not be flagged as unused.\n\nThe patch introduces a new function, '_is_name_used_in_type_comment', which recursively checks whether a given name is referenced in any type comment or annotation in the module. The patch then integrates this check into the logic that determines whether an import is unused, skipping the 'unused-import' warning if the import is found in a type comment or annotation. The patch also adds a functional test that covers various cases, including imports used only in type comments and annotations.\n\nThe changes are focused on the described issue, and the new logic is only invoked in the relevant code paths. The patch does not introduce unrelated changes, and the test demonstrates that the false positive is now avoided. While there is some added logging and debug output, these are not strictly necessary for the fix but are not functionally disruptive.\n\nTherefore, the patch is correct and precise in addressing the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-4661": {
        "explanation": "The issue requests that pylint stop storing its data in $HOME/.pylint.d and instead use the XDG Base Directory Specification, specifically $HOME/.local/share/pylint (or the appropriate platform-specific equivalent). The patch does the following:\n\n- Replaces the logic for determining PYLINT_HOME in pylint/config/__init__.py to use appdirs.user_data_dir(\"pylint\", \"pylint\") when the PYLINTHOME environment variable is not set, which is the correct XDG-compliant location for user data.\n- Ensures the directory exists when accessed.\n- Adds tests to verify that PYLINT_HOME is set to the XDG-compliant directory, that the directory is created, and that the PYLINTHOME environment variable is respected.\n- Adds appdirs as a dependency.\n\nThe patch does not make unnecessary changes and directly addresses the issue by moving data storage to the correct XDG location. The tests are relevant and check the new behavior. There are no unrelated or missing aspects with respect to the issue description.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-4970": {
        "explanation": "The issue requests that setting `min-similarity-lines` to 0 should disable the duplicate code check, rather than treating every line as a duplicate. The patch adds a check at the start of the `_compute_sims` method: if `self.min_lines == 0`, it returns an empty list, effectively disabling the similarity check. This is a minimal and targeted change that directly addresses the issue without introducing unrelated modifications. The logic is clear and matches the desired behavior described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-6386": {
        "explanation": "The issue describes that the short option '-v' for 'verbose' incorrectly expects an argument, while the long option '--verbose' works as a flag (does not expect an argument). The patch changes the 'verbose' option's action from a custom '_DoNothingAction' to 'store_true', removes unnecessary kwargs, and sets a default of False. This makes both '-v' and '--verbose' behave as boolean flags, which matches the expected behavior described in the issue. The help text is also updated to be more concise and accurate. No unrelated changes are made. The patch is precise and directly addresses the problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-6528": {
        "explanation": "The issue describes that Pylint's recursive mode does not respect the --ignore, --ignore-paths, or --ignore-patterns options, leading to files like .a/foo.py not being ignored as expected. The patch introduces a new _should_ignore function that checks all three ignore mechanisms (by basename, by regex on basename, and by regex on path) and applies it both to the initial list of files/modules and recursively to submodules discovered during expansion. This replaces the previous logic, which only checked basenames and did not properly apply ignore-paths or ignore-patterns recursively. The patch also adds a test script and test files to verify the behavior. The changes are focused and directly address the described bug, ensuring that all ignore options are respected in recursive mode, as per the documentation and user expectation. There are no unnecessary changes, and the patch is precise in its scope.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-6903": {
        "explanation": "The issue describes a bug where _query_cpu() can return 0 (specifically when cpu.shares is very low, e.g., 2), which causes pylint to crash because multiprocessing requires at least 1 process. The patch changes the return statement to 'return max(1, avail_cpu) if avail_cpu is not None else None', ensuring that the function never returns 0 (it will return at least 1 if avail_cpu is not None). This directly addresses the issue described, as it prevents returning 0 and thus avoids the crash. The patch also adds targeted tests to verify that _query_cpu never returns 0 and returns None when appropriate. No unrelated changes are made. The solution is both correct and precise for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-7080": {
        "explanation": "The issue describes that when running Pylint recursively, the 'ignore-paths' setting from the configuration file is ignored, resulting in files under 'src/gen/*' being linted when they should not be. The patch modifies the config file parser to resolve all paths in 'ignore-paths' to their absolute paths using Path(p).resolve(), and ensures that the config_content['ignore-paths'] is always a list of strings representing absolute paths. This is a necessary step for later matching logic to work correctly, especially in recursive runs where relative paths may not match as expected. The patch is focused, only changes the handling of 'ignore-paths', and does not introduce unrelated changes. It does not address edge cases such as regex patterns (the example uses a regex), but if the rest of the codebase expects resolved paths, this is a reasonable and precise fix for the described problem. Therefore, the patch is correct and precise for the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-7277": {
        "explanation": "The issue describes that pylint unconditionally removes the first item from sys.path, which can cause problems if the first item is not '', '.', or os.getcwd(). The expected fix is to only remove the first item if it matches one of those values. The patch changes the code so that sys.path.pop(0) is only called if sys.path[0] is '', '.', or cwd, which matches the requested behavior. No unrelated changes are made, and the logic is precise and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pylint-dev__pylint-8898": {
        "explanation": "The issue describes that the 'bad-names-rgxs' option, which is supposed to accept regular expressions, incorrectly splits on commas, thus mangling regexes that contain commas (e.g., '(foo{1,3})'). The patch changes the _regexp_csv_transfomer function so that it no longer splits the input string on commas, but instead treats the entire string as a single regex pattern. This directly addresses the problem: now, a regex with commas is not split and is compiled as intended. The patch also updates/adds tests to ensure that regexes with commas are handled correctly and do not cause errors. No unrelated changes are made. Therefore, the patch is correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-10051": {
        "explanation": "The issue describes a divergence between caplog.get_records() and caplog.records after caplog.clear() is called: get_records() returns stale data because the underlying list is replaced, not cleared, so references held by get_records() are not updated. The patch adds logic to clear all lists in records_by_phase (which is where get_records() pulls its data from), ensuring that after caplog.clear(), both caplog.records and caplog.get_records() are in sync and reflect the cleared state. The patch does not introduce unrelated changes and directly addresses the described problem by clearing the lists in place rather than replacing them, which is the correct approach for maintaining reference integrity. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-10081": {
        "explanation": "The issue describes that when running pytest with --pdb, the tearDown method of unittest.TestCase classes decorated with @unittest.skip is still executed, which is incorrect. The patch modifies the condition under which tearDown is replaced with a no-op when --pdb is used: it now checks that neither the test method nor the parent (the class) is skipped. This ensures that if the class is skipped, tearDown is not executed, which directly addresses the issue. The new test added in the patch specifically covers the reported scenario, verifying that the test is skipped and tearDown is not run (no error is raised). The change is minimal and targeted, with no unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-10356": {
        "explanation": "The issue describes that when a class inherits from multiple base classes, each with their own pytest markers, only one set of markers is preserved due to the way attribute lookup works (MRO). The expectation is that all markers from all base classes should be merged, as demonstrated by the metaclass workaround in the issue description.\n\nThe patch modifies `get_unpacked_marks` to recursively collect `pytestmark` from the object and all its base classes (using `__bases__`), and then normalizes the combined list. This approach ensures that markers from all base classes are included, not just the first one found via MRO. This directly addresses the problem described in the issue.\n\nThe patch does not introduce unrelated changes and is focused solely on the described problem. It does not attempt to deduplicate marker names by MRO order, as the issue only suggests this as a possible improvement, not a strict requirement. The patch matches the behavior of the metaclass workaround in the issue, which simply concatenates all markers from the MRO.\n\nTherefore, the patch is correct and precise for the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-10552": {
        "explanation": "The issue describes that pytest inconsistently discovers and runs staticmethod and classmethod tests, especially when combined with @pytest.mark.parametrize. Some tests are not collected, some produce warnings, and some silently fail to run. The expectation is that such tests should either be collected and run, or at least a warning should be issued if they cannot be run.\n\nThe patch introduces logic to better handle staticmethod and classmethod test functions, including those wrapped with @pytest.mark.parametrize. It attempts to unwrap the decorators, correctly identify the underlying function, and reconstruct the test items so that they are properly collected. It also issues warnings when a function cannot be collected, and adds debug prints (presumably for development, though these would be removed in production).\n\nThe patch is focused on the collection logic and does not introduce unrelated changes. It specifically addresses the cases described in the issue: staticmethod/classmethod tests, with and without parametrize, and ensures that they are either collected or a warning is issued. The handling of decorated functions and the reconstruction of test items for staticmethod/classmethod is precise and directly targets the problem described.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-11143": {
        "explanation": "The issue describes a bug where, if the first expression in a Python file is a number (e.g., '42'), the assertion rewrite machinery mistakes it for a docstring, leading to a TypeError when it tries to check for substring membership in an int. The patch adds a test case to ensure that a file starting with a number followed by an assert statement is handled correctly by the rewrite function, i.e., the number is not treated as a docstring. However, the patch only adds a test and does not modify the actual code that would prevent the TypeError. Therefore, while the test is correct and useful, it does not itself fix the underlying bug described in the issue.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "pytest-dev__pytest-5262": {
        "explanation": "The issue describes two main problems: (1) EncodedFile's mode property includes 'b', which causes youtube-dl to treat it as a binary stream and write bytes, but EncodedFile's write() method does not accept bytes, leading to a TypeError; (2) EncodedFile's write() method should be able to handle both str and bytes gracefully, as youtube-dl may write either depending on the mode.\n\nThe patch addresses both:\n- It sets EncodedFile.mode = 'w', ensuring 'b' is not present, so youtube-dl will treat it as a text stream and write str.\n- It updates the write() method to accept bytes as well as str, encoding non-bytes/non-str objects as needed, so it will not raise a TypeError if bytes are written.\n- It adds a test that simulates youtube-dl's behavior, checking that writing str, bytes, and other types all work, and that the mode does not include 'b'.\n\nThe changes are minimal and directly address the issue without unrelated modifications. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5404": {
        "explanation": "The issue describes a KeyError: '__wrapped__' occurring during test collection, specifically when using pytest's doctest plugin and importing certain modules. The traceback shows that the error arises in the _mock_aware_unwrap function, which wraps inspect.unwrap. The patch adds a try/except block around the call to real_unwrap, catching KeyError and returning the original object if such an error occurs. This is a targeted fix: it prevents the KeyError from propagating, which matches the reported problem. The patch does not introduce unrelated changes and only modifies the relevant function. The only possible concern is whether returning the original object is always the correct fallback, but given the context (doctest collection) and the fact that the error is due to missing '__wrapped__', this is a reasonable and minimal fix. Therefore, the patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5550": {
        "explanation": "The issue describes that when using 'junit_family=xunit2', the generated XML report should have a root <testsuites> tag, but currently it only outputs a <testsuite> root, which is the legacy format. The patch modifies the code so that when 'self.family == \"xunit2\"', it wraps the <testsuite> element in a <testsuites> root element before writing to the logfile. For other formats, it preserves the original behavior. The change is minimal, targeted, and directly addresses the requirement described in the issue without introducing unrelated changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5555": {
        "explanation": "The issue describes that when using pytest's stepwise plugin with strict xfail, tests that unexpectedly pass (XPASS(strict)) are not handled correctly: they are reported as failures, but stepwise does not stop after the first XPASS(strict) as it should. The patch modifies the stepwise plugin to detect XPASS(strict) outcomes by checking for '[XPASS(strict)]' in the report's longrepr and by adding logic in pytest_runtest_makereport to handle xfail exceptions with wasxfail set, marking them as lastfailed and stopping the session. The patch also adds a test and configures pytest.ini to use strict xfail, matching the issue's scenario. The changes are focused and directly address the problem described, without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5631": {
        "explanation": "The issue describes a ValueError that occurs when using unittest.mock.patch with a numpy array as the 'new' argument, due to the check 'p.new in sentinels' in num_mock_patch_args. The error arises because numpy arrays do not support element-wise truth evaluation in 'in' checks, leading to ambiguity. The patch addresses this by adding a guard: it only performs 'p.new in sentinels' if 'p.new' does not have a '__array__' attribute (i.e., is not a numpy array or array-like object). This prevents the ValueError for numpy arrays and similar objects. The patch also adds tests for numpy arrays, empty arrays, None, DEFAULT, and custom array-like objects, covering relevant edge cases. The change is minimal and directly targets the problematic check without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5692": {
        "explanation": "The issue requests that the generated JUnit XML reports from pytest include the 'hostname' and 'timestamp' attributes in the <testsuite> element, as seen in standard JUnit XML outputs. The patch adds two lines to the dictionary of attributes passed to the Junit.testsuites() call: one for 'timestamp', formatted in the correct ISO format, and one for 'hostname', using os.uname().nodename. These changes are minimal, directly address the missing attributes, and do not introduce unrelated modifications. The patch is both correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5787": {
        "explanation": "The issue describes that when running tests with exception chains (using 'raise ... from ...' or implicit chaining), pytest's output (when using xdist) only shows the last exception, omitting the full chain. The patch introduces logic to serialize and deserialize the exception chain, adding an 'exception_chain' field to the report serialization. It traverses __cause__ and __context__ to build the chain, and includes this in the report dict. The patch also updates the deserialization logic to reconstruct the chain. The changes are focused on the serialization/deserialization of exception chains and do not introduce unrelated modifications. The approach is correct and precise for the described issue, as it ensures the full exception chain is included in the report data, which is what is needed for xdist to display the full chain. Edge cases (such as very complex or circular chains) are not explicitly handled, but for the described use case, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5809": {
        "explanation": "The issue describes that using 'lexer=python3' when submitting to bpaste.net can cause HTTP 400 errors for some contents, and that using 'lexer=text' avoids this problem and is more appropriate since the content is arbitrary text, not Python code. The patch changes the 'lexer' parameter from a conditional 'python3'/'python' to always 'text' in the code that submits to bpaste.net. The corresponding test is also updated to expect 'lexer=text'. No unrelated changes are made. This directly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-5840": {
        "explanation": "The issue describes a regression in pytest 5.1.2 on Windows, where conftest.py files are loaded with their paths converted to lower case, causing import errors due to incorrect casing (e.g., 'python' vs 'Python'). The patch addresses this in two main ways:\n\n1. In `src/_pytest/pathlib.py`, the `unique_path` function is modified so that on Windows, it does not apply `normcase` (which lowercases the path), but instead preserves the original case by using `realpath` directly. This directly targets the root cause: path case normalization on Windows.\n\n2. In `src/_pytest/config/__init__.py`, the patch adds logic to import conftest.py files using `importlib.util.spec_from_file_location` if the package path is None, which is a more robust way to import modules by filename and avoids issues with import machinery that may be sensitive to path casing. It falls back to the previous method if this fails.\n\nThe other changes (new empty files) are unrelated but do not affect the core logic.\n\nThe patch is focused, addresses the described issue, and does not introduce unnecessary changes. It is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-6197": {
        "explanation": "The issue describes a regression where pytest 5.2.3 collects and tries to import arbitrary __init__.py files, even when they are not explicitly specified or matched by python_files patterns, leading to errors if those files are not meant to be collected (e.g., platform-specific code). The expected behavior (as in 5.2.2) is that __init__.py files are only collected if they are explicitly specified or their containing directory is specified as a test target.\n\nThe patch modifies the collection logic in pytest_collect_file and pytest_pycollect_makemodule to:\n- Only collect __init__.py if it is explicitly passed or its directory is explicitly passed as an argument.\n- Otherwise, only collect files matching python_files patterns.\n- For __init__.py, if not explicitly passed, collect as a Package (not as a Module).\n\nThis matches the expected behavior described in the issue and does not introduce unrelated changes. The changes are targeted and precise, restoring the previous behavior and preventing the unwanted collection of arbitrary __init__.py files.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-6202": {
        "explanation": "The issue describes that in pytest's test report headlines, occurrences of '.[' in test names (such as 'test_boo[..[') are being replaced with just '[' (resulting in 'test_boo[.['), which is incorrect. The root cause is identified as the line 'return s.replace('.[', '[')' in 'getmodpath', which alters the string representation of the test name. The patch removes this replacement, changing it to 'return s', which directly addresses the described problem. The patch does not introduce unrelated changes and only modifies the problematic behavior. Additional tests are added to cover edge cases, but these are relevant and do not constitute unnecessary changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-7205": {
        "explanation": "The issue describes a BytesWarning that occurs when pytest's --setup-show option tries to display a bytes parameter using str(), which is not safe under Python's -bb flag. The patch changes the relevant line in _show_fixture_action to use saferepr instead of str() (via format), which is the correct and recommended way to safely represent bytes (and other potentially problematic types) in output. The patch also adds tests that parametrize over bytes, including edge cases, to ensure the fix is robust. No unrelated changes are made. The patch is precise and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-7236": {
        "explanation": "The issue describes that when running pytest with --pdb, unittest.TestCase.tearDown is executed even for skipped tests, which is incorrect behavior. The patch modifies the logic in TestCaseFunction to check if the test is skipped (via __unittest_skip__) and only overrides tearDown (to postpone it for debugging) if the test is not skipped. This prevents tearDown from being called on skipped tests when --pdb is used, directly addressing the issue. The patch also adds a targeted regression test to ensure that tearDown is not called for skipped tests with --pdb, and that normal behavior is unchanged. No unrelated or unnecessary changes are present. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-7283": {
        "explanation": "The issue describes that when running a skipped unittest.TestCase with --pdb, the tearDown method is still executed, and if it fails, post-mortem debugging is triggered, which is not expected for skipped tests. The patch attempts to prevent post-mortem debugging for skipped tests by walking the traceback frames and checking for a '_skipped_by_mark' attribute on 'self' in the frame's locals. If found, it returns early, skipping the debugger.\n\nThis approach is targeted and does not introduce unrelated changes. However, it relies on the presence of '_skipped_by_mark' in the frame's locals, which may not be a robust or general solution for all ways a test can be skipped (e.g., via unittest.skip, pytest.skip, or other mechanisms). There may be skipped tests where this attribute is not set, so the patch could miss some edge cases. Still, it does address the main scenario described in the issue.\n\nTherefore, the patch is correct for the described case but may not be fully comprehensive for all skip scenarios.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "pytest-dev__pytest-7324": {
        "explanation": "The issue describes a crash in Python debug builds (3.8+) when compiling the string 'False' as an expression, due to an assertion in the CPython compiler. The patch introduces a _safe_compile method in AssertionRewritingHook that, in debug mode, detects when the source is 'True', 'False', or 'None' and instead constructs the appropriate AST node and compiles that, avoiding the problematic code path. For other code, it falls back to the normal compile. The patch also adds a test that exercises both the special case ('False') and a normal expression, verifying that both compile and evaluate as expected. The changelog entry is clear and accurate. The patch is minimal and directly addresses the described crash, without unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-7432": {
        "explanation": "The issue describes that when using --runxfail, the location reported for skipped tests marked with @pytest.mark.skip is incorrect: it points to the skipping machinery in pytest's own code instead of the test's location. The hint points to the pytest_runtest_makereport hook in skipping.py. The patch removes two blank lines at the end of the relevant block, but does not change any logic or fix any code related to how the skip location is determined or reported. Therefore, this patch does not address the issue at all\u2014it is a no-op with respect to the described bug.",
        "label": "INCORRECT"
    },
    "pytest-dev__pytest-7490": {
        "explanation": "The issue describes a regression in pytest 6.x where dynamically adding an xfail marker to a test via request.node.add_marker no longer causes the test to be treated as xfailed (i.e., expected to fail), but instead the test fails as normal. In pytest 5.x, this worked as expected.\n\nThe patch adds logic to Node.add_marker such that when an xfail marker is added, it sets self._evalxfail = marker_.mark. This is the mechanism used by pytest to track whether a test is expected to fail (xfail). By restoring this assignment when the xfail marker is dynamically added, the patch ensures that dynamically added xfail markers are handled the same as statically declared ones, restoring the previous behavior and directly addressing the regression described in the issue.\n\nThe patch is minimal and precise, only affecting the handling of the xfail marker in add_marker, and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-7521": {
        "explanation": "The issue describes a regression in pytest 6.0.0rc1 where captured output via capfd.readouterr() converts carriage returns ('\\r') to newlines ('\\n'), breaking tests that expect the original '\\r' to be preserved. The patch modifies the FDCaptureBinary.read() method to replace a trailing '\\n' with '\\r\\n' only if the result ends with '\\n' and not with '\\r\\n'. However, this does not restore the original carriage return ('\\r') that was present in the output; instead, it changes a trailing '\\n' to a Windows-style '\\r\\n'. The issue is about preserving '\\r', not about converting '\\n' to '\\r\\n'. Therefore, the patch does not address the core problem described in the issue and misunderstands the requirements.",
        "label": "INCORRECT"
    },
    "pytest-dev__pytest-7571": {
        "explanation": "The issue describes that after using caplog.set_level in one test, the log handler's level is not restored for the next test, which is a regression from previous pytest versions. The patch addresses this by:\n\n1. Saving the initial handler level (self._initial_handler_level) the first time set_level is called.\n2. Restoring the handler's level in _finalize if it was changed.\n3. Also saving and restoring the root logger's level, which is a reasonable extension to ensure full restoration of logging state.\n4. The new test file explicitly checks that after a test that sets the level, the handler's level is restored to NOTSET, and that the root logger's level is also restored.\n\nThe patch is focused, does not introduce unrelated changes, and the new tests directly verify the fix. The restoration logic is precise and only applies to the aspects changed by the fixture. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-7982": {
        "explanation": "The patch only updates the changelog to document that the issue with symlinked directories not being collected has been fixed. However, it does not include any code changes that actually address the problem described in the issue (i.e., removing or changing the 'follow_symlinks=False' behavior in the directory traversal code). Therefore, this patch does not address the issue itself, but merely documents a fix that is not present in this patch.",
        "label": "INCORRECT"
    },
    "pytest-dev__pytest-8022": {
        "explanation": "The issue describes that when running doctest collection on an __init__.py file, only a single doctest is collected, even if there are multiple examples. The patch modifies the collection logic in _pytest/doctest.py so that for __init__.py files, each example (i.e., each '>>>' block) is split into its own DoctestItem, rather than grouping all examples into a single test. This directly addresses the problem described in the issue. The patch also updates the session collection logic in _pytest/main.py to ensure all DoctestModule items are yielded for __init__.py files. The included tests in test_doctest_init.py verify that multiple examples in __init__.py are collected as separate tests, and that empty docstrings or SKIP directives are handled correctly. The changes are focused and do not introduce unrelated modifications. Therefore, the patch is correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-8399": {
        "explanation": "The issue describes that, starting from v6.2.0, the auto-generated unittest setUpClass fixtures are no longer \"private\" (i.e., their names do not start with an underscore), causing them to be shown in the default output of `pytest --fixtures` and breaking CI scripts that expect only public fixtures to be shown unless `-v` is used. The patch changes the fixture name from `unittest_{setup_name}_fixture_{obj.__qualname__}` to `_unittest_{setup_name}_fixture_{obj.__qualname__}` by prepending an underscore, restoring the previous behavior. This is a minimal, targeted change that directly addresses the issue without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-8641": {
        "explanation": "The issue is that the regex used to match '%(levelname)' format specifiers in log format strings does not handle precision formatting (e.g., '-5.5s'). The original regex only matches an optional sign and width, but not the precision part. The patch updates the regex to '%(levelname)([+-]?\\d*)(\\.?\\d*)(s)', which now captures an optional sign and width, an optional precision (with a dot), and the 's' type. This change directly addresses the issue by allowing the regex to match format strings like '-5.5s', as shown in the user's example. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-9681": {
        "explanation": "The issue describes a regression in pytest 7.0.0 where the presence of a conftest.py file in a package (e.g., tests/conftest.py) causes importlib-based test collection to fail to recognize the package structure, resulting in ModuleNotFoundError for imports like 'import tests.subpath.helper'. The patch modifies the import_path function to first attempt to resolve the package path and construct the module name relative to the package root, only falling back to the previous root-relative logic if not part of a package. This directly addresses the problem: it ensures that when a test file is part of a package (even if conftest.py is present), the correct fully-qualified module name is used for importlib, restoring the expected import behavior. The patch is focused, does not introduce unrelated changes, and precisely targets the described regression.",
        "label": "CORRECT_AND_PRECISE"
    },
    "pytest-dev__pytest-9798": {
        "explanation": "The issue is that when a test using pytest.approx fails, the error reporting code tries to import numpy (np), causing a ModuleNotFoundError if numpy is not installed, even though pytest.approx itself does not require numpy. The patch removes the unnecessary import of numpy and replaces 'np.inf' with 'float('inf')', which is a standard Python way to represent infinity and does not require numpy. This change is minimal, directly addresses the problem, and does not introduce unrelated changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-10297": {
        "explanation": "The issue described is about the RidgeClassifierCV class in scikit-learn not accepting the 'store_cv_values' parameter, even though the documentation claims it should. The patch, however, modifies the LassoLarsIC class in least_angle.py, specifically changing the calculation of the 'criterion_' attribute. This change is unrelated to RidgeClassifierCV or the 'store_cv_values' parameter. Therefore, the patch does not address the described issue at all.",
        "label": "INCORRECT"
    },
    "scikit-learn__scikit-learn-10452": {
        "explanation": "The issue describes that PolynomialFeatures does not support sparse data, and requests that it should. The patch updates the relevant methods to accept sparse input (by passing accept_sparse=True to check_array), and then adds logic to handle both dense and sparse cases in the transformation. For the sparse case, it allocates a sparse output matrix, computes the polynomial features using sparse operations, and returns the result in CSR format. The dense case is left unchanged. The patch does not introduce unrelated changes and appears to fully address the issue as described, enabling support for sparse input in PolynomialFeatures.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-10844": {
        "explanation": "The issue describes an integer overflow in the calculation of fowlkes_mallows_score when pk and qk are large, specifically in the expression tk / np.sqrt(pk * qk). The proposed patch changes this to np.sqrt(tk / float(pk)) * np.sqrt(tk / float(qk)), which is mathematically equivalent (since sqrt(a*b) = sqrt(a)*sqrt(b)), but avoids the direct multiplication of pk and qk, thus reducing the risk of integer overflow. The patch also ensures floating point division by casting pk and qk to float. No unrelated changes are made, and the patch directly addresses the overflow issue as described. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-10908": {
        "explanation": "The issue describes that when a user provides a vocabulary to CountVectorizer at initialization, calling get_feature_names() before fitting raises a NotFittedError, even though transform() works (because it calls _validate_vocabulary, which sets vocabulary_). The expectation is that get_feature_names() should also work in this case, initializing vocabulary_ if needed, rather than raising an error.\n\nThe patch changes get_feature_names() so that if self.vocabulary is not None and self.vocabulary_ is not set, it calls _validate_vocabulary() (which sets vocabulary_ from the provided vocabulary), otherwise it calls _check_vocabulary() (which raises NotFittedError if vocabulary_ is not set). This mirrors the logic in transform(), ensuring that get_feature_names() works as expected when a vocabulary is provided at initialization, even if fit() has not been called.\n\nThe patch is minimal and directly addresses the issue without introducing unrelated changes. It ensures that get_feature_names() behaves consistently with transform() in the presence of a user-provided vocabulary.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-10986": {
        "explanation": "The issue describes a bug in warm-starting a LogisticRegression model with binary outcomes and multi_class='multinomial', where the coef shape is (1, n_features) but w0 expects (2, n_features). The original code incorrectly broadcasts coef into w0, leading to incorrect warm starts and degraded performance on repeated fits. The proposed patch adds a conditional: if multi_class is 'multinomial', there are 2 classes, and coef has shape (1, n_features), then it sets w0[0] to -coef[0] and w0[1] to coef[0], which matches the expected initialization for multinomial logistic regression in the binary case. Otherwise, it falls back to the original assignment. This directly addresses the bug as described, is limited in scope, and does not introduce unrelated changes. The logic matches the suggested fix in the issue and is precise for the described scenario.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-11310": {
        "explanation": "The patch introduces a new attribute, `refit_time_`, to `BaseSearchCV` and its subclasses, which records the time taken to refit the best estimator on the entire dataset when `refit=True`. The timing is implemented by capturing the time before and after the refit operation and storing the elapsed time in `self.refit_time_`. The patch also updates the class docstring to document the new attribute. Furthermore, it adds comprehensive tests to ensure that `refit_time_` is present and a positive float when `refit=True`, and absent when `refit=False`. This directly and precisely addresses the issue described, which requested a way to retrieve the refit time. The changes are minimal and focused, without unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-11578": {
        "explanation": "The issue describes that LogisticRegressionCV with multi_class='multinomial' was not using the correct probability calculation for scoring, because the internal LogisticRegression instance was always created with the default multi_class='ovr'. The proposed patch changes the instantiation of LogisticRegression in _log_reg_scoring_path to pass multi_class=multi_class, as well as other relevant parameters (penalty, dual, solver, intercept_scaling) that may affect the model's behavior. This ensures that the scoring uses the same configuration as the main estimator, and thus the probabilities and scores will be consistent with the user's intent and the behavior of LogisticRegressionCV. The patch does not introduce unrelated changes and directly addresses the root cause described in the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-12421": {
        "explanation": "The issue describes a discrepancy between the scikit-learn and R implementations of OPTICS regarding the value of core distances for points that can never be core points. According to the documentation, such points should have a core distance of infinity, but the current implementation does not set them to inf. The patch adds a line that sets core_distances to np.inf for points where the computed core distance exceeds self.max_eps, which is the correct criterion for points that cannot be core points. This change is minimal, directly addresses the issue, and does not introduce unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-12585": {
        "explanation": "The issue describes a failure in scikit-learn's `clone` function when an estimator parameter is a class (type) rather than an instance. The error occurs because the code attempts to call `get_params()` on a class, which is invalid. The proposed patch modifies the `clone` function to treat objects that are classes (i.e., `isinstance(estimator, type)`) as non-estimators, thus avoiding the erroneous call to `get_params()`. The patch is minimal and directly targets the problematic condition. It also includes comprehensive tests for the new behavior, covering both the specific case in the issue and related scenarios (nested, lists, etc.), and asserts that standard cloning is unaffected. No unrelated changes are introduced. The patch is therefore correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-12682": {
        "explanation": "The patch adds a `max_iter` parameter to the `SparseCoder` class, exposes it in the constructor, and ensures it is passed through to the underlying Lasso estimator when `transform_algorithm='lasso_cd'`. The default is set to 1000, matching the previous implicit default, and the docstring is updated accordingly. The patch also adds comprehensive tests to verify that the parameter is set and used correctly, including checks for convergence warnings and correct defaulting. No unrelated changes are made. This directly and precisely addresses the issue described, which was the inability to set `max_iter` for the underlying Lasso in `SparseCoder`.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-12973": {
        "explanation": "The issue describes a problem where the `copy_X` parameter is duplicated: it exists both as a class attribute (set at initialization) and as a parameter to the `fit` method (with a default of True). This can lead to unintuitive behavior, as the class-level setting can be silently overridden by the method default. The proposed solution is to have `fit` accept `copy_X=None` by default, and only override the class attribute if the user explicitly passes a value to `fit`.\n\nThe patch implements exactly this: it changes the `fit` signature to `copy_X=None`, updates the docstring to explain the new behavior, and in the method body, sets `copy_X = self.copy_X if copy_X is None else copy_X`. This ensures that the class attribute is used unless the user explicitly passes a value to `fit`, which is the intended and standard behavior in scikit-learn estimators. No unnecessary changes are made, and the patch is precise and limited to the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13124": {
        "explanation": "The issue describes that StratifiedKFold's shuffle parameter does not actually shuffle the samples within each class/stratum, but only shuffles the order of the folds, which is not useful for cross-validation. The expected behavior is that, when shuffle=True, the samples within each class should be shuffled before being split into folds, and that different random_state values should yield different splits.\n\nThe patch modifies the _make_test_folds method of StratifiedKFold. Instead of using KFold.split on dummy arrays (which did not shuffle within classes), it now explicitly collects the indices for each class, shuffles them (if shuffle=True), and then assigns them to folds in a round-robin fashion. This ensures that the samples within each class are shuffled and distributed across folds, and that different random_state values will yield different splits. The patch also updates the documentation string for the shuffle parameter to clarify its behavior.\n\nAdditionally, a new test script is added to verify that the splits change with different random_state values when shuffle=True, and that the splits are stratified.\n\nThere are no unnecessary changes, and the patch directly addresses the core of the issue as described. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13135": {
        "explanation": "The issue is that KBinsDiscretizer with strategy='kmeans' can produce unsorted cluster centers, which leads to unsorted bin_edges, causing np.digitize to throw a ValueError because it requires monotonically increasing or decreasing bins. The patch addresses this by sorting the cluster centers before computing the bin edges, ensuring that bin_edges will be sorted and np.digitize will work as expected. The patch makes a minimal, targeted change: it adds np.sort to the cluster centers before calculating bin_edges, and does not introduce unrelated changes. The added reproduce_error.py file is just a reproduction script and does not affect the core logic. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13142": {
        "explanation": "The issue describes a discrepancy between the results of fit_predict(X) and predict(X) in GaussianMixture when n_init > 1. The root cause is that fit_predict was returning the argmax over the log responsibilities (log_resp) from the best run, while predict uses the model parameters (means, covariances, weights) from the best run to compute the weighted log probabilities and then takes the argmax. These two can differ, especially when n_init > 1 and the best run is selected based on log likelihood, but the log_resp from that run is not recomputed with the final parameters. \n\nThe patch changes fit_predict to return self._estimate_weighted_log_prob(X).argmax(axis=1), which matches the behavior of predict. This ensures that both methods use the final model parameters to assign cluster labels, resolving the inconsistency described in the issue. The patch is minimal and does not introduce unrelated changes.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13328": {
        "explanation": "The issue describes that HuberRegressor.fit fails with a boolean X, raising a TypeError, and that the expected behavior is for boolean X to be automatically cast to float, as is done in LinearRegression. The patch modifies the call to check_X_y in HuberRegressor.fit to specify dtype=[np.float64, np.float32], which will cause check_X_y to cast boolean (and other compatible) dtypes to float, matching the behavior of LinearRegression. The included test script in the patch demonstrates that the original float X, boolean X (which should now work), and boolean X cast to float all work as expected. The patch is minimal and directly addresses the issue without unrelated changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13368": {
        "explanation": "The issue describes a problem where `cross_val_predict` with `predict_proba` returns all-zero probability vectors for some samples when a class is missing from a training split (e.g., due to very few samples of that class). The expected behavior is that the output should still be a valid probability distribution (summing to 1), even if some classes are missing in a split.\n\nThe patch modifies the code so that after filling the `predictions_for_all_classes` array (which is zero for classes not seen in the split), it normalizes each row to sum to 1, avoiding division by zero. This ensures that the output is always a valid probability distribution, addressing the issue where all-zero vectors were returned.\n\nThe patch is minimal and directly targets the described problem, without introducing unrelated changes. It also includes a safeguard against division by zero. While it does not attempt to \"guess\" probabilities for missing classes (which would be a more complex and potentially controversial change), it ensures the output is always a valid probability distribution, which is the main requirement described in the issue.\n\nTherefore, the patch is correct and precise for the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13439": {
        "explanation": "The issue requests that the Pipeline class implement the __len__ method so that len(pipe) returns the number of steps in the pipeline, which is currently not possible and raises an error. The patch adds a __len__ method to the Pipeline class that returns len(self.steps), which is the correct and expected behavior. The patch does not introduce any unrelated changes and is minimal. The included test script demonstrates the intended usage. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13447": {
        "explanation": "The issue describes that when using sample_weight in label_ranking_average_precision_score, samples with all-true or all-false labels (the special case) are not properly weighted: the code always adds 1.0 to the output, regardless of the sample_weight for that sample. The patch changes this so that, in the special case, if sample_weight is provided, it adds sample_weight[i] instead of 1.0. This directly addresses the bug described, ensuring that the contribution of these special-case samples is properly weighted. The patch is minimal and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13485": {
        "explanation": "The issue describes the inability of CalibratedClassifierCV to handle 3D input arrays, which are common in metric learning scenarios (e.g., pairs of samples). The request is to make CalibratedClassifierCV more tolerant of such inputs, similar to other meta-estimators like GridSearchCV.\n\nThe patch addresses this by:\n- Adding a private method _reshape_3d_to_2d that flattens 3D inputs to 2D (n_samples, n_dim1 * n_dim2) before passing them to check_array or using them in fit/predict methods.\n- Modifying the docstrings to indicate that 3D input is now accepted.\n- Applying this reshaping in fit, predict_proba, and predict methods of CalibratedClassifierCV, as well as in the internal _CalibratedClassifier class.\n- Adding a test script that verifies both 2D and 3D inputs are accepted and processed without error.\n\nThe patch does not remove check_array, but instead preprocesses the input to make it compatible, which is a safe and minimal change. It does not introduce unrelated changes or miss any key aspect of the issue. The approach is consistent and precise, and the added test demonstrates the intended behavior.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13496": {
        "explanation": "The patch exposes the 'warm_start' parameter in the IsolationForest class's __init__ method, adds it to the docstring with wording consistent with RandomForestClassifier, and passes it to the superclass constructor. This directly addresses the first two bullet points of the issue: exposing the parameter and documenting it. The patch does not add a test or update the example documentation, but those are additional suggestions rather than requirements for the core functionality. The patch is focused and does not introduce unrelated changes. Therefore, it is correct and precise for the main request.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13584": {
        "explanation": "The issue is that when using print_changed_only=True, the new repr for estimators attempts to compare numpy arrays with '!=', which raises a ValueError due to the ambiguous truth value of arrays. The patch adds a check: if both the parameter and its default are numpy arrays, it uses np.array_equal to compare them, otherwise it falls back to the original logic. This directly addresses the problem described in the issue, and the change is minimal and targeted. No unnecessary changes are introduced, and the logic is correct for the described scenario.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-13779": {
        "explanation": "The issue describes a failure in VotingClassifier's fit method when an estimator is set to None and sample_weight is provided. The original code does not check if the estimator is None before checking for sample_weight support, leading to an AttributeError. The patch adds a check to skip None estimators in the relevant loop, preventing the AttributeError. The rest of the patch ensures that None estimators are also skipped during parallel fitting, which is consistent with the intended behavior. The changes are minimal and directly address the issue without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14053": {
        "explanation": "The issue describes an IndexError in export_text when the tree has only one feature and feature_names is provided. The patch modifies the construction of feature_names_ to handle cases where the feature index is _tree.TREE_UNDEFINED, assigning 'undefined' in such cases, which prevents out-of-range indexing. This directly addresses the root cause of the IndexError. The patch also adds comprehensive tests, including the single-feature case, with and without feature names, and with show_weights, ensuring the fix is robust. No unrelated changes are introduced. Therefore, the patch is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14087": {
        "explanation": "The issue describes an IndexError that occurs in LogisticRegressionCV when refit=False, specifically in the code path that averages over coefs_paths for the multinomial case. The error message 'too many indices for array' suggests that the code is sometimes indexing coefs_paths as if it has four dimensions, when in some cases it only has three. The patch addresses this by checking the dimensionality of coefs_paths: if it is 3D, it uses the appropriate indexing; if it is 4D, it uses the original indexing. This conditional logic directly targets the root cause of the IndexError, and the rest of the patch only adds a minor, related safeguard for l1_ratio_ assignment. No unrelated changes are made. Therefore, the patch is both correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14141": {
        "explanation": "The issue requests that 'joblib' be added to the dependencies listed in show_versions. The patch modifies sklearn/utils/_show_versions.py to include 'joblib' in the list of dependencies whose versions are reported by show_versions(). This directly addresses the issue. The addition of a new script (reproduce_error.py) that simply calls sklearn.show_versions() is not strictly necessary for the fix, but it does not alter the core library and can be seen as a minimal demonstration or test. The main change is precise and does not introduce unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14496": {
        "explanation": "The issue describes a bug where, if min_samples is passed as a float (e.g., 0.1), the code computes min_samples = max(2, min_samples * n_samples), which results in a float. This float is then passed to NearestNeighbors, which expects an integer, causing a TypeError. The proposed fix is to ensure min_samples is an integer by rounding and casting to int. The patch changes the line to min_samples = max(2, int(round(min_samples * n_samples))), which matches the suggested fix in the issue description. This change is minimal, directly addresses the bug, and does not introduce unnecessary modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14629": {
        "explanation": "The issue describes an AttributeError when using cross_val_predict with method='predict_proba' and a MultiOutputClassifier, because the code tries to access estimator.classes_ when it should access estimator.estimators_[i].classes_ for each output. The patch updates the relevant code paths: when predictions is a list (i.e., multi-output), it now checks if the estimator has an 'estimators_' attribute and uses estimator.estimators_[i_label].classes_ if so, otherwise falls back to estimator.classes_[i_label]. Similarly, for the single-output case, it uses estimator.estimators_[0].classes_ if available. This directly addresses the bug described, is limited to the relevant code, and does not introduce unrelated changes. The patch is precise and correct for the described scenario.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14710": {
        "explanation": "The issue describes a bug where HistGradientBoostingClassifier fails with string targets when early stopping is enabled, because the scorer receives integer-encoded y_true but predict returns string labels, causing a type mismatch. The proposed patch updates the _check_early_stopping_scorer method to decode y_small_train and y_val back to their original string labels using self.classes_ before passing them to the scorer. This ensures y_true and y_pred are of the same type, directly addressing the root cause of the error. The patch is minimal, targeted, and does not introduce unrelated changes. It also adds a test script that would fail on the original code and pass with the patch, further confirming the fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14894": {
        "explanation": "The issue describes a ZeroDivisionError occurring when the number of support vectors (n_SV) is zero, specifically in the calculation of dual_coef_indptr, which divides by n_class. The patch adds a conditional check for n_SV == 0, and in that case, directly assigns self.dual_coef_ to an empty sparse matrix of the correct shape, avoiding the problematic division. If n_SV is not zero, the original logic is preserved. This change is minimal, directly addresses the error, and does not introduce unrelated modifications. It also matches the expected behavior described in the issue (self.dual_coef_ = sp.csr_matrix([])), but with the correct shape (n_class, n_SV). Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-14983": {
        "explanation": "The issue describes that RepeatedKFold and RepeatedStratifiedKFold lack a custom __repr__ method, resulting in the default object representation being shown. The expected behavior is for __repr__ to return a string with the class name and the values of n_splits, n_repeats, and random_state. The patch adds a __repr__ method to both classes, returning a string in the format 'ClassName(n_splits=..., n_repeats=..., random_state=...)', using the appropriate attributes. The implementation is concise and directly addresses the issue without introducing unrelated changes. The use of self.cvargs['n_splits'], self.n_repeats, and self.random_state matches the expected output and the class structure. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-15094": {
        "explanation": "The issue describes that MaxAbsScaler always outputs float64, even when the input is float32, due to the use of check_array with dtype=FLOAT_DTYPES, which upcasts to float64. The patch changes the dtype argument in check_array to use the dtype of X if it is in FLOAT_DTYPES, otherwise defaults to FLOAT_DTYPES. This means that if the input is float32, check_array will preserve float32, and the output will also be float32, as expected. The patch only changes the dtype handling in fit, transform, and inverse_transform, and does not introduce unrelated changes. It directly and precisely addresses the upcasting issue described in the issue, without unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-15100": {
        "explanation": "The issue describes that `strip_accents_unicode` fails to remove accents from strings that are already in NFKD form, because the function previously returned the string unchanged if it was already normalized. The expected behavior is that both precomposed (e.g., '\u00f1') and decomposed (e.g., 'n' + COMBINING TILDE) forms should be stripped to the same result ('n').\n\nThe patch removes the conditional check `if normalized == s: return s` and always applies the accent-stripping logic: it returns `''.join([c for c in normalized if not unicodedata.combining(c)])` for all inputs. This ensures that even if the input is already in NFKD form (like 'n' + COMBINING TILDE), the combining characters are removed, and the function returns the correct accent-stripped string.\n\nNo unrelated or unnecessary changes are made. The patch is minimal and directly addresses the described bug.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-25102": {
        "explanation": "The patch introduces an optional 'preserve_dtypes' argument to set_output and propagates this through the output configuration and wrapping logic. When 'preserve_dtypes' is True and the input is a DataFrame, the dtypes from the original input are passed to _wrap_in_pandas_container, which then uses astype to set the dtypes of the output DataFrame columns that match. This directly addresses the issue: it allows users to preserve the dtypes of the input DataFrame when using transformers that do not modify the input values, as requested. The patch is careful to only apply dtypes to columns present in both the dtypes and the output, and does not introduce unrelated changes. The docstrings are updated accordingly. Therefore, the patch is correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-25232": {
        "explanation": "The patch adds a `fill_value` parameter to the `IterativeImputer` class, updates its docstring, includes it in the parameter validation, stores it as an attribute, and passes it to the internal `SimpleImputer` instance used for initial imputation. This directly addresses the issue: it allows users to specify `fill_value` when `initial_strategy='constant'`, mirroring the behavior of `SimpleImputer` as requested. The patch does not introduce unrelated changes and is limited to the necessary modifications. There is no evidence of missing edge cases in the patch, such as support for `np.nan`, but since `SimpleImputer` already supports this and the patch simply passes the value through, this is sufficient. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-25570": {
        "explanation": "The issue is that when using ColumnTransformer with pandas output, if a transformer is assigned to an empty set of features (e.g., categorical_features = []), the resulting DataFrame list (Xs) contains an empty DataFrame. The original code does not filter these out, leading to a length mismatch when setting column names after concatenation, as the number of columns in the concatenated DataFrame does not match the number of feature names generated (since the empty transformer is skipped in self._iter(fitted=True)).\n\nThe patch addresses this by:\n- Filtering out empty DataFrames from Xs before concatenation.\n- If all DataFrames are empty, returning an empty DataFrame with the correct index.\n- Adjusting the logic for transformer_names and feature_names_outs to only use the non-empty DataFrames, ensuring the lengths match and avoiding the ValueError.\n\nThe patch does not introduce unrelated changes and directly targets the described bug. It also handles the edge case where all transformers are empty. The changes are minimal and precise, only affecting the relevant code path for pandas output.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-25747": {
        "explanation": "The patch only adds a new file, 'reproduce_error.py', which contains a script to reproduce the error described in the issue. It does not modify any existing code or provide a fix for the problem. The issue is about FeatureUnion (make_union) not working with custom transformers that aggregate data when 'transform_output' is set to 'pandas'. The patch simply demonstrates the problem, but does not address or resolve it in any way. Therefore, it does not address the issue described.",
        "label": "INCORRECT"
    },
    "scikit-learn__scikit-learn-25931": {
        "explanation": "The issue arises because, when contamination != 'auto', the IsolationForest fit method internally calls score_samples(X) to compute the offset. If X was a DataFrame with feature names, but the internal call uses a transformed or validated version of X (possibly an ndarray), this can trigger a warning about feature names mismatch. The patch addresses this by storing the original input X as self._fit_X before any validation/transformation, and then uses self._fit_X in the offset computation, ensuring that the feature names are preserved and the warning is not triggered. The patch also deletes self._fit_X after use to avoid unnecessary memory usage. The change is minimal, targeted, and does not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-25973": {
        "explanation": "The issue describes that passing an iterable of splits (as produced by a cross-validator's split method) to the SequentialFeatureSelector's cv parameter results in an IndexError, even though the documentation suggests this should be supported. The patch addresses this by checking if self.cv is an iterable but not a sized container (i.e., has __iter__ but not __len__), and if so, converts it to a list at the start of fit(). This ensures that the splits can be iterated over multiple times as required by downstream code, preventing the IndexError. The patch does not introduce unrelated changes and is focused on the described problem. The included test script demonstrates the fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-26194": {
        "explanation": "The issue describes that when using probability estimates (i.e., all thresholds <= 1), the current implementation prepends a threshold value of thresholds[0] + 1, which can result in a threshold greater than 1. This is not appropriate for probability estimates, which should be bounded by [0, 1]. The patch checks if all thresholds are <= 1 and, in that case, prepends 1.0 instead of thresholds[0] + 1. Otherwise, it retains the original behavior. This directly and precisely addresses the issue without introducing unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-26323": {
        "explanation": "The issue describes that when using set_output on a ColumnTransformer, the output setting is not propagated to the 'remainder' transformer if it is an estimator (i.e., not 'passthrough' or 'drop'). This leads to inconsistent output types when the remainder is an estimator, as shown in the provided example. The patch adds logic to check if the remainder is not 'passthrough' or 'drop', and if so, calls _safe_set_output on it, just as is done for the explicitly listed transformers. This directly and precisely addresses the described bug, without introducing unrelated changes or missing any described aspect of the issue. The patch is minimal and targeted.",
        "label": "CORRECT_AND_PRECISE"
    },
    "scikit-learn__scikit-learn-9288": {
        "explanation": "The issue describes that KMeans produces slightly different results (inertia values) when run with n_jobs=1 versus n_jobs>1, even with the same random_state and data. The expected behavior is that results should be identical regardless of n_jobs, for reproducibility.\n\nThe patch addresses this by generating a list of random seeds (one per n_init run) at the start, using the provided random_state, and then using these seeds for both the single-threaded and parallel code paths. Previously, the single-threaded path would use the same random_state object for each run, while the parallel path would generate a list of seeds and use those. This could lead to different initializations and thus different results. The patch unifies the seeding logic, ensuring that both code paths use the same seeds in the same order, making the results consistent across n_jobs values.\n\nThe patch also adds documentation and two tests (one in a new file and one in the main test suite) that check for consistency of inertia across different n_jobs values, which directly tests the issue described.\n\nThere are no unnecessary changes, and the patch is focused on the described problem. The solution is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-10097": {
        "explanation": "The issue describes a problem where long option names in the LaTeX output (specifically in the index) overflow the line, making the output unreadable. The patch modifies the LaTeXTranslator to insert '\\allowbreak{}' after commas, equal signs, and opening square brackets in option strings. This allows LaTeX to break lines at these points, which are natural break opportunities in option syntax. The changes are targeted and do not introduce unrelated modifications. The patch directly addresses the described overflow issue by enabling line breaks at logical places, which should resolve the problem as described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-10323": {
        "explanation": "The issue describes that when using the :prepend: (and :append:) options with the literalinclude directive, the leading whitespace is not preserved to match the indentation of the included code, resulting in poorly formatted output. The expected behavior is that prepended (and appended) lines should match the indentation of the included code block.\n\nThe patch modifies the prepend_filter and append_filter methods in LiteralIncludeReader to:\n- Split the prepend/append text into lines.\n- Detect the indentation of the first (for prepend) or last (for append) line of the included content.\n- Apply this indentation to each non-blank line in the prepend/append content.\n- Add the processed lines to the beginning/end of the included lines.\n\nThis directly addresses the issue: prepended and appended lines will now match the indentation of the included code, as requested. The patch does not introduce unrelated changes (except for some test and whitespace cleanups), and the new test specifically checks that the indentation is preserved as expected.\n\nThere are some extra lines in append_filter (e.g., replacing comments, removing 'def bar()', and adding a coding line) that seem unrelated to the prepend/append indentation issue. These changes are not described in the issue and may be considered unnecessary or even problematic, as they alter the included content in ways not requested by the user.\n\nTherefore, while the patch does address the main issue, it also introduces unrelated modifications, which is not ideal.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "sphinx-doc__sphinx-10435": {
        "explanation": "The issue describes unwanted whitespace at the start and end of inline code in LaTeX output, due to the way the LaTeX macros are emitted. The desired fix is to use a percent sign (%) at the end of the opening macro and at the start of the closing macro to suppress the insertion of spaces by LaTeX. The patch changes '\\sphinxcode{\\sphinxupquote{' to '\\sphinxcode{\\sphinxupquote{%' and '}}' to '%}}', which matches the recommended LaTeX markup in the issue description. No unrelated changes are made. This directly and precisely addresses the whitespace problem as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-10449": {
        "explanation": "The issue is that when using autodoc_typehints = \"description\", Sphinx's autoclass includes a return type annotation for the class itself, which is not expected or correct. The patch modifies the merge_typehints function in sphinx/ext/autodoc/typehints.py to exclude the 'return' annotation when processing classes (objtype == 'class'), ensuring that return type information is not added to class documentation. The patch also adds a test that verifies that the class documentation does not include a return type annotation, while method documentation does. The changes are minimal and directly address the described issue without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-10466": {
        "explanation": "The issue describes that the 'locations' list in message entries is being duplicated, resulting in repeated file:line references in the generated .pot file. The proposed patch modifies the __init__ method of the relevant class to deduplicate the 'locations' list by converting it to a set and back to a list. This directly addresses the problem of duplicate locations. The patch does not introduce unrelated changes and is limited to the deduplication logic. While the issue author notes that similar deduplication may be needed elsewhere (e.g., in babel), the patch as submitted is scoped to the Sphinx codebase and the described bug. Thus, the patch is correct and precise for the Sphinx side of the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-10614": {
        "explanation": "The issue is that SVG inheritance diagrams generated by Sphinx have relative links that are incorrect when the diagram is embedded in a document not at the root directory. The links in the SVG are relative to the SVG file, but the actual link targets are relative to the embedding HTML file, causing 404s when navigating from subdirectories.\n\nThe patch modifies the `fix_svg_relative_paths` function to take an additional `docpath` argument, and updates the logic for rewriting relative links in SVG files. It attempts to adjust the URLs so that they are correct relative to the embedding document. The patch also updates all call sites to pass the current document path.\n\nThe core of the fix is in the new logic that checks if the URL is relative and then prepends '../' or '..' as appropriate. However, the logic for adjusting the relative paths is simplistic: it prepends '../' or '..' based on some string checks, but does not actually compute the correct relative path from the SVG file's location to the target HTML file. For example, if the SVG is in 'my_package/' and the link is '../my_class_1.html', the correct link should be 'my_class_1.html' (relative to 'my_package/'), but the patch would prepend '..' again, resulting in '../../my_class_1.html', which is incorrect.\n\nTherefore, while the patch attempts to address the issue and makes progress, it does not robustly compute the correct relative path between the SVG file and the target HTML file. It may work in some cases but will fail in others, especially with deeper directory structures or more complex relative paths.\n\nThus, the patch is CORRECT_BUT_INCOMPLETE: it addresses the issue in some cases but does not handle all edge cases and may still produce incorrect links in certain directory layouts.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "sphinx-doc__sphinx-10673": {
        "explanation": "The issue describes that users want to include 'genindex', 'modindex', and 'search' in their toctree without Sphinx emitting warnings about nonexisting documents. The patch addresses this by:\n\n1. Modifying the toctree parsing logic to recognize these special pages and not emit warnings for them.\n2. Ensuring these special pages are not treated as regular documents for dependency tracking and toctree includes.\n3. Adjusting the TOC collector to handle these special pages by inserting an empty bullet list and returning early, preventing further processing.\n4. Adding a test that verifies no warnings are emitted when these special pages are included in a toctree, and that they do not appear as regular document links in the TOC.\n\nThe changes are tightly scoped to the described problem, do not introduce unrelated modifications, and the test covers the main scenario. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-11192": {
        "explanation": "The issue describes that when building Sphinx documentation with parallelism enabled (using -j auto), the generated search index is much smaller and incomplete compared to a serial build, leading to broken search functionality. This suggests that the search index is not being correctly aggregated from all parallel processes.\n\nThe patch introduces a multiprocessing.Manager().list() (result_list) to be shared among parallel processes, and passes this list to the _write_parallel method. After the parallel writing is done, it calls _finalize_search_index(result_list) to aggregate the results from all processes. This approach directly addresses the problem of missing or incomplete aggregation of search index data when building in parallel, which is the root cause described in the issue.\n\nThe patch is focused and does not introduce unrelated changes. It uses a standard multiprocessing pattern to ensure data integrity across processes. The only possible concern is whether all relevant data is correctly appended to result_list in the parallel workers and whether _finalize_search_index properly merges them, but based on the patch and the issue, this is the correct and precise fix for the described problem.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-11445": {
        "explanation": "The issue describes that when 'rst_prolog' is set, and a document starts with a domain directive (e.g., ':mod:`mypackage2`') as the first heading, the heading is not rendered correctly. The root cause is that the prolog is prepended with a blank line, which interferes with parsing when a heading with a domain directive is at the top.\n\nThe patch addresses this in two ways:\n1. In 'sphinx/util/rst.py', it only inserts a blank line after the prolog if the next line does not start with '=' (which is a common heading underline in reST). This prevents an extra blank line before a heading.\n2. In 'sphinx/parsers.py', it adds logic to skip prepending the prolog entirely if the first line is a domain directive (starts with ':') and the document is empty or does not start with a description node. This prevents the prolog from interfering with the heading parsing in these specific cases.\n\nBoth changes are tightly scoped to the described problem and do not introduce unrelated modifications. The logic is directly tied to the issue's reproduction steps and the root cause. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-11510": {
        "explanation": "The issue describes that when using the 'source-read' event in a Sphinx extension to modify the source of included files, the modifications are not reflected in the final output. The root cause is that, although the event is fired and the source is modified in memory, the modified content is not written back or used when the included file is processed as part of another document.\n\nThe patch modifies the 'note_included' method to:\n1. Re-emit the 'source-read' event for the included file.\n2. Read the source of the included file, pass it through the event, and then write the possibly modified source back using 'self.app.env.write_source'.\n\nThis ensures that any modifications made by extensions listening to 'source-read' are persisted and used when the included file is processed, thus addressing the issue described. The patch is focused and does not introduce unrelated changes. The docstring is also slightly improved for clarity.\n\nTherefore, the patch correctly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7440": {
        "explanation": "The issue describes a warning about duplicate glossary terms in Sphinx when the only difference is case (e.g., 'MySQL' vs 'mysql'). The expectation is that 'MySQL' and 'mysql' should be treated as distinct terms, but Sphinx currently treats them as duplicates, likely due to case-insensitive handling.\n\nThe patch changes the call to 'std.note_object' so that the 'termtext' is lowercased before being used as the key. This would make the system treat 'MySQL' and 'mysql' as the same term, enforcing case-insensitive uniqueness. However, the issue reporter expects 'MySQL' and 'mysql' to be treated as different terms (case-sensitive), not as duplicates. Therefore, this patch enforces the opposite of the expected behavior: it makes the system more strict about case-insensitive duplicates, rather than allowing case-sensitive distinction.\n\nThus, the patch fundamentally misunderstands the requirement and does not address the issue as described.",
        "label": "INCORRECT"
    },
    "sphinx-doc__sphinx-7454": {
        "explanation": "The issue describes an inconsistency: when autodoc_typehints is set to 'description', a function returning None generates a clickable link to None's documentation, but when set to 'signature', the None in the signature is not clickable. The expected behavior is that None should always be a clickable link to its documentation, regardless of the autodoc_typehints setting.\n\nThe patch modifies the FunctionDocumenter in sphinx.ext.autodoc.__init__.py to check if the return annotation is None. If so, it replaces the return annotation with ':py:obj:`None`' (which Sphinx will render as a link to None's documentation) in both 'description' and 'signature' modes. In 'description' mode, it also adds a :rtype: field with the same markup. This directly addresses the inconsistency described in the issue.\n\nThe patch does not introduce unrelated changes to the codebase. The changes to setup.py are unrelated to the issue but are limited to dependency pinning and do not affect the logic under review. The test change in test_ext_autodoc.py is a minor robustness improvement for warning output and does not affect the core fix.\n\nTherefore, the patch is correct and precise in addressing the described issue: it ensures that None is always rendered as a clickable link in both signature and description modes, as requested.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7462": {
        "explanation": "The issue describes an IndexError caused by calling result.pop() on an empty list when parsing an empty tuple type annotation (Tuple[()]). The patch adds a conditional check to ensure result is not empty before calling result.pop(). This directly prevents the IndexError in the described scenario, and the change is minimal and targeted. There are no unnecessary changes, and the patch does not alter unrelated logic. The fix is precise and addresses the root cause as described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7590": {
        "explanation": "The issue describes that Sphinx's C++ domain parser does not support user-defined literals (UDLs) such as '6.62607015e-34q_J' or '1q_s', resulting in a parsing error. The patch introduces a new regular expression 'udl_suffix_re' to match UDL suffixes and updates the number literal parsing logic in 'DefinitionParser' to recognize and consume UDL suffixes after numeric literals. The changes are targeted and only affect the relevant parsing logic, without introducing unrelated modifications. The patch directly addresses the inability to parse UDLs as described in the issue, and the approach is both correct and precise for the problem at hand.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7748": {
        "explanation": "The issue describes that autodoc currently only picks up the first signature from the docstring when there are multiple overloaded method signatures (as is common with SWIG-wrapped C++ classes). The patch modifies the MethodDocumenter to extract all signatures from the docstring metadata (if present) and appends them to the list of signatures to be rendered. The test added in test_ext_autodoc.py specifically checks that all overloaded signatures are included in the output, matching the expected behavior described in the issue. The changes are focused and do not introduce unrelated modifications. Therefore, the patch correctly and precisely addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7757": {
        "explanation": "The issue describes that default values for positional-only arguments (e.g., 'b=0' in 'foo(a, b=0, /, c=1)') are not being displayed in the generated documentation. The relevant code in 'sphinx/domains/python.py' is responsible for rendering function signatures, including default values. The patch modifies the logic so that the '=' operator and the default value are always added when a parameter has a default, regardless of whether it has an annotation. Previously, the '=' and default value were only added if there was no annotation, or with an extra space if there was. The new logic ensures the default value is always rendered, fixing the bug described. The changes in 'setup.py' are unrelated to the issue but are limited to dependency pinning and do not affect the correctness of the fix. Therefore, the patch addresses the described issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7889": {
        "explanation": "The issue describes a TypeError in autodoc's mock.py when handling generically-typed classes, specifically due to attempting to concatenate a string with a TypeVar (which is not a string). The problematic code is in _make_subclass, where 'module + '.' + name' is used, and 'name' can be a TypeVar. The patch changes this to 'module + '.' + str(name)', ensuring that even if 'name' is a TypeVar, it will be stringified, thus preventing the TypeError. The patch also updates the type() call to use str(name) for the class name, which is a safe and reasonable change. \n\nAdditionally, the patch adds a test that creates a generic class using TypeVar and checks that autodoc can process it without error, confirming the fix. The other changes in setup.py are unrelated to the issue but do not affect the fix for the described bug. They are dependency pinning and do not introduce unnecessary changes to the code relevant to the issue.\n\nTherefore, the patch directly and precisely addresses the described issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7910": {
        "explanation": "The issue describes that when a class's __init__ method is decorated (even with functools.wraps), Sphinx autodoc fails to document it, because the decorated method is not recognized as a method (due to the way Sphinx checks for method-ness and retrieves the class). The patch addresses this by unwrapping decorated methods using inspect.unwrap before checking if they are methods or functions, and before extracting their docstrings. This ensures that autodoc can correctly recognize and document decorated __init__ (and __new__) methods, as well as other decorated methods. The changes are targeted and do not introduce unrelated modifications. The changes to setup.py are unrelated to the issue but do not affect the logic in question. Therefore, the patch is correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-7985": {
        "explanation": "The issue requests that Sphinx's linkcheck builder should also check local (internal) links, not just external ones. The patch attempts to address this by adding a check for local links in the link checking logic. However, the implementation simply identifies local links and marks them as 'local' without actually checking if the referenced local file or anchor exists. The expected behavior is to verify the existence of the local target (e.g., a file or section), but the patch only categorizes and logs them as local, skipping any real validation. Therefore, the patch does not fulfill the core requirement of the issue.",
        "label": "INCORRECT"
    },
    "sphinx-doc__sphinx-8035": {
        "explanation": "The issue requests that the `:private-members:` option in autodoc accept a list of specific private members to document, rather than being a simple boolean that includes all private members. The patch changes the handling of `private-members` in two key ways: (1) it updates the options dictionary to use `members_option` for `private-members`, which allows it to accept a list of member names (mirroring how `:members:` works), and (2) it updates the logic in the member filtering to check if `self.options.private_members` is a list, and if so, only includes the member if its name is in that list. This directly implements the requested feature, and the changes are minimal and targeted. There are no unnecessary changes, and the patch does not miss any key aspects of the request. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8056": {
        "explanation": "The issue describes a rendering problem in Sphinx's Napoleon extension when multiple parameters are listed on a single line in a Numpy-style docstring (e.g., `x1, x2 : array_like, optional`). The expected output is that both parameter names are shown together, with their type and 'optional' status clearly indicated, and the description is associated with both.\n\nThe patch makes substantial changes to the docstring parsing logic in both GoogleDocstring and NumpyDocstring, specifically:\n- It updates the `_consume_field` and `_consume_fields` methods to correctly split multiple parameter names (e.g., `x1, x2`) and associate them with the type and description.\n- It updates the formatting methods to group parameter names and types together in the output.\n- It adds/updates tests to check that the output matches the expected format, both for Numpy and Google style docstrings.\n\nThe new test in `test_napoleon.py` and the updated test in `tests/test_ext_napoleon_docstring.py` both check for the correct rendering of multiple parameters on a single line, including the 'optional' marker. The output format matches the expectation described in the issue.\n\nThere are no unnecessary unrelated changes, and the patch is focused on the described problem. The changes are precise and comprehensive, covering both parsing and rendering, and are validated by tests.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8120": {
        "explanation": "The issue describes that user-provided translations in locale/<language>/LC_MESSAGES/sphinx.po are ignored in favor of the built-in Sphinx translations, even though the documentation suggests user locales should be respected and prioritized. The patch addresses this by:\n\n1. Updating the documentation to clarify that the first directory in locale_dirs is prioritized for custom translations.\n2. Modifying the CatalogRepository class in sphinx/util/i18n.py so that it yields the first locale directory (assumed to be the user's custom directory) before the others, ensuring user translations are loaded first and can override built-in ones.\n3. Adding a test to verify that the first directory's translation is prioritized.\n\nThe patch is focused, addresses the described bug directly, and does not introduce unrelated changes. The changes to setup.py are unrelated to the i18n issue but are limited to version pinning and do not affect the i18n logic. The main logic and test changes are correct and precise for the issue at hand.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8265": {
        "explanation": "The issue is that default arguments which are tuples (e.g., color=(1, 1, 1)) are rendered incorrectly in the generated documentation, appearing as color=1, 1, 1 instead of color=(1, 1, 1). The patch adds a special case in the object_description function to handle tuples, ensuring they are rendered with parentheses and their elements joined by commas, recursively using object_description for each element. This directly addresses the problem described in the issue, and the change is minimal and targeted. No unnecessary changes are made, and the patch does not appear to break or miss any key aspects of the described issue. Edge cases like single-element tuples (which require a trailing comma) are not explicitly handled, but the original issue does not mention them, so this is not required for a precise fix to the reported problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8269": {
        "explanation": "The issue describes that when linkcheck_anchors is True, linkcheck always reports 'Anchor not found' even if the HTTP request itself failed (e.g., 404), and the expected behavior is to report the HTTP error instead. The patch changes the logic so that response.raise_for_status() is always called immediately after the GET request, before any anchor checking. This means that if the server returns an error status (like 404), an HTTPError will be raised and handled, and the anchor check will not be performed. This matches the expected behavior described in the issue. The patch does not introduce unrelated changes in the linkcheck logic. The changes in setup.py are unrelated to the issue but do not affect the correctness of the linkcheck fix. Therefore, the patch is correct and precise regarding the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8435": {
        "explanation": "The issue describes that autodoc_type_aliases is not being applied to variables and attributes, specifically for type annotations that use string aliases (e.g., 'String'). The expected behavior is that when autodoc_type_aliases is set in conf.py, Sphinx autodoc should resolve these aliases for both module-level variables and class attributes.\n\nThe patch modifies sphinx/ext/autodoc/__init__.py, specifically the AttributeDocumenter, to update the global namespace with autodoc_type_aliases and passes it as localns to get_type_hints. This is the correct place to hook in the aliasing for attribute and variable type annotations, including those written as string forward references. The patch also adds relevant tests that check for the correct resolution of the type alias in the generated documentation, and the tests are focused and relevant.\n\nNo unrelated or unnecessary changes are made to the core logic. The changes to setup.py and tox.ini are minor and related to dependency pinning and test output, not affecting the core fix. The new files (conf.py, example/__init__.py, index.rst, test_type_aliases.py, and the new test in tests/test_ext_autodoc/test_type_aliases.py) are all directly related to testing and demonstrating the fix.\n\nTherefore, the patch is CORRECT_AND_PRECISE: it addresses the described issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8459": {
        "explanation": "The issue describes that when 'autodoc_typehints' is set to 'description', the 'autodoc_type_aliases' configuration is not respected, and type aliases are not substituted in the generated documentation. The expected behavior is that type aliases (e.g., 'JSONObject') are used in the documentation instead of their expanded types (e.g., 'Dict[str, Any]'), regardless of the 'autodoc_typehints' setting.\n\nThe patch modifies the 'modify_field_list' function to accept 'type_aliases' as an argument and, when processing each parameter, checks if the parameter name is in 'type_aliases'. If so, it replaces the annotation with the alias value. The patch also updates the call to 'modify_field_list' to pass the 'autodoc_type_aliases' config value.\n\nThis directly addresses the issue: it ensures that when generating the parameter type descriptions, the type alias is used if configured, even when 'autodoc_typehints' is set to 'description'. The patch is focused and does not introduce unrelated changes.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8475": {
        "explanation": "The issue describes a bug where the linkcheck builder marks links as broken if a HEAD request results in a TooManyRedirects exception, without attempting a GET request as a fallback. The patch updates the exception handling in CheckExternalLinksBuilder to catch TooManyRedirects alongside HTTPError, and then retries with a GET request. If the GET request also results in TooManyRedirects, the link is marked as broken with an appropriate message. The patch also adds tests to verify both the fallback to GET and the case where both HEAD and GET fail with TooManyRedirects. The documentation and changelog are updated accordingly. The changes are minimal, targeted, and directly address the described issue without introducing unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8548": {
        "explanation": "The issue describes that autodoc's 'inherited-members' option does not work for inherited attributes (data members), because autodoc only checks for attributes in the immediate class and not in base classes. The patch modifies the logic in autodoc's Documenter and ClassDocumenter to explicitly traverse the MRO (method resolution order) and collect inherited attributes from base classes when 'inherited-members' is enabled. It also updates the test suite to add a class with an inherited attribute and asserts that the attribute is documented when 'inherited-members' is specified. The patch is focused on the described problem, adds appropriate debug output, and does not introduce unrelated changes. The changes in setup.py are unrelated to the issue but are limited to dependency pinning and do not affect the correctness of the fix for the described issue. Therefore, the patch is correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8551": {
        "explanation": "The issue describes that unqualified type names in :type: and :rtype: fields are resolved ambiguously, leading to warnings and incorrect links (e.g., 'A' resolves to 'mod.A' instead of 'mod.submod.A' in the context of 'mod.submod'). The expected behavior is that unqualified names should resolve in the current module context, matching how explicit cross-references work.\n\nThe patch modifies the PythonDomain.find_obj method to add a block that, if no matches are found and the target is not qualified (does not start with '.'), attempts to resolve the name first in the current module (modname), then in the current class (clsname). This is precisely the missing logic described in the issue: it ensures that unqualified names are resolved in the current module context, reducing ambiguity and matching the behavior of explicit xrefs.\n\nThe changes in setup.py and tox.ini are unrelated to the issue but do not detract from the correctness of the fix in python.py. The core logic change is minimal, targeted, and directly addresses the described bug without unnecessary changes.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8593": {
        "explanation": "The issue describes that variables with a docstring containing ':meta public:' (i.e., metadata marking them as public) are not being included in the generated documentation, even though the user expects them to be. The relevant code in the patch adds 'keep = True' when 'public' is found in the metadata, ensuring that such members are not filtered out as private. This change is minimal and directly targets the described problem, making sure that variables marked as public via metadata are included. There are no unnecessary changes, and the patch does not appear to miss any edge cases described in the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8595": {
        "explanation": "The issue describes that when a module's __all__ attribute is an empty list, autodoc should not document any members, but currently it documents all. The patch changes the logic in get_object_members: it now distinguishes between __all__ being None (not set) and __all__ being an empty list. If __all__ is None, it keeps the old behavior (show all members). If __all__ is an empty list, it returns False, []\u2014meaning no members are documented. This directly and precisely addresses the issue, with no unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8621": {
        "explanation": "The issue describes that the current implementation treats '-', '+', and '^' as separators in all contexts, which leads to incorrect HTML output when these characters are intended as standalone keystrokes (e.g., :kbd:`-`). The expected behavior is that if the entire kbd text is just one of these characters, it should be treated as a single keystroke, not split into empty keystrokes around a separator.\n\nThe patch introduces a new static method 'split' that uses re.finditer to split the text into keys and separators, and then in the run method, it only splits if the result has more than one part. If the result is a single part (e.g., just '-'), it leaves it as a single kbd node, which is the correct behavior. For compound keystrokes, it alternates between wrapping keys in <kbd> and inserting the separator as text, which matches the expected output.\n\nThe patch does not introduce unrelated changes and directly addresses the described problem, ensuring that single-character keystrokes that are also separators are not split. The logic is precise and does not appear to miss edge cases described in the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8633": {
        "explanation": "The issue is that Sphinx's _restify_py37 function assumes that the object passed in (cls) always has a __name__ attribute, but in the case of requests.codes (a LookupDict), this is not true, leading to an AttributeError. The patch adds a check: if the object does not have a __name__ attribute, it falls back to returning ':obj:`%s`' % str(cls), which avoids the AttributeError and provides a reasonable fallback representation. The patch is minimal, directly addresses the error, and does not introduce unrelated changes. It is both correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8638": {
        "explanation": "The issue describes a problem where Sphinx's Python domain links instance variables (attributes) to other variables of the same name elsewhere in the project, even when they are unrelated (e.g., a class attribute 'limit' in one class linking to a global or another class's 'limit'). The expected behavior is that instance variables should only link within their own class scope, not to unrelated variables elsewhere.\n\nThe patch modifies the name resolution logic in the PythonDomain class, specifically for the 'attribute' (instance variable) type. It restricts the search for attributes to only the current class scope, both for exact and fuzzy matches. For other types, the original broader search is preserved. The patch also ensures that when resolving attributes, it checks for the presence of the class name (and module name if available) and only considers matches within that scope.\n\nThis directly addresses the issue: it prevents instance variables from being auto-linked to unrelated variables of the same name elsewhere in the project, as requested. The changes are targeted and do not introduce unrelated modifications.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8721": {
        "explanation": "The issue describes that viewcode is generating module pages for epub output even when the configuration option 'viewcode_enable_epub' is set to False. The expected behavior is that these pages should not be created for epub by default. The patch adds a conditional check in the 'collect_pages' function: if the builder's name starts with 'epub' and 'viewcode_enable_epub' is False, the function returns early, preventing the creation of these pages. This change is minimal, directly addresses the described issue, and does not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-8801": {
        "explanation": "The issue describes that autodoc fails to document class attributes that are only annotated (not assigned) in a superclass, unless :undoc-members: is specified. The patch modifies the update_annotations method in AttributeDocumenter to traverse the MRO and collect annotations from all base classes, not just the immediate parent. It merges annotations from module analyzers and class-level annotations, ensuring inherited annotated attributes are included. The patch also adds a test that verifies both the subclass and inherited superclass annotated attributes are documented. No unrelated changes are made, and the requirements changes are unrelated to the bug but are not excessive. The patch is focused and directly addresses the issue as described, ensuring that inherited annotation-only members are documented as expected.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9171": {
        "explanation": "The issue requests a way to suppress the constructor signature from the class header generated by the autodoc extension's autoclass directive. The patch introduces a new config value ('autodoc_hide_class_signature') and a new option ('hide-signature') for the autoclass directive. In the ClassDocumenter.format_signature method, it checks for either the config value or the directive option and, if set, returns an empty string instead of the signature. This directly addresses the user's request, is implemented in a minimal and targeted way, and does not introduce unrelated changes. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9229": {
        "explanation": "The issue describes that Sphinx's autodoc extension inconsistently displays docstrings for type aliases: sometimes the docstring is shown, sometimes only the default 'alias of ...' message appears. The expectation is that all type aliases with docstrings should have those docstrings rendered in the documentation.\n\nThe patch modifies the DataDocumenter class in sphinx.ext.autodoc to:\n- In get_doc, check if the object is a type alias (by checking if it's a function and has a __supertype__ attribute), and if so, extract its docstring using inspect.getdoc.\n- In the main documentation generation logic, if the object is a type alias, it adds the docstring lines to the output after the value line.\n\nThis directly targets the described problem: it ensures that type aliases with docstrings have those docstrings included in the generated documentation, rather than being ignored in favor of the default 'alias of ...' message. The patch does not introduce unrelated changes, and the logic is scoped to the handling of type aliases' docstrings. The changes to setup.py are unrelated to the issue but are limited to dependency pinning and do not affect the correctness of the fix for the described bug.\n\nTherefore, the patch is CORRECT_AND_PRECISE: it addresses the issue as described, ensuring docstrings for type aliases are always included in the documentation output.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9230": {
        "explanation": "The issue describes that type annotations like dict(str, str) are rendered incorrectly in the generated documentation, with misplaced parentheses and commas. The root cause is that the AST node for function calls (ast.Call), which represents constructs like dict(str, str), was not being handled in the _parse_annotation function. The patch adds explicit handling for ast.Call nodes: it unparses the function name, adds an opening parenthesis, recursively unparses each argument (adding commas as needed), and closes the parenthesis. This directly addresses the rendering issue for types like dict(str, str), ensuring the output matches the expected format. The patch is focused and does not introduce unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9231": {
        "explanation": "The issue describes a problem where the 'man_make_section_directory' configuration option was enabled by default, causing breakage in downstream projects that expect man pages to be placed directly in the output directory. The patch explicitly adds 'man_make_section_directory' as a config value and sets its default to False, with a comment explaining the rationale for backward compatibility. This directly addresses the issue by ensuring the option is disabled by default, restoring the previous behavior and preventing the breakage described. The patch does not introduce unrelated changes and is precise in its scope.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9246": {
        "explanation": "The issue describes a crash due to an unhandled 'pending_xref_condition' node reaching the HTML writer, which raises NotImplementedError for unknown nodes. The root cause is that some references are not being resolved to a known node type, and the suggestion is that the Sphinx core should handle resolving the fallback content for pending_xref_condition nodes, so that extensions (like sphinx-qt-documentation) do not have to duplicate this logic.\n\nThe patch addresses this by refactoring the code in ReferencesResolver to always resolve pending_xref_condition nodes via a new helper method handle_pending_xref_condition. This method attempts to extract the fallback content using find_pending_xref_condition, and if it fails (including catching AttributeError, ValueError, or any Exception), it falls back to the original content node. This ensures that after the transform, the doctree will not contain any pending_xref_condition nodes, thus preventing the NotImplementedError in the writer. The patch also removes an obsolete type annotation block and adds some logging, but these are not unrelated changes\u2014they are minor cleanups and diagnostics.\n\nThe patch does not introduce unnecessary changes and directly addresses the issue by ensuring that pending_xref_condition nodes are handled and replaced before writing, as suggested in the issue. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9258": {
        "explanation": "The issue requests support for union type specification using the pipe (|) syntax, e.g., 'bytes | str', in docstrings and type representations. The patch updates the relevant functions in sphinx/util/typing.py to use the pipe syntax when rendering union types, replacing the previous 'Union[...]' or 'Optional[Union[...]]' forms with the new 'type1 | type2' and 'Optional[type1 | type2]' forms. The changes are targeted and do not introduce unrelated modifications. The patch directly addresses the issue as described, enabling the requested syntax in the generated documentation.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9281": {
        "explanation": "The issue describes that enum default values in function signatures are rendered as their full repr (e.g., <MyEnum.ValueA: 10>) instead of the expected, cleaner MyEnum.ValueA. The patch modifies the _parse_annotation function in sphinx.domains.python to improve how AST Attribute nodes are rendered, specifically checking for enum-like patterns and formatting them as 'MyEnum.ValueA'. It also adds a test that verifies the rendered HTML contains the correct, expected string '<span class=\"pre\">MyEnum.ValueA</span>', and not the previous, verbose representation. The patch is focused, only changes the relevant code, and adds a precise test for the described issue. No unnecessary or unrelated changes are present. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9320": {
        "explanation": "The issue describes that when running `sphinx-quickstart` in a directory with an existing `conf.py`, the prompt says pressing Enter should exit, but instead, it asks for a valid path name. The expected behavior is that pressing Enter should exit the process. \n\nThe patch changes the input validation logic: previously, the validator was always called, even if the user pressed Enter (resulting in an empty string), which would trigger a validation error. The patch now only calls the validator if the input is non-empty (`if x:`), so if the user presses Enter (and x is empty), it skips validation and breaks out of the loop, returning the empty string. This matches the prompt's instruction and the expected behavior described in the issue.\n\nThe patch is minimal and directly addresses the described problem without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9367": {
        "explanation": "The issue describes that 1-element tuples are rendered incorrectly as '(1)' instead of '(1,)', i.e., the trailing comma is missing. The patch modifies the 'visit_Tuple' method to explicitly check for the case where there is exactly one element in the tuple. In that case, it returns the string with a trailing comma: '(' + element + ',)'. For empty tuples, it returns '()', and for tuples with more than one element, it joins them with commas as before. This directly and precisely addresses the issue described, without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9386": {
        "explanation": "The issue describes that, even when 'autodoc_typehints' is set to 'none', Sphinx autodoc still renders type hints for properties. The expected behavior is that no type hints should be rendered in the output when this setting is 'none'.\n\nThe patch modifies the code so that the block responsible for adding the ':type:' line for properties is now guarded by a check: 'if self.config.autodoc_typehints in (\"both\", \"description\")'. This means that if 'autodoc_typehints' is set to 'none', the code will not add the type hint line for properties, which matches the expected behavior described in the issue.\n\nThe patch does not introduce unrelated changes and directly addresses the described bug. The logic is precise and scoped only to the relevant code path.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9461": {
        "explanation": "The issue describes that methods decorated with both @classmethod and @property (i.e., class-level properties) are not being documented by Sphinx autodoc, even though regular @property methods are. The patch adds a check in MethodDocumenter: if the object is an instance of classmethod and its __func__ is a property, it adds a ':property:' line to the documentation. This directly targets the described issue: it recognizes the special case of classmethod-wrapped properties and marks them as properties in the generated documentation. The patch is minimal, does not introduce unrelated changes, and precisely addresses the problem as described in the issue. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9464": {
        "explanation": "The issue describes that PEP 585 generics (like list[str]) are not rendered correctly by autodoc\u2014they appear as 'list' instead of 'list[str]'. The suggested fix is to check if the annotation has __args__ and, if so, return repr(annotation). The patch adds a check for annotations whose class is 'types.GenericAlias' (which is the type of PEP 585 generics) and returns repr(annotation) in that case. This is a targeted fix that directly addresses the rendering issue for PEP 585 generics without affecting other annotation types or making unrelated changes. The patch is precise and does not introduce unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9467": {
        "explanation": "The issue describes that, starting with Sphinx 4.1.0, linkcheck fails for GitHub links with line anchors (e.g., #L232), which previously worked. The problem is that Sphinx's linkcheck now rewrites GitHub anchors, possibly incorrectly, causing valid links to fail.\n\nThe patch adds logging to the `rewrite_github_anchor` function, but does not change its logic. The function still rewrites any GitHub anchor that does not start with 'user-content-' by prepending 'user-content-'. However, for links like `#L232` (which are line anchors in GitHub source files), this is not correct: GitHub does not use the 'user-content-' prefix for these anchors. The function should not rewrite such anchors, but the patch does not address this logic error.\n\nAdditionally, the patch adds a test file and some dependency pinning, but the core bug (incorrect anchor rewriting for line anchors) is not fixed. The logging may help debug, but it does not resolve the issue described.\n\nTherefore, the patch fails to address the actual problem described in the issue.",
        "label": "INCORRECT"
    },
    "sphinx-doc__sphinx-9591": {
        "explanation": "The issue describes that Sphinx does not properly cross-reference the return type annotation of a property (e.g., the 'end' property returning 'Point'). The expected behavior is that the documented type in a property's type annotation should be cross-referenced, just as it is for regular methods or attributes.\n\nThe patch modifies 'sphinx/util/typing.py', specifically the 'restify' function, which is responsible for converting type annotations into a string suitable for documentation and cross-referencing. The patch adds a check:\n\n    elif isinstance(cls, property):\n        return restify(cls.fget.__annotations__.get('return'))\n\nThis means that if the object is a property, it will extract the return type annotation from the property's getter function and recursively call 'restify' on it. This is the correct approach, as it ensures that the property's return type is processed in the same way as other type annotations, enabling cross-referencing.\n\nThe rest of the patch (setup.py changes) is unrelated to the issue and appears to be dependency pinning, which is not relevant to the described bug. However, these changes do not detract from the correctness of the fix for the actual issue.\n\nTherefore, the patch correctly and precisely addresses the described issue: it enables cross-referencing of property return type annotations in Sphinx documentation.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9602": {
        "explanation": "The issue is that Sphinx, when rendering type annotations involving typing.Literal, incorrectly treats the values inside Literal (such as True, False, or strings) as py:class references, which triggers nitpick warnings because these are not classes. The patch addresses this by modifying the logic in sphinx/util/typing.py (and via a monkey-patch in sphinx/util/typing_fix.py) so that when restifying Literal arguments, it only uses :class: markup for actual types, and uses repr() for other values. This prevents Sphinx from generating py:class references for non-class Literal values, which is exactly what the issue describes. The patch also adds tests to verify that Literal[True], Literal['example'], and mixed Literals are rendered correctly and do not trigger warnings. The changes are focused and do not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9658": {
        "explanation": "The issue is that when documenting classes that inherit from mocked classes (e.g., torch.nn.Module being mocked), the 'Bases' section in the generated documentation shows 'torch.nn.' instead of 'torch.nn.Module'. This is because the mock object does not have the expected __name__ and __module__ attributes, or the code that generates the reference does not handle mocked classes correctly.\n\nThe patch addresses this by:\n- Importing the 'ismock' function from 'sphinx.ext.autodoc.mock'.\n- Adding a check in the 'restify' function: if the class is a mock (ismock(cls)), it returns a reST reference using both __module__ and __name__ (':py:class:`%s.%s`').\n- Adding a test that creates a mock class with __sphinx_mock__, __name__, and __module__ attributes, and verifies that restify(mocked_class) returns the correct string (':py:class:`torch.nn.Module`').\n\nThis directly targets the described bug: the 'Bases' section will now show the correct base class name for mocked classes. The patch does not introduce unrelated changes and the new test covers the edge case described in the issue. The changes to setup.py are unrelated to the core bug but are limited to dependency pinning and do not affect the logic under review.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9673": {
        "explanation": "The issue is that autodoc_typehints_description_target = \"documented\" does not work as expected with Napoleon: when a return description is present in the docstring (parsed by Napoleon), the return type is not shown in the generated documentation. The root cause is that Napoleon uses 'Returns:' (plural) in Google-style docstrings, but the Sphinx autodoc typehints code only checks for 'return' (singular) when determining if a return description is present. As a result, it fails to recognize the return description and omits the return type.\n\nThe patch changes the relevant line in sphinx/ext/autodoc/typehints.py from:\n  elif parts[0] == 'return':\nto:\n  elif parts[0] in ('return', 'returns'):\n\nThis means both 'return' and 'returns' will be recognized as a return description, fixing the logic so that autodoc_typehints_description_target = \"documented\" works as intended with Napoleon/Google style docstrings. The patch is minimal, targeted, and does not introduce unrelated changes.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9698": {
        "explanation": "The issue describes that when using the `py:method` directive with the `:property:` option, the generated index entry incorrectly includes parentheses (parens), which is not appropriate for properties. The patch changes the formatting in the relevant code path: for properties, it removes the '()' from the index entry string, so that properties are listed without parens, as expected. The change is minimal and directly targets the described problem, without affecting other cases (classmethod, staticmethod, etc.). There are no unrelated or unnecessary changes. The extra newline at the end of the file is inconsequential. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sphinx-doc__sphinx-9711": {
        "explanation": "The issue describes a bug where version comparisons in the needs_extensions check are performed as string comparisons, leading to incorrect results (e.g., '0.10.0' being considered less than '0.6.0'). The patch addresses this by importing 'parse' from 'packaging.version' and using it to compare versions semantically rather than as strings. The logic is updated so that both the required version and the extension's version are parsed before comparison, ensuring correct version ordering. The patch also adds a more explicit error for the 'unknown version' case, which is a reasonable improvement. No unrelated or unnecessary changes are present in the patch. Therefore, the patch precisely and correctly addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-11618": {
        "explanation": "The issue describes that the distance calculation between points of different dimensions (e.g., 2D and 3D) is incorrect because the current implementation zips the coordinates, ignoring extra dimensions. The patch modifies the distance method to pad the shorter coordinate list with zeros so both points are treated as if they exist in the same dimensional space, which is mathematically correct for Euclidean distance. The patch also adds a test in the main test suite and two standalone test scripts to verify the fix. The code change is minimal and directly addresses the described bug without introducing unrelated changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-12096": {
        "explanation": "The issue describes that evalf does not call _imp_ recursively, so compositions of implemented_function do not evaluate numerically as expected. The patch modifies Function._eval_evalf to call evalf(prec) on the result of self._imp_(*self.args), instead of wrapping it in Float directly. This ensures that if the result is itself an unevaluated function (e.g., f(g(2))), evalf will be called recursively, allowing for full numeric evaluation. The patch also adds targeted tests that demonstrate the fix, including the exact scenario from the issue (f(g(2)).evalf() == 16). The changes are minimal and directly address the described problem without unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-12419": {
        "explanation": "The issue describes that summing all elements of an identity matrix (of size n x n) in SymPy returns 0, but the expected result is n. The patch adds a sum_elements method to the Identity matrix class, returning self.rows (which is n for an n x n identity matrix). This directly and precisely addresses the issue: the sum of all elements in an identity matrix is indeed n. The patch does not introduce unrelated changes and is limited to the required fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-12481": {
        "explanation": "The issue describes that the Permutation constructor should allow non-disjoint cycles (e.g., Permutation([[0,1],[0,1]])), applying them in left-to-right order, rather than raising a ValueError. The patch modifies the code to only raise a ValueError for repeated elements if 'is_cycle' is False, which means that for the case of cycles (is_cycle=True), the check is skipped and the constructor proceeds. This change allows non-disjoint cycles to be passed in, as requested. The patch does not introduce unrelated changes and directly addresses the described issue. The addition of the test script (reproduce_error.py) is not part of the core library and is just for demonstration, so it does not affect the evaluation. The patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-12489": {
        "explanation": "The issue describes a problem with subclassing the Permutation class in sympy.combinatorics, specifically that the static method _af_new always returns instances of Permutation, not of the subclass, due to the use of staticmethod and hardcoded class references. The proposed patch changes _af_new from a staticmethod to a classmethod and replaces the hardcoded 'Perm' (i.e., Permutation) with 'cls', ensuring that when called from a subclass, an instance of the subclass is created. This directly and precisely addresses the issue described, without introducing unrelated changes or missing any key aspects of the problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-12977": {
        "explanation": "The issue describes a ValueError when calling a nullary function (i.e., a function with zero arguments) in SymPy, specifically due to the use of max() and min() on an empty sequence in the __new__ method of Function. The patch adds a check after constructing the result: if result.args is empty (i.e., the function is nullary), it returns the result immediately, thus avoiding the problematic max() and min() calls. This directly addresses the error described in the issue. Additionally, the patch sets nargs = len(args) in AppliedUndef, which is a minor but relevant addition for tracking the number of arguments, and does not introduce unrelated changes. The patch is focused and precise in addressing the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13031": {
        "explanation": "The issue describes a regression in the behavior of Matrix.hstack when stacking zero-row matrices: in sympy 1.0, hstack of (0,0), (0,1), (0,2), (0,3) matrices yields a (0,6) matrix, but in sympy 1.1 it yields (0,3). The patch changes an internal call from self.hstack to self._hstack in the MatrixBase class, and also adds MatrixShaping as a base class to MatrixBase. The likely intent is to ensure the correct hstack implementation is used, possibly restoring the old behavior. The patch is minimal and directly targets the method used for stacking, without unrelated changes. However, without seeing the implementation of _hstack and hstack, it's not possible to guarantee that all edge cases (such as stacking more than two matrices, or matrices with more complex shapes) are handled as in 1.0. Still, the patch is focused and addresses the described regression.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "sympy__sympy-13091": {
        "explanation": "The issue requests that SymPy's Basic.__eq__ (and by extension, __ne__) return NotImplemented instead of False when it cannot handle the comparison, so that Python's comparison machinery can delegate to the other object's __eq__ method, ensuring symmetric and correct behavior. The patch changes the return value from False to NotImplemented in the relevant place in __eq__, and updates __ne__ to propagate NotImplemented if received from __eq__, which is the correct and minimal change required. No unnecessary changes are made, and the patch directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13372": {
        "explanation": "The issue describes an UnboundLocalError in evalf_mul, specifically that variables like 'reprec' (or 're_acc', 'im_acc' in the patch) are referenced before assignment when certain argument orders are used in Mul expressions. The issue suggests that the elif clauses that define these variables should have an 'else: raise NotImplementedError' to avoid the UnboundLocalError, but the patch instead rewrites evalf_mul (and adds evalf_add) to handle the evaluation more robustly, ensuring that 're', 'im', 're_acc', and 'im_acc' are always defined before being used. The new logic covers all branches, and in the case where neither 're' nor 'im' is set, it returns a tuple of Nones, preventing the UnboundLocalError. The patch does not introduce unrelated changes and directly addresses the root cause by restructuring the logic to avoid uninitialized variables. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13480": {
        "explanation": "The issue is a NameError due to the use of an undefined variable 'cotm' in the coth class's eval method. The patch replaces 'cotm' with 'cothm', which is the correct variable name as defined just above in the code (cothm = coth(m)). This change directly addresses the error without introducing any unrelated modifications. The logic of the code remains consistent with the intended behavior, and the patch is minimal and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13551": {
        "explanation": "The patch directly addresses the issue: it adds a special-case evaluation for Product(n + 1/2**k, (k, 0, n-1)), which was previously computed incorrectly. The new logic checks for terms of the form (symbol + 1/2**k) and, when matched, computes the product using a formula that matches the expected result (as verified by the new test cases for n=2 and n=3). The patch does not introduce unrelated changes and confines itself to the specific case described in the issue. The added test file confirms the fix for the reported cases. Thus, the patch is both correct and precise for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13581": {
        "explanation": "The patch introduces a custom _eval_simplify method for the Mod class, specifically targeting the simplification of nested Mod expressions, especially for modulo 2. It handles the case where Mod(Mod(x + 1, 2) + 1, 2) should simplify to Mod(x, 2), as described in the issue. The logic identifies Mod terms within an Add, combines them, and applies modulo 2 arithmetic to constants (e.g., counting 1s and reducing them mod 2). This directly addresses the main example in the issue.\n\nHowever, the patch does not appear to handle the more general case of expressions like foo*Mod(x + 1, 2) + non_mod_terms + 1, as it only looks for Mod terms that are direct arguments of Add, not those multiplied by coefficients. Thus, while the patch is correct for the main example, it is incomplete regarding the recursive/general simplification requested in the issue description.\n\nTherefore, the patch is CORRECT_BUT_INCOMPLETE.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "sympy__sympy-13615": {
        "explanation": "The issue describes that Complement(FiniteSet(x, y, 2), Interval(-10, 10)) should return FiniteSet(x, y), i.e., remove the numbers in the interval from the set, but keep the symbols. The original code did not handle this case, likely because it tried to check membership for all elements, and for symbols, this is indeterminate.\n\nThe patch adds a special case in Complement.reduce: if A is a FiniteSet and B is an Interval, it constructs a new FiniteSet containing elements from A that are either not numbers or not contained in B. This directly implements the desired behavior: numbers in the interval are removed, symbols are kept. The patch also adds comprehensive tests for this behavior, including edge cases with more elements and different intervals.\n\nThe patch does not introduce unrelated changes and the logic is precise and limited to the described issue. The docstring is improved for clarity, and the tests are expanded appropriately. Therefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13647": {
        "explanation": "The issue describes a bug in Matrix.col_insert() where, after inserting columns into a matrix, the columns to the right of the inserted block are incorrectly shifted. Specifically, the identity matrix's columns are not preserved in their correct positions after the insertion. The patch modifies the logic in sympy/matrices/common.py, changing the index calculation from 'self[i, j - pos - other.cols]' to 'self[i, j - other.cols]'. This change directly addresses the off-by-pos error in the column index calculation for columns after the inserted block. The patch also adds new test scripts (reproduce_col_insert.py and test_edge_cases.py) that demonstrate the correct behavior for various insertion positions, including the one described in the issue. The code change is minimal and directly targets the bug without introducing unrelated changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13757": {
        "explanation": "The issue describes an inconsistency: when multiplying a Poly by an expression (Poly(x) * x), the result is a Poly, but when the expression is on the left (x * Poly(x)), the result is an unevaluated product. The patch modifies the Poly.mul method so that if the right operand is an Expr (but not a Poly), it returns f.as_expr() * g, which will produce an expression rather than a Poly. This ensures that Poly(x) * x and x * Poly(x) both return the same type of result (an expression), resolving the inconsistency. The patch is minimal and directly targets the described issue without introducing unrelated changes. It does not attempt to change the behavior of ground element multiplication or other Poly operations, so it is both correct and precise for the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13798": {
        "explanation": "The issue requests that the `latex()` function accept arbitrary strings for the `mul_symbol` argument, rather than restricting it to a fixed set of options. The patch modifies the relevant code in `sympy/printing/latex.py` to allow any string to be used as the multiplication symbol, falling back to the previous behavior for the known options. It also updates the documentation and adds both unit and integration tests to verify that custom symbols like '\\,' and '\\thinspace' work as expected. The changes are minimal, targeted, and do not introduce unrelated modifications. The patch is therefore correct and precise in addressing the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13852": {
        "explanation": "The patch directly addresses both main points raised in the issue:\n\n1. It adds a special-case expansion for polylog(2, 1/2), returning the expected closed form -log(2)**2/2 + pi**2/12, as requested. This is implemented by checking for s == 2 and z == Rational(1, 2) in _eval_expand_func.\n\n2. It changes the expansion of polylog(1, z) from -log(1 + exp_polar(-I*pi)*z) to -log(1 - z), removing the unnecessary exp_polar(-I*pi) and matching the mathematical equivalence and user expectation described in the issue. This resolves the confusion and derivative inconsistency described.\n\nThe patch does not introduce unrelated changes and only modifies the relevant expansion logic. The included test script (reproduce.py) demonstrates that the new expansions match expectations and that derivatives behave as expected. Edge cases are also considered, but the patch itself only targets the described issues.\n\nTherefore, the patch is both correct and precise in addressing the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13877": {
        "explanation": "The issue describes that computing the determinant of certain symbolic matrices using SymPy's Bareiss algorithm leads to 'Invalid NaN comparison' errors, and questions whether the Bareiss algorithm is appropriate for symbolic (non-integer) matrices. The patch addresses this by modifying the '_eval_det_bareiss' method: it now checks if any matrix entry is symbolic (i.e., an instance of Expr and not a number), and if so, delegates to '_eval_det_berkowitz', which is more robust for symbolic matrices. This is a targeted and minimal change that directly addresses the root cause of the error (using Bareiss on symbolic matrices), without introducing unrelated modifications. The rest of the patch consists of minor imports modernization (using 'collections.abc.Iterable'), which is unrelated but harmless and does not detract from the main fix. Therefore, the patch is correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13878": {
        "explanation": "The issue requests the addition of precomputed _cdf methods for a list of continuous distributions in SymPy where symbolic integration of the PDF to obtain the CDF is unreliable or slow. The patch adds a _cdf method to each of the distributions mentioned in the issue: Arcsin, Dagum, Erlang, Frechet, Gamma, GammaInverse, Kumaraswamy, Laplace, Logistic, Nakagami, StudentT, and UniformSum. The formulas used in the patch match the standard closed-form CDFs for these distributions, using special functions (e.g., lowergamma, uppergamma, hyper) where appropriate, and simple expressions otherwise. The patch does not introduce unrelated changes and does not omit any of the distributions listed in the issue. Therefore, the patch is both correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13915": {
        "explanation": "The issue describes that substituting b with a in the expression r = (1/(a+b) + 1/(a-b))/(1/(a+b) - 1/(a-b)) should not return 1, because the substitution makes the denominator zero, leading to an undefined expression. The correct behavior is to compute the limit as b approaches a, which yields -1. The patch modifies the subs method so that after each substitution, if the result contains S.NaN (i.e., an undefined expression), it attempts to compute the limit instead. This directly addresses the issue: when substitution leads to an undefined expression, the code now tries to compute the limit, which is the expected mathematical behavior. The patch is focused and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-13974": {
        "explanation": "The issue describes that powers of TensorProduct expressions are not being evaluated as expected: expressions like TP(2,3)**3 or TP(1,1)*TP(1,1) remain unevaluated (e.g., 1x1**2 instead of 1x1), and tensor_product_simp does not simplify them. The patch introduces logic in tensor_product_simp to handle powers of TensorProduct, distributing the power to each argument, and also adds a _eval_power method to TensorProduct. The new tests cover the problematic cases from the issue (e.g., TP(1,1)*TP(1,1), TP(2,3)**3, TP(1,Pauli(3))*TP(1,Pauli(3)), etc.), and the expected outputs match the desired simplifications. The patch does not introduce unnecessary changes and directly targets the described problem. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-14070": {
        "explanation": "The issue is that logcombine(log(3) - log(2)) does not simplify to log(3/2) as expected. The patch modifies the handling of Add terms in the logcombine logic. Previously, only positive log terms were grouped, but the patch adds logic to also recognize and group negative log terms (i.e., terms like -log(x)), by checking if a term could extract a minus sign and if its negation is a log. This allows logcombine to correctly identify and combine expressions like log(3) - log(2) into log(3/2). The change is minimal and directly targets the described issue without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-14248": {
        "explanation": "The issue is that differences like A - B are printed as sums with explicit (-1) coefficients, e.g., '-1*B + A', instead of using the subtraction operator, e.g., 'A - B'. The patch attempts to address this in the pretty printer by iterating over the terms, checking if the coefficient is negative, and then using a 'pretty_negative' function to format the term accordingly. However, the patch as written appears to have several problems:\n\n1. The new code is inserted inside the 'pretty_negative' function, but it is not clear that this is the correct place for handling the sum of terms. The function seems to be intended for formatting a single negative term, not for iterating over all terms in an expression.\n2. The patch introduces a new loop and logic, but leaves unreachable code after a 'return' statement, and the indentation is inconsistent. This suggests the patch is either incomplete or incorrectly placed.\n3. The patch does not address the LaTeX or str printers, only the pretty printer.\n4. The patch does not clearly show how it would transform '-1*B' into '- B' in the output, as it still relies on the existing 'pretty_negative' function, which may not be sufficient for all cases (e.g., nested negatives, or more complex expressions).\n\nTherefore, the patch misses key aspects of the issue and is not a correct or complete solution.",
        "label": "BROAD_MISSING_KEY_ASPECTS"
    },
    "sympy__sympy-14317": {
        "explanation": "The issue describes that the LaTeX printer for Poly objects does not print the monomials in the same (descending degree) order as the str and pretty printers. The patch changes the LaTeX printer's _print_Poly method to use poly.as_ordered_terms() instead of poly.as_expr(). The as_ordered_terms() method returns the terms in the correct (descending degree) order, matching the behavior of the other printers. This change is minimal and directly addresses the issue without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-14333": {
        "explanation": "The issue describes two main problems: (1) SymPy's mod_inverse does not handle negative moduli (e.g., mod_inverse(2, -5)), and (2) the sign of the result should match the sign of the modulus, as in Python and WolframAlpha. The patch changes the check from 'm > 1' to 'm != 0', allowing negative moduli, and uses 'abs(m)' in igcdex and modulo operations, ensuring the result is in the correct range. It also adjusts the result for negative moduli to ensure the sign matches the modulus, as described in the issue. The symbolic branch is also updated to match this logic. The patch is focused and does not introduce unrelated changes. Therefore, it correctly and precisely addresses the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-14531": {
        "explanation": "The issue describes two main problems: (1) the 'sympy_integers' setting is not respected in subexpressions when printing with sstr (e.g., in Eq or Limit), and (2) the python() printer does not generate symbol/function declarations as expected.\n\nThe patch addresses (1) by updating the _print_Limit and _print_Relational methods in StrPrinter to use self._print on all subexpressions, ensuring that settings like 'sympy_integers' are propagated recursively. The Rational printer is also adjusted to check the setting first, so that S(1)/2 is printed as such when requested, even in nested contexts. The new tests in test_str_printer_settings.py confirm that the correct output is produced for both top-level and nested expressions.\n\nFor (2), the patch rewrites the python() function to collect symbols from expr.free_symbols and generate the appropriate Symbol(...) declarations, as well as handling reserved keywords. The test confirms that the generated code now includes the expected symbol declarations and the correct expression assignment.\n\nNo unnecessary changes are made, and the patch directly targets the described issues with clear, focused modifications and comprehensive tests. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-14711": {
        "explanation": "The issue describes that adding 0 (as in sum([N.x, 0*N.x])) to a Vector raises a TypeError, because the __add__ method expects the other operand to be a Vector, but 0 is not. The patch adds a check in Vector.__add__ to return self if other == 0, which prevents the TypeError and allows sum([N.x, 0*N.x]) to work as expected. The patch does not introduce unnecessary changes and directly addresses the described problem. The added test file (edge_cases.py) demonstrates the fix for several relevant cases. No unrelated code is changed. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-14817": {
        "explanation": "The issue describes a traceback when pretty-printing a MatAdd involving a MatrixSymbol with a name containing a '*', due to the use of S(item.args[0]) (i.e., sympify) on a string that is not a valid SymPy expression. The patch replaces the problematic sympification with a check for the 'could_extract_minus_sign' method, which is a standard SymPy way to check if an expression is negative, and thus avoids the problematic sympify call. The patch also adds a test that covers the original error case and a negative matrix case, ensuring the fix works and that negative signs are handled. No unrelated changes are made. The patch is precise and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-14976": {
        "explanation": "The issue describes that when using lambdify with modules='mpmath', rational numbers (like 232/3) are not wrapped in mpmath.mpf, leading to reduced precision. The patch adds a _print_Rational method to the MpmathPrinter, which outputs rationals as mpmath.mpf(p)/mpmath.mpf(q), ensuring both numerator and denominator are high-precision floats. This directly addresses the issue by ensuring rationals are represented with mpmath's arbitrary precision. The rest of the patch contains a small unrelated typo fix (delimite -> delimiter), but this is a harmless correction and not an unrelated feature or bugfix. The main change is precise and correct for the issue described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15011": {
        "explanation": "The issue is that lambdify fails with MatrixSymbol names containing curly braces (e.g., '{v}'), even when dummify=True, because the generated argument names are not valid Python identifiers. The patch modifies the _is_safe_ident method to treat identifiers as safe if curly braces are removed and the result is a valid identifier. More importantly, in the _preprocess method, when dummify is True or the identifier is not safe, it replaces curly braces in the dummy variable's name with underscores, ensuring the generated argument names are valid Python identifiers. This directly addresses the root cause of the SyntaxError described in the issue. The patch does not introduce unrelated changes and is focused on the described problem. The added print statements are for debugging and do not affect correctness. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15017": {
        "explanation": "The issue describes that for rank-0 (scalar) SymPy arrays, len(a) returns 0, but it should return 1, matching the behavior of numpy (where a scalar array has size 1). The patch changes the initialization of self._loop_size from 0 to 1 when shape is empty (i.e., rank-0), in two places in the code. This is the correct place to make the change, as _loop_size is used by __len__ to determine the length of the array. The patch does not introduce unrelated changes and directly addresses the described issue. The included test script (reproduce.py) is not part of the library code and does not affect the patch's correctness. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15225": {
        "explanation": "The issue describes that `bool_map` incorrectly returns a mapping between two logically non-equivalent expressions (Xor and its negation), simply because their structure allows a mapping, without verifying logical equivalence. The patch modifies the function to, after finding a mapping, substitute it and check for logical equivalence using `.equals()`. Only if the mapped expression is logically equivalent does it return the mapping; otherwise, it returns `(False, None)`. This directly addresses the issue by ensuring that only logically equivalent expressions are considered a match, and does not introduce unrelated changes. The patch is both correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15345": {
        "explanation": "The issue is that the SymPy Mathematica printer outputs 'Max(2, x)' instead of the correct Mathematica syntax 'Max[x,2]'. The patch addresses this by:\n\n1. Adding an entry for 'max' in the known_functions dictionary, mapping it to 'Max'.\n2. Modifying the _print_Function method to check if the function name is 'max' (case-insensitive) and, if so, return 'Max[args]' with the arguments stringified and separated by commas, matching Mathematica's syntax.\n3. The patch also updates the lookup in known_functions to be case-insensitive, which is a minor but reasonable improvement.\n\nNo unrelated or unnecessary changes are made, and the patch directly addresses the issue as described. The output for Max(x,2) will now be 'Max[x,2]', as expected.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15349": {
        "explanation": "The issue describes an error in the output of Quaternion.to_rotation_matrix(), specifically that both sin(x) terms in the resulting matrix are positive, whereas one should be negative. The patch changes the sign in the computation of m12 from '+' to '-', which directly addresses the sign error in the rotation matrix. This is a minimal and targeted fix, and it does not introduce unrelated changes. The patch is therefore correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15599": {
        "explanation": "The issue describes that Mod(3*i, 2) should simplify to Mod(i, 2), and provides a suggested patch that modifies the logic in sympy/core/mod.py to ensure that non_mod_l is replaced with [cls(x, q) for x in non_mod_l] and that the simplification is triggered if this replacement changes the list, or if the existing mod_l logic applies. The submitted patch matches this logic, introducing the 'was' and 'changed' variables and updating the condition to trigger the distributive simplification. This directly addresses the issue, and does not introduce unrelated changes. The patch is precise and limited to the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15625": {
        "explanation": "The issue describes a problem where SymPy's LaTeX output in Jupyter notebooks is wrapped in double dollar signs ($$...$$), which causes errors when the output itself contains LaTeX environments like equation*, as these are not valid inside math mode. The proposed solution is to replace the $$...$$ wrapping with $\\displaystyle ...$, which keeps the output in inline math mode but with display style, avoiding the problematic nesting of math environments.\n\nThe patch changes the return value from '$$%s$$' to '$\\displaystyle %s$', which directly implements the suggested fix in the issue description. It does not introduce any unrelated changes and precisely targets the problematic wrapping. This should resolve the LaTeX error described, and the change is in line with the discussion and references in the issue.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15809": {
        "explanation": "The patch modifies the MinMaxBase class so that when Min() or Max() are called with no arguments, they return oo or -oo, respectively, instead of raising a ValueError. This directly addresses the issue described, which requests that Min() with no arguments return oo and Max() with no arguments return -oo, in line with mathematical convention. The patch also updates the tests to check for this new behavior, ensuring correctness. No unrelated changes are made, and the implementation is precise and complete for the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15875": {
        "explanation": "The issue describes that `is_zero` on certain complex expressions (specifically, sums involving complex numbers) incorrectly returns `False` when it should return `None` if undecidable. The patch changes the logic so that, in the relevant branch, instead of returning `False` for complex expressions that might simplify to zero, it now returns `None`. This directly addresses the problem described in the issue, ensuring that `is_zero` does not incorrectly return `False` for undecidable cases. The change is minimal and targeted, with no unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-15976": {
        "explanation": "The issue is that symbols with numeric suffixes (like 'x2') are rendered invisibly in MathML output, because the code previously split off trailing digits as subscripts, so 'x2' was interpreted as 'x' with subscript '2', and the main symbol name became empty in the MathML output. The patch modifies the 'split_super_sub' function to accept an 'allow_digits' parameter, and in the MathML printer, it sets 'allow_digits=True' so that trailing digits are not split off as subscripts. This ensures that 'x2' is treated as a single symbol name, not as 'x' with subscript '2'. The patch also adds tests to verify that 'x2' is rendered as 'x2' in both content and presentation MathML. The changes are minimal and directly address the issue without unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-16450": {
        "explanation": "The issue describes that the `posify` function, when replacing a symbol with a positive dummy, fails to preserve the `finite` assumption (and possibly others) from the original symbol. The patch modifies the `posify` function so that, when creating the replacement dummy symbol, it copies all assumptions from the original symbol and then overrides `positive=True`. This ensures that the `finite` assumption (and any other existing assumptions) are preserved in the new dummy symbol, directly addressing the reported problem. The patch does not introduce unrelated changes and is focused solely on the described issue. The included test script (in a new file) demonstrates the preservation of the `finite` assumption and does not affect the main codebase. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-16597": {
        "explanation": "The issue describes that a Symbol with the assumption even=True should have is_finite == True, but currently it returns None. The patch introduces a _handle_assumptions class method that sets 'integer' to True if 'even' is True, which is correct because 'even' implies 'integer', and in SymPy, 'integer' implies 'finite'. However, the patch also adds a _eval_is_finite method that directly returns True if is_even is True, which is a more direct fix and ensures that is_finite is True for even symbols. The test added in test_assumptions.py checks both the positive and negative cases, confirming the fix. The patch is focused and does not introduce unrelated changes. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-16601": {
        "explanation": "The issue arises because the code attempts to evaluate a relational expression (x < self.focus.args[0]) directly in an if statement, which fails when x or the focus coordinate is symbolic, as SymPy cannot determine the truth value of symbolic inequalities. The patch replaces these direct comparisons with a Piecewise expression, which is the correct symbolic way to represent conditional logic in SymPy. This allows the p_parameter property to return a symbolic expression that can be further manipulated, rather than raising an error. The patch is minimal and directly addresses the described issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-16766": {
        "explanation": "The issue describes that PythonCodePrinter does not support printing of Indexed objects, resulting in a warning and a comment in the output. The proposed solution is to add a _print_Indexed method that formats the Indexed object as base[index0, index1, ...], which matches Python's syntax for indexed access. The patch adds exactly this method, with no extraneous changes, and the implementation matches the one suggested in the issue description. Therefore, the patch directly and precisely addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-16792": {
        "explanation": "The issue is that when a MatrixSymbol argument is passed to autowrap but does not appear in the expression, the generated C function signature incorrectly uses a scalar type (e.g., 'double x') instead of a pointer (e.g., 'double *x'). This causes runtime errors when numpy arrays are passed. The patch addresses this by, in the Argument class's __init__, checking if the argument is a MatrixSymbol and, if so, setting its dimensions accordingly. This ensures that the code generation logic later recognizes it as an array and emits the correct pointer type in the C function signature, regardless of whether the argument appears in the expression. The rest of the patch consists of debug print statements, which are not necessary for the fix but do not affect correctness. No unrelated or missing aspects are present. The patch is precise and directly addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-16886": {
        "explanation": "The patch directly addresses the issue described: it changes the Morse code mapping for '1' from the incorrect '----' to the correct '.----' in the dictionary within sympy.crypto.crypto. No unrelated changes are made, and the fix is precise. The addition of a test script (reproduce_morse_error.py) is not part of the core library and does not affect the correctness of the fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-16943": {
        "explanation": "The issue is that calling sympy.fps on a polynomial (like x**2) returns the polynomial itself, not a FormalPowerSeries object, which leads to interface inconsistencies (e.g., indexing fails). The patch changes the behavior so that, even if the result is None (i.e., no expansion is needed), a FormalPowerSeries object is always returned, wrapping the original expression. This ensures that the return type is consistent and supports the expected interface. The patch does not introduce unnecessary changes and directly addresses the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-17139": {
        "explanation": "The issue is that when simplifying expressions like cos(x)**I, the code attempts to compare rv.exp (which is I, the imaginary unit) to 0, resulting in a TypeError because complex numbers cannot be ordered. The patch adds a check to ensure that rv.exp is real before performing the comparison. If rv.exp is not real, it returns rv early, thus avoiding the invalid comparison. This directly and precisely addresses the issue described, without introducing unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-17318": {
        "explanation": "The issue describes an IndexError in the function split_surds, which is called during sqrtdenest when the input expression cannot be denested. The error occurs because the surds list is empty, leading to an attempt to unpack or index into an empty sequence. The patch adds a check: if not surds, return S.One, S.Zero, expr. This prevents the IndexError by returning default values when there are no surds to process, allowing the calling code to handle the undenestable case gracefully. The patch is minimal and directly addresses the root cause without introducing unrelated changes. It matches the described fix and the new behavior (returning the unchanged expression) aligns with the issue's requirements.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-17630": {
        "explanation": "The issue is that after block-multiplying BlockMatrix objects containing ZeroMatrix blocks, the resulting blocks that should be ZeroMatrix are instead sympy.core.numbers.Zero (i.e., scalar zero), which causes attribute errors when further block operations are attempted (since Zero does not have .cols or .rows attributes). The patch modifies the _blockmul method so that after multiplying the blocks, it scans the resulting block matrix and replaces any scalar zeros (S.Zero) with ZeroMatrix objects of the appropriate size, using the block sizes from the original BlockMatrix. This directly addresses the root cause described in the issue: it ensures that zero blocks remain ZeroMatrix objects, preserving the expected interface and preventing the AttributeError. The patch is focused and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-17653": {
        "explanation": "The issue describes that when a subclass of Symbol (such as TypedSymbol) is passed as the label to IndexedBase, the current code upcasts it to a plain Symbol, thereby losing any additional information or attributes from the subclass. The problematic line is 'label = Symbol(label.name)', which reconstructs the label as a base Symbol, discarding subclass-specific data. The patch simply removes this line, so that if a Symbol (or subclass) is passed, it is used as-is after merging assumptions, preserving any subclass-specific information. This directly and precisely addresses the issue without introducing unrelated changes or missing any described requirements.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-17655": {
        "explanation": "The issue describes that adding a Point to a scalar-multiplied Point (e.g., point1 + 2.0*point2) raises an exception, while the reverse order (point1 + point2*2.0) works. The patch addresses this by modifying the __add__ method to detect when 'other' is a Mul (i.e., a multiplication expression), and if so, checks if one of the arguments is a Point. It then rewrites the addition in terms of point + (point * scalar), which is supported. Additionally, the patch adds __rmul__ and as_coeff_Mul to make Point more compatible with SymPy's multiplication and coefficient extraction mechanisms. These changes are minimal, directly address the issue, and do not introduce unrelated modifications. The patch is therefore correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-17770": {
        "explanation": "The patch updates the _eval_is_extended_real and _eval_is_finite methods for hyperbolic functions (sinh, cosh, tanh, and their reciprocals) to more accurately determine when these functions are real and finite, especially for imaginary and infinite arguments. The changes are focused on the logic for these properties, and the patch does not introduce unrelated changes. The logic is more nuanced than before, handling cases like arguments that are imaginary multiples of pi, and checking for finiteness and extended reality in a more granular way. The patch also acknowledges that the logic may still be incomplete for all possible cases, but it is a clear improvement and directly addresses the issue described. Therefore, the patch is correct and precise for the scope of the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18033": {
        "explanation": "The patch removes the global Permutation.print_cyclic flag and replaces it with a property that issues a deprecation warning when set, as required by the deprecation policy. The new property fetches the print_cyclic setting from the printing defaults, aligning with the requirement that printing configuration should be handled by the printer, not the object. In the str printer, the code is updated to always print the Python-valid representation, regardless of the print_cyclic flag, which matches the issue's request that the str printer should always print a Python-valid representation and not the cycle notation. The patch does not make unnecessary changes and directly addresses the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18109": {
        "explanation": "The issue is that when evaluation is disabled (with evaluate(False)), the expression 'oo - oo' should not be automatically evaluated to 'nan'; instead, it should remain unevaluated as 'oo - oo'. The patch adds a check at the start of the __sub__ methods for the relevant number classes: if global_evaluate[0] is False, it returns Add(self, -other), which constructs the unevaluated subtraction. This is the correct and minimal change to ensure that with evaluation disabled, the subtraction is not automatically evaluated, and the pretty printer will display 'oo - oo' as expected. The patch does not introduce unrelated changes and directly addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18189": {
        "explanation": "The issue describes that the output of diophantine depends on the order of the 'syms' argument when 'permute=True', leading to incomplete results for some orderings. The patch modifies the code so that, when a custom 'syms' order is provided, it first computes the solution in the default variable order, then reorders each solution tuple to match the requested 'syms' order. This ensures that all solutions are present regardless of the order of 'syms', and only the tuple order changes. The patch does not introduce unrelated changes and directly addresses the described problem by decoupling the solution generation from the output ordering. The logic is clear and precise, and no edge cases are missed based on the issue description. Therefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18191": {
        "explanation": "The issue describes a recursion error when evaluating sqrt(1 / tan(1 + I)), which ultimately results from repeated application of the transformation Pow(b.conjugate()/Abs(b)**2, other) in the power evaluation logic. The patch adds a check to prevent this transformation from being applied recursively by verifying that the base is not already of the form (something)**-1 (i.e., an inverse). This directly addresses the infinite recursion by ensuring the transformation is not reapplied to its own result. The change is minimal, targeted, and does not introduce unrelated modifications. Therefore, the patch is correct and precise in addressing the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18199": {
        "explanation": "The issue describes that nthroot_mod does not return 0 as a root when a % p == 0, even though x = 0 mod p is always a solution in this case. The patch adds a check for a % p == 0, and if so, returns 0 (or [0] if all_roots is True) before proceeding with the rest of the logic. This directly and precisely addresses the issue without introducing unrelated changes. The rest of the function remains untouched, so no unnecessary modifications are made.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18211": {
        "explanation": "The issue describes that for equations like Eq(n*cos(n) - 3*sin(n), 0), solveset currently raises NotImplementedError, but it should instead return a ConditionSet representing the solution set. The patch modifies the relevant section in solveset.py so that if both rational and transcendental solvers fail (i.e., both return ConditionSet), it explicitly returns a ConditionSet for the equation. This directly addresses the issue: instead of raising an error, solveset will now return the expected ConditionSet. The added test checks this exact scenario, ensuring that the fix works as intended. The patch does not introduce unnecessary changes and is focused on the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18650": {
        "explanation": "The issue is that sqrt(8)**Rational(2, 3) currently returns 2**(1/3)*2**(2/3) instead of simplifying to 2. The patch adds logic to combine exponents when both are rational and not integers, so (b**e)**other becomes b**(e*other), and if the new exponent is integer, it returns b**new_exp. For sqrt(8)**Rational(2,3), this means (8**(1/2))**(2/3) = 8**(1/3), and since 8**(1/3) = 2, the patch will return 2 as desired. The change is targeted and does not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18698": {
        "explanation": "The issue describes that the output of sqf_list is inconsistent: in some cases, it returns multiple factors with the same multiplicity (e.g., two separate (x-3, 3) and (x-2, 3)), when it should combine them into a single factor of that multiplicity (e.g., ((x-3)*(x-2), 3)). The patch introduces a factor_dict keyed by multiplicity, accumulating (by multiplication) all factors of the same multiplicity. At the end, it returns a list of (factor, multiplicity) pairs, sorted by multiplicity. This directly addresses the issue by ensuring that for each multiplicity, all corresponding factors are multiplied together, so the output will have at most one factor per multiplicity. The patch does not introduce unrelated changes and is precise in its fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18763": {
        "explanation": "The issue describes that when printing a multiplication involving a Subs object whose argument is an expression (like -x + y), the LaTeX output does not parenthesize the expression inside Subs, leading to ambiguous or incorrect rendering. The patch changes the LaTeX printer's _print_Subs method to use self.parenthesize(expr, PRECEDENCE[\"Mul\"], strict=True) instead of self._print(expr). This ensures that the expression inside Subs is parenthesized appropriately when necessary, such as when it is an addition or subtraction, matching the desired output shown in the issue. The change is minimal and directly targets the described problem without introducing unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18810": {
        "explanation": "The issue is that generate_derangements assumes the input permutation is sorted, using the first permutation generated (which is sorted) as the reference for derangement, rather than the original input order. This causes incorrect results when the input is not sorted. The patch fixes this by converting the input to a list (to ensure consistent indexing and comparison) and then always comparing generated permutations to the original input order (perm), not the first permutation from the generator. This directly and precisely addresses the issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-18903": {
        "explanation": "The issue describes that nested floor (and ceiling) expressions like floor(floor(x) + 1) are not being fully evaluated, i.e., floor(floor(x)) should simplify to floor(x). The patch adds a check in the eval method for instances where a term is itself a floor (or ceiling) operation, and recursively evaluates its argument, thus flattening nested operations. This directly addresses the issue by ensuring that nested floor/ceiling are simplified as expected, and the change is minimal and targeted. No unrelated code is touched.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-19007": {
        "explanation": "The issue describes that accessing an element of a BlockMatrix with symbolic indices (like C[i, 0]) incorrectly returns a single block's element (e.g., (A)[i, 0]), even when the index could refer to multiple blocks. The correct behavior is to return an unevaluated or piecewise expression reflecting the possible blocks the index could refer to.\n\nThe patch modifies the _entry method of BlockMatrix to always return a Piecewise expression, where each piece corresponds to a block and is guarded by a condition that the indices fall within that block's range. This ensures that for symbolic indices, the result is a Piecewise over all possible blocks, which is the correct and general solution. The patch does not introduce unrelated changes and directly addresses the described problem.\n\nTherefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-19040": {
        "explanation": "The issue describes that factoring a polynomial with extension=True (e.g., over complex numbers) drops a factor (specifically, 'y-1' in the example). The problematic behavior is that 'factor(z, extension=[I])' returns only 'x-1' instead of '(x-1)*(y-1)'.\n\nThe patch removes an early return in 'dup_ext_factor' that would return immediately if only one factor is found by 'dup_factor_list_include'. This early return would bypass the rest of the factoring logic, which could be necessary for correctly handling multivariate polynomials or polynomials over extension fields. By removing this early return, the function will continue processing, which is likely to ensure all factors are considered, including those that were previously dropped.\n\nThe change is minimal and directly targets the described issue without introducing unrelated modifications. The logic aligns with the problem: the early return was causing incomplete factorization in the extension case. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-19346": {
        "explanation": "The issue describes that srepr prints dict and set elements using their default representations (e.g., {x, y} instead of {Symbol('x'), Symbol('y')}). The patch adds _print_set and _print_dict methods to the repr printer, ensuring that sets and dicts are printed with their elements recursively passed through the printer (e.g., {Symbol('x'), Symbol('y')}). The patch also adds comprehensive tests for sets and dicts, including edge cases (empty, single element, nested). The changes are minimal and directly address the issue without unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-19495": {
        "explanation": "The issue describes that substituting a parameter (like y) in a ConditionSet whose base set is an ImageSet does not work as expected: instead of substituting y in both the condition and the base set, the substitution seems to go awry, possibly confusing the dummy variable with the substitution target. The expected behavior is that subs(y, Rational(1,3)) should replace y with 1/3 everywhere in the ConditionSet, including inside the ImageSet base set and the condition.\n\nThe patch rewrites the subs logic for ConditionSet, ensuring that substitutions are applied to the condition and the base set, and that the dummy variable (the bound variable of the ConditionSet) is only replaced if the substitution target is a Symbol. The new tests specifically check the case where the base set is an ImageSet and confirm that substituting y with 1/3 in the ConditionSet correctly updates both the condition and the base set, matching the expected output described in the issue.\n\nThe patch does not introduce unrelated changes and directly addresses the described problem, including the edge case with ImageSet. The logic for handling the dummy variable is careful to avoid accidental replacement unless appropriate, and the test coverage is expanded to include the problematic scenario from the issue.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-19637": {
        "explanation": "The issue is that the variable 'kern' may be referenced before assignment in the function kernS, specifically in the line 'hit = kern in s'. The patch moves the assignment of 'hit = kern in s' inside the 'if' block where 'kern' is defined, ensuring that 'kern' is only used after it has been assigned. This directly addresses the UnboundLocalError described in the issue, and the change is minimal and targeted, without introducing unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-19783": {
        "explanation": "The issue describes that multiplying a Dagger (adjoint) object by an IdentityOperator does not simplify to just the Dagger object, whereas multiplying a regular Operator by IdentityOperator does simplify as expected. The patch adds a __mul__ method to the adjoint class that checks if the right operand is an IdentityOperator and, if so, returns self (the Dagger object), otherwise defers to the superclass multiplication. This directly and precisely addresses the issue described, without introducing unnecessary changes or missing the described behavior.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-19954": {
        "explanation": "The issue describes an IndexError occurring in the original sylow_subgroup() implementation for certain DihedralGroup instances, specifically when calling minimal_blocks() as part of the computation. The patch introduces a new module with 'safe' versions of minimal_blocks and sylow_subgroup, and a SafePermutationGroup class that exposes these methods. The new safe_sylow_subgroup implementation avoids the problematic code path by not calling minimal_blocks, and instead implements its own logic for finding Sylow subgroups, including special handling for dihedral groups. The patch also includes comprehensive tests that cover the problematic cases (e.g., DihedralGroup(18), DihedralGroup(50)), and these tests assert that the correct subgroup orders are returned, which would not be possible if the IndexError still occurred. The patch does not make unnecessary changes and is focused on the described issue. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-20049": {
        "explanation": "The issue describes that Point.vel() should, when possible, compute the velocity of a point relative to a reference frame by differentiating its position vector, even if the velocity was not explicitly set. The original code only returns a velocity if it was explicitly set, otherwise it raises a ValueError. The patch modifies this behavior: if the velocity is not explicitly set, it attempts to compute the velocity by differentiating the position vector with respect to a reference point (if available), and adds the reference point's velocity if it is defined. If neither is available, it falls back to differentiating the position vector alone. This matches the expected behavior described in the issue and does not introduce unrelated changes. The patch is thus correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-20139": {
        "explanation": "The issue requests that MatrixSymbol should use a string (Str) for its name, rather than a Symbol. The patch enforces that the 'name' argument must be a string, raising a TypeError otherwise, and stores it directly as the first argument. It also updates the 'name' property to return this string directly, rather than accessing .name on a Symbol. This is a precise and minimal change that directly addresses the issue, with no unrelated modifications. The patch does not attempt to handle edge cases like backward compatibility or existing code that may have passed a Symbol, but this is consistent with the issue's intent to enforce string usage. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-20154": {
        "explanation": "The issue describes a problem where the `partitions()` iterator yields the same dictionary object multiple times, mutating it between yields. This can cause confusing and incorrect results when the output is collected, as all results reference the same dictionary object. The patch addresses this by yielding a copy of the dictionary (`ms.copy()` or `{0: 1}.copy()`) at every yield point, ensuring that each yielded value is a distinct object and not affected by subsequent mutations. The patch does not introduce unrelated changes and directly targets the described problem. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-20428": {
        "explanation": "The issue describes a bug where, after calling clear_denoms() on a polynomial that is mathematically zero, the resulting Poly object prints as zero but behaves inconsistently: its .is_zero property is False, and methods like terms_gcd() and primitive() either fail or return inconsistent results. The root cause is that the internal DMP representation contains unstripped leading zeros (e.g., DMP([EX(0)], EX, None) instead of DMP([], EX, None)).\n\nThe patch addresses this by:\n- Adding a static method _strip_zeros to the Poly class, which removes leading zeros from the DMP representation.\n- Ensuring that Poly.new() calls _strip_zeros on the rep before constructing the Poly object, so all new Poly instances have a properly stripped DMP.\n- Updating primitive() to handle the zero case explicitly, returning the correct content and zero Poly.\n- Adding a regression test that exercises the problematic case from the issue, asserting that the resulting Poly is zero, its as_expr() is 0, its rep is DMP([], ...), and that terms_gcd() and primitive() behave as expected.\n\nThe patch is focused, only changes what is necessary, and the new test covers the reported issue. No unrelated changes are made. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-20438": {
        "explanation": "The issue describes two main problems: (1) `is_subset` gives wrong results for `ProductSet` and `FiniteSet` combinations, and (2) an AttributeError is raised because `Complement` lacks an `equals` method, which is called during simplification of set equalities.\n\nThe patch addresses both:\n\n1. In `sympy/sets/sets.py`, it adds `_eval_is_subset` and `_eval_is_superset` methods to `ProductSet`, allowing correct subset/superset checks between `ProductSet` and `FiniteSet`. The logic for `_eval_is_subset` is to check that all elements of the `ProductSet` are contained in the `FiniteSet`, which is correct for the case described in the issue.\n\n2. In `sympy/sets/sets.py`, it adds an `equals` method to `Complement`, which prevents the AttributeError when `dif.equals(0)` is called during simplification. The implementation is minimal but sufficient to avoid the crash described in the issue.\n\nOther changes in the patch are minor and do not introduce unrelated modifications. The patch is focused and directly addresses the two problems described in the issue.\n\nTherefore, the patch is CORRECT_AND_PRECISE.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-20590": {
        "explanation": "The issue describes that, starting from version 1.7, Symbol instances have a __dict__ attribute, which they did not have in 1.6.2. The likely cause is that a parent class (Boolean) does not define __slots__, causing __dict__ to be present in subclasses. The patch removes Boolean from the base classes of Symbol, leaving only AtomicExpr. This change would restore the previous behavior, as AtomicExpr defines __slots__, and thus Symbol instances would not have a __dict__ attribute. The patch is minimal and directly addresses the issue without unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-20801": {
        "explanation": "The issue describes an inconsistency: `S(0.0) == S.false` returns True, but `S.false == S(0.0)` returns False. The expectation, based on the behavior of `S(0) == S.false` (which returns False), is that `S(0.0) == S.false` should also return False. The patch changes the behavior so that when comparing a Float to a Boolean, it returns `self.is_zero and not other`. For `S(0.0) == S.false`, this evaluates to `True and True` (since `S.false` is False, so `not other` is True), which is True. This preserves the current (incorrect) behavior, rather than making it consistent with `S(0) == S.false`. Therefore, the patch does not address the issue as described, and in fact, it seems to reinforce the undesired behavior.",
        "label": "INCORRECT"
    },
    "sympy__sympy-20916": {
        "explanation": "The issue describes that when pretty-printing with unicode, subscripts on Greek letters (such as \u03c90) are not formatted correctly: the digit is not rendered as a subscript, but as a normal digit. The patch addresses this by:\n\n1. Creating a reverse mapping from Unicode Greek characters to their names, allowing for better handling of direct Unicode input.\n2. Modifying the subscript mapping to include all Greek letters and their Unicode forms, so that both 'omega' and '\u03c9' can be mapped to their subscript forms.\n3. Adjusting the pretty_list function to process subscripts character by character, ensuring that digits are converted to their subscript Unicode equivalents, while non-digit characters are left as is or mapped if possible.\n4. Ensuring that the output for subscripts and superscripts is joined without spaces, preserving the correct formatting.\n5. Updating the line_width function to not count subscript digits in the width calculation, which is a minor but relevant formatting fix.\n\nAll these changes are directly related to the issue and do not introduce unrelated modifications. The patch is precise and comprehensive for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-21101": {
        "explanation": "The issue is that when printing an unevaluated Mul like Mul(6, S.Half, evaluate=False), the LaTeX output is '6 1/2', which lacks the expected separation (numbersep) between the numbers, making it ambiguous. The patch modifies the logic in convert_args to check if there is more than one numeric argument in the Mul, and if so, it recursively calls convert_args on the args, which is the same behavior as when the first argument is S.One. This change ensures that cases like Mul(6, S.Half, evaluate=False) are handled in the same way as other cases where numbers are combined, thus producing the correct LaTeX output with proper separation. The patch is minimal and directly targets the described issue without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-21171": {
        "explanation": "The issue is that _print_SingularityFunction() does not accept the 'exp' keyword argument, but the LaTeX printer's _print_Pow method may call it with exp=... for function bases. The patch modifies _print_SingularityFunction and _print_Heaviside to accept an optional 'exp' argument, and adds logic to wrap their output in a LaTeX exponent if exp is provided. This directly addresses the TypeError described in the issue. The patch also makes similar changes to other function printers for consistency. There are some redundant lines (e.g., double 'if exp:' blocks in _print_Heaviside), but these do not affect correctness. The patch is focused and does not introduce unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-21208": {
        "explanation": "The issue describes an inconsistency: Matrix.diff applies differentiation elementwise but does not apply the chain rule for nested functions, while diff on an element or applyfunc(lambda ij: ij.diff(x)) does. The patch addresses this by clarifying the docstring of Matrix.diff to state that it does not apply the chain rule and introduces a new method, deep_diff, which applies differentiation to each element including the chain rule (by using applyfunc with diff). This directly addresses the confusion in the issue, provides a method for the desired behavior, and does not introduce unnecessary changes. The patch is precise and correct.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-21379": {
        "explanation": "The issue describes an unexpected 'PolynomialError: Piecewise generators do not make sense' when substituting values in expressions involving Piecewise, particularly after certain substitutions and with real symbols. The problematic code in polytools.py raises a PolynomialError whenever a generator is a Piecewise instance. The patch changes this to only raise the error if the Piecewise's args are not all instances of Basic. However, in SymPy, Piecewise.args are always tuples of (expr, cond), and both expr and cond are typically Basic instances. Thus, the new check will almost always pass, and the error will not be raised, allowing the code to proceed. This change prevents the error in the user's case, but it does not address the underlying reason why Piecewise generators are problematic for polynomials, nor does it ensure that all edge cases are handled correctly. It is possible that some Piecewise objects with non-Basic args could still cause issues, and the patch may allow invalid generators to pass through. Therefore, the patch addresses the immediate symptom but may not be robust for all cases involving Piecewise generators.",
        "label": "CORRECT_BUT_INCOMPLETE"
    },
    "sympy__sympy-21596": {
        "explanation": "The issue is that after intersecting an ImageSet with Reals, the 'in' operator incorrectly returns True for 2, even though 2 is not in the set. The patch adds a check in the _contains method: if the base set (or _intersect_set) is a subset of Reals, and the element being checked is complex but not real, it immediately returns False. This prevents non-real numbers from being considered as members of a set that is a subset of the reals. This directly addresses the issue where 2 (a real number) was incorrectly considered in the intersection, but the patch ensures that only real numbers are considered for sets that are subsets of Reals. The change is minimal and targeted, with no unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-21612": {
        "explanation": "The issue is that parsing a nested fraction like \\frac{\\frac{a^3+b}{c}}{\\frac{1}{c^2}} yields ((a**3 + b)/c)/1/(c**2) instead of the correct ((a**3 + b)/c)/(1/(c**2)). The difference is that the denominator should be grouped as a single entity (1/(c**2)), not as a sequence of divisions. The patch modifies the convert_frac function in _parse_latex_antlr.py so that, instead of returning expr_top * expr_bot**-1 for a general fraction, it always constructs the denominator as a Pow(expr_bot, -1, evaluate=False) and then multiplies by the numerator. This ensures that the denominator is always grouped as a single entity, preventing the flattening of nested divisions. The patch does not introduce unnecessary changes and directly addresses the root cause of the issue. The rest of the patch consists of generated files and does not affect the logic. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-21847": {
        "explanation": "The issue describes that `itermonomials` with a `min_degrees` argument only returns monomials where the maximum exponent of any variable is at least `min_degrees`, rather than monomials whose total degree is at least `min_degrees` (and at most `max_degrees`). The patch changes the filtering condition from `if max(powers.values()) >= min_degree:` to `if min_degree <= sum(powers.values()) <= max_degree:`, which correctly implements the documented and expected behavior: it now checks the total degree of the monomial, not just the maximum exponent. The patch is minimal and directly addresses the bug without unrelated changes. The added script in `reproduce_bug.py` is a test script to demonstrate the fix and does not affect the core code. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-21930": {
        "explanation": "The issue describes a LaTeX rendering problem where expressions like b^\\dagger_{0}^{2} are not grouped correctly, leading to incorrect display in Jupyter. The patch modifies the _latex methods for CreateBoson and Commutator to ensure that when a power of a creation operator is printed, the base (b^\\dagger_{0}) is wrapped in parentheses or curly braces before the exponent is applied, resulting in (b^{\\dagger}_{0})^{2}. This directly addresses the issue described, and the changes are limited to the relevant LaTeX printing logic without introducing unrelated modifications. The patch is precise and correct for the described problem.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-22080": {
        "explanation": "The issue describes a bug where, when using lambdify with modules=[], the generated code for Mod expressions incorrectly moves multipliers inside the Mod, resulting in incorrect output. The patch introduces a custom MyPythonCodePrinter that overrides the _print_Mod method to always generate '({} % {})' for Mod(a, b), ensuring the correct translation to Python's modulo operator. The patch also ensures that this custom printer is used when modules is empty or contains 'sympy' or 'math', which matches the problematic case described in the issue. The change is minimal, targeted, and does not introduce unrelated modifications. Therefore, the patch correctly and precisely addresses the described issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-22456": {
        "explanation": "The issue describes that the `String` class in `codegen.ast` does not support argument invariance via positional arguments (`expr.func(*expr.args) == expr`), which is expected for any `Basic` subclass. The original code had `not_in_args = ['text']`, which excluded 'text' from being part of `args`, thus breaking this invariance. The patch changes `not_in_args` to an empty list, ensuring 'text' is included in `args`, thereby restoring the expected behavior. The rest of the patch adds a `matches` method, which is unrelated to the described issue but does not detract from the main fix. The change is minimal and directly addresses the argument invariance problem without unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-22714": {
        "explanation": "The issue describes that using 'with evaluate(False)' causes Point2D construction to raise 'Imaginary coordinates are not permitted.' even for valid real coordinates, while it works as expected without evaluate(False) or with evaluate=False. The patch changes the check for imaginary coordinates in Point2D.__new__ to only raise the error if 'evaluate' is True. This means that when evaluation is suppressed (evaluate=False), the check is skipped, which matches the behavior of other SymPy objects and resolves the issue. The patch does not introduce unrelated changes and is precise. The added test script demonstrates the fixed and expected behaviors, but does not affect the core code. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-22914": {
        "explanation": "The issue describes that PythonCodePrinter does not support the SymPy Min and Max functions, and provides example methods to add to the printer to generate Python's min() and max() calls. The patch adds exactly these two methods, using the same logic as suggested in the issue, and does not introduce any unrelated changes. The implementation is precise and directly addresses the problem described, enabling correct code generation for Min and Max expressions.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-23262": {
        "explanation": "The issue describes a regression in the Python code printer for tuples with a single element: the generated code omits the trailing comma, so (1,) is printed as (1), which is not a tuple in Python. The patch adds a _print_Tuple method to the LambdaPrinter class, which checks if the tuple has a single element and, if so, prints it with a trailing comma. For tuples with more than one element, it prints them as a comma-separated list in parentheses. This directly and precisely addresses the issue described, without introducing unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-23296": {
        "explanation": "The issue describes that mathematical functions like exp should only be applied to dimensionless arguments, and that previously, the dimension system would incorrectly propagate dimensions through such functions, leading to errors. The patch modifies the _collect_factor_and_dimension method so that when a Function is encountered, it checks if all arguments are dimensionless. If so, it returns the function applied to the factors and sets the dimension to Dimension(1) (i.e., dimensionless), which is correct. If any argument is not dimensionless, it raises a ValueError, which is also correct behavior. This directly addresses the issue described, and the change is limited to the relevant logic without unnecessary modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-23413": {
        "explanation": "The issue describes that the Hermite Normal Form (HNF) computation is incorrectly removing rows, likely due to misidentifying zero rows or rank-deficiency, especially when the matrix is manipulated with flips and transposes. The patch introduces a ZERO_TOLERANCE threshold and changes the check from 'if b == 0' to 'if abs(b) < ZERO_TOLERANCE' in the HNF algorithm. This is a floating-point style fix, but the matrices in question are over ZZ (integers), as enforced by the domain check. Introducing a floating-point tolerance in an integer domain is not appropriate and could mask other issues. The patch does not address the root cause of the row removal or rank misidentification in the integer domain context. Therefore, the patch is incorrect for the described issue.",
        "label": "INCORRECT"
    },
    "sympy__sympy-23534": {
        "explanation": "The issue is that when passing a tuple of strings (e.g., ('q:2', 'u:2')) to symbols with cls=Function, the returned objects are of type Symbol instead of Function. The patch modifies the handling of non-string inputs in the symbols function, specifically ensuring that when the input is a tuple, list, or set, each element is processed with the correct cls argument, and the container type is preserved. For sets, it also handles the case where elements might be single-element tuples. This directly addresses the issue described, ensuring that the correct class (Function) is used even with an extra layer of parentheses. The changes are focused and do not introduce unrelated modifications.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-23824": {
        "explanation": "The issue describes that kahane_simplify() reverses the order of leading uncontracted gamma matrices when reinserting them after simplification. The original code inserts these matrices in the order 0 to first_dum_pos-1, which results in the first leading gamma matrix being inserted first, but since each is inserted at position 0, the final order is reversed. The patch changes the loop to iterate from first_dum_pos-1 down to 0, so the last leading gamma matrix is inserted first, and the first is inserted last, thus preserving the original order. The patch also updates the comment to correctly refer to 'leading' instead of 'trailing' gamma matrices. No unrelated changes are made. This directly and precisely addresses the described bug.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-23950": {
        "explanation": "The issue describes that Contains.as_set() incorrectly returns a Contains object, which is not a set, leading to downstream errors (e.g., in Piecewise, which expects a set with as_relational). The patch changes Contains.as_set() to return self.args[1], which is the set argument to Contains, thus making as_set() return the actual set as expected. The test is updated to check that as_set() returns the correct set, and a new test is added to ensure Piecewise works with Contains as a condition, addressing the error in the issue. The patch is minimal and directly addresses the problem without unrelated changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24066": {
        "explanation": "The issue describes that when passing an expression like exp(second/(farad*ohm)) to SI._collect_factor_and_dimension, the function fails to recognize that the exponent is dimensionless, and thus raises a ValueError. The patch modifies the handling of Function instances in _collect_factor_and_dimension: it now checks if all arguments to the function are dimensionless, and if so, returns the function of the scale factors and sets the dimension to Dimension(1). This is the correct behavior for mathematical functions like exp, sin, etc., which require dimensionless arguments and yield dimensionless results. The patch does not introduce unrelated changes and directly addresses the described bug. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24213": {
        "explanation": "The issue describes that _collect_factor_and_dimension currently only allows addition of terms with exactly the same dimension object, but not with dimensions that are equivalent (e.g., velocity and acceleration*time). The patch modifies the relevant check in _collect_factor_and_dimension to use the equivalent_dims method, which allows for dimensionally equivalent (but not necessarily identical) dimensions to be considered valid for addition. The patch also adds a targeted test that reproduces the issue and asserts the correct behavior. No unrelated changes are made. Therefore, the patch is both correct and precise in addressing the issue as described.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24370": {
        "explanation": "The issue is that performing floor division (//) between a sympy.Integer and a symbolic sympy.Symbol (e.g., sympy.Integer(1024)//s0) raises a TypeError, because the Integer class tries to construct an Integer from a symbolic floor expression, which is not a numeric type. The patch modifies the __floordiv__ method of Integer so that if the right-hand side is a symbolic expression (Expr) but not an Integer, it returns floor(self / other) instead of trying to create an Integer from the result. This matches the behavior of Python ints in sympy, and ensures that the result is a symbolic floor expression rather than an invalid Integer. The patch is minimal and directly addresses the described issue without introducing unrelated changes. Therefore, it is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24443": {
        "explanation": "The issue describes a bug in `_check_homomorphism` where, for `PermutationGroup`, the code fails to recognize inverted generators (e.g., `r[i]` being the inverse of a generator) because the `in gens` test only matches direct generators. The patch addresses this by adding an `elif` branch: if `r[i]**-1` is in `gens`, it uses the inverse of the corresponding generator. This directly addresses the described failure mode. The rest of the logic is unchanged, and the patch does not introduce unrelated changes. The patch is focused and precise in its fix, handling both direct and inverted generators as required by the issue.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24539": {
        "explanation": "The issue describes that PolyElement.as_expr() ignores the user-supplied symbols even when the correct number is provided, always using self.ring.symbols instead. The original code uses an 'else' block, so even if the user provides the correct number of symbols, it still sets 'symbols = self.ring.symbols', thus ignoring the input. The patch changes 'else' to 'if not symbols', so now, only if no symbols are provided will it use self.ring.symbols; otherwise, it will use the user-supplied symbols. This directly and precisely addresses the issue without unnecessary changes.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24562": {
        "explanation": "The issue describes that Rational('0.5', '100') produces an incorrect value (1/100100) instead of the expected 1/200, while Rational(0.5, 100) works as expected. The patch adds a special case in the Rational constructor to handle when both p and q are strings, converting them to fractions and dividing them, then constructing the Rational from the resulting numerator and denominator. This directly addresses the issue described, ensuring that string inputs are handled in a way consistent with float inputs. The patch does not introduce unnecessary changes and includes relevant tests to verify the fix. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24661": {
        "explanation": "The patch directly addresses the issue that 'parse_expr' with 'evaluate=False' was not respecting the flag for relational operators (e.g., '<', '>', '==', etc.), and would return evaluated Python booleans instead of unevaluated SymPy relational objects. The patch adds handling for relational operators in the EvaluateFalseTransformer, mapping Python AST comparison nodes to their SymPy equivalents (Lt, Gt, Le, Ge, Eq, Ne) and ensuring 'evaluate=False' is passed. It also adds comprehensive tests for all relevant relational operators, verifying that the output is the correct unevaluated SymPy object. The changes are focused and do not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    },
    "sympy__sympy-24723": {
        "explanation": "The issue describes that the PDF of the Matrix Normal distribution in SymPy has an incorrect normalization constant: the denominator is off by a factor of 2. The Wikipedia formula shows the denominator as (2\u03c0)^{np/2} * |U|^{p/2} * |V|^{n/2}, but the code had (2\u03c0)^{np/2} * |U|^p/2 * |V|^n/2, which is not the same as the Wikipedia formula. The patch changes the denominator to (2\u03c0)^{np/2} * sqrt(|U|^p * |V|^n), which is mathematically equivalent to |U|^{p/2} * |V|^{n/2}, matching the Wikipedia formula. The patch makes only this change and does not introduce unrelated modifications. Therefore, the patch is correct and precise.",
        "label": "CORRECT_AND_PRECISE"
    }
}