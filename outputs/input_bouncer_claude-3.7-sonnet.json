{
    "astropy__astropy-11693": {
        "explanation": "This issue is well-specified with clear information about the problem, expected behavior, and steps to reproduce:\n\n1. The problem is clearly defined: When plotting an image with a WCS projection that contains non-linear distortions, it fails with a `NoConvergence` error.\n\n2. The expected behavior is explicitly stated: The user wants to be able to get a plot without having to modify the library code by adding `quiet=True` to the `all_world2pix` call.\n\n3. The actual behavior is well-documented with a detailed error message showing the exact point of failure.\n\n4. Complete steps to reproduce are provided with a self-contained code example that can be run to recreate the issue.\n\n5. System details are comprehensively provided, including versions of all relevant libraries.\n\n6. The user has even identified a potential solution (adding `quiet=True` parameter) and demonstrated that it works, showing a screenshot of the successful result.\n\nThe issue has all the necessary information to understand the problem and implement a solution. The solution would likely involve modifying the plotting functionality to handle the `NoConvergence` error gracefully, possibly by adding a parameter to control the `quiet` behavior when using WCS projections with non-linear distortions.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12544": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The user explains:\n\n1. The problem: In Astropy 5, `Table.read()` automatically creates a `MaskedColumn` when encountering values like `NaN`, converting the table to a `MaskedTable`.\n2. Why it's a problem: This automatic behavior complicates data pipelines where downstream functions need to replace `NaN` values, requiring extra steps to work with masked data.\n3. A specific solution proposal: Add a keyword parameter like `Table.read(filename, ..., mask=False)` to disable the automatic masking behavior.\n\nThe issue provides enough context about the current behavior, why it's problematic in certain workflows, and a clear suggestion for how to resolve it. A developer could implement this feature without needing additional clarification. The solution would involve adding a parameter to control the masking behavior in the `Table.read()` method.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12825": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code example, error message, and expected behavior.\n\nThe issue describes a specific bug: when a column of `SkyCoord` objects is included in an Astropy `Table`, the `aggregate` function fails to work on grouped tables. The error occurs because the `SkyCoord` object doesn't have a `groups` attribute that the aggregation process is trying to access.\n\nThe reproduction steps are complete and include:\n1. A minimal code example that demonstrates the issue\n2. The exact error message that occurs\n3. System details including OS, Python version, and library versions\n\nThe expected behavior is clearly stated: aggregation should work, only failing to aggregate columns where the operation doesn't make sense.\n\nA developer would have all the information needed to investigate and fix this issue without requiring further clarification. The solution would likely involve modifying how the aggregation process handles SkyCoord columns in grouped tables.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12842": {
        "explanation": "This issue is well-specified with clear details about the problem, reproduction steps, and expected behavior. The user has:\n\n1. Clearly identified the problem: After upgrading from Astropy 4.2.1 to 5.0+, they can no longer read BinnedTimeSeries with datetime64 columns saved as ECSV files.\n\n2. Provided the exact error message: `ValueError: datatype 'datetime64' of column 'time_bin_start' is not in allowed values...`\n\n3. Identified the specific code change that caused the issue: A commit that added strict type checking in ecsv.py line 177.\n\n4. Included reproduction steps: How the file was created and how they're trying to read it.\n\n5. Included relevant file format details: The ECSV header showing the datetime64 datatype.\n\n6. Provided full error logs and system details for both working and non-working environments.\n\n7. Even suggested potential solutions: Either scaling back the strict type checking or adding datetime64 as an allowed type.\n\nA developer would have all the necessary information to understand the issue, reproduce it, and implement a fix by either modifying the ECSV_DATATYPES constant to include datetime64 or adding special handling for datetime64 columns when reading ECSV files.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12880": {
        "explanation": "This issue is well-specified with clear details about the problem, reproduction steps, and expected behavior. The user has:\n\n1. Clearly identified the problem: After upgrading from Astropy 4.2.1 to 5.0+, they can no longer read BinnedTimeSeries with datetime64 columns saved as ECSV files.\n\n2. Provided the exact error message: `ValueError: datatype 'datetime64' of column 'time_bin_start' is not in allowed values...`\n\n3. Identified the specific code change that caused the issue: A commit that added strict type checking in ecsv.py line 177.\n\n4. Provided reproduction steps: How the file was created and how they're trying to read it.\n\n5. Included relevant file format details: The ECSV header showing the datetime64 datatype.\n\n6. Included full error logs and system details for both working and non-working environments.\n\n7. Even suggested potential solutions: Either scaling back the strict type checking or adding datetime64 as an allowed type.\n\nA developer would have all the necessary information to understand the issue, reproduce it, and implement a fix by either modifying the ECSV_DATATYPES constant to include datetime64 or adding special handling for datetime64 columns in the ECSV reader.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12891": {
        "explanation": "This issue is reasonably well-specified. The user clearly identifies a specific problem: the `where` keyword argument of `np.mean` (which was added in NumPy 1.20.0) is not supported for `astropy.units.Quantity` instances. \n\nThe issue provides:\n1. A clear identification of the feature gap (lack of support for `where` keyword in `np.mean` for Quantity objects)\n2. Context about when this feature was added to NumPy (version 1.20.0)\n3. An indication that the user has already searched for existing issues on this topic\n\nWhat makes this not fully \"well-specified\" is the lack of:\n1. A code example demonstrating the problem\n2. Error messages or specific behavior when attempting to use this feature\n3. Expected behavior details\n\nHowever, a developer familiar with both NumPy and Astropy would likely understand what needs to be done: update the `astropy.units.Quantity` class to properly handle the `where` keyword argument when passed to NumPy reduction functions like `np.mean`. The solution would involve examining how NumPy implements this feature and ensuring the Quantity class properly passes this parameter through to the underlying NumPy functions.\n\nThe user even offers to contribute to implementing this feature, which suggests they have a good understanding of what's needed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-12907": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user has:\n\n1. Provided a specific function (`separability_matrix`) that's not working as expected\n2. Included complete code examples demonstrating the issue\n3. Shown the expected behavior with simple models\n4. Demonstrated the unexpected behavior when nesting compound models\n5. Included the actual output that appears incorrect\n6. Explained why they believe this is a bug (the separability matrix shows inputs/outputs as not separable when they should be)\n\nThe issue is focused on a specific technical problem with the `separability_matrix` function not correctly handling nested CompoundModels. The expected behavior is clear - the separability matrix should show the same separability relationships regardless of whether models are nested or not. The solution would involve fixing the `separability_matrix` function to correctly handle nested CompoundModels.\n\nA developer could reproduce this issue and understand what needs to be fixed without requiring additional information.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-12962": {
        "explanation": "This issue is well-specified with clear requirements and context. The user is requesting functionality to convert a CCDData object to an ImageHDU object, which is currently not directly possible. The issue:\n\n1. Clearly explains the current behavior and limitations\n2. Demonstrates the asymmetry in the API (you can convert ImageHDU to CCDData but not vice versa)\n3. Provides code examples showing the current functionality\n4. Proposes three specific implementation options with code examples\n5. Includes additional context by comparing to similar functionality with Table/BinTableHDU\n\nThe issue provides enough technical detail for a developer to understand the problem and implement one of the proposed solutions. The three options are well-defined:\n- Option A: Add a parameter to CCDData.to_hdu()\n- Option B: Create a new convenience function\n- Option C: Add an append parameter to CCDData.write()\n\nA developer could implement any or all of these options without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13032": {
        "explanation": "This issue is well-specified with clear examples demonstrating the bug. The reporter has:\n\n1. Provided a clear description of the problem: `ModelBoundingBox` doesn't correctly handle ignored inputs, specifically when ignoring the \"x\" parameter.\n\n2. Included complete code examples that reproduce the issue, showing both the problematic case (ignoring \"x\") and a working case (ignoring \"y\").\n\n3. Shown the expected output versus the actual output, making it clear what's wrong.\n\n4. Identified that the issue exists in both the current release (5.03) and the development version.\n\n5. Explained the likely cause: the ignored status is being accounted for during validation but getting \"lost\" afterward.\n\nThe issue provides all the necessary information to understand the problem and attempt a fix. A developer would need to investigate the `ModelBoundingBox` class implementation to see why the \"x\" parameter is being treated differently from \"y\" when marked as ignored.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13033": {
        "explanation": "This issue is well-specified with clear information about what's wrong and what needs to be fixed. The issue describes:\n\n1. The specific problem: When a required column is removed from a TimeSeries object, the error message is misleading.\n2. The expected behavior: An exception that clearly informs users that required columns are missing.\n3. The actual behavior: The current confusing error message is provided verbatim.\n4. A complete code example that reproduces the issue, showing how to set up a TimeSeries object with required columns and trigger the error.\n5. System details including versions of relevant packages.\n\nThe solution would involve modifying the error handling in the TimeSeries class to provide a more informative error message when a required column is removed. The code example makes it clear where the issue occurs and what needs to be fixed. A developer could easily reproduce this issue and implement a solution without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13068": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible example. The user is experiencing precision issues when converting time formats using astropy's Time object. They've provided:\n\n1. A specific code example showing the conversion from ISO time format to Julian Date (JD)\n2. A second example showing the reverse conversion back to ISO format\n3. The exact results they're getting\n4. A clear statement of what they expect: to get back the original time value after the round-trip conversion\n\nThe issue is that precision is being lost during these conversions, and the user wants to maintain the original precision. This is a concrete, technical problem with a clear definition of success: maintaining the original time precision through format conversions.\n\nThe solution would likely involve understanding how precision is handled in astropy.time.Time and finding the correct way to preserve it during format conversions. This might involve examining the precision parameter usage or other Time object configuration options.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13075": {
        "explanation": "This issue is reasonably well-specified. It clearly describes the goal: to add HTML export functionality to the Cosmology.write method with MathJax support for mathematical expressions. The issue provides:\n\n1. Context about the existing functionality (Cosmology can read and write to files)\n2. Links to relevant examples and documentation\n3. A suggested implementation approach (using cosmology.io.table.to_table() and QTable.write)\n4. An additional suggestion about storing MathJax representations in Parameter objects\n\nWhile the issue provides good direction, there are some details that would need to be filled in during implementation:\n- Exact formatting of the HTML output\n- How to handle MathJax integration specifically\n- Which parameters should have MathJax representations\n- Error handling considerations\n\nHowever, these are implementation details that a developer could reasonably determine based on the existing codebase and the provided documentation links. The core requirements are clear enough to begin work on a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13132": {
        "explanation": "This issue is well-specified with clear objectives. The issue requests implementing `__array_func__` for the `astropy.time.Time` class to enable NumPy array functions to work with Time objects. The description provides:\n\n1. A specific example of the functionality being requested (using `np.linspace` with Time objects)\n2. A reference to a related GitHub issue (#8610) for additional context\n3. A working code example demonstrating the expected behavior\n4. A clear checklist of remaining tasks (tests, documentation, etc.)\n\nThe implementation goal is unambiguous: make NumPy array functions work with Time objects through the `__array_func__` protocol. This is a standard Python protocol for NumPy interoperability, and the example clearly shows what functionality is expected.\n\nThe PR author has already started implementation and is seeking feedback, which indicates they have a clear understanding of what needs to be done. The remaining tasks (tests, documentation) are standard parts of any implementation and don't indicate ambiguity in the requirements.\n\nA developer with knowledge of NumPy's array function protocol and Astropy's Time class would have sufficient information to implement this feature based on the description provided.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13158": {
        "explanation": "This issue is well-specified with a clear problem description, a minimal reproducible example, expected behavior, actual error output, and even a pointer to the likely source of the problem in the codebase.\n\nSpecifically:\n1. The issue is clearly defined: model evaluation fails when a parameter has a `MagUnit` type value\n2. A complete code example is provided that reproduces the error\n3. The expected output is stated: `-20.0 * unit`\n4. The actual error traceback is included, showing exactly where the failure occurs\n5. The reporter has identified the likely problematic code in `astropy.modeling.core` with a link to the specific lines\n6. System details are provided for context\n\nThe issue even includes a suggestion about what might need to be fixed - the logic for handling parameters with special unit types needs to be improved in the `._param_sets` method.\n\nA developer with knowledge of the astropy codebase, particularly the units and modeling components, would have all the information needed to investigate and fix this issue without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13234": {
        "explanation": "This issue is well-specified with clear details about the bug and how to reproduce it. The description explains that a structured column with a field name of \"name\" cannot be round-tripped through ECSV format, suggesting a problem in the serialization format. The issue includes:\n\n1. A clear description of the problem\n2. Complete code to reproduce the issue\n3. The exact error output showing the TypeError: \"unhashable type: 'SerializedColumn'\"\n4. A suggested solution approach (modifying the serialization format)\n5. System details including versions of relevant packages\n\nThe expected behavior is clearly stated: \"It should work!\" meaning the round-trip serialization should succeed without errors. The issue also references a related issue (#13231) which provides additional context.\n\nThe error occurs in the table serialization code when trying to use a SerializedColumn as a dictionary key, which is not possible because it's not hashable. The issue suggests a specific format change to fix the problem.\n\nA developer would have all the information needed to understand, reproduce, and fix this issue without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13236": {
        "explanation": "This issue is well-specified with clear context and a concrete proposal. The issue describes:\n\n1. The current behavior: structured numpy arrays are automatically transformed into NdarrayMixin when added to a Table\n2. The rationale for why this behavior exists and why it may no longer be necessary (after PR #12644)\n3. A specific two-step proposal for changing the behavior:\n   - Add a FutureWarning in the current version\n   - Remove the auto-transformation behavior in version 5.2\n\nThe issue includes the exact code snippet that would need to be modified, making it very clear where the change needs to be made. The desired outcome is explicit: stop automatically converting structured arrays to NdarrayMixin and instead add them as Column objects.\n\nThe timeline expectations are also clear - it's not critical for version 5.1 but would be nice to include if there are other critical bugfixes happening.\n\nA developer could implement this change without needing additional clarification about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13306": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a clear description of the problem: using `table.vstack` on tables containing columns backed by numpy structured arrays fails.\n\n2. Included a minimal, complete code example that demonstrates both the working case and the failing case.\n\n3. Included the full error traceback showing exactly where and how the operation fails.\n\n4. Provided comprehensive system details including OS, Python version, and all relevant library versions.\n\nThe issue is specifically about a TypeError that occurs when trying to stack tables where one contains a structured array column. The error message \"Cannot cast array data from dtype([('val', '<f4'), ('min', '<f4'), ('max', '<f4')]) to dtype('V12') according to the rule 'unsafe'\" clearly indicates a type casting problem during the vstack operation.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed - the vstack operation needs to properly handle structured array columns.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13390": {
        "explanation": "This issue is well-specified with clear information about what's failing. The issue describes test failures in the astropy library that are occurring with NumPy 1.23.0rc3. The error logs show detailed information about the failures:\n\n1. The main problem appears to be related to comparing string types (Unicode vs bytes) in Column objects\n2. The errors include:\n   - `ufunc 'not_equal' did not contain a loop with signature matching types`\n   - `FutureWarning: elementwise comparison failed; returning scalar instead`\n   - `TypeError: '>' not supported between instances of 'MaskedColumn' and various types`\n\nThe issue is clearly a compatibility problem between astropy's table/column implementation and changes in NumPy 1.23.0rc3's handling of string type comparisons. The test failures are well-documented with stack traces showing exactly where the problems occur.\n\nA developer would be able to:\n1. Identify the specific functions that need to be fixed (`_compare` method in column.py)\n2. Understand the nature of the comparison issues between different string types\n3. See that the code has special handling for certain cases that may need to be updated\n4. Recognize that NumPy's behavior has changed regarding elementwise comparisons\n\nThe solution would involve updating the comparison logic in the Column and MaskedColumn classes to handle the new NumPy behavior correctly.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13398": {
        "explanation": "This issue is well-specified for an experienced developer familiar with the astropy codebase and astronomical coordinate transformations. The issue describes:\n\n1. A clear problem: The current ITRS to AltAz transformation is causing confusion for users who want to observe nearby objects (satellites, airplanes, etc.)\n2. A specific technical explanation of the issue: The problem relates to geocentric versus topocentric aberration\n3. A proposed solution: A more direct approach that stays within ITRS and converts between ITRS, AltAz, and HADec coordinates\n4. Working code that demonstrates the solution\n5. A specific question: Would the team be receptive to this approach?\n\nThe issue includes detailed implementation code showing exactly how the solution would work. The author has already tested the approach and is ready to submit a PR if the team agrees with the direction.\n\nThe only minor ambiguity is whether refraction should be added to the implementation, but the author acknowledges this and is prepared to add it if needed.\n\nFor someone familiar with the domain, this issue provides all the necessary information to understand the problem and evaluate the proposed solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13404": {
        "explanation": "This issue is well-specified with clear information about what's not working and how to reproduce it. The issue describes a specific problem: the `_array2string` function in `masked.function_helpers` doesn't work on arrays with structured dtypes, while it should work like their non-masked counterparts.\n\nThe issue provides:\n1. A clear description of the problem\n2. Expected behavior: masked arrays with structured dtypes should print without issues\n3. Actual behavior: it errors because of the structured dtype\n4. A specific code example to reproduce the issue\n5. System details including OS, Python version, and relevant library versions\n\nThe reproduction steps are complete and include a concrete example showing both the failing case and a working case for comparison. A developer would be able to reproduce this issue and understand what needs to be fixed without needing additional information. The solution would involve modifying the `_array2string` function to properly handle structured dtypes in masked arrays.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13453": {
        "explanation": "This issue is well-specified with clear information about the problem, expected behavior, and steps to reproduce. The user has:\n\n1. Clearly identified the bug: The `formats` parameter is ignored when writing an astropy table to HTML format, while it works for other formats.\n\n2. Provided the expected behavior: The HTML output should respect the formatting specified in the `formats` argument.\n\n3. Included complete, runnable code examples that demonstrate both the problem (HTML output) and working examples (CSV and RST formats).\n\n4. Shown the actual output versus expected output.\n\n5. Included detailed system information including versions of Python, astropy, and other relevant packages.\n\nThe issue contains all the necessary information for a developer to understand the problem, reproduce it, and implement a fix. The problem is specific and focused on a single functionality that isn't working as expected. A developer would be able to locate the relevant code in the astropy codebase that handles HTML table output and modify it to respect the formats parameter, similar to how the other output formats handle it.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13462": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The specific failing test: `test_two_sum_symmetric` in `time/tests/test_precision.py`\n2. The exact error: an assertion failure where `two_sum(f1, f2)` is not equal to `two_sum(f2, f1)` for specific floating-point values\n3. The failing example with exact values: `f1=-3.089785075544792e+307, f2=1.7976931348623157e+308`\n4. Links to both failing and successful test runs for comparison\n5. Reference to the PR (#10373) where the test was added\n\nThe issue is about the `two_sum` function not being symmetric for certain extreme floating-point values, which is causing test failures in PyInstaller builds. The function is returning `nan` in one direction but `-9.9792015476736e+291` in the other.\n\nWhat's missing is:\n1. The implementation of the `two_sum` function itself, though it could be found in the codebase\n2. Explicit guidance on how the function should handle these extreme values\n3. Whether the test should be modified or the function fixed\n\nHowever, a developer with access to the codebase could reasonably:\n1. Examine the `two_sum` function implementation\n2. Understand the numerical precision issues with extreme floating-point values\n3. Either fix the function to handle these edge cases symmetrically or modify the test to account for expected behavior with extreme values\n\nThe issue provides enough context to understand the problem and make a reasonable attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13469": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly described: when trying to convert a list of Astropy Tables to a NumPy array with dtype=object, it fails with a \"Datatype coercion is not allowed\" error. The issue points to the specific source of the problem in the table.__array__() method, which explicitly checks for any dtype parameter and raises an error if it's not None.\n\nThe request is to consider making an exception for dtype=object to allow this conversion, particularly for the use case of converting a list of Tables to a NumPy array.\n\nWhat makes this reasonably (rather than perfectly) specified:\n1. It doesn't provide the exact code in table.__array__() that's causing the issue\n2. It doesn't specify exactly how the exception for dtype=object should be implemented\n3. It doesn't provide a complete test case that demonstrates the issue\n\nHowever, an experienced developer familiar with Astropy could locate the relevant code, understand the issue, and implement a solution that allows dtype=object to pass through in the __array__() method while maintaining the existing behavior for other dtype values. The issue provides enough context about the problem and the desired outcome to guide implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-13477": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug: comparing a Frame instance with data to a SkyCoord instance with the same data raises a TypeError exception.\n\n2. The issue provides complete reproduction steps with a minimal code example that clearly demonstrates the problem:\n   - Import the necessary classes\n   - Create a Frame instance with data\n   - Create a SkyCoord instance from that Frame\n   - Try to compare them with `==` which raises an exception\n\n3. The error message and traceback are included, showing exactly where the problem occurs in the code.\n\n4. The expected behavior is clearly stated: the comparison should return true or false based on the data, rather than raising an exception.\n\n5. The issue provides context about why this is problematic: the user notes that SkyCoord and Frame instances with data are sometimes used interchangeably, making this inconsistent behavior surprising.\n\nA developer would have all the information needed to understand the problem, locate the issue in the codebase, and implement a fix that would allow comparison between Frame and SkyCoord objects without raising an exception.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13579": {
        "explanation": "This issue is well-specified with a clear description of the problem and complete steps to reproduce it. The user has:\n\n1. Provided a detailed explanation of the inconsistent behavior they're experiencing with `world_to_pixel` in `SlicedLowLevelWCS`\n2. Included complete, runnable code examples that demonstrate both the expected behavior and the problematic behavior\n3. Shown the exact outputs they're getting and what they expected instead\n4. Provided system details including OS, Python version, and relevant library versions\n5. Identified that the issue occurs specifically when using `world_to_pixel` on a sliced WCS, while `pixel_to_world` works correctly in both cases\n6. Included a specific test case with a 3D WCS that has a non-trivial PCij matrix coupling spectral and spatial dimensions\n\nThe issue is clearly a bug in the `SlicedLowLevelWCS` implementation when handling the `world_to_pixel` transformation for sliced WCS objects with certain matrix configurations. The expected behavior is well-defined, and the actual behavior is clearly demonstrated. A developer would have all the information needed to investigate and fix this issue.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13668": {
        "explanation": "This issue is well-specified with clear examples of the problem. The issue describes that the `wcslint` tool in Astropy is crashing when processing certain valid WCS (World Coordinate System) files. The description includes:\n\n1. A clear identification of the problem: `wcslint` crashes on valid WCS files\n2. Links to the relevant code in the repository where the issue might be occurring\n3. Three specific example files with their exact paths/URLs that reproduce the issue\n4. Complete command-line examples showing how to reproduce the crashes\n5. The exact error messages and stack traces produced by each crash\n\nThe issue shows three different failure modes:\n- A malloc assertion failure with the first file\n- A ValueError about HDUList requirements with the second file\n- A malloc checksum error with the third file\n\nThe description also suggests potential causes - either an issue inside the `WCS` class or how the validator calls `WCS` in complex cases.\n\nWith this information, a developer would have everything needed to reproduce the issue, investigate the crashes, and implement a fix. The examples provide diverse test cases to ensure any solution addresses all failure modes.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13745": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code examples, and expected behavior. The issue describes how the float32 representation of pi/2 is slightly larger than the actual value when converted back to float64, causing Astropy's Latitude validation to reject it as being greater than 90 degrees.\n\nThe problem is clearly demonstrated with code that shows:\n1. The float32 representation of pi/2 is 1.5707964, which is slightly larger than the float64 value of 1.5707963267948966\n2. When this float32 value is passed to Latitude(), it's rejected with a ValueError because the validation is done at float64 precision\n3. The exact error message and traceback are provided\n\nThe expected behavior is also reasonably clear - the suggestion is to be more lenient in the comparison, perhaps by only comparing up to float32 precision when the input is a float32 value.\n\nThe issue includes all necessary system details and version information. A developer would have enough information to reproduce the issue and implement a solution that handles float32 values near the boundaries appropriately.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13803": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code examples, and expected behavior. The issue describes how the float32 representation of pi/2 is slightly larger than the actual value when converted back to float64, causing Astropy's Latitude validation to reject it as being greater than 90 degrees.\n\nThe problem is clearly demonstrated with code that shows:\n1. The float32 representation of pi/2 is 1.5707964, which is slightly larger than the float64 value of 1.5707963267948966\n2. When this float32 value is passed to Latitude(), it's rejected with a ValueError because the validation is done at float64 precision\n3. The exact error message and traceback are provided\n\nThe expected behavior is also reasonably clear - the suggestion is to be more lenient in the comparison, perhaps by only comparing up to float32 precision when the input is a float32 value.\n\nThe issue includes all necessary system details and version information. A developer would have enough information to reproduce the issue and implement a solution that handles float32 values near the boundaries appropriately.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13838": {
        "explanation": "This issue is well-specified with clear details about the problem. The user has:\n\n1. Provided a clear description of the bug: printing tables with 0-length array cells causes an IndexError\n2. Included a complete error traceback showing exactly where the error occurs\n3. Supplied a minimal reproducible example with sample data that demonstrates the issue\n4. Shown screenshots of the behavior with different inputs\n5. Explained the expected behavior (table should print fine with \"bad\" events)\n6. Included system details and version information\n\nThe issue is specifically about the Astropy Table class failing to properly handle tables that contain array columns with zero-length arrays. The error occurs in the formatting/printing code when trying to access elements of these empty arrays.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed - the table printing functionality needs to handle zero-length arrays properly without trying to access out-of-bounds indices.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13933": {
        "explanation": "This issue is well-specified with clear information about the bug and how to reproduce it. The description clearly states the problem: after pickling and unpickling an Angle object, the to_string() method fails with a specific error message. The reporter has provided:\n\n1. A concise description of the bug\n2. Complete, minimal code to reproduce the issue\n3. Expected vs. actual behavior\n4. The exact error message\n5. Detailed system information including OS, Python version, and all relevant package versions\n\nThe reproduction steps are straightforward and can be executed directly to verify the issue. The problem is clearly related to the serialization/deserialization of Angle objects in astropy.coordinates, specifically affecting how unit information is preserved during pickling. This provides enough information for a developer to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-13977": {
        "explanation": "This issue is well-specified with a clear problem statement and a proposed solution. The user has:\n\n1. Provided a detailed description of the problem they're encountering\n2. Included a minimal working example that reproduces the issue\n3. Shown the exact error message they're receiving\n4. Explained their reasoning for why `Quantity.__array_ufunc__()` should return `NotImplemented` instead of raising a `ValueError`\n5. Referenced the numpy documentation that supports their argument\n6. Offered to create a PR to implement the solution\n\nThe issue is about changing the behavior of `Quantity.__array_ufunc__()` in astropy to return `NotImplemented` instead of raising a `ValueError` when it encounters incompatible inputs. This would allow for proper duck typing and enable the reflected versions of arithmetic operators to be called when appropriate.\n\nThe solution would involve modifying the `_condition_arg` function in astropy's units module to return `NotImplemented` instead of raising a `ValueError` when it encounters a value that isn't scalar compatible or convertible to an array. This is a well-defined change with clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14096": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is subclassing `SkyCoord` from the astropy.coordinates module and adding a custom property. When this property tries to access a non-existent attribute, the error message incorrectly states that the property itself doesn't exist rather than indicating the actual missing attribute.\n\nThe issue includes:\n1. Complete code to reproduce the problem\n2. The exact error message received\n3. A clear explanation of what's wrong (error message says 'prop' doesn't exist when it should say 'random_attr' doesn't exist)\n4. The expected behavior (error should point to the actual missing attribute)\n\nThe problem is in how the `__getattr__` method in the parent `SkyCoord` class handles attribute access for subclasses. A solution would involve modifying how attribute errors are propagated when accessing properties in subclasses of `SkyCoord`. This is a specific, technical issue with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14182": {
        "explanation": "This issue is well-specified with a clear problem and expected solution. The user wants to add support for the `header_rows` parameter in the RestructuredText output format of Astropy tables, similar to how it works in the `ascii.fixed_width` format. \n\nThe issue includes:\n1. A clear example demonstrating the current functionality\n2. The exact error message when attempting to use the unsupported parameter\n3. The expected behavior (by showing how it works with another format)\n4. A use case explaining why this feature would be valuable (for autogenerated documentation)\n\nThe solution would require adding support for the `header_rows` parameter to the RST writer class in Astropy's ASCII I/O module. The implementation would likely follow a similar pattern to how it's implemented in the fixed_width writer. The error message even points to the specific class that needs modification (RST.__init__()).\n\nA developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14253": {
        "explanation": "This issue is reasonably well-specified. The author is discussing when the `info` attribute should be propagated to new objects in the `Quantity` class. They provide a clear categorization of different operations and their opinion on whether `info` should be propagated in each case:\n\n1. Operations where `info` should \"surely\" be propagated (views, reshapes, insertions)\n2. Operations where `info` \"probably\" should be propagated (scalar selection, copies, equivalent unit changes)\n3. Operations where `info` \"probably\" should not be propagated (arithmetic operations, real unit changes)\n\nThe author also notes that currently all of these operations propagate `info` due to the implementation in `__array_finalize__` and `_new_view`.\n\nWhile the issue provides a good starting point for discussion and implementation, there are some aspects that could benefit from further clarification:\n1. The exact definition of what constitutes a \"real unit change\" vs an \"equivalent unit change\"\n2. Whether there are other operations not listed that need consideration\n3. Whether there are specific use cases or examples that motivate these distinctions\n\nHowever, these are details that could reasonably be worked out during implementation, and the core request is clear enough to begin work on a solution. The issue is directed at a specific person (@taldcroft) who likely has context on the codebase and design decisions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14295": {
        "explanation": "This issue is well-specified with clear details about the problem, expected behavior, actual behavior, and steps to reproduce. The issue describes a specific bug where the presence of SIP keywords in a FITS header causes PV keywords to be ignored by astropy.wcs, which is different from the behavior in WCSLIB.\n\nThe issue includes:\n1. A clear description of the problem\n2. A complete code example to reproduce the issue\n3. Visual evidence showing the difference between expected and actual behavior\n4. Detailed steps to reproduce the issue\n5. System details for context\n6. A reference to the PR that aims to fix the issue\n\nThe PR description provides additional context, explaining that the issue is caused by the `_fix_scamp` function which removes PV keywords when SIP distortions are present. The PR proposes to remove this heuristic code rather than adding more special cases.\n\nA developer would have all the necessary information to understand the issue, reproduce it, and implement a solution by removing or modifying the problematic `_fix_scamp` function in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14309": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific error: IndexError in the `identify_format` function from astropy's io.registry module.\n\n2. It provides the exact error traceback showing where the error occurs: in the `is_fits` function when trying to access `args[0]` which is out of range.\n\n3. It includes a minimal reproducible example with code that can be run to trigger the error.\n\n4. It references a specific commit that likely caused the issue (2a0c5c6f5b982a76615c544854cd6e7d35c67c7f).\n\n5. It explains the suspected cause: a change in behavior where previously the function returned None for non-FITS files, but now it tries to execute `isinstance(args[0], ...)` which fails because `args` is empty or doesn't have enough elements.\n\nThe issue provides all the necessary context for a developer to understand the problem, reproduce it, and identify where the fix needs to be applied. The solution would likely involve modifying the `is_fits` function in `astropy/io/fits/connect.py` to properly handle the case when `args` is empty or doesn't have enough elements.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14365": {
        "explanation": "This issue is well-specified with clear information about the problem and expected behavior:\n\n1. The problem is clearly defined: The `ascii.qdp` reader in Astropy only accepts uppercase QDP commands (like \"READ SERR 1 2\"), but QDP itself is case-insensitive and can handle lowercase commands.\n\n2. There's a clear reproduction case with:\n   - Sample input file content\n   - Exact commands to run\n   - The error message received\n   - Confirmation that the actual QDP program works with the same file\n\n3. The expected behavior is explicitly stated: The reader should accept lowercase commands and successfully read the file with errors.\n\n4. All relevant version information is provided.\n\nThe solution path is straightforward: modify the `ascii.qdp` reader to be case-insensitive when parsing QDP commands. This would likely involve changing the command recognition logic to convert commands to uppercase before comparison or making the comparisons case-insensitive.\n\nA developer could implement this solution without needing any additional information or clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14369": {
        "explanation": "This issue is well-specified with clear details about what's wrong and how to reproduce it. The user has:\n\n1. Clearly identified the problem: When reading MRT files (CDS format) with astropy.table, composite units are incorrectly parsed, specifically the order of division is jumbled.\n\n2. Provided the expected behavior: Units in the resulting Table should match those in the input MRT file.\n\n3. Included complete reproduction steps:\n   - Provided a sample MRT file with specific unit formats\n   - Included the exact code to reproduce the issue\n   - Showed the incorrect output that demonstrates the problem\n\n4. Specified the environment:\n   - OS: macOS-12.5-arm64-arm-64bit\n   - Python version: 3.9.12\n   - Astropy version: 5.2.1\n\n5. Referenced the relevant standard: CDS standard for units (with link)\n\nThe issue is focused on a specific bug with clear examples showing the incorrect behavior. A developer would have all the necessary information to investigate and fix the unit parsing problem in the astropy.table CDS format reader.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14371": {
        "explanation": "This issue is reasonably well-specified. The request is to add an `atol` (absolute tolerance) parameter to the `is_O3` function in the astropy codebase, or alternatively to use the maximum precision of the matrix dtype instead of the hardcoded 1e-15 value.\n\nThe issue provides:\n1. A clear feature request (add `atol` parameter)\n2. An alternative solution (use matrix dtype precision)\n3. The exact file and line numbers where the change should be made\n\nWhat makes this reasonably specified rather than well-specified:\n- It doesn't specify the default value for the new `atol` parameter\n- It doesn't explain the rationale for why this change is needed\n- It doesn't specify if the parameter should be optional or required\n- It doesn't provide examples of cases where the current implementation fails\n\nHowever, these gaps can be filled with reasonable assumptions:\n- The `atol` parameter would likely be optional with a default value\n- The implementation would be similar to other numpy/scipy functions that use tolerance parameters\n- The rationale is likely to allow users to adjust precision requirements based on their specific needs\n\nA developer familiar with the codebase should be able to implement this feature without additional clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14379": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. The current behavior: `Angle.to_string()` doesn't include a space between the value and unit (e.g., \"1.08e+07mas\")\n2. The desired behavior: Add an option to include a space between the value and unit\n3. A specific implementation suggestion: Add a boolean keyword argument to control this behavior\n4. An alternative approach: Change the default behavior to match `Quantity` which already includes a space by default\n\nThe issue includes concrete code examples demonstrating both the current behavior and the desired behavior. It also references the existing behavior of a related class (`Quantity`) for consistency consideration.\n\nA developer would have enough information to implement this feature without needing additional clarification. The scope is narrow and well-defined - modify the `to_string` method of the `Angle` class to either add a new parameter or change the default behavior to include a space between value and unit.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14413": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes an inconsistency in how spaces are handled when formatting units in Astropy, specifically with the `unicode` and `console` formats. The problem is clearly demonstrated with code examples showing:\n\n1. When using `unicode` format, sometimes a space is included at the beginning of the string representation\n2. The behavior is inconsistent - sometimes a space is included, sometimes it's not, depending on whether a unit scale factor is present\n\nThe expected behavior is clearly stated: `unit.to_string(format)` should never start with a space, regardless of the format used.\n\nThe reproduction steps are comprehensive, with multiple examples showing different scenarios where the issue occurs. The issue also references related discussions in a PR, providing additional context.\n\nA developer would have enough information to locate the relevant code in the Astropy codebase that handles unit string formatting, identify the conditions causing the inconsistent space insertion, and implement a fix to ensure consistent behavior without leading spaces.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14439": {
        "explanation": "This issue is well-specified with a clear problem statement and solution expectation. The user identifies that:\n\n1. The current implementation of astropy units exports \"Jy/beam\" as \"beam-1 Jy\" in FITS format\n2. This differs from the convention used in radio astronomy packages, which prefer \"Jy/beam\"\n3. The space after \"beam-1\" causes parsing difficulties\n4. The user provides specific examples and links to related issues in other packages\n\nThe expected solution is clear: modify the FITS string representation of the Jy/beam unit to output \"Jy/beam\" instead of \"beam-1 Jy\". This is a specific, targeted change to the unit string representation system with a well-defined expected outcome. The issue provides enough context about the problem, the current behavior, and the desired behavior to implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14508": {
        "explanation": "This issue is well-specified with clear details about the problem, expected behavior, and reproduction steps:\n\n1. The problem is clearly defined: `io.fits.Card` uses an unnecessarily verbose string representation of floats (e.g., \"0.009124999999999999\" instead of \"0.009125\"), which causes comments to be truncated due to FITS card length limitations.\n\n2. Expected behavior is explicitly stated: Being able to create any valid FITS Card via `io.fits.Card`.\n\n3. Reproduction steps are provided with:\n   - A sample FITS file\n   - Complete code examples showing the issue\n   - The exact output demonstrating the problem\n   - A link to the specific code in the codebase causing the issue\n\n4. The submitter has even investigated the root cause and suggested a potential solution: using `str(value)` before falling back to `f\"{value:.16G}\"`.\n\n5. Environment details are provided, including OS, Python version, and relevant package versions.\n\nThis issue contains all the necessary information to understand the problem, reproduce it, and implement a solution without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14528": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a minimal reproducible example. The issue describes that when an `ImageHDU` is created with zero-dimensional data (a scalar numpy array with shape `()`), and then written to a file using `io.fits`, it creates a corrupt FITS file without any error or warning. The expected behavior is clearly stated: `io.fits` should not silently create corrupt FITS files. The reproduction steps are precise, including a complete code example that demonstrates the issue. The environment details (OS, Python version, and relevant library versions) are also provided. A developer would have all the necessary information to reproduce the issue and implement a fix that either prevents zero-dimensional arrays from being used or handles them properly when writing to FITS files.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14539": {
        "explanation": "This issue is well-specified with clear information about the problem, expected behavior, and a complete reproduction case. The issue describes a bug in `io.fits.FITSDiff` where it incorrectly reports differences when comparing identical FITS files, particularly when those files contain variable-length arrays (VLAs).\n\nThe description includes:\n1. A clear problem statement: `FITSDiff` reports differences between identical files\n2. Expected behavior: comparing a file to itself should never yield differences\n3. A complete, minimal reproduction script that demonstrates the issue\n4. The exact output showing the incorrect behavior\n5. A hypothesis about the cause (VLA handling)\n6. Complete version information for the environment\n\nThe reproduction case is particularly valuable as it provides a self-contained example that creates a FITS file with a variable-length array column and then demonstrates the incorrect comparison behavior. The output clearly shows that `FITSDiff` is reporting differences in the VLA column data when comparing the file to itself.\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14566": {
        "explanation": "This issue is well-specified with a clear problem description and a complete reproduction script. The user reports that the \"decimalyear\" format produces errors when applied to MaskedColumn type, while \"jyear\" format works correctly. The issue includes:\n\n1. A clear description of the problem\n2. The expected behavior\n3. A complete, self-contained reproduction script that demonstrates the issue\n4. Environment information (OS, Python version, astropy version)\n\nThe reproduction script shows exactly how to trigger the problem by creating a MaskedColumn from Gaia query results and then attempting to convert it to Time objects using both 'jyear' and 'decimalyear' formats. The script also includes additional tests with scalar values to demonstrate that 'decimalyear' works with regular values.\n\nWhile the exact error message isn't included, the reproduction script is sufficient to run and observe the error. A developer could easily reproduce this issue and understand what needs to be fixed - namely, ensuring that the 'decimalyear' format works correctly with MaskedColumn objects just like 'jyear' does.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14578": {
        "explanation": "This issue is well-specified with clear examples of what works and what doesn't work. The problem is clearly identified: writing a Table containing objects (specifically None values) to a FITS file fails with a specific error message, while writing to ASCII format works fine. The error occurs in the `io/fits/column.py` file when it encounters a column with `dtype('O')` (Python object type).\n\nThe issue includes:\n1. Complete code examples showing the working and non-working cases\n2. The exact error message and traceback\n3. A clear explanation of the root cause (FITS writer doesn't know how to handle object dtypes)\n4. A suggestion for potential solutions (either write objects as their string representation or provide a better error message)\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed. The solution would involve either:\n1. Adding support for object dtypes in the FITS writer by converting them to strings\n2. Improving the error message to be more informative about why object types can't be written to FITS\n3. Documenting this limitation clearly\n\nThe scope is well-defined and the problem is specific enough to be addressed without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14590": {
        "explanation": "This issue is reasonably well-specified. It provides:\n\n1. A clear error message: \"TypeError: cannot write to unmasked output\"\n2. A complete stack trace showing where the error occurs\n3. The specific test that's failing: TestUfuncLike.test_fix\n4. Context that this started happening in numpy-dev jobs (suggesting a compatibility issue with a newer version of NumPy)\n\nThe issue appears to be that in astropy's masked array implementation, there's a problem when using np.fix with numpy-dev. The error occurs in the __array_ufunc__ method when trying to write to an unmasked output.\n\nWhile the issue doesn't explicitly state what the solution should be, there's enough information for a developer familiar with the codebase to:\n1. Understand the specific failure\n2. See that it's related to handling masked arrays with NumPy's ufuncs\n3. Identify the specific code location that needs to be modified\n\nA developer would need to understand how masked arrays work in astropy and how they interact with NumPy's ufuncs, but the error message and stack trace provide sufficient context to start investigating and implementing a solution. The comment \"TODO: allow writing to unmasked output if nothing is masked?\" in the code even suggests a potential direction for the fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14598": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is clearly described: there's an inconsistency in how FITS Card handles double single-quotes ('') in certain string lengths. The issue provides:\n\n1. Detailed reproduction code that demonstrates the problem\n2. Clear output showing when the issue occurs (at specific string lengths)\n3. Multiple test cases showing the pattern of the issue (both with trailing '' and with '' embedded in a string)\n4. Expected behavior (\"All card values should be handled properly\")\n5. Complete version information\n\nThe issue appears to be that when a FITS Card is created with a value containing double single-quotes ('') and then converted to a string and back using Card.fromstring(), the double quotes are sometimes incorrectly reduced to a single quote, but only at specific string lengths.\n\nA developer would have all the information needed to investigate and fix this bug without requiring further clarification. The issue is specific to the FITS Card handling in astropy.io.fits and involves string handling and serialization/deserialization.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14701": {
        "explanation": "This issue is reasonably well-specified. It clearly describes:\n\n1. The goal: Register a method for exporting a Cosmology object to a LaTeX table\n2. The context: Cosmology already has read/write capabilities to files\n3. The implementation approach: Use `cosmology.io.table.to_table()`, format the table to include LaTeX formatting, then call `QTable.write(..., format='latex')`\n4. References to existing code and documentation for guidance\n\nThe issue provides a good starting point with specific implementation suggestions and references to existing code patterns. \n\nThere are some details that would need to be determined during implementation:\n- The exact formatting of the LaTeX output (only a simple example of H0 formatting is given)\n- Whether any configuration options should be exposed to users\n- How to handle different cosmology parameters and units consistently\n\nHowever, these are reasonable implementation details that can be determined by examining the existing code and following established patterns. The core requirements and approach are clear enough to begin work on a solution.\n\nThe issue provides sufficient information for an experienced developer familiar with the codebase to implement a solution without needing further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14702": {
        "explanation": "This issue is well-specified with a clear problem and proposed solution. The issue describes that:\n\n1. There's confusion because `votable.tree.Table` and `table.Table` objects have the same string representation (`<Table length=4>`) despite having different behaviors and methods.\n\n2. The issue provides a concrete example demonstrating the problem, showing how a VOTable result from PyVO displays as `<Table length=4>` but is not an instance of `Table` (as verified with `isinstance()`).\n\n3. The proposed solution is specific: change the representation of VOTable objects to something like `<VOTable length=4>` to visually distinguish them from regular Table objects.\n\nThe issue includes all necessary context with code examples showing the current behavior and the confusion it causes. A developer could implement this change without needing additional clarification, as it's clear what needs to be modified (the string representation method of the VOTable class) and what the expected outcome should be (a different representation string that includes \"VOTable\" instead of just \"Table\").",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14938": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: Cosmology write methods that use Table should support the same format keys as Table, specifically adding \"ascii.latex\" since Table supports both \"latex\" and \"ascii.latex\"\n2. The desired outcome: Register the existing method a second time under the name \"ascii.latex\"\n\nThe issue provides enough context for someone familiar with the codebase to understand what needs to be done. It appears to be a straightforward task of adding an additional registration label for an existing method.\n\nWhat's missing is specific implementation details like:\n- Which specific file(s) need to be modified\n- The exact code that needs to be added\n- Any tests that should be updated\n\nHowever, these details could be reasonably inferred by someone familiar with the codebase by looking at how the \"latex\" format is currently registered and following the same pattern for \"ascii.latex\". The task seems to involve adding a registration for an existing method under an additional name, which is a common pattern in software development.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-14966": {
        "explanation": "This issue is well-specified with a clear problem description and expected behavior. The user provides:\n\n1. A complete code example that reproduces the issue\n2. The current output showing that QTable group keys for quantity columns only contain float values without units\n3. A clear statement of expected behavior: \"The keys should be complete, i.e. for a quantity column it should be a quantity with the proper unit\"\n4. Detailed version information for the relevant software\n\nThe issue is about ensuring that when grouping a QTable by a column that contains quantities with units (using astropy.units), the group keys should preserve the units rather than just containing the float values. This is a specific, technical issue with a clear path to resolution - modify the QTable.group_by functionality to preserve units in the group keys.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14991": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The reporter:\n\n1. Identifies the specific file (`w0wzcdm.py`) and line number (205) where the error occurs\n2. Points out the exact equation that they believe is incorrect\n3. Provides the expected correction (changing `-3.0 * self._wz * z` to `3.0 * self._wz * z` in the exponent)\n4. Mentions they verified this through manual calculation and Wolfram Alpha\n\nThe issue is focused on a mathematical error in the implementation of distance calculations for w0wz Cosmologies, specifically in the de_density_scale calculation. The sign in the exponent of the equation appears to be wrong.\n\nWhile the \"How to Reproduce\" section has placeholder text and lacks a complete code snippet, the mathematical nature of this issue means the core problem is clearly identified in the equation itself. A developer familiar with the codebase would be able to locate the file, verify the equation, and implement the fix by changing the sign in the exponent as suggested.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-14995": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and version information. The problem is clearly defined: in Astropy v5.3, when performing arithmetic operations on NDDataRef objects where one operand has a mask and the other doesn't, using handle_mask=np.bitwise_or causes a TypeError. The issue includes:\n\n1. Precise reproduction code that demonstrates the error\n2. Clear examples of what works and what doesn't work\n3. Comparison with previous version (v5.2) where this worked correctly\n4. Expected behavior: \"When one of the operand does not have mask, the mask that exists should just be copied over to the output\"\n5. Complete version information for all relevant dependencies\n\nThe error is specific: \"TypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\" which indicates that the bitwise_or operation is trying to operate between an integer and None.\n\nA developer could use this information to locate the issue in the mask propagation logic of NDDataRef arithmetic operations and fix it to handle the case where one operand doesn't have a mask, likely by ensuring None values are properly handled before applying bitwise operations.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-6938": {
        "explanation": "This issue is well-specified. The reporter has identified a potential bug in the `fitsrec.py` file where the code is using `replace()` on a `chararray` as if it were an in-place operation, when it actually returns a copy. The issue clearly points to:\n\n1. The exact file and code snippet where the problem exists\n2. The specific nature of the bug (using `replace()` incorrectly assuming it modifies in-place)\n3. Evidence that the code might not be properly tested (commenting it out doesn't cause test failures)\n\nA developer can easily locate the code, understand the problem, and implement a fix by properly capturing the return value of the `replace()` operation. The solution would be straightforward: change the line to `output_field = output_field.replace(encode_ascii('E'), encode_ascii('D'))` or similar, depending on the surrounding context.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7008": {
        "explanation": "This issue is well-specified. It clearly describes a feature request to implement a context manager for setting constant versions in what appears to be an astronomy-related library. The issue provides:\n\n1. A clear description of the desired functionality: a context manager to temporarily set the version of constants\n2. A specific example of the expected syntax: `with constants_set(astropyconst13): ... code goes here ...`\n3. A clear use case: temporarily changing the constant set for specific code blocks\n\nThe implementation would involve creating a context manager that:\n- Takes a constant set as an argument\n- Sets that constant set as active when entering the context\n- Restores the previous constant set when exiting the context\n\nThis is a common pattern in Python libraries that need to temporarily modify global state, and the request follows standard Python conventions. A developer familiar with the codebase should be able to implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7166": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a problem with the InheritDocstrings metaclass not working for properties because it uses `inspect.isfunction` which returns `False` for properties. \n\nFrom this description, I can understand:\n1. There is an InheritDocstrings metaclass in the codebase\n2. This metaclass is supposed to handle docstring inheritance\n3. It's not working for properties because it uses `inspect.isfunction`\n4. Properties return False when checked with `inspect.isfunction`\n\nThe solution would likely involve modifying the InheritDocstrings metaclass to properly detect and handle properties in addition to regular functions.\n\nWhile the issue doesn't provide specific code examples or the exact location of the InheritDocstrings metaclass in the codebase, the problem is clear enough that an experienced developer with access to the codebase could locate the relevant code and implement a fix. The solution would likely involve either using a different inspection method or adding an additional check for properties.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-7218": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `HDUList.copy()` method of the astropy.io.fits package. The problem is that the method returns a Python list instead of an HDUList object as would be expected. The issue includes a reproducible code example that demonstrates the problem, showing that when `hdulist.copy()` is called, it returns an empty list (`[]`) and the type of the returned object is `list` rather than `HDUList`. The Python version (3.6) is also specified. \n\nThe fix would involve modifying the `copy()` method to return an `HDUList` object instead of a list. This is a straightforward issue with a clear expected outcome: the `copy()` method should return an object of the same type as the original.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7336": {
        "explanation": "This issue is well-specified with a clear problem description, a complete reproducer, and error messages. The user is reporting that the `units.quantity_input` decorator in Astropy fails when used with a constructor method that has a type hint indicating it returns `None`. The error occurs because the decorator attempts to call the `.to()` method on the `None` return value.\n\nThe issue includes:\n1. A clear summary of the problem\n2. A minimal, complete reproducer code example\n3. The exact error message and traceback\n4. Environment information (Fedora 27, Python 3.6.3, Astropy 2.0.2, NumPy 1.13.3)\n5. A workaround (removing the return type hint)\n6. A suggested fix (checking if None is returned before attempting unit conversion)\n\nThis provides all the necessary information to understand, reproduce, and fix the issue. A developer would be able to locate the problem in the Astropy codebase (specifically in the units.decorators module) and implement a fix that handles the None return type case.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7441": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The main request is to add a `to_timedelta` method to the `astropy.time.TimeDelta` class that would convert a `TimeDelta` object to a `datetime.timedelta` object.\n\n2. The issue points out that the current inheritance gives `TimeDelta` a `to_datetime` method which is \"useless\" (implying it doesn't make sense for a time delta).\n\n3. There's a secondary request to add conversion methods to NumPy's time types: `np.datetime64` for `Time` and `np.timedelta64` for `TimeDelta`.\n\nThe issue clearly identifies the problem (missing conversion functionality), the desired solution (specific conversion methods), and even explains why the current situation is problematic. A developer would know exactly what needs to be implemented and could proceed with creating these conversion methods without needing further clarification.\n\nThe only minor detail that might need interpretation is whether the `to_timedelta` method should handle arrays of `TimeDelta` objects (returning multiple `datetime.timedelta` objects), but this can be reasonably inferred from the plural \"objects\" in the description and would follow patterns established in similar conversion methods.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7606": {
        "explanation": "This issue is well-specified with a clear problem description and expected behavior. The issue shows that when comparing an UnrecognizedUnit object with None using the equality operator (==), it raises a TypeError instead of returning False as expected in Python. \n\nThe error trace is provided, showing exactly where the problem occurs in the code: in the `__eq__` method of what appears to be a Unit class, it tries to convert None to a Unit object, which raises a TypeError. \n\nThe expected behavior is clearly stated: `x == None` should return False rather than raising an exception. This follows Python's standard behavior where comparing any object with None typically returns False rather than raising an error.\n\nThe solution would involve modifying the `__eq__` method in the Unit class to handle the case when `other` is None, returning False directly instead of trying to convert it to a Unit object.\n\nThe issue provides all necessary information to understand and fix the problem, including the specific code location that needs to be modified.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7671": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific bug in the `minversion` function that was introduced by PR #7647.\n2. It provides a clear error case with a complete traceback showing the exact error: `TypeError: '<' not supported between instances of 'int' and 'str'`.\n3. It identifies the root cause: a bug in Python's `LooseVersion` class when comparing version strings with certain formats (specifically when comparing version strings where one has a numeric component and another has a string component like 'dev').\n4. It provides examples of both failing and working cases, which helps understand the scope of the issue.\n5. It mentions that `pkg_resources.parse_version` (which was previously used but removed) works correctly for this case.\n\nThe solution would need to fix the `minversion` function to properly handle version strings with 'dev' or similar string components when comparing with numeric version components. This could involve either modifying how `LooseVersion` is used or potentially reverting to using `pkg_resources.parse_version` or implementing a custom version comparison logic that handles these cases correctly.\n\nThe issue provides all the necessary information to reproduce and understand the problem, making it well-specified for a developer to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7737": {
        "explanation": "This issue is well-specified with a clear problem and proposed solution:\n\n1. The problem is clearly identified: `biweight_location` returns `nan` when given a constant array (array with zero variance).\n2. The cause is identified: a 0/0 division when the MAD (median absolute deviation) becomes zero.\n3. A specific example is provided showing the current behavior.\n4. A clear solution is proposed: catch this special case and return the median instead of `nan`.\n\nThe issue provides enough context to understand what's happening, why it's happening, and what needs to be fixed. The solution would involve adding a check for when the MAD is zero and returning the median in that case. This is a straightforward fix that doesn't require additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-7973": {
        "explanation": "This issue is reasonably well-specified for a developer to work on. The problem is clearly articulated: there's a need to permanently fix how data size is recorded in the WCS object. Two specific solution approaches are proposed:\n\n1. Add public `naxisj` attributes populated by FITS header `NAXISj` keywords\n2. Create a subclass of `WCS` that adds these attributes while keeping the original WCS class focused on the FITS WCS standard\n\nThe issue also mentions that a deprecation process will accompany the change, which is an important implementation detail.\n\nWhile the issue is collecting votes rather than specifying which approach to implement, a developer could reasonably proceed with either approach once a decision is made. The issue references previous discussions (#4662, #5411, #5454) that provide context and background information.\n\nWhat makes this not fully \"well-specified\" is that the exact implementation details aren't provided - the developer would need to review the referenced issues to understand the full context and requirements. Additionally, the deprecation process isn't detailed. However, an experienced developer familiar with the codebase could make a reasonable attempt at implementing either solution after reviewing the referenced materials.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-8005": {
        "explanation": "This issue is well-specified and clearly describes a consistency problem in the codebase. The issue identifies that:\n\n1. The `thermodynamic_temperature` equivalency hardcodes the use of Planck15 cosmology for the T_cmb parameter\n2. The `with_H0` equivalency uses the default cosmology (currently WMAP9)\n3. This inconsistency should be addressed for the 3.1 release\n\nThe issue also provides context by referencing related PR #7970 and issue #8003, and suggests a potential solution path: changing the `thermodynamic_temperature` equivalency to use the default cosmology, especially if the default cosmology is changed to Planck 2015 as discussed in the related issue.\n\nThe code snippets clearly show the inconsistency, and the issue explains why this is problematic. The goal is clear: make the cosmology usage consistent across equivalencies. This is specific enough to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8263": {
        "explanation": "This issue is well-specified with clear details about the problem. The description provides:\n\n1. A specific error message: `astropy.units.core.UnitConversionError: 's / m' and 's / m' are not convertible`\n2. A reproducible example with code snippets showing the issue\n3. The exact environment where it occurs (Travis CI tests)\n4. The specific version of Astropy (3.1) where the issue appears\n5. The git commit that introduced the bug (identified through git bisect)\n6. The specific file/module affected: `plasmapy.physics.distribution.Maxwellian_1D`\n\nThe issue is clearly a bug where identical units ('s / m') are not recognized as convertible to each other in Astropy 3.1. The problem is well-isolated and the information provided is sufficient for someone to investigate and fix the issue without needing additional clarification. The issue reporter has done thorough investigation work to narrow down the problem.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8339": {
        "explanation": "This issue is well-specified with clear information about the bug and its solution. The reporter has:\n\n1. Identified the specific bug: a variable 'ncp_prior' is being referenced before assignment in the Bayesian Blocks algorithm in astropy.stats\n2. Provided the exact error message and traceback showing where the error occurs\n3. Shown the code that reproduces the issue\n4. Pinpointed the exact location of the bug in the code\n5. Provided a specific solution: adding an else clause to properly assign the ncp_prior variable when self.ncp_prior is not None\n\nThe solution is straightforward - when the ncp_prior parameter is passed to the function, it needs to be properly assigned to a local variable before being used. The current code only handles the case when ncp_prior is None, but doesn't handle the case when it has a value.\n\nA developer could implement this fix without needing any additional information or clarification.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8519": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible code example. The issue describes a bug in Astropy's unit handling system where mathematical operations (addition/subtraction) between ABmag Quantities lose the correct magnitude type information.\n\nThe user provides:\n1. A complete code example that demonstrates the issue\n2. The exact error messages received\n3. Clear expected behavior (that the difference between two ABmag quantities should also be an ABmag)\n4. Alternative scenario showing how the error changes when using addition instead of subtraction\n\nThe problem is clearly defined: when performing arithmetic operations on ABmag quantities, the resulting unit either loses its magnitude type (becoming just \"mag\") or gets an incorrect type (becoming \"mag(AB2)\"), which prevents further conversions to other units like nJy.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed in Astropy's unit handling system for ABmag quantities.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8707": {
        "explanation": "This issue is well-specified with clear details about the problem and a suggested solution:\n\n1. The problem is clearly identified: `Header.fromstring` only accepts Python 3's unicode `str` type but not `bytes`, despite the documentation saying it accepts a \"byte string\".\n\n2. The issue provides context about the difference between Python 2 and Python 3 string handling, explaining why this is a problem (in Python 2, `str` could contain binary data, but in Python 3 it cannot).\n\n3. A specific solution is proposed: modify `Header.fromstring` to accept both unicode strings and bytes objects, similar to how `Header.fromfile` works with both text and binary mode files.\n\n4. The issue also identifies a related method (`Card.fromstring`) that likely needs the same treatment.\n\nThe issue provides enough information for a developer to understand the problem, locate the relevant code, and implement the suggested solution without needing additional clarification. The solution approach is reasonable and specific.",
        "label": "WELL_SPECIFIED"
    },
    "astropy__astropy-8747": {
        "explanation": "The issue is reasonably specified but lacks some details. It clearly states the problem: with NumPy 1.17, `np.clip` will be based on a ufunc, and we need to ensure it works properly with `Quantity` objects. The motivation is also clear - without this support, there might be failures with `numpy-dev`.\n\nHowever, the issue lacks specific details about:\n1. What exactly needs to be implemented (though someone familiar with the codebase would likely understand)\n2. How to test the implementation\n3. What specific failures are occurring or might occur\n\nFor an experienced developer familiar with the codebase and NumPy's ufunc system, this would likely be enough information to start working on a solution. They would understand that `Quantity` objects need to properly interact with NumPy's new ufunc-based `clip` function, which probably involves implementing or updating some compatibility layer or method.\n\nThe issue is not vague in its intent, but it assumes familiarity with both NumPy's ufunc system and the codebase's `Quantity` class.",
        "label": "REASONABLY_SPECIFIED"
    },
    "astropy__astropy-8872": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user has identified that when creating a `Quantity` object from a `np.float16` value, the data type is automatically upgraded to `np.float64`, which doesn't happen with other float types like `np.float32`, `np.float64`, or `np.float128`. The issue includes specific code examples showing the unexpected behavior and contrasting it with the expected behavior for other float types. The goal would be to modify the code to preserve the `float16` data type when creating a `Quantity` object, consistent with how other float types are handled. The issue also references a related issue (#6389) which could provide additional context. This is a specific, reproducible issue with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10097": {
        "explanation": "This issue is well-specified with clear requirements for a solution. The problem is precisely defined: Django's URLValidator currently accepts URLs with unencoded special characters (specifically \":\", \"@\", or \"/\") in the username and password portions, which violates RFC 1738 section 3.1. \n\nThe issue provides:\n1. Clear context: When the issue was introduced (#20003)\n2. Specific technical requirements: Special characters in username/password must be %-encoded\n3. Concrete examples of problematic URLs: \"http://foo/bar@example.com\"\n4. Reference to existing test cases that would be affected\n5. A link to a pull request that implements a solution\n\nThe issue also addresses potential concerns about implementation complexity and justifies why this validation is important by noting it was encountered in production.\n\nA developer would have enough information to understand the problem, locate the relevant code in Django's validators, and implement a solution that enforces proper URL encoding in the username and password portions of URLs.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10213": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly stated: Django currently has a `--no-colors` flag to disable colors, but lacks a complementary flag to force colors when they would normally be disabled (like when output is piped).\n\n2. The requested solution is specific: Add a `--force-colors` flag to django-admin that will ensure color output even when piped to another command.\n\n3. A real-world use case is provided: The user wants to pipe colored output from a custom Django command to the `aha` utility to create HTML logs with colors preserved.\n\n4. There's even a reference to similar functionality in other tools (`ls --color=always`) which provides a clear pattern to follow.\n\nThe implementation would involve modifying Django's command-line handling to add this new flag and ensure the color output is preserved when this flag is used, regardless of whether the output is being piped. This is a straightforward feature addition with a well-defined scope.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10301": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem: SQLite functions crash when dealing with NULL values, specifically when using the power operator (**) with NULL values in a Django query. The error message and the exact query that causes the problem are provided, which helps in reproducing the issue.\n\nThe issue shows that when trying to annotate a queryset with a NULL value and then using that NULL value in a power operation (** operator), an \"OperationalError\" is raised with the message \"user-defined function raised exception\".\n\nWhat makes this not fully well-specified:\n1. It doesn't explicitly state the expected behavior (should NULL ** NULL return NULL or something else?)\n2. It doesn't mention which version of Django this occurs in\n3. It doesn't provide information about the TestModel2 structure\n\nHowever, there's enough information to investigate the issue by:\n1. Understanding how Django handles NULL values in SQLite functions\n2. Examining how the power operator is implemented for SQLite in Django\n3. Implementing a fix that properly handles NULL values in this context\n\nA solution would likely involve modifying how Django translates the power operator to SQLite to properly handle NULL values.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10316": {
        "explanation": "This issue is well-specified with a clear problem description and solution. The issue explains that:\n\n1. When using `settings.configure()` instead of the environment variable `DJANGO_SETTINGS_MODULE`, the `diffsettings` command fails.\n2. The specific error message is provided: \"django.core.exceptions.ImproperlyConfigured: Requested settings, but settings are not configured...\"\n3. The root cause is identified: `diffsettings` calls `settings._setup()` unconditionally.\n4. A specific solution is proposed: check if settings are not already configured before calling `_setup()` by using `if not settings.configured: settings._setup()`.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement the suggested fix. The solution is straightforward and the expected behavior is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10390": {
        "explanation": "This issue is well-specified with clear details about the problem and expected behavior. The issue describes that when using Django's Trunc() function to truncate datetimes that fall during daylight saving time transitions, it raises unhandled pytz exceptions (NonExistentTimeError/AmbiguousTimeError). The reporter provides a complete test case that reproduces the issue, showing exactly how the error occurs when truncating to an hour during the DST transition in the Europe/Stockholm timezone.\n\nThe issue clearly states the expected behavior: \"to not check the validity of truncated dates\" or in other words, to allow passing an is_dst parameter to resolve these ambiguous/nonexistent time situations. The error stack trace is included, showing exactly where the problem occurs in the code path.\n\nA developer would have enough information to:\n1. Understand the problem (Trunc() doesn't handle DST transitions properly)\n2. Reproduce the issue (using the provided test case)\n3. Identify where the fix is needed (in the Trunc implementation, likely needing to pass the is_dst parameter to timezone.make_aware())\n4. Implement a solution (modify Trunc to accept and pass through an is_dst parameter)\n\nThe issue provides all necessary context for implementing a solution without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10426": {
        "explanation": "The issue is reasonably well-specified. It clearly states that app label validation needs to be added to the showmigrations command, similar to how it was added to other commands in the referenced tickets (#29469, #29518, #29506). \n\nThe issue provides context by referencing previous tickets where similar validation was implemented for other commands. This gives a developer a clear path to understand what kind of validation is needed and how it should be implemented.\n\nWhile the issue doesn't provide specific implementation details, a developer familiar with the codebase could:\n1. Look at the referenced tickets to understand how app label validation was implemented in other commands\n2. Apply a similar pattern to the showmigrations command\n3. Ensure consistency with the existing validation approach\n\nThe only minor ambiguity is that the issue doesn't explicitly state what app label validation does or why it's needed, but this can be inferred from the referenced tickets. A developer would need to review those tickets to understand the exact validation logic, but the task itself is clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10531": {
        "explanation": "This issue is well-specified with clear details about what's wrong and what the expected behavior should be:\n\n1. The problem is clearly identified: In the admin section's model history, field names (e.g., \"pub_date\") are being displayed instead of their verbose names (e.g., \"date published\").\n\n2. A concrete example is provided: The issue shows a model with a DateTimeField that has a verbose name \"date published\", but when changed, the history shows \"Changed pub_date\" instead of the expected \"Changed date published.\"\n\n3. The expected behavior is explicitly stated: The history should show human-readable messages using verbose names rather than field names.\n\n4. There's even a reference point: The issue mentions that older versions of Django displayed the verbose names correctly.\n\nThis provides enough information for a developer to understand the problem, locate the relevant code in Django's admin history functionality, and implement a fix that ensures verbose names are used instead of field names in the history display.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10554": {
        "explanation": "This issue is well-specified with a clear reproduction case and error message. The problem occurs when using Django's QuerySet union operation with ordering, and then applying additional ordering operations on the resulting queryset. The issue description provides:\n\n1. A specific reproduction case with code examples\n2. The exact error message and traceback\n3. The expected behavior (evaluating the queryset directly works)\n4. The context (Django ORM)\n\nThe error \"ORDER BY position 4 is not in select list\" suggests that there's a bug in how Django handles ordering in union querysets. When a union queryset with ordering is created and then another ordering operation is applied, the original queryset becomes unusable.\n\nThe issue is specific enough that a developer could attempt to reproduce it and understand the underlying problem in the Django ORM's handling of union querysets with ordering operations. The solution would likely involve fixing how Django handles the SQL generation for these cases.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10606": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug: Using RelatedOnlyFieldListFilter with a reverse ManyToMany relation causes an exception.\n2. The exact error message is provided: \"get_choices() got an unexpected keyword argument 'limit_choices_to'\"\n3. The root cause is identified: ForeignObjectRel.get_choices is missing a parameter that Field.get_choices has.\n4. A pull request addressing the issue is linked: https://github.com/django/django/pull/10606\n5. A demo repository showing how to reproduce the bug is provided: https://github.com/mgrdcm/django-bug-reverse-related/blob/master/rrbug/rrapp/admin.py#L11-L15\n\nWith this information, a developer would be able to:\n- Understand the specific issue\n- Reproduce the bug using the provided demo\n- See the proposed solution in the pull request\n- Implement a fix by adding the missing parameter to ForeignObjectRel.get_choices\n\nThe issue provides all the necessary context to understand and solve the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10643": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly described: Django's admin search functionality doesn't work consistently across database backends when searching for UUIDs with dashes. Specifically:\n\n1. The issue occurs with Django 2.1 in a model admin that has a UUIDField in both list_display and search_fields.\n2. The problem is that searching for UUIDs with dashes works in PostgreSQL but fails in Oracle.\n3. The reason is identified: Oracle backend stores UUIDs as strings without dashes.\n4. The expected behavior is that users should be able to search using the UUID format displayed in the admin (with dashes).\n5. The user suggests a potential workaround (customizing get_search_results) but believes this should be handled internally by Django.\n\nWhat makes this reasonably specified rather than well-specified:\n- It doesn't provide specific code examples showing the model definition or the exact search implementation\n- It doesn't show the exact error or failure mode when searching in Oracle\n- It doesn't specify if the solution should modify how UUIDs are stored in Oracle or how the search is performed\n\nHowever, there's enough information to understand the problem and develop a solution that would allow the icontains lookup to work with UUIDs regardless of whether they contain dashes or not, across different database backends.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10737": {
        "explanation": "The issue is reasonably specified, though it could benefit from more details. It clearly identifies:\n\n1. The problem: FieldError exceptions in Django don't include the name of the field that caused the error\n2. The location: django/db/models/sql/compiler.py file has many instances of this issue\n3. The proposed solution: Include the field name in the error message when raising FieldError exceptions\n\nWhat makes this reasonably (rather than well) specified:\n- It doesn't provide specific examples of the current error messages\n- It doesn't identify exactly which FieldError instances in the file need modification\n- It doesn't specify the exact format for including the field name in the error message\n- It uses \"where possible\" which introduces some ambiguity about which cases should be addressed\n\nHowever, a developer familiar with Django could:\n1. Locate all instances of FieldError in the compiler.py file\n2. Identify which ones have access to field information\n3. Modify the error messages to include the field name in a sensible format\n\nThe task is clear enough that a developer could make a meaningful attempt at implementing the solution without further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10853": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies that Django doesn't check SQLite version compatibility before attempting to use window functions, which are only available in SQLite 3.25.0+.\n\n2. It explains the current behavior: when window functions are used with older SQLite versions, a confusing OperationalError is raised rather than a more helpful Django-specific error.\n\n3. It provides specific code examples that reproduce the issue, both at the Django ORM level and with raw SQLite.\n\n4. It specifies the environment (Python 3.6.6, Windows 10 x64) and Django version (master branch with specific commit).\n\n5. The expected behavior is clearly implied: Django should check SQLite version compatibility and raise an appropriate error message when window functions are used with incompatible SQLite versions, rather than letting the operation fail with a generic database error.\n\nThe solution would involve adding version checking for SQLite before attempting to use window functions, and raising a more descriptive error when the SQLite version is too old to support them. This is a common pattern in Django for other database features that have version-specific support.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10880": {
        "explanation": "The issue is well-specified enough to understand the problem and attempt a solution. It clearly describes:\n\n1. The specific problem: A SQL query syntax error occurs when using both a Case condition and distinct=True parameter in a Count annotation\n2. The exact error condition: A space is missing between \"DISTINCT\" and \"CASE\" in the generated SQL (resulting in \"DISTINCTCASE WHEN\")\n3. The environment: Django 2.2, regardless of database backend\n\nFrom this description, I can understand that the issue is in Django's ORM query generation, specifically when combining Count annotations with both Case expressions and the distinct parameter. The solution would involve fixing the SQL generation to properly include a space between \"DISTINCT\" and \"CASE\" in the generated query.\n\nThis is a concrete, reproducible issue with a clear expected outcome (proper SQL syntax with a space between DISTINCT and CASE).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10904": {
        "explanation": "This issue is well-specified with a clear objective: replace all instances of OSError aliases (IOError, EnvironmentError, WindowsError, mmap.error, socket.error, select.error) with OSError in the Django codebase. The issue provides:\n\n1. Clear context: Starting with Python 3.3, these exceptions are all aliases of OSError\n2. Specific exceptions to target: EnvironmentError, IOError, WindowsError, socket.error, select.error, mmap.error\n3. Additional information about SMTPException being a subclass of OSError since Python 3.4\n4. Links to relevant Python documentation for reference\n\nThe task is straightforward - find all occurrences of these exception aliases in the Django codebase and replace them with OSError. This is a code cleanup task with a well-defined scope and objective. A developer would know exactly what to look for and how to make the changes.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10910": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Clearly identified the problem: When using `datetime.timezone(datetime.timedelta(...))` with database functions, Django generates SQL with timezone strings like \"UTC+05:00\" which PostgreSQL interprets incorrectly as a POSIX-style timezone specification.\n\n2. Provided detailed reproduction steps with a sample model and code snippets showing the issue.\n\n3. Included the exact versions of Django, PostgreSQL, and psycopg2 used.\n\n4. Demonstrated the problem with concrete examples showing:\n   - The expected behavior (using pytz timezone)\n   - The actual incorrect behavior (using datetime.timezone)\n   - The SQL query being generated\n\n5. Referenced PostgreSQL documentation explaining why this happens (PostgreSQL interprets \"UTC+05:00\" as a POSIX-style timezone where +05:00 means 5 hours west of UTC, not east).\n\nThe issue is specific enough that a developer could understand the problem, reproduce it, and implement a fix without needing additional clarification. The fix would likely involve changing how Django serializes datetime.timezone objects to SQL to ensure PostgreSQL interprets them correctly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10914": {
        "explanation": "This issue is reasonably well-specified. The reporter is describing a specific problem with file permissions when uploading files using Django's FileSystemStorage. They've identified that:\n\n1. When files are uploaded, the permissions can be inconsistent (0o600 vs expected permissions)\n2. This depends on whether a MemoryUploadedFile or TemporaryUploadedFile is used\n3. The issue occurs on specific systems (CentOS 7.4.1708 with Python 3.6.5)\n4. The issue is related to how tempfile.NamedTemporaryFile + os.rename sets permissions to 0o600\n\nThe title suggests a solution: set the default FILE_UPLOAD_PERMISSION to 0o644 (readable by owner and group, writable by owner).\n\nWhat's missing is:\n1. The exact code path where this issue occurs\n2. A minimal reproduction case\n3. Confirmation that 0o644 is the appropriate default for all systems\n\nHowever, a developer familiar with Django's file upload system should be able to locate the relevant code, understand the issue, and implement a solution based on this description. The issue is specific enough about the problem and suggests a reasonable solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10939": {
        "explanation": "The issue description is somewhat vague and lacks specific details that would make it easier to understand and solve. Here's what I can gather:\n\n1. There's a ModelAdmin with:\n   - Custom widgets\n   - Inlines\n   - filter_horizontal fields\n\n2. The problem involves:\n   - An inline with a field that has its own media JS (which doesn't need jQuery)\n   - A one-to-many field using filter_horizontal\n   - A MediaOrderConflictWarning being raised\n   - inlines.js loading before jQuery (which would cause errors since inlines.js likely depends on jQuery)\n\nHowever, the description is missing critical information:\n- No code examples showing the ModelAdmin setup\n- No exact error messages or stack traces\n- No steps to reproduce the issue\n- No information about the Django version\n- No clear explanation of what the expected behavior should be\n\nWhile I can infer that this is likely a JavaScript dependency ordering issue in Django's admin interface, the lack of specific details makes it difficult to pinpoint the exact problem and solution without making assumptions. A developer would need to investigate how Django's Media class merges and orders JavaScript files in the admin interface, particularly when dealing with custom widgets, inlines, and filter_horizontal fields.",
        "label": "VAGUE"
    },
    "django__django-10973": {
        "explanation": "The issue is reasonably specified, but lacks some details that would make it completely clear. \n\nThe issue suggests replacing the current implementation in Django's postgres backend client with `subprocess.run()` and using the environment variable `PGPASSWORD` to handle PostgreSQL authentication. This would simplify the code and make it more reliable.\n\nFrom the description, I can understand:\n1. The goal is to use `subprocess.run()` which was added in Python 3.5\n2. The target file is in `django.db.backends.postgres.client`\n3. The change involves setting the `PGPASSWORD` environment variable\n4. The motivation is code simplification and reliability\n\nWhat's missing:\n1. The specific code that needs to be changed (though it can be found by examining the codebase)\n2. The exact implementation details of how `subprocess.run()` should be used\n3. Any specific edge cases or considerations that need to be handled\n\nDespite these gaps, a developer familiar with Django's codebase could reasonably implement this change by:\n1. Locating the relevant code in the postgres backend client\n2. Identifying where subprocess calls are currently made\n3. Refactoring to use `subprocess.run()` with an environment dict containing `PGPASSWORD`\n\nThe issue provides enough direction to make a meaningful attempt at implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-10989": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue is clearly identified: Django crashes on Windows when trying to display strings containing ellipsis characters (specifically Unicode character \\u2026) because Windows terminals default to code page 437 (ASCII-like) which can't display this character.\n\n2. The exact error is provided with a complete traceback showing where the problem occurs.\n\n3. A workaround is mentioned (changing the code page to UTF-8 with \"chcp 65001\").\n\n4. The issue reporter even identifies the likely solution path: finding where the horizontal ellipsis character was used instead of three periods \"...\" and replacing it.\n\n5. The issue is contextualized as a regression from Django 2.1.7.\n\nA developer could easily reproduce this issue on a Windows system and has a clear path to fix it by searching the codebase for instances of \\u2026 and replacing them with \"...\" to ensure compatibility with Windows default terminal encoding.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10997": {
        "explanation": "This issue is well-specified with a clear objective: add the applied datetime value to the output of the Django `showmigrations` command. The description explains:\n\n1. The specific problem being solved: difficulty determining which migrations are new when switching between branches with different migrations\n2. The proposed solution: show the applied datetime in the command output\n3. Current workaround: manually checking the django_migrations table sorted by applied column\n4. Implementation details: initially implemented for the --list option with --verbosity of 2 and above\n5. The author has already created a PR with an implementation\n\nThe issue includes references to previous discussions and a PR with the implementation, which provides additional context. The only minor ambiguity might be around the exact format of the datetime display and whether it should be configurable, but these are implementation details that can be reasonably determined during development.\n\nOverall, this is a well-defined feature request with a clear purpose and implementation path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-10999": {
        "explanation": "This issue is well-specified with clear information about what's wrong and how to fix it. The problem is identified in the `parse_duration()` function where the regular expression pattern doesn't properly handle negative durations. The issue specifically points out that the lookahead assertion in the regex pattern for hours is missing the optional negative sign (`-?`), which prevents it from matching negative durations correctly. \n\nThe issue even provides the exact fix needed: changing `(?=\\d+:\\d+)` to `(?=-?\\d+:-?\\d+)` in the regular expression pattern to account for possible negative values in the minutes and seconds components.\n\nWith this information, a developer can:\n1. Locate the exact file (django/utils/dateparse.py)\n2. Find the specific regex pattern that needs modification\n3. Apply the suggested fix\n4. Create tests to verify that negative durations are now parsed correctly\n\nThe issue provides both the problem and solution with sufficient technical detail.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11003": {
        "explanation": "This issue is reasonably well-specified. The user describes a clear problem: when a ModelChoiceField is set to disabled=True and initialized with a valid value, form validation fails with a \"Select a valid choice\" error. The user has also identified a workaround (adding to_field_name) and referenced a potentially related bug (#28387).\n\nThe issue provides enough context to understand:\n1. What's happening: A disabled ModelChoiceField with a valid initial value fails validation\n2. The error message received\n3. A workaround that fixes the issue\n4. A potential related bug\n\nWhat's missing is:\n1. The exact code that reproduces the issue\n2. The Django version being used\n3. The specific model and field definitions\n\nHowever, these gaps can be reasonably filled in by examining the Django codebase, particularly how ModelChoiceField handles disabled fields and validation. The issue is specific enough that a developer could reproduce it and investigate a solution based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11019": {
        "explanation": "This issue is well-specified with clear details about the problem. The description:\n\n1. Provides a complete code example that reproduces the issue\n2. Explains the expected behavior (proper ordering of JS files)\n3. Shows the actual behavior (incorrect ordering and warning)\n4. Identifies the root cause (the order of media merging operations)\n5. Points out specific inconsistencies in the warning message\n\nThe issue relates to Django's Media class handling when merging multiple media objects, particularly how it handles dependencies between JavaScript files. The problem occurs when three or more media objects are merged, resulting in incorrect ordering and unnecessary MediaOrderConflictWarnings.\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided example\n2. Understand the expected behavior\n3. Locate the relevant code in Django's forms/widgets.py\n4. Implement a solution that properly handles dependencies when merging multiple media objects\n\nThe solution would likely involve improving the algorithm for merging media objects to better handle transitive dependencies across multiple widgets.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11057": {
        "explanation": "This issue is well-specified with clear details about the problem. The user explains:\n\n1. They're upgrading from Django 2.1 to 2.2b1\n2. They have a custom authentication middleware that extends/wraps Django's AuthenticationMiddleware\n3. In Django 2.1, their custom middleware worked fine\n4. In Django 2.2b1, they're getting an admin.E408 error requiring the base AuthenticationMiddleware to be in MIDDLEWARE\n5. The error message is clearly provided: \"django.contrib.auth.middleware.AuthenticationMiddleware must be in MIDDLEWARE in order to use the admin application\"\n\nThe issue is asking whether this is a bug/regression in Django 2.2 or if their approach was incorrect all along. The problem is clearly defined - Django 2.2 is not recognizing subclasses of AuthenticationMiddleware as valid for admin authentication checks.\n\nA solution would need to modify Django's admin checks to recognize subclasses of AuthenticationMiddleware, not just the exact class. This is a common pattern in object-oriented programming where isinstance() checks would be more appropriate than exact class equality checks.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11062": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information. The user provides:\n\n1. A clear model definition showing the relevant fields\n2. The exact code they're trying to run that causes the error\n3. The complete error traceback showing the SQL error\n4. Version information for Django (2.1.7, 2.2b1), Python (3.6.7), and PostgreSQL (10.6)\n5. A comparison point (works in Django 1.8)\n\nThe issue is specifically about using Django's Extract function in an annotation and then trying to use that annotation in an aggregate with a filter. The SQL error \"column '__col2' does not exist\" suggests there's a bug in how Django is generating the SQL when combining these operations.\n\nA developer would have all the necessary information to reproduce the issue, investigate the SQL generation in Django's ORM, and implement a fix. The expected behavior is also clear - the user wants to aggregate usage data by hour.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11066": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The reporter has:\n\n1. Identified the specific code location causing the issue (with GitHub links to the exact commit and line numbers)\n2. Provided context about their environment (dynamic database router setup)\n3. Explained the specific error they're encountering (\"OperationalError, stating that no such table: django_content_types exists\")\n4. Traced through the code to identify the root cause (the content_type.save() call not using the specified database)\n5. Proposed a specific fix (adding the \"using=db\" parameter to the save method call)\n\nThe issue is related to Django's content type system and database routing, where a save operation is being performed on the default database instead of the specified one. The reporter has done thorough investigation and provided a clear, specific solution that can be implemented and tested.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11070": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The task is to add autocomplete attributes to Django's contrib.auth form fields\n2. Specific autocomplete values are mentioned: username, email, current-password, new-password\n3. The most important one is highlighted (autocomplete=new-password)\n4. The purpose is clearly stated: to prevent browsers from prefilling with current password and to enable Chrome's strong password suggestion feature\n5. References to relevant documentation are provided\n6. A pull request is already linked, which likely contains an implementation that can be reviewed\n\nThe issue provides enough context about what needs to be done, why it's needed, and even points to resources that explain how to implement it. A developer could take this issue and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11087": {
        "explanation": "This issue is well-specified with clear details about the problem and what needs to be fixed. The issue describes:\n\n1. A specific error scenario: UnicodeDecodeError during .delete() operations when migrating from Python 2.7 to Python 3.6\n2. Complete steps to reproduce the issue with links to relevant code\n3. Expected vs actual behavior\n4. Detailed error traceback\n5. Database queries being executed\n6. Root cause analysis identifying two specific issues:\n   - mysqlclient-python behavior differences between Python versions\n   - Django unnecessarily fetching fields not required for deletion\n\nThe issue specifically requests optimizing .delete() to use only required fields (issue #2 in their analysis), which would both fix the UnicodeDecodeError and improve performance. The solution path is clear: modify Django's deletion process to only fetch the minimum required fields (primary keys, etc.) rather than all fields when performing cascading deletes.\n\nThe issue provides sufficient context about the codebase, the specific models involved, and the exact location where the error occurs. A developer could implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11088": {
        "explanation": "This issue is well-specified with clear reproduction steps and a detailed error message. The problem is clearly defined: when a BinaryField has a default value of '' (empty string) in an initial migration, and is later changed to b'' (empty bytes), attempting to reverse the migration fails with a \"can't escape str to binary\" error. \n\nThe issue includes:\n1. Complete reproduction steps with exact versions (Python 3.6.0, Django 1.10.6, Postgres 9.5.4)\n2. Clear model definitions and migration commands\n3. Full error traceback showing where the problem occurs\n4. Analysis of the root cause in the Django codebase\n5. A suggested fix with specific code location and implementation\n\nA developer would have all the information needed to reproduce the issue, understand the problem, and implement a solution. The suggested fix even provides a good starting point, indicating that the effective_default method needs to handle BinaryField defaults differently when they're empty strings.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11095": {
        "explanation": "The issue is reasonably well-specified. The request is to add a new hook method `get_inlines(request, obj=None)` to the Django ModelAdmin class that would allow developers to dynamically set inlines based on the request or model instance.\n\nThe issue explains:\n1. The specific functionality needed: a hook to set inlines dynamically\n2. The current workaround: overriding `get_inline_instances` method\n3. The problem with the current approach: requires copying a for loop\n4. The proposed solution: add a new hook method with specific parameters (request, obj=None)\n\nWhile the issue doesn't provide implementation details or code examples, it clearly describes what needs to be added and why. A Django developer familiar with the ModelAdmin class would understand what's being requested. The only minor ambiguity is exactly how this hook would be integrated with the existing code, but this can be determined by examining the ModelAdmin implementation.\n\nFor a developer familiar with Django's admin system, this provides enough information to implement a solution without needing further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11099": {
        "explanation": "This issue is very well-specified with clear details about:\n\n1. The exact problem: UsernameValidator allows trailing newlines in usernames due to a quirk in Python regex where $ matches before a trailing newline\n2. The specific files affected: contrib.auth.validators containing ASCIIUsernameValidator and UnicodeUsernameValidator\n3. The exact regex pattern currently used: r'^[\\w.@+-]+$'\n4. The exact solution proposed: Change the regex to r'\\A[\\w.@+-]+\\Z' to properly reject usernames with trailing newlines\n5. The technical explanation of why this change is needed: $ matches before a trailing newline, while \\Z doesn't have this behavior\n\nThe issue provides all the necessary information to implement a fix without requiring additional clarification. The change is straightforward and the reasoning is sound. A developer could immediately locate the affected code and make the necessary changes based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11115": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The user is experiencing an `AlreadyRegistered` exception when a model is being registered twice in Django's admin site, and they want the exception to provide more details about which package/app is causing the clash.\n\nThe issue includes:\n1. A clear error message and stack trace showing the exact problem\n2. The specific exception (`AlreadyRegistered: The model Site is already registered`)\n3. A clear request: enhance the exception to provide more information about which package/app previously registered the model\n\nThe solution would involve modifying the Django admin site registration process to include more detailed information in the `AlreadyRegistered` exception, such as which app originally registered the model. This would help developers quickly identify conflicting registrations without having to bisect their dependencies.\n\nThe context is sufficient to understand what needs to be done, where the code needs to be modified (likely in Django's admin site registration logic), and what the expected outcome should be (more informative error messages).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11119": {
        "explanation": "This issue is well-specified with clear information about what's wrong and what needs to be fixed. The problem is precisely identified: when using `Engine.render_to_string()`, the method creates a `Context` without respecting the engine's `autoescape` attribute. This means that even if an engine is created with `autoescape=False`, the `render_to_string()` method will still apply autoescaping.\n\nThe issue also provides context by referencing a specific commit ([19a5f6da329d58653bcda85]) where this problem was likely introduced, suggesting it was an oversight during that change.\n\nTo fix this issue, the solution would clearly involve modifying the `render_to_string()` method to pass the engine's `autoescape` setting to the `Context` it creates. This is a straightforward task with a well-defined scope and expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11129": {
        "explanation": "This issue is reasonably well-specified. The user describes a specific problem with Django's QuerySet.update() method when using an F expression that references an annotated field which includes a join. They provide:\n\n1. A clear error message (ProgrammingError: missing FROM-clause entry for table \"myapp_mymodel\")\n2. A traceback showing where the error occurs\n3. Their understanding of the root cause: \"trying to update a field using an F expression on an annotated field which included a join in it\"\n4. A suggestion that the error message should be improved to be more consistent\n\nWhat's missing is:\n1. The actual code that reproduces the issue (they mention a test is attached but it's not included in the description)\n2. The expected behavior vs. actual behavior\n3. The specific version of Django (only mentioned as Django 1.11)\n\nHowever, there's enough information to understand the problem: Django's QuerySet.update() method is failing with a misleading error message when trying to use an F expression that references an annotated field involving a join. The solution would likely involve either improving the error message to be more descriptive or fixing the underlying issue to properly handle this case.\n\nA developer familiar with Django's ORM internals would likely be able to reproduce this issue and work on a solution based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11133": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is trying to write a BinaryField from a database into an HttpResponse. They've discovered that when using PostgreSQL, the field contents are returned as a memoryview object, which Django's HttpResponse doesn't handle correctly. \n\nThe issue includes a complete code example demonstrating:\n1. HttpResponse works correctly with string content\n2. HttpResponse works correctly with bytes content\n3. HttpResponse fails when given memoryview content (returning the memory address string instead of the actual content)\n\nThe expected behavior is clearly stated: when passing a memoryview object to HttpResponse, the response.content should contain the actual binary data (b'My Content') rather than a string representation of the memoryview object (b'<memory at 0x7fcc47ab2648>').\n\nThis is a specific bug with a clear reproduction case and expected outcome, making it well-specified for a solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11138": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a bug where the TIME_ZONE value in DATABASES settings is not being used correctly when making dates timezone-aware on MySQL (and also mentions SQLite and Oracle).\n\n2. The reporter provides a complete reproduction case with:\n   - Exact settings configuration\n   - Code examples showing the expected vs. actual behavior\n   - SQL queries demonstrating the problem\n   - Explanation of why the current behavior is incorrect\n\n3. The reporter even identifies the specific line in the Django codebase that's causing the issue: https://github.com/django/django/blob/stable/1.11.x/django/db/backends/mysql/operations.py#L49 where 'UTC' is hardcoded instead of using the database's TIME_ZONE setting.\n\n4. The issue explains the expected behavior: when using date lookups with TIME_ZONE specified in database settings, the conversion should be from the database timezone to the Django app timezone, not from UTC to the Django app timezone.\n\n5. The issue also notes that when both timezones are the same, no conversion should be needed at all.\n\nThe problem is clearly defined, the expected behavior is specified, and even the location of the bug in the code is identified. A developer would have all the information needed to fix this issue without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11141": {
        "explanation": "This issue is well-specified with clear technical details and context. It describes:\n\n1. The problem: The migrate command checks for a __file__ attribute on migrations packages, which prevents it from working with Python 3 namespace packages (which don't have __init__.py files and thus no __file__ attribute).\n\n2. The history: The check was added in PR #21015 because __file__ was used for migration file discovery.\n\n3. The current situation: PR #23406 changed migration file discovery to use pkgutil.iter_modules() which uses __path__ instead of __file__, making the __file__ check unnecessary.\n\n4. The solution direction: Remove the __file__ check since it's no longer needed and is preventing migrations from working with namespace packages.\n\nThe issue provides sufficient technical context, references to related PRs, and a clear understanding of what needs to be fixed. A developer would need to locate the __file__ check in the migrate command code and remove it, ensuring that namespace packages work correctly with migrations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11149": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: users with view-only permissions can edit ManyToManyField relationships in admin inlines, which should not be possible.\n\n2. It provides context by referencing a previous ticket (#8060) where this regression was noted.\n\n3. It includes specific code examples showing the model structure (Photo and Report models with a M2M relationship) and the admin configuration (TabularInline using the through model).\n\n4. It clearly identifies the Django version where the issue occurs (at least 2.1).\n\n5. The expected behavior is implied but clear: users with only view permissions should not be able to add or remove items in a ManyToManyField inline.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - the permission checking for ManyToManyField inlines needs to respect view-only permissions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11155": {
        "explanation": "This issue is well-specified with clear requirements. The user wants to add three new settings for the language cookie in Django:\n\n1. LANGUAGE_COOKIE_SECURE (default: False)\n2. LANGUAGE_COOKIE_HTTPONLY (default: False)\n3. LANGUAGE_COOKIE_SAMESITE (default: None)\n\nThe issue provides:\n- Exact names for the new settings\n- Default values for each setting that maintain backward compatibility\n- Rationale for why these settings are needed (auditor requirements, personal preference, browser trends)\n- Context that these settings don't provide much security value since language isn't sensitive\n- A reference to a previous discussion on the topic\n\nA developer would have a clear understanding of what needs to be implemented: add these three new settings to Django's configuration system and modify the language cookie creation code to respect these settings. The implementation would be straightforward - find where the language cookie is set and add the new parameters based on these settings.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11163": {
        "explanation": "This issue is well-specified with a clear problem statement and solution. The issue describes:\n\n1. The specific function with the bug: `model_to_dict()`\n2. The exact behavior that's incorrect: when called with `fields=[]`, it returns all fields instead of an empty dict\n3. The root cause: a conditional check that doesn't properly handle empty lists\n4. The exact line that needs to be fixed: `if fields and f.name not in fields:`\n5. The proposed solution: change it to `if fields is not None and f.name not in fields:`\n6. A PR link is even provided with the fix\n\nThe issue provides enough context to understand the problem, locate the code that needs to be changed, and implement the fix. The expected behavior is clearly stated - an empty list of fields should result in an empty dictionary. This is a straightforward bug fix with a well-defined scope.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11166": {
        "explanation": "This issue is well-specified with clear context and a proposed solution. The problem is that the Django admin app has a hard dependency check on django.contrib.sessions being in INSTALLED_APPS, which prevents users from using alternative session management apps. The issue provides:\n\n1. Clear context: When the dependency was introduced (Django 2.2, commit 371ece2f0682e51f2f796854d3e091827a7cea63)\n2. The specific problem: Admin app checks for django.contrib.sessions in INSTALLED_APPS rather than checking for session functionality\n3. A real-world use case: Projects using alternative session apps like django-qsessions\n4. Current workaround: Adding django.contrib.sessions back to INSTALLED_APPS, but this has the side effect of creating unnecessary database tables\n5. Proposed solution: Replace the app check with a middleware check that looks for SessionMiddleware or a subclass\n\nThe solution approach is reasonable and specific enough to implement. The developer would need to modify the admin app's checks to look for session middleware functionality rather than the specific app. This is a well-defined task with a clear success criterion: the admin app should work with alternative session management apps without requiring django.contrib.sessions in INSTALLED_APPS.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11169": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: When a custom error handler with a path like 'path.to.view' cannot be imported, it currently raises ModuleNotFoundError or ViewDoesNotExist without a helpful system check message.\n\n2. The solution is explicitly described: Add a new system check that catches these exceptions and displays a more helpful error message with a specific code (urls.E008).\n\n3. The exact format of the error message is provided: \"The custom ``handlerXXX`` view ``'path.to.view'`` cannot be imported.\"\n\n4. There's context provided with a reference to a related issue (#29642) and a Stack Overflow question that demonstrates the problem.\n\n5. The implementation would involve adding exception handling around the import of custom error handlers and adding a new check code in the system checks framework.\n\nThis is a straightforward enhancement to improve the developer experience by providing more helpful error messages when custom error handlers can't be imported. The implementation path is clear and doesn't require additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11170": {
        "explanation": "The issue description contains several key elements but lacks some clarity and structure:\n\n1. The reporter encountered a UnicodeDecodeError when Django was trying to load debug templates (specifically technical_500.html).\n2. They identified that the issue might be related to an ellipsis character (\u2026) in line 239 of technical_500.html that was changed from \"dotdotdot\" in Django 2.1.8 to \"ellipsis\" in Django 2.2.\n3. They mention two potential solutions: either changing the ellipsis back to dotdotdot, or setting the encoding format to utf-8 when reading the technical_500.html file in debug.py.\n\nHowever, the issue lacks:\n- Clear steps to reproduce the error\n- The exact error message with traceback\n- The specific Django version being used\n- Whether this is a regression that needs fixing or just a configuration issue\n- The final question \"Did you intentionally modify the technical_html file?\" is unclear about what specific change they're referring to\n\nDespite these gaps, there's enough information to understand the core problem (Unicode decoding error with debug templates) and investigate the potential causes mentioned (character encoding issues with ellipsis). A developer could examine the debug.py file and technical_500.html to verify the encoding handling and the character changes between versions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11177": {
        "explanation": "This issue is reasonably well-specified. The user has provided:\n\n1. Clear context: Django 2.2 autoreloader not consistently detecting file changes\n2. Environment details: Python 3.7 slim Docker container, not using watchman\n3. Specific command being used: `./manage.py runserver 0.0.0.0:8001`\n4. Comparison to previous behavior: Worked in Django up to 2.1.7\n5. Observed behavior: Changes to .py files are detected once but subsequent changes to the same file don't trigger reloads\n6. Attempted workarounds: Editing different files, which doesn't consistently help\n\nWhat's missing:\n- Exact Docker setup details that might be relevant\n- Specific file system details that could affect file watching\n- Exact timing patterns of when it works vs. doesn't work\n- Whether this happens with all .py files or specific ones\n\nHowever, there's enough information to investigate the Django 2.2 autoreloader implementation changes compared to 2.1.7, look for known issues with file watching in containerized environments, and test potential solutions. The issue is specific enough to identify the component (autoreloader) and the behavior change.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11179": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a problem: when deleting a model instance that has no dependencies, the primary key (PK) is not being set to None after the delete() call. The issue also points to a specific location in the code (Django.db.models.deletion:276-281) and suggests that line 280 should be updated to fix this issue.\n\nThe description provides enough context to understand:\n1. What the problem is (PKs not being cleared after deletion)\n2. When it occurs (for models without dependencies)\n3. Where in the code the fix should be applied (Django.db.models.deletion:276-281, specifically line 280)\n\nWhile the issue doesn't provide a complete code sample or the exact implementation details of the fix, it gives enough direction for a developer familiar with Django's ORM to locate the issue and implement a solution. The fix would likely involve modifying the code to ensure that after a model instance is deleted, its primary key is set to None, similar to how Django handles this for models with dependencies.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11185": {
        "explanation": "The issue description is quite vague and lacks important details needed for a clear understanding of the problem and what a solution would entail:\n\n1. It references a previous partial fix (#27407) without explaining what that fix addressed or what specific functionality it implemented.\n\n2. It mentions that Model.delete(keep_parents=True) \"does not preserve all parent reverse relationships\" but doesn't:\n   - Provide any examples of the current behavior\n   - Explain what the expected behavior should be\n   - Describe any specific test cases that fail\n   - Detail which types of relationships are affected\n\n3. The statement about \"relationships toward parents of parents\" is unclear without context about how the current implementation works and what specific limitation exists.\n\nWithout additional information such as code examples, error cases, or a more detailed explanation of the current implementation's limitations, it would be difficult to implement a solution that correctly addresses the issue. A developer would need to spend significant time investigating the existing code and the previous fix (#27407) to understand what needs to be done.",
        "label": "VAGUE"
    },
    "django__django-11194": {
        "explanation": "The issue is reasonably specified, though it could benefit from more details. Here's what we can understand:\n\n1. The issue is about RSS feeds in Django and specifically about the language tag used in these feeds.\n2. Currently, Django's RSS views.py uses settings.LANGUAGE_CODE as the language tag for RSS feeds.\n3. The user wants to be able to specify a different language for the feed than what's in settings.LANGUAGE_CODE.\n4. The user has already created a patch/PR (#9879) to address this issue.\n\nWhat's missing or could be clearer:\n- The exact use case or scenario where this would be needed isn't fully explained\n- The specific implementation details of their solution aren't described in the issue itself (though they link to a PR)\n- There's no mention of how they envision the API for this feature (though we can assume it would be some parameter to override the default language)\n\nHowever, the core problem is clear enough: allow specifying a different language for RSS feeds than the global Django language setting. This is a reasonable feature request with a clear goal, and the linked PR likely provides additional context about the implementation approach.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11206": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies the specific function that has the problem: `utils.numberformat.format`\n2. It provides a concrete example demonstrating the issue with exact inputs and outputs\n3. It explains the current behavior: small decimals below a certain threshold are rendered in exponential notation\n4. It specifies the expected behavior: when `decimal_pos` is supplied and the number is smaller than what can be represented with that precision, it should return \"0\" followed by the appropriate number of decimal places\n5. It even hints at the cause: \"a hardcoded cut-off point in the internal logic\"\n\nA developer would be able to:\n1. Locate the specific function in the Django codebase\n2. Reproduce the issue using the provided examples\n3. Understand what changes are needed to fix the behavior\n4. Implement and test the solution based on the clear expected behavior\n\nThe issue provides all the necessary information to understand the problem and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11211": {
        "explanation": "This issue is well-specified with clear reproduction steps. The problem is clearly defined: prefetch_related is not working when using GenericForeignKey (GFK) with a model that uses a UUID field as primary key. \n\nThe issue provides:\n1. Exact code examples showing how to create the models involved\n2. The specific query that fails (Bar.objects.all().prefetch_related('foo'))\n3. The observed behavior (django orm engine returns None for the attribute foo)\n4. A link to a related bug report that might provide additional context\n\nThe issue is specific enough that a developer could reproduce the problem and understand what's happening. The expected behavior is implied (prefetch_related should work with GFK to UUID primary keys), and the actual behavior is stated (it returns None).\n\nThis is a technical issue with Django's ORM that has a clear scope and reproduction steps, making it well-specified for someone to investigate and fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11214": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is clearly described: Django's migration system continuously deletes and re-adds check constraints when the constraint contains a range object.\n\n2. A specific code example is provided showing the problematic constraint definition:\n   ```python\n   models.CheckConstraint(\n       check=models.Q(month__in=range(1, 13)),\n       name='check_valid_month',\n   )\n   ```\n\n3. The generated migration code is shown, illustrating how the range object is converted to a tuple, which appears to be causing the issue.\n\n4. A sample project repository is linked for reference.\n\n5. The reporter has even suggested potential solutions:\n   - Keep the range method call in the generated migration file\n   - Disallow using ranges in check constraints\n   - Use a database-specific range expression (for PostgreSQL)\n\nThe issue is focused on a specific Django ORM behavior with a reproducible example and clear potential solutions. A developer could work on this issue with the information provided without needing additional clarification. The problem scope is well-defined and the expected behavior is clear - the migration system should not continuously delete and re-add the same constraint when using range objects.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11216": {
        "explanation": "This issue is well-specified with a clear problem statement and suggested solution path. The issue describes that:\n\n1. ManifestStaticFilesStorage is generating duplicate versions of static files during collectstatic (with multiple hashed versions of the same file)\n2. This causes problems with tools like WhiteNoise that need to process each file\n3. The issue provides concrete examples of the problem (multiple versions of admin/css/base.css)\n4. It references previous discussions where this was considered \"working as intended\"\n5. It proposes a specific solution: adding a `keep_intermediate_files` property to HashedFilesMixin that would be set to False for CachedStaticFilesStorage and True for ManifestStaticFilesStorage\n\nThe issue provides enough context about the Django static files system, the specific problem, and a clear direction for implementing a solution. A developer familiar with Django's static file handling system would have sufficient information to implement the proposed solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11239": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: The Django `dbshell` command doesn't support PostgreSQL client certificates and keys for mutual TLS connections, even though Django's database configuration already supports these parameters.\n\n2. The context is provided: The issue references a similar bug (#28322) and explains that mutual TLS is a common security procedure requiring server certificate, client certificate, and client key.\n\n3. The current configuration is shown: A complete example of the database configuration that works for regular connections but not with `dbshell` is provided.\n\n4. The specific parameters that need to be supported are identified: `sslcert` and `sslkey` (in addition to the already supported `sslrootcert`).\n\n5. The scope is clear: Add support for these PostgreSQL client certificate parameters to the `dbshell` command.\n\n6. The complexity is estimated: The issue description mentions it \"should be a trivial fix,\" suggesting the implementation would involve passing these additional SSL parameters to the PostgreSQL command-line client.\n\nA developer could implement this solution without needing additional clarification, as it's clear what needs to be done and what a successful implementation would look like.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11260": {
        "explanation": "The issue description is somewhat brief but provides enough information to understand the problem:\n\n1. The issue is about Django's `inspectdb` command, which is a tool that generates Django models from an existing database.\n2. The problem is that `inspectdb` is generating `ForeignKey` fields with a `unique=True` constraint instead of using `OneToOneField`.\n3. This is causing Django to raise `fields.W342` warnings.\n\nFrom a Django developer's perspective, this makes sense because:\n- A `ForeignKey` with `unique=True` is functionally equivalent to a `OneToOneField`\n- Django's best practice is to use `OneToOneField` when a one-to-one relationship is intended\n- The warning `fields.W342` is likely telling developers to use `OneToOneField` instead of a unique `ForeignKey`\n\nThe solution would involve modifying the `inspectdb` command to generate `OneToOneField` instead of unique `ForeignKey` when appropriate.\n\nWhile the description is brief, it provides enough context for a Django developer to understand the issue and implement a solution. The issue is specific and technical, and the expected behavior is clear: `inspectdb` should generate `OneToOneField` instead of unique `ForeignKey`.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11265": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: using `exclude()` on a queryset with an annotated `FilteredRelation` causes a `FieldError` on the annotation name.\n\n2. The reporter provides a concrete example by modifying an existing Django test, showing both the working code (using `filter()`) and the failing code (using `exclude()`).\n\n3. A complete error traceback is included, which points to the specific error and helps identify where in the code the problem occurs.\n\n4. The reporter even provides their understanding of where the issue might be in the code: the `split_exclude()` function not properly handling the extra data from the original query.\n\nThis issue contains all the necessary information to reproduce the bug and provides good clues about where to look for a solution. A developer could take this information and start working on a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11270": {
        "explanation": "The issue is reasonably well-specified. It describes a clear problem: the Django `startapp` command allows specifying a target directory, but currently lacks validation to ensure this directory has a valid name that would allow the app to be imported properly.\n\nThe request is to add validation for the target directory name similar to the existing validation for app names. This implies:\n\n1. There is already validation logic for app names in the codebase\n2. The same or similar validation should be applied to the target directory argument\n\nWhile the issue doesn't provide specific details about what constitutes a \"valid name\" or exactly how the validation should be implemented, a developer familiar with Django would understand that Python module naming rules would apply (valid identifiers that can be imported). The existing app name validation code would serve as a reference for implementing this feature.\n\nA developer could reasonably implement this solution by:\n1. Locating the existing app name validation code\n2. Finding where the target argument is processed in the startapp command\n3. Applying similar validation logic to the target argument\n\nThe issue is specific enough to understand what needs to be done, though some details need to be inferred from the existing codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11276": {
        "explanation": "This issue is well-specified with clear requirements. The task is to replace Django's custom implementation of `django.utils.html.escape()` with Python's standard library function `html.escape()`. The issue provides:\n\n1. Clear identification of the problem: Django's function duplicates functionality already available in Python's standard library\n2. Evidence that the standard library version is faster (with benchmark results)\n3. Specific technical details about the implementation\n4. A note about a small backwards compatibility concern (the different encoding of apostrophes)\n\nThe solution path is straightforward: modify Django's `django.utils.html.escape()` function to use Python's `html.escape()` internally instead of its current implementation. The issue even highlights the one potential backwards compatibility issue to be aware of during implementation.\n\nA developer could implement this change without needing additional clarification, as the scope and requirements are clearly defined.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11278": {
        "explanation": "The issue is reasonably well-specified, though it could benefit from some additional details. Here's what we understand:\n\n1. The issue concerns adding a system check for uniqueness of names for:\n   - Constraints (check and unique constraints)\n   - Partial indexes\n\n2. The name field is mandatory for these elements and must be unique within the database scope.\n\n3. This is based on a discussion in issue #30362, which provides additional context.\n\nWhat's clear:\n- The problem: Names of constraints and partial indexes must be unique, and we need a system check for this\n- The scope: Database-wide uniqueness check\n- The elements affected: Constraints (check and unique) and partial indexes\n\nWhat could be clearer but can be reasonably inferred:\n- The implementation details: While not specified, a system check in Django typically refers to a validation that runs during the system check framework execution\n- The exact validation logic: We can infer that we need to check for duplicate names across these database objects\n- The error message or handling: Not specified, but can follow standard Django system check patterns\n\nThis issue provides enough information for a developer familiar with Django's system check framework and database constraints to implement a solution, though they would likely need to reference issue #30362 for additional context.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11279": {
        "explanation": "The issue is well-specified and provides clear requirements for implementation. It asks to enhance the `name` argument for BaseConstraint (CheckConstraint, UniqueConstraint) and Index to support interpolation of `%(app_label)s` and `%(class)s` placeholders, similar to how these placeholders work in other parts of Django's ORM.\n\nThe issue clearly explains:\n1. What needs to be implemented: Allow `%(app_label)s` and `%(class)s` placeholders in the `name` argument\n2. Where it should be implemented: BaseConstraint (including CheckConstraint, UniqueConstraint) and Index\n3. What the placeholders should do: Replace with lowercased class name and app label\n4. The purpose: To allow specifying constraints and indexes with mandatory names in abstract models\n\nThe issue also references a related discussion (#30362) which provides additional context.\n\nThis is a well-defined feature request with clear implementation requirements that follows an existing pattern in Django (as these placeholders are already used elsewhere in the framework). A developer familiar with Django would understand what needs to be done without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11281": {
        "explanation": "The issue is asking to improve typography in user-facing strings throughout the Django codebase. It specifically mentions replacing certain typographical elements with more professional alternatives:\n\n1. Using proper double quotes (\"\") instead of straight quotes\n2. Using apostrophes (') for contractions\n3. Using em dashes (\u2014) rather than double hyphens (--)\n4. Using ellipsis character (\u2026) rather than three periods (...)\n\nWhile the request is clear in terms of what typographical changes are desired, there are several significant gaps in the specification:\n\n1. No scope is defined - which parts of the codebase need these changes? All user-facing strings? Just documentation? Templates? Error messages?\n2. No implementation guidance - should this be done manually or with a script?\n3. No mention of backward compatibility concerns or potential impacts\n4. No clear definition of \"user-facing\" - does this include developer-facing documentation?\n5. No mention of internationalization considerations\n\nWithout these details, a developer would need to make significant assumptions about the scope and approach, which could lead to an incomplete or inappropriate solution. The issue provides the \"what\" but lacks the \"where\" and \"how\" needed for a complete implementation.",
        "label": "VAGUE"
    },
    "django__django-11292": {
        "explanation": "This issue is well-specified. It clearly describes:\n\n1. The current state: Management commands have a \"skip_checks\" stealth option that exists but is not exposed via command line\n2. The proposed change: Expose this option as a \"--skip-checks\" command line argument\n3. The rationale: To allow developers to bypass system checks when needed during development\n4. The expected outcome: Users should be able to use \"--skip-checks\" flag when running management commands\n\nThe implementation would involve adding this option to the command line parser for management commands. This is a straightforward feature request with a clear purpose and implementation path. The developer would need to modify the management command infrastructure to expose the existing functionality through a command line flag.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11299": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in Django's CheckConstraint implementation when using OR operators in combination with AND clauses.\n\n2. The problem is clearly explained: Django is incorrectly including fully qualified field names in part of the check constraint SQL, which causes migration failures when tables are renamed.\n\n3. A complete reproducible example is provided, including:\n   - The model definition with the constraint\n   - The migration code\n   - The actual SQL being generated\n   - The specific error message received\n   - The expected SQL that would work correctly\n\n4. The issue affects specific databases (SQLite and Oracle) which are mentioned.\n\n5. The root cause is even hinted at: \"It appears that this has to do with the AND clause items using Col while the OR clause uses SimpleCol.\"\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided example\n2. Understand the expected behavior\n3. Identify where in Django's code the problem might be occurring (in the SQL generation for CheckConstraint with mixed AND/OR operators)\n4. Develop and test a fix\n\nThis is a well-documented bug report with all the necessary details to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11333": {
        "explanation": "This issue is well-specified with clear technical details about the problem and a proposed solution:\n\n1. The problem is clearly identified: Multiple URLResolver instances are being constructed by `django.urls.resolvers.get_resolver` when `django.urls.base.set_urlconf` hasn't been called yet.\n\n2. The cause is explained in detail: `get_resolver` is cached with lru_cache, but it's being called with different values (None initially, then settings.ROOT_URLCONF after request handling), causing multiple expensive `URLResolver._populate` calls.\n\n3. The impact is specified: Performance degradation due to redundant expensive computations and duplicate caching, especially in applications with many routes.\n\n4. A specific solution is proposed: Modify `get_resolver` to look up settings.ROOT_URLCONF before the memoized function call.\n\nThe issue provides enough context about the Django URL resolution system, the specific functions involved, and the performance implications. A developer familiar with Django would have sufficient information to implement the proposed solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11354": {
        "explanation": "This issue is well-specified with clear details about the problem and a potential solution. The reporter has:\n\n1. Provided a clear description of the bug: QuerySet.count() fails when using RawSQL annotations on inherited model fields\n2. Included minimal model definitions to reproduce the issue\n3. Provided specific code to reproduce the bug\n4. Identified the root cause in Django's codebase with line references\n5. Suggested a potential fix with code changes\n6. Specified the Django versions affected (1.8.18 and 1.11.0)\n7. Included the specific error message received\n\nThe issue contains all the necessary information for a developer to understand the problem, reproduce it, and implement a solution. The reporter has even done the investigative work to identify where in the codebase the issue occurs and proposed a potential fix. This makes it very straightforward for someone to create a PR that addresses this issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11377": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue occurs in Django 2.2 when using query expressions in Model.ordering\n2. The specific error is identified: TypeError when the deprecation warning tries to join OrderBy objects as strings\n3. The exact location of the error is provided (django/db/models/sql/compiler.py, line 558)\n4. A sample model definition that reproduces the issue is provided\n5. A proposed quick fix is suggested (joining string representations)\n6. The expected output format is clearly shown with a before/after example\n7. A regression test is even provided via a GitHub link\n\nThe issue has all the necessary information to understand the problem and implement a solution. The fix would involve modifying how the deprecation warning handles query expressions in Meta.ordering to properly format them as Python code rather than just strings. The solution needs to ensure that the warning message contains valid Python code that users can copy into their .order_by() calls.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11396": {
        "explanation": "This issue is well-specified. The reporter provides:\n\n1. A clear code example showing what they're trying to do: `MyModel.objects.annotate(my_column=Value('asdf')).order_by('my_column').values_list('id')`\n2. The exact error message they're receiving: `ProgrammingError: non-integer constant in ORDER BY`\n3. The problematic SQL line: `LINE 1: ...odel\".\"id\" FROM \"mymodel\" ORDER BY 'asdf' ASC...`\n4. The database system where the issue occurs: PostgreSQL (mentioned in the title)\n\nThe issue is that when trying to order by a constant string value in a PostgreSQL query, Django is generating SQL that puts the string literal directly in the ORDER BY clause rather than referencing the annotated column. This is causing PostgreSQL to reject the query with a syntax error.\n\nA developer would have enough information to reproduce this issue and understand what needs to be fixed: Django's query generation for PostgreSQL needs to handle constant values in ORDER BY clauses correctly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11400": {
        "explanation": "This issue is well-specified with clear technical details about a problem in Django's admin filters. The issue identifies two specific problems:\n\n1. `RelatedFieldListFilter` doesn't use the ordering defined in `Model._meta.ordering` as a fallback when no ordering is defined in the ModelAdmin class. It specifically points to the line in the code where ordering is set to an empty tuple.\n\n2. `RelatedOnlyFieldListFilter` doesn't apply any ordering to the related model, even when ordering is defined in the ModelAdmin class, because the call to `field.get_choices()` doesn't include the ordering parameter.\n\nThe issue provides specific file paths and line numbers in the Django codebase, making it easy to locate the problematic code. It also clearly states what the expected behavior should be (falling back to the ordering defined in the related model's Meta.ordering field).\n\nA developer would be able to understand the problem, locate the relevant code, and implement a solution that makes both filter classes respect the ordering defined in the model's Meta class when appropriate.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11405": {
        "explanation": "The issue describes a bug where QuerySet ordering and Meta.ordering are mutable when using the reverse() method. The description provides specific information:\n\n1. The bug can be reproduced by running a specific test command: `./runtests.py ordering.test --reverse`\n2. It mentions a specific commit hash where the bug can be reproduced: a2c31e12da272acc76f3a3a0157fae9a7f6477ac\n3. It points to a potential root cause: order mutates on queryset execution in SQLCompiler.get_order_by()\n4. It references a previous commit (f218a2ff455b5f7391dd38038994f2c5f8b0eca1) that added a test which wasn't correct\n\nThis provides enough context for a developer to:\n1. Reproduce the issue using the provided test command\n2. Examine the specific code in SQLCompiler.get_order_by() that's causing mutation\n3. Review the history of the related test to understand what was intended vs. what's happening\n\nThe issue is about unexpected mutation of ordering when reverse() is called, which is a clear technical problem with a specific area to investigate. A developer would need to ensure that reverse() doesn't mutate the original ordering but creates a new one instead.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11417": {
        "explanation": "The issue is reasonably specified but lacks some details. It clearly identifies:\n\n1. The component to update: django.core.mail.message.sanitize_address\n2. The current implementation: uses email.utils.parseaddr\n3. The proposed change: use email.headerregistry.parser.get_mailbox() instead\n4. The rationale: it's \"less error-prone\" on Python 3\n\nHowever, it doesn't provide:\n- Specific examples of errors or issues with the current implementation\n- Details on how the new implementation would handle edge cases differently\n- Whether there are compatibility concerns to consider\n- Any specific requirements for backward compatibility\n\nA developer familiar with Django's mail system and Python's email libraries would likely understand what needs to be done, but would need to research the specific differences between the two parsing approaches to ensure a proper implementation. The task is straightforward enough that a competent developer could proceed with implementation after some investigation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11423": {
        "explanation": "This issue is well-specified with a clear description of the problem and steps to reproduce it. The user has:\n\n1. Created a minimal reproducible example in a GitHub repository\n2. Provided clear instructions on how to run the example (migrate, runserver with --noreload)\n3. Explained the data model and relationships clearly (Tag with GenericRelation to Book and CD models)\n4. Described the expected behavior: tag.content_object.tag should equal tag\n5. Shown the actual incorrect behavior with specific output examples\n6. Identified the specific issue with prefetch_related and caching when using cyclic prefetching with GenericRelation\n7. Explained the context of why this matters (performance optimization in a real application)\n8. Even tracked down that the issue is related to _prefetched_objects_cache holding incorrect values\n\nA developer could take this information and work on a solution without needing additional clarification. The issue appears to be a bug in Django's prefetch_related functionality when dealing with cyclic relationships through GenericRelation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11428": {
        "explanation": "This issue is well-specified with clear reproduction steps and a detailed explanation of the problem. The issue describes:\n\n1. A specific bug in Django's autoreloader when dealing with custom exceptions that have non-standard signatures\n2. Complete code to reproduce the issue\n3. The exact file and function where the problem occurs (django.utils.autoreload.raise_last_exception)\n4. The specific error that occurs (TypeError about missing required argument)\n5. Historical context about when the behavior changed (referencing a specific commit)\n6. The expected behavior (it should handle custom exception signatures properly)\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and identify where in the codebase the fix would need to be made. A developer could immediately start working on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11433": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with Django forms where fields with default values on the model cannot be overwritten using the `cleaned_data` dictionary if the field wasn't included in the original form submission data. The issue includes a reference to a specific GitHub pull request comment for additional context.\n\nThe issue clearly identifies:\n1. The current behavior: default values from models cannot be overwritten with values in `cleaned_data` if the field wasn't in the original form data\n2. Why this is problematic: it doesn't follow the expected paradigm of being able to modify data in `cleaned_data`\n3. The current workaround: users have to copy and overwrite the raw data submitted with the form\n\nWhile the issue doesn't provide specific code examples or a detailed technical explanation of the underlying mechanism, it provides enough information for a developer familiar with Django's form processing to understand the problem and attempt a solution. The reference to the PR comment would likely provide additional technical context.\n\nA solution would involve modifying Django's form processing logic to allow `cleaned_data` values to override model default values even when the field wasn't in the original form submission.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11446": {
        "explanation": "This issue is well-specified. The problem is clearly described: Django's default error pages (specifically the 404 page) in production mode (DEBUG=False) are not properly formatted HTML documents. The issue provides:\n\n1. The current output: Just the bare `<h1>` and `<p>` tags without proper HTML document structure\n2. The expected output: A complete HTML document with `<html>` and `<body>` tags\n3. A comparison with DEBUG=True mode, where the HTML is properly formatted\n\nThe solution would require modifying Django's error handling to ensure that error pages in production mode are properly formatted HTML documents with the appropriate structure tags. This is a straightforward issue with a clear problem statement and expected outcome.\n\nThe scope is well-defined and the solution approach is evident - modify the templates or handlers that generate these error pages to include the proper HTML document structure.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11451": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The problem is clearly identified: ModelBackend.authenticate() makes an unnecessary database query when username is None, which is inefficient.\n\n2. The current code is provided with explanatory comments showing exactly where the issue occurs.\n\n3. A specific solution is proposed: add a check to return early if username or password is None.\n\n4. The submitter provides context about how they discovered the issue (while writing tests for django-sesame).\n\n5. They include the actual SQL query that's being executed unnecessarily.\n\n6. They address potential concerns about timing attacks and explain why their proposed change shouldn't compromise security.\n\nThe issue contains all the information needed to understand the problem, verify it exists, and implement the suggested fix. A developer could create a PR with this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11477": {
        "explanation": "The issue description is quite vague and lacks important details needed to understand and solve the problem effectively. Here's what's missing:\n\n1. No clear explanation of what `translate_url()` is or what it's supposed to do\n2. No examples of the incorrect behavior\n3. No information about what \"optional named groups\" means in this context\n4. No details about what URL patterns look like or how they're structured\n5. No information about what the expected correct behavior should be\n6. Only a vague reference to \"it's seen in test case of the patch\" without providing the actual test case or patch\n\nWhile there is a hint that this relates to URL translation with optional arguments, there's not enough context to understand the specific problem or how to approach a solution. Without examples of the incorrect behavior and expected behavior, it would be difficult to implement a fix confidently.",
        "label": "VAGUE"
    },
    "django__django-11490": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue describes that when using composed queries (like union operations) with values() or values_list(), changing the requested columns in subsequent calls doesn't work as expected. \n\nThe example clearly demonstrates the bug:\n1. A query is created and a union operation is performed\n2. When values_list('name', 'order') is called, it correctly returns ('a', 2)\n3. But when values_list('order') is called on the same union query, it still returns ('a', 2) instead of just (2)\n\nThe issue even points to the specific code location in compiler.py (lines 428-433) that likely contains the bug. This gives a clear starting point for investigation.\n\nA developer would be able to:\n1. Reproduce the issue using the provided example\n2. Locate the relevant code in compiler.py\n3. Understand that the problem is related to how composed queries handle column selection in values/values_list calls\n4. Implement a fix that ensures composed queries respect subsequent column selection changes\n\nThe expected behavior is clear: when values_list('order') is called, it should only return the 'order' column value.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11514": {
        "explanation": "This issue is well-specified with a clear problem and solution:\n\n1. Problem: Django's `never_cache` decorator doesn't prevent CDN caching because it lacks the `Cache-Control: private` header.\n2. Solution: Add the `Cache-Control: private` header to the `never_cache` decorator's response.\n3. Context: The issue provides a clear explanation of why this is needed (CDN providers like Fastly require this specific header) and even includes documentation reference.\n\nThe issue describes a specific technical change with a clear rationale. A developer would know exactly what code to modify (the `never_cache` decorator) and what change to make (add the `private` directive to the Cache-Control header). This is a straightforward enhancement that can be implemented without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11517": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in Django's `call_command` function when working with subparsers.\n2. It provides a complete code example showing how to create a command with subparsers.\n3. It clearly explains the expected behavior (that worked in Django 1.11).\n4. It details the exact error message and line number where the error occurs.\n5. It explains the root cause: `_SubParsersAction.option_strings` is an empty array, causing `min()` to fail.\n6. It also describes alternative approaches that fail and their error messages.\n\nThe issue provides enough context to understand the problem and reproduce it. A developer would be able to:\n1. Create a test case to reproduce the issue\n2. Locate the problematic code in `django/core/management/__init__.py`\n3. Understand that the issue is with handling subparsers in `call_command`\n4. Implement a fix that properly handles subparser destinations in options\n\nThe solution would likely involve modifying the argument validation in `call_command` to properly handle subparser destinations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11525": {
        "explanation": "This issue is well-specified with a clear problem and expected solution. The user describes a specific bug where Django's mail_managers() function doesn't properly validate the format of the MANAGERS setting, leading to cryptic SMTP errors instead of helpful exceptions. \n\nThe issue includes:\n1. A clear example of the incorrect setting format (a list of email strings instead of tuples)\n2. The exact error that occurs when using this incorrect format\n3. A reference to documentation that clarifies the correct format\n4. A specific request: Django should \"fail early\" with a helpful exception when settings are in incorrect formats\n\nFrom this description, I can understand that the solution would involve adding validation in the mail_managers() and mail_admins() functions to check if MANAGERS and ADMINS settings are properly formatted (as tuples with name and email) before attempting to send emails, and raising appropriate exceptions with helpful error messages when they're not.\n\nThe scope is well-defined and the expected behavior change is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11527": {
        "explanation": "This issue is well-specified with a clear problem and suggested solution. The issue describes that the `sqlsequencereset` command in Django provides no output when there are no sequences to reset, which can confuse users. The expected behavior is clearly stated: the command should raise a CommandError when there are no sequences to reset, informing the user that there's nothing to do. There's also a bonus suggestion to detect if the database backend supports sequences at all.\n\nThe issue provides:\n1. The specific command affected (`sqlsequencereset`)\n2. The current behavior (silent output when no sequences found)\n3. Why this is problematic (users can't tell if it worked or failed)\n4. A specific solution (raise CommandError when no statements)\n5. A link to the relevant code\n6. A bonus enhancement (detect if backend supports sequences)\n\nWith this information, a developer could locate the relevant code, understand the problem, and implement the suggested solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11532": {
        "explanation": "This issue is well-specified with clear information about the problem, steps to reproduce, and a suggested fix:\n\n1. Problem: Email messages crash when the computer hostname contains non-ASCII characters and the email encoding is set to a non-Unicode format like iso-8859-1. This happens because Django attempts to include the hostname in the Message-ID header without properly encoding it.\n\n2. Steps to reproduce: The issue provides clear steps - set hostname to a non-ISO-8859-1 value (e.g., \"\u6b63\u5b97\") and run the mail tests.\n\n3. Root cause: The issue points to the specific code location in Django's codebase (django/core/mail/message.py line 260) where the Message-ID is being set with the DNS_NAME that contains non-ASCII characters.\n\n4. Suggested fix: Convert the domain name to punycode before using it in the Message-ID header.\n\n5. Test case: A complete test case is provided to verify the fix works correctly.\n\n6. Traceback: A detailed error traceback is included showing exactly where and how the error occurs.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The suggested fix (converting to punycode) is a standard approach for handling internationalized domain names in email headers.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11543": {
        "explanation": "This issue is well-specified with clear technical details. It describes a specific problem with Django's runserver command when using the --nothreading option. The issue explains:\n\n1. The exact environment (OS, Python version, Django version, browsers)\n2. The specific command-line option causing the problem (--nothreading)\n3. The technical reason for the failure (keep-alive connections preventing new connections from being accepted)\n4. The sequence of events that leads to the server becoming unresponsive\n\nThe problem is clearly a bug in how Django's development server handles keep-alive connections when threading is disabled. A developer would understand that the solution requires modifying how the runserver command handles connections when the --nothreading option is specified, likely by either:\n1. Properly closing connections after requests complete\n2. Handling multiple keep-alive connections correctly in non-threaded mode\n3. Or potentially adding a timeout mechanism for idle connections\n\nThe issue provides enough context for a developer to reproduce the problem and understand what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11551": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in Django's admin validation system that was introduced in version 2.2.1 (upgrading from 2.0.7).\n2. It provides a complete code example that reproduces the issue.\n3. It explains the exact error message: admin.E108 being incorrectly raised for fields accessible only via instance.\n4. It identifies the specific commit that introduced the bug (with link).\n5. It provides a detailed analysis of the problem, including a truth table showing different scenarios and how they're handled in different versions.\n6. It proposes a specific solution with complete replacement code for the `_check_list_display_item` function.\n\nThe issue is highly technical but thoroughly documented. A developer would have all the information needed to understand the problem, verify it, and implement the suggested fix or an alternative solution. The issue description even includes a detailed analysis of how the proposed solution would handle all the edge cases compared to the current implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11555": {
        "explanation": "This issue is reasonably well-specified with enough information to understand the problem and attempt a solution. The user has:\n\n1. Clearly identified the bug: using a query expression for ordering during multi-table inheritance causes a crash\n2. Provided a reproducible test repository (https://github.com/JonnyWaffles/djangoordermetabug)\n3. Mentioned that the bug occurs because a field is an OrderBy object instead of a string during get_order_dir\n4. Noted a specific circumstance where the bug manifests (during test db setup but not during repl or script use)\n5. Included a reference to a stacktrace in the repository\n\nWhile the issue doesn't explicitly detail the expected behavior or provide the exact code for a fix, it provides enough context and a reproducible example that would allow a developer familiar with Django's ORM and inheritance model to investigate and develop a solution. The repository would likely contain the necessary context about the Meta.ordering expressions that are causing the issue.\n\nA developer would need to:\n1. Examine the repository and stacktrace\n2. Understand how Django processes Meta.ordering with expressions\n3. Fix the get_order_dir method to properly handle OrderBy objects\n\nThis is sufficient information to make a meaningful attempt at a solution, though some investigation would be required.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11559": {
        "explanation": "This issue is reasonably well-specified with enough information to understand the problem and attempt a solution. The user has:\n\n1. Clearly identified the bug: using a query expression for ordering during multi-table inheritance causes a crash\n2. Provided a reproducible test repository (https://github.com/JonnyWaffles/djangoordermetabug)\n3. Mentioned that the bug occurs because a field is an OrderBy object instead of a string during get_order_dir\n4. Noted a specific behavior difference: it only fails during test db setup, not during REPL or script use\n5. Included a reference to a stacktrace in the repository\n\nWhile the issue doesn't explicitly detail the exact fix needed, it provides enough context and a reproducible example that would allow a developer to investigate the issue, understand the root cause, and implement a solution. The specific mention of the OrderBy object vs string issue in get_order_dir gives a strong hint about where to look for the problem.\n\nThe only minor gap is that the issue doesn't specify which version of Django is affected, but this can be determined from the test repository.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11560": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue identifies a specific bug: ExtractYear lookup doesn't work properly with DurationField.\n2. It provides a concrete code example showing the problematic usage pattern.\n3. It explains the technical reason for the failure: ExtractYear has an optimization that compares durations with dates, which is not a valid comparison.\n4. The SQL query that's being generated is shown, making it clear why the operation fails.\n\nThe solution would need to:\n1. Identify where in the ExtractYear implementation this optimization occurs\n2. Add a check to detect when the field is a DurationField\n3. Raise a ValueError with an appropriate message when this incompatible operation is attempted\n\nThe issue provides enough context to locate the relevant code in Django's ORM and implement a proper fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11564": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem: when Django is running on a sub-path (using WSGI's SCRIPT_NAME parameter), the static URLs generated by the {% static %} tag don't include this prefix, resulting in incorrect URLs.\n\nThe issue also suggests a specific solution approach: modifying the render method of the static tag to include the SCRIPT_NAME from request.META. It further identifies that FileSystemStorage and StaticFilesStorage also need similar modifications.\n\nWhat makes this not fully well-specified:\n1. It doesn't provide complete implementation details for all affected components\n2. It doesn't specify how to handle edge cases (e.g., when SCRIPT_NAME is empty or already included in STATIC_URL)\n3. It doesn't address backward compatibility concerns\n4. It doesn't specify test cases or validation methods\n\nHowever, an experienced Django developer would understand the problem and could implement a solution based on the information provided, potentially filling in these gaps with reasonable assumptions based on Django's conventions and architecture.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11584": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is clearly defined: FileNotFoundError when running Django's development server in a Docker container with volume-mounted source.\n\n2. The exact error is provided with a complete stack trace showing where the failure occurs.\n\n3. The issue includes version information: reproducible with Django 2.2.3 but not with Django 2.1.4.\n\n4. The complete configuration is provided, including the Dockerfile and docker-compose.yml file.\n\n5. The error appears to be related to the autoreload functionality in Django 2.2.3 when running in a Docker container with mounted volumes.\n\nThe issue provides all the necessary information to reproduce the problem and narrow down the cause. A developer would have enough information to investigate why the autoreloader in Django 2.2.3 is failing to find a directory when running in a Docker container, likely related to how the container's working directory is handled during file change detection.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11591": {
        "explanation": "This issue is well-specified and clear about what needs to be done:\n\n1. The problem is clearly identified: After using QuerySet.union(), intersection(), or difference() methods, only certain operations (LIMIT, OFFSET, ORDER BY) are supported according to the documentation, but this isn't strictly enforced in the code.\n\n2. The requested solution is explicit: Implement proper error handling to raise descriptive errors when unsupported operations are attempted on the resulting QuerySet.\n\n3. Context is provided: The issue references two related tickets (#27982 and #27990) that demonstrate user confusion about this behavior.\n\n4. The scope is well-defined: The change would affect the QuerySet methods union(), intersection(), and difference(), specifically adding validation for operations performed on their results.\n\nA developer could implement this solution by:\n1. Identifying which operations are unsupported after these set operations\n2. Adding appropriate checks in the code\n3. Raising descriptive error messages when unsupported operations are attempted\n\nThis is a straightforward enhancement to improve the developer experience by making the API behavior match the documented limitations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11592": {
        "explanation": "This issue is well-specified with a clear problem and solution path. The issue identifies that Django's FileResponse class has a block_size attribute that can be customized, but this attribute is not being passed to the wsgi.file_wrapper when it's used. The issue provides specific links to the relevant code locations:\n\n1. The FileResponse class definition showing the block_size attribute: https://github.com/django/django/blob/415e899dc46c2f8d667ff11d3e54eff759eaded4/django/http/response.py#L393\n\n2. The location where wsgi.file_wrapper is called without passing the block_size: https://github.com/django/django/blob/415e899dc46c2f8d667ff11d3e54eff759eaded4/django/core/handlers/wsgi.py#L144\n\nThe solution would involve modifying the WSGI handler to pass the block_size attribute from the FileResponse object to the wsgi.file_wrapper function. This is a straightforward change with clear requirements and locations to modify.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11603": {
        "explanation": "This issue is well-specified for implementation. It clearly states:\n\n1. The problem: Avg and Sum aggregates need DISTINCT support, which was previously ignored but now throws an exception (since version 2.2)\n2. The context: This is an extension of issue #28658 (which presumably added DISTINCT support for some other aggregates)\n3. The solution approach: Set the relevant aggregate classes as allowing DISTINCT\n4. Additional information: The same approach could be applied to Min and Max (though noted as pointless)\n\nThe issue provides enough technical context for a developer familiar with the codebase to understand what needs to be done. The implementation would involve finding the Avg and Sum aggregate classes and modifying them to allow the DISTINCT parameter, similar to what was done in the referenced issue #28658. This is a straightforward task with a clear success criterion: the Avg and Sum aggregates should accept DISTINCT parameters without throwing exceptions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11605": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes a limitation in Django's validation system where it fails to detect window expressions in certain filter conditions:\n\n1. The issue clearly identifies that Django has a check for window expressions in filters, but it's not comprehensive.\n2. Three specific code examples are provided:\n   - The first example shows the current behavior where a direct window expression in a filter raises NotSupportedError\n   - The second example shows a case where referencing a window expression on the right side of a filter condition doesn't raise an error\n   - The third example shows a more complex case with a derived expression based on a window function\n\nThe expected behavior is implied: Django should detect and raise a descriptive error for all cases where window expressions are used in filters, regardless of whether they're direct or indirect references.\n\nThe solution would involve enhancing Django's validation to check for window expressions more thoroughly, including the right side of expressions and combined expressions. This is a well-defined task with clear examples of what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11612": {
        "explanation": "This issue is well-specified with clear information to understand and reproduce the problem:\n\n1. The problem is clearly defined: SQLite3 migrations fail when a model's Meta db_table is quoted (e.g., '\"table_with_quoted_name\"').\n\n2. The specific error is provided: django.db.utils.OperationalError with \"near 'table_with_quoted_name': syntax error\"\n\n3. The problematic SQL query is included, showing how Django is incorrectly generating:\n   `CREATE TABLE \"new__\"table_with_quoted_name\"\"`\n\n4. Reproduction steps are provided: A table with a quoted name that has at least one foreign key.\n\n5. A complete traceback is included, showing exactly where in Django's code the error occurs.\n\n6. There's a reference to Django's documentation stating that quoted names should be supported.\n\nThe issue appears to be in Django's SQLite3 schema migration code, specifically how it handles quoted table names when creating new tables during migrations. The problem occurs in the `_remake_table` method in `django/db/backends/sqlite3/schema.py` which is called when adding fields to existing tables.\n\nThis is a specific bug with clear reproduction steps and enough context to identify where the fix needs to be applied.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11618": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug where ManifestFilesMixin.read_manifest() silently catches PermissionErrors, leading to confusing behavior.\n\n2. The reporter provides a detailed error trace and explains the root cause: the code catches OSError broadly, which includes PermissionError, and then proceeds with empty content instead of failing properly.\n\n3. The reporter has done thorough debugging and verified the issue by testing with root permissions.\n\n4. The issue includes specific suggestions for improvement:\n   a) Not catching OSError at all\n   b) Having more granular error handling for OSError subclasses\n   c) Adding logging to indicate the problem\n\n5. The reporter points to the exact line of code causing the issue (https://github.com/django/django/blob/master/django/contrib/staticfiles/storage.py#L385).\n\nThe problem is clearly defined, the cause is identified, and reasonable solutions are proposed. A developer could implement a fix based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11622": {
        "explanation": "The issue is reasonably well-specified. It describes a clear problem: when invalid values are passed to numeric fields like AutoField, FloatField, or IntegerField, Django currently produces an unhelpful error message that doesn't indicate which field caused the problem. The desired solution is to enhance the exception to include the field name in the error message, making it easier for developers to identify and fix the issue.\n\nThe issue provides:\n1. The current behavior: A generic TypeError without field information\n2. The desired behavior: An exception that includes the field name\n3. The specific field types affected: AutoField, FloatField, IntegerField\n\nWhat's missing is:\n1. Specific code examples showing the current behavior\n2. The exact format of the desired error message\n3. Whether other field types should also be included\n\nHowever, these gaps can be reasonably filled in by examining the Django codebase, understanding how field validation works, and implementing a solution that follows Django's error handling patterns. The core requirement - to improve error messages by including field names - is clear enough to guide implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11630": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The exact error message is provided: \"table_name: (models.E028) db_table 'table_name' is used by multiple models: base.ModelName, app2.ModelName.\"\n\n2. The architecture is clearly described: They have a Base app connected to a central database and multiple other apps connected to their own databases, with some apps sharing the same table names.\n\n3. The context is clear: The setup worked in Django 2.0 but after upgrading to Django 2.2, they're getting an error about not being allowed to have two apps with different models using the same table names.\n\n4. The question is specific: \"Is this correct behavior?\" - they want to know if this is expected in Django 2.2 or if it's a bug.\n\nThe issue provides enough information to:\n1. Understand the problem\n2. Reproduce the issue\n3. Research the changes between Django 2.0 and 2.2 that might have caused this\n4. Determine if this is expected behavior or a bug\n5. Suggest potential solutions or workarounds\n\nA developer would have enough information to investigate and address this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11638": {
        "explanation": "This issue is well-specified with clear information about what needs to be improved. The user wants to enhance the exception message in Django's test client and urlencode() function when None is passed as data. They've provided:\n\n1. The specific context: upgrading from Django 2.0.5 to 2.2.1\n2. The exact problem: tests failing due to a change in the test client that now throws an exception when None is passed as data\n3. A reference to the Django documentation explaining the change\n4. A clear proposal: improve the exception message to display the offending key and value\n5. A link to their fork with a proposed implementation\n\nWith this information, a developer could understand the issue, verify the problem, and evaluate the proposed solution or implement their own. The scope is narrow and well-defined - enhancing an error message to be more informative.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11669": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a specific inconsistency in Django's class-based views: TemplateView automatically passes kwargs to the context, while ListView does not. The reporter suggests this inconsistency should be fixed.\n\nFrom this description, I can understand:\n1. The specific component affected (TemplateView)\n2. The behavior that needs to be changed (stop automatically passing kwargs to context)\n3. The rationale (consistency with other views like ListView)\n\nWhat's missing is:\n1. The preferred solution - should TemplateView stop passing kwargs, or should ListView start passing them?\n2. Potential backward compatibility concerns - changing TemplateView's behavior could break existing code\n3. Whether this is a bug fix or a feature request (seems more like a design consistency issue)\n\nHowever, despite these gaps, there is enough information to make a reasonable interpretation: the solution would likely involve modifying TemplateView to stop automatically passing kwargs to the context, bringing it in line with ListView's behavior. A developer familiar with Django's codebase would be able to locate the relevant code and implement a solution, while being mindful of backward compatibility concerns.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11680": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes how Django unnecessarily performs an UPDATE query before an INSERT when saving a new model instance that has a primary key with a default value (in this case, a UUID field). \n\nThe description provides three different scenarios:\n1. Using a model's instance with `save()` - shows the problematic behavior with both UPDATE and INSERT queries\n2. Using a model's manager method `create()` - shows the desired behavior with only an INSERT query\n3. Using a model's instance with `save(force_insert=True)` - shows a workaround that achieves the desired behavior\n\nThe issue is asking to modify Django's behavior so that when saving a new model instance with a primary key that has a default value, it should skip the unnecessary UPDATE query and directly perform an INSERT, similar to how the `create()` method or `save(force_insert=True)` behave.\n\nThe issue also references a potentially related Django ticket (#29129) which provides additional context.\n\nThis is a clear optimization request with reproducible examples, expected behavior, and even a workaround. A developer would have enough information to investigate and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11692": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user is trying to use Django's OuterRef in a union of QuerySets (using either the union() method or the | operator), and then wrap that union in a Subquery for annotation. They've provided specific code examples demonstrating the issue and the exact error message they're receiving: \"ValueError: This queryset contains a reference to an outer query and may only be used in a subquery.\"\n\nThe issue is clearly a limitation or bug in Django's ORM where OuterRef references don't work properly when combined with union operations, even when wrapped in a Subquery. The user has demonstrated multiple approaches they've tried (both union() method and | operator) with the same result.\n\nThe expected behavior is implicit but clear: the user expects to be able to use OuterRef in union subqueries since they're wrapping the result in Subquery, which should theoretically make it valid. A solution would involve either fixing Django's ORM to support this use case or providing a workaround that achieves the same result.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11695": {
        "explanation": "The issue is well-specified with a clear problem and solution:\n\n1. Problem: The Ajax autocomplete widget using Select2 in Django is triggering requests on every keypress, causing unnecessary server load.\n\n2. Solution: Implement rate-limiting by adding a delay using Select2's ajax.delay option.\n\n3. Context: A reference to an existing PR (#11695) is provided, which likely contains the implementation details.\n\nThe issue provides enough information to understand what needs to be done. The solution approach is explicitly mentioned (using Select2's ajax.delay option), and there's even a reference to an existing pull request that likely contains the implementation. A developer familiar with Django and Select2 would have sufficient information to implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11701": {
        "explanation": "This issue is well-specified with clear reproduction steps and detailed error information. The problem occurs when a null character (%00) is included in the search query parameter in the Django admin interface when using PostgreSQL as the database. The error message \"A string literal cannot contain NUL (0x00) characters\" is clear, and the full traceback is provided showing exactly where the error occurs in the Django code path.\n\nThe issue includes:\n1. Exact reproduction steps (accessing http://localhost/admin/auth/user/?q=%00)\n2. Complete environment details (Django 2.1.4, Python 3.6.7)\n3. Full list of installed applications and middleware\n4. Complete traceback showing the error path\n5. Clear error message indicating the problem is with null characters in PostgreSQL string literals\n\nFrom this information, it's evident that the Django admin search functionality needs to handle or sanitize null characters in search queries when using PostgreSQL, which doesn't allow null bytes in string literals. A solution would involve properly handling or filtering out null characters from search parameters before they reach the database query.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11728": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a bug in the `simplify_regexp()` function where it fails to replace trailing named groups in URL patterns that don't end with a '/'. The issue provides:\n\n1. A specific example of the problematic input: `r'entries/(?P<pk>[^/.]+)/relationships/(?P<related_field>\\w+)'`\n2. The specific problem: the final named group \"related_field\" is not properly replaced\n3. A workaround: adding a trailing '/' makes it work correctly\n4. Mentions that this affects Django REST Framework (issue #6888)\n\nThe only missing information is:\n- The exact code of the `simplify_regexp()` and `replace_named_groups()` functions\n- The expected output after replacement\n- The actual output that's currently produced\n\nHowever, since the issue mentions that code reproducing the issue is attached (though not visible in this description), and it references a specific Django REST Framework issue, a developer familiar with the codebase should be able to locate the relevant code and understand what's expected. The problem description is clear enough to identify where to look and what to fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11734": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible test case. The issue describes a bug where using OuterRef() within exclude() or ~Q() causes an error, while it works correctly in filter(). The error message is provided (\"ValueError: This queryset contains a reference to an outer query and may only be used in a subquery\"), and the test case clearly demonstrates three scenarios:\n\n1. Using OuterRef in filter() - works correctly\n2. Using OuterRef in exclude() - crashes with the error\n3. Using OuterRef in filter(~Q()) - crashes with the error\n\nThe issue is specific to Django's ORM functionality, particularly with the OuterRef class when used in negative conditions. A developer familiar with Django would understand what's happening and could investigate why OuterRef behaves differently in exclude() and ~Q() contexts compared to filter().\n\nThe solution would need to fix the OuterRef implementation to properly handle references to outer queries when used in negative conditions. This is a well-defined bug with a clear reproduction case that can be addressed without additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11740": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user has:\n\n1. Provided complete model definitions showing the initial state (UUIDField) and the changed state (ForeignKey)\n2. Clearly described the expected behavior (creating a dependency on App2 in the migration)\n3. Described the actual behavior (no dependency created)\n4. Included the specific error message: \"ValueError: Related model 'testapp2.App2' cannot be resolved\"\n5. Mentioned the Django version (2.2) and database (PostgreSQL)\n6. Indicated they've attached an archive with source code for reproduction\n\nThe issue is about Django's migration system not correctly creating dependencies when changing a UUIDField to a ForeignKey that references another model. This is a specific technical problem with a clear reproduction case and expected outcome. A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11742": {
        "explanation": "The issue is reasonably well-specified. It describes a clear problem: there's no validation to ensure that Field.max_length is large enough to accommodate the longest value in Field.choices. This can lead to runtime errors when saving records with values that exceed the max_length.\n\nThe solution would involve adding a validation check that compares the max_length of a Field against the length of each choice value, and either raising an error or warning if any choice exceeds the max_length.\n\nWhile the issue doesn't specify exactly where this check should be implemented (during field initialization, model validation, etc.) or what the exact behavior should be (warning vs. error), these are reasonable implementation details that can be determined by examining the codebase and following existing patterns. The core requirement is clear enough to proceed with a solution.\n\nThe issue also doesn't specify which Field class is being referred to, but it's likely referring to a field in a web framework like Django, where fields have both max_length and choices attributes. This can be confirmed by examining the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11749": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in Django's `call_command` function when dealing with mutually exclusive argument groups.\n2. The exact error message is provided: \"django.core.management.base.CommandError: Error: one of the arguments --shop-id --shop is required\"\n3. The code that fails is clearly shown: `call_command('my_command', shop_id=1)`\n4. The working alternative is demonstrated: `call_command('my_command, '--shop-id=1')`\n5. The relevant argument parser configuration is included, showing how the mutually exclusive group is set up.\n6. The reporter has even identified the likely cause in Django's source code, pointing to the specific logic that handles required arguments but doesn't properly handle required argument groups.\n\nWith this information, a developer could reproduce the issue, understand the root cause, and implement a fix to make `call_command` properly handle arguments that are part of a required mutually exclusive group when passed as keyword arguments.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11751": {
        "explanation": "This issue is well-specified with clear objectives. The author wants to make three security headers (SECURE_CONTENT_TYPE_NOSNIFF, SECURE_BROWSER_XSS_FILTER, and X_FRAME_OPTIONS) enabled by default in Django projects. The issue provides:\n\n1. Clear context: References a talk and blog post about security headers\n2. Current state: Shows the output of security checks on a fresh Django project\n3. Specific headers to change: Identifies the three headers that should be enabled by default\n4. Implementation approach: Suggests making them default in startproject settings and changing global defaults with a deprecation period\n\nThe issue provides enough information for a developer to understand what needs to be changed, where the changes should be made, and why they're important. The solution would involve modifying Django's default settings to enable these security headers, which is a straightforward task for someone familiar with the Django codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11754": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies the problem: the `ExceptionReporter` class is hardcoded in Django's `technical_500_response` view, and the requester wants to make it configurable similar to what was done in ticket #29714 for the `AdminEmailHandler`.\n\nThe issue outlines specific tasks:\n1. Make the `ExceptionReporter` class a parameter in the `technical_500_response` view\n2. Provide an example of using `functools.partial` to configure a subclass when specifying `handler500`\n3. Add documentation for `ExceptionReporter`, explaining its relationship with the filter it uses\n4. Add a `DEFAULT_EXCEPTION_REPORTER` setting\n\nThere's also a question about potentially deprecating `DEFAULT_EXCEPTION_REPORTER_FILTER`, but this is presented as an optional consideration.\n\nThe issue provides enough context to understand what needs to be done, references related tickets (#29714 and #25167), and gives clear direction on the implementation approach. While some details about the exact implementation might need to be determined during development, the overall requirements are clear enough to proceed with a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11772": {
        "explanation": "This issue is well-specified with clear problems and a proposed solution. The author identifies three specific issues with the current `make_template_fragment_key` function:\n\n1. Unnecessary use of `urllib.quote` for vary_on args\n2. MD5 hashing not being FIPS compliant on certain systems\n3. Inefficient string joining before hashing instead of using hashlib's update method\n\nThe author has already implemented a solution that addresses these issues by switching to SHA256 and optimizing the function. They've provided links to:\n- Their implementation branch\n- A pull request\n- A repository demonstrating performance improvements\n\nThe author acknowledges the downside that this change would invalidate existing cache keys and suggests potentially changing other MD5-based cache key functions at the same time to minimize disruption.\n\nThe issue provides enough context and specific technical details for a developer to understand the problem, evaluate the proposed solution, and implement or refine it. The only minor ambiguity is whether to proceed with SHA256 or consider other hash algorithms, but this is presented as a discussion point rather than a lack of specification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11790": {
        "explanation": "This issue is well-specified with clear information about what's wrong and where the problem originated. The issue states that:\n\n1. The AuthenticationForm's username field no longer renders with the maxlength HTML attribute\n2. It identifies the specific regression that introduced this issue (#27515 and commit 5ceaf14686ce626404afb6a5fbd3d8286410bf13)\n3. It provides links to discussions about the issue on Google Groups and Django forum\n\nWith this information, a developer can:\n- Examine the identified commit to see what changes were made that affected the maxlength attribute\n- Check the current implementation of AuthenticationForm\n- Understand what needs to be fixed (restore the maxlength HTML attribute to the username field)\n- Implement and test a solution\n\nThe issue provides enough context to understand the problem and its origin, making it straightforward to develop a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11797": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue demonstrates that when using a filtered queryset as a subquery, the GROUP BY clause is incorrectly modified from the original query.\n\nSpecifically:\n1. The user creates a queryset `a` that groups by 'email' and gets the maximum 'id' for each group\n2. When using `a[:1]` as a filter condition in another queryset `b`, the GROUP BY clause in the subquery changes from 'email' to 'id'\n3. The expected behavior is that the GROUP BY clause should remain 'email' in the subquery\n\nThe issue provides:\n- Complete code to reproduce the problem\n- The actual SQL queries generated (with print(a.query) and print(b.query))\n- A clear indication of what's wrong (\"GROUP BY U0.\"id\" should be GROUP BY U0.\"email\"\")\n\nThis is a specific bug in Django's ORM query generation that can be investigated and fixed with the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11808": {
        "explanation": "This issue is well-specified with clear information about what's wrong and how it should be fixed. The issue describes:\n\n1. The specific problem: Django's Model.__eq__ method doesn't return NotImplemented when comparing with objects it doesn't know how to compare against, instead returning False.\n\n2. The exact location of the problematic code: A link to the specific line in Django's codebase (base.py line 526).\n\n3. The expected behavior: According to Python's data model documentation (with link provided), __eq__ should return NotImplemented in these cases.\n\n4. The rationale: The current implementation breaks expected behavior with tools like mock.ANY in testing.\n\n5. The solution is implied: Modify Model.__eq__ to return NotImplemented instead of False when comparing with non-Model objects.\n\nThis provides all the necessary information to understand the issue and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11810": {
        "explanation": "This issue is very well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a complete code example with models and test code\n2. Shown the actual output that demonstrates the problem\n3. Clearly stated the expected behavior: \"The expectation is that the original QuerySet is not mutated, and the two queries are different\"\n4. Identified that the issue occurs with both `select_related` and `prefetch_related`\n5. Referenced the Django documentation that states QuerySet methods return \"a new QuerySet instance\"\n\nThe issue is that when creating a new QuerySet from an existing one with `select_related` and then applying another `select_related` to the new QuerySet, it unexpectedly mutates the original QuerySet. This is clearly a bug since Django's documentation states that QuerySet methods return new instances and shouldn't modify the original.\n\nA developer would have all the information needed to investigate and fix this issue, including a complete reproduction case and clear expectations of correct behavior.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11815": {
        "explanation": "This issue is well-specified with a clear problem description and a concrete example. The issue explains:\n\n1. The problem: When using an Enum object as a default value for a CharField, Django migrations use the value of the Enum object instead of its name, which causes problems with translations.\n\n2. A specific example with code: The issue provides a complete code example showing the model definition with an Enum class and how it's used in a model field.\n\n3. The current behavior: The migration file uses `Status('Good')` which references the value.\n\n4. The expected behavior: The migration should use `Status['GOOD']` which references the name of the enum member.\n\n5. The reason why this is a problem: When translations are applied, the value changes but the name remains constant, causing migrations to fail with a specific error message.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. A developer would need to modify how Django's migration system serializes Enum objects used as default values, ensuring it uses the name rather than the value for stability across translations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11820": {
        "explanation": "This issue is reasonably specified but lacks some details. It clearly identifies a specific error (models.E015) that occurs when Meta.ordering contains \"__pk\" of a related field, and provides an example of the error message. It also identifies a specific commit (440505cb2cadbe1a5b9fba246bcde6c04f51d07e) that introduced the regression.\n\nWhat's missing:\n1. The expected behavior - it's implied but not explicitly stated that this should work rather than raise an error\n2. More context about what the code was doing before the regression\n3. Specific details about the model structure that triggers this issue\n\nHowever, a developer familiar with Django's ORM would understand that:\n- This is about Django models' Meta.ordering attribute\n- The issue is that ordering by a related model's primary key using \"__pk\" syntax is raising an error\n- The error message suggests Django is not recognizing \"option__pk\" as a valid field reference\n- Since it's labeled as a regression, the expected behavior is that this should work\n\nWith this information, a developer could:\n1. Examine the identified commit to understand what changed\n2. Create test models that reproduce the issue\n3. Fix the code to properly handle \"__pk\" in related field references in ordering\n\nThis is enough information to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11829": {
        "explanation": "This issue is well-specified with clear technical details about a bug in Django's `patch_cache_control` function. The issue explains:\n\n1. The problem: `patch_cache_control` doesn't properly handle the \"no-cache\" directive according to HTTP specification (RFC 2616)\n2. The expected behavior based on the HTTP spec: \"no-cache\" can either be a boolean flag or specify field names\n3. The current incorrect behavior: \n   - It doesn't treat \"no-cache\" as a list when field names are specified\n   - Later calls with different field names overwrite earlier ones\n   - `no_cache=True` doesn't take precedence over `no_cache=\"foo\"`\n4. A real-world use case: integration with AWS CloudFront requiring `Cache-Control: no-cache=\"Set-Cookie\"`\n5. Reference to related ticket #13008\n\nThe issue provides enough context about the HTTP specification, the current implementation's shortcomings, and the expected behavior to implement a solution. A developer would need to modify `patch_cache_control` to handle \"no-cache\" as a special case that can be either a boolean flag or a list of field names, with the boolean flag taking precedence.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11848": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific function with the issue: `django.utils.http.parse_http_date`\n2. The exact problem: The function's handling of two-digit years doesn't follow RFC 7231 guidelines\n3. The current behavior: Years 0-69 are interpreted as 2000-2069, and 70-99 as 1970-1999\n4. The expected behavior: Two-digit years should be interpreted according to the RFC 7231 rule - \"more than 50 years in the future\" should be interpreted as the most recent year in the past with the same last two digits\n\nThe issue provides enough context about the RFC requirements and the current implementation to understand what needs to be fixed. A developer would need to modify the function to calculate the appropriate century based on the current year rather than using the hardcoded logic currently in place. This is a straightforward task with a clear success criteria: the function should follow the RFC 7231 guideline for interpreting two-digit years.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11880": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is precisely identified: The `__deepcopy__` method in Django form fields doesn't properly deep copy the `error_messages` dictionary.\n\n2. The exact location of the issue is provided with a GitHub link to the specific line in the codebase.\n\n3. The consequence of the bug is clearly explained: All copies of the same field share the same error_messages dictionary, causing modifications to affect all instances.\n\n4. A specific use case where this becomes problematic is described: Forms that modify error messages dynamically.\n\n5. The issue includes information about which versions are affected (Django 1.11 and later).\n\nThe solution would require modifying the `__deepcopy__` method to properly deep copy the `error_messages` dictionary. This is a straightforward fix with a clear success criterion: after the fix, modifying error messages on one form field instance should not affect other instances.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11883": {
        "explanation": "This issue is well-specified. It clearly requests that the `cache.delete()` method be modified to return a boolean value indicating whether the deletion was successful or not. The description provides:\n\n1. A clear goal: Make `cache.delete()` return success/failure information\n2. Rationale: Useful for complex caching/locking systems and logging\n3. Feasibility context: Other cache implementations (Memcache, Redis) already provide this functionality\n4. Implementation hints: For file, in-memory, and database backends, it should be based on the number of returned rows\n\nThe issue doesn't provide specific code examples or implementation details, but for an experienced developer with access to the codebase, this is sufficient information to implement the requested change. The task is straightforward - modify the existing `cache.delete()` method to return a boolean value indicating success instead of returning nothing (or whatever it currently returns).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11885": {
        "explanation": "This issue is well-specified with clear requirements for implementation. The issue describes:\n\n1. The current behavior: When using on_delete=models.CASCADE, the deletion.Collector performs separate DELETE queries for each foreign key relationship.\n\n2. The desired behavior: Combine DELETE queries that target the same table but with different conditions into a single query using OR.\n\n3. A concrete example with model definitions and the exact SQL queries that are currently executed versus what should be executed after the fix.\n\nThe issue provides enough technical detail about the Django ORM's deletion mechanism, specifically mentioning the deletion.Collector class that would need to be modified. The example clearly illustrates the problem and the expected solution.\n\nA developer would be able to:\n- Locate the deletion.Collector class in the Django codebase\n- Identify the logic that generates the \"fast delete\" queries\n- Modify this logic to combine queries targeting the same table\n- Test the implementation using the example models provided\n\nThe optimization goal is also clear: reduce database roundtrips by combining related DELETE queries.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11891": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific component (ConditionalGetMiddleware) and its interaction with apache x-sendfile (django-sendfile).\n\n2. It clearly explains the bug: When using x-sendfile, response.content is empty, causing the same ETag to be generated regardless of file changes. This leads to 304 responses even when the file has changed.\n\n3. The expected behavior is stated: Django shouldn't return 304 if ETag is the same but Last-Modified has changed.\n\n4. There's a reference to a related issue (#29241) which provides additional context.\n\nThe issue provides enough information for a developer to:\n- Understand the problem scenario\n- Identify the components involved\n- Understand the current incorrect behavior\n- Know what the correct behavior should be\n- Have a starting point for investigation (the ConditionalGetMiddleware's get_conditional_response() method)\n\nA developer could proceed with a solution by examining how the middleware handles ETags and Last-Modified headers, and modifying the logic to consider Last-Modified even when ETags match.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11893": {
        "explanation": "This issue is well-specified. It clearly identifies the problem: Django's DateTimeField does not accept ISO 8601 formatted date strings that use the 'T' character as a separator between date and time components (YYYY-MM-DDTHH:MM:SS), only accepting spaces as separators. \n\nThe issue provides:\n1. The specific field type affected (DateTimeField)\n2. The exact format that's not working (ISO 8601 with 'T' separator)\n3. The current behavior (only space separators are accepted)\n4. The expected behavior (should accept ISO 8601 standard format with 'T' separator)\n\nA developer would be able to locate the DateTimeField implementation in Django, identify the parsing logic that handles datetime strings, and modify it to accept the 'T' separator in addition to spaces. This is a straightforward enhancement request with a clear scope and acceptance criteria.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11894": {
        "explanation": "This issue is well-specified with clear context and requirements. The problem is clearly defined: Django's cookie-setting methods (.set_cookie and set_signed_cookie) accept a samesite parameter that defaults to None, but Django doesn't actually set the SameSite attribute to \"None\" in the cookie when this value is provided. This is becoming problematic because Chrome (and potentially other browsers) now require the SameSite attribute to be explicitly set to \"None\" for cross-origin requests, rather than assuming it from the absence of the attribute.\n\nThe issue includes a reference to Chrome's documentation about this change, providing additional context. The solution would involve modifying Django's cookie-setting functionality to explicitly set \"SameSite=None\" in the cookie when the samesite parameter is None, rather than omitting the attribute entirely.\n\nA developer with access to Django's codebase would be able to locate the relevant cookie-setting methods, understand how they currently handle the samesite parameter, and modify them to explicitly set \"SameSite=None\" when appropriate.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11903": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue identifies a specific bug in the `fetch_command()` method of `ManagementUtility` where it incorrectly reports \"No Django settings specified\" even when settings are configured.\n\n2. It clearly explains the root cause: the method only checks for `DJANGO_SETTINGS_MODULE` environment variable but doesn't account for settings configured programmatically via `settings.configure()`.\n\n3. It points out that the parent `execute` method correctly checks `settings.configured` flag, suggesting this approach should be used in `fetch_command()` as well.\n\n4. The issue provides code snippets showing the problematic implementation and hints at the solution (using `settings.configured` instead of just checking the environment variable).\n\nA developer can understand exactly what needs to be fixed: modify the `fetch_command()` method to check `settings.configured` rather than only checking for the presence of `DJANGO_SETTINGS_MODULE` in environment variables. This would ensure the method correctly recognizes settings configured programmatically.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11905": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem: the `__isnull` lookup in Django's ORM currently accepts non-boolean values, which can lead to unexpected behavior. The issue explains that:\n\n1. Using truthy/falsey values doesn't promote INNER JOIN to OUTER JOIN as expected\n2. This behavior is undocumented and untested\n3. The proposed solution is to raise an error for non-boolean values\n\nWhat makes this reasonably specified rather than well-specified is that it lacks some implementation details:\n- It doesn't specify exactly where in the codebase this change should be made\n- It doesn't provide examples of the problematic behavior\n- It doesn't specify what type of error should be raised\n- It doesn't address backward compatibility concerns\n\nHowever, an experienced Django developer would be able to locate the relevant code for the `__isnull` lookup, implement validation for boolean values, and raise an appropriate exception (likely ValueError or TypeError) when non-boolean values are provided. The goal of preventing confusion and maintaining consistency is clear enough to guide the implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-11916": {
        "explanation": "This issue is well-specified with clear performance problems and proposed solutions. The description provides:\n\n1. A detailed explanation of the data model relationships (one-to-one, many-to-many, many-to-one)\n2. Comprehensive profiling data showing exactly where the bottleneck is (57% of runtime spent creating unnecessary querysets)\n3. Two specific solution approaches with a clear preference for option 2\n4. Expected performance improvement (50% speedup for some prefetch_related queries)\n5. Technical challenges that need to be addressed (creating a proxy object that behaves like a queryset)\n\nThe issue author has done thorough analysis and provided concrete metrics. A developer would have enough information to implement the preferred solution (lazy instantiation of querysets for prefetch_related) without needing further clarification. The technical requirements are clear: create a proxy object that behaves like a queryset but doesn't actually create the queryset until needed, while maintaining type compatibility.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11951": {
        "explanation": "This issue is well-specified with clear information about the problem and a proposed solution:\n\n1. The issue identifies a specific bug in Django's bulk_create method where the batch_size parameter overrides the compatible batch size calculation.\n2. It provides exact file and line references to the problematic code in the Django codebase.\n3. It points to a similar implementation in bulk_update that correctly handles this situation by taking the minimum of the user-provided batch_size and the calculated max_batch_size.\n4. It proposes a specific code change: replacing the current logic with `batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size`.\n\nThe issue description provides enough context to understand the problem, locate the affected code, and implement the suggested fix. The solution approach is clearly defined and reasonable, making it straightforward to create a PR that addresses this issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11964": {
        "explanation": "This issue is well-specified with clear examples and expected behavior. The problem is clearly defined: when using Django's TextChoices or IntegerChoices with model fields, the value returned by a field getter has an inconsistent type behavior between freshly created objects and retrieved objects.\n\nThe issue includes:\n1. A complete code example showing the model definition with TextChoices\n2. Test cases that demonstrate the problem\n3. The actual test output showing the failure\n4. A clear explanation of the expected behavior (that str(my_object.my_str_value) should return \"first\" rather than \"MyChoice.FIRST_CHOICE\")\n5. The practical impact of this inconsistency (problems when communicating with external APIs)\n\nThe issue demonstrates that when creating a new object with an enum value (MyChoice.FIRST_CHOICE), the field retains the enum type, but when retrieving from the database, it becomes a string. This inconsistency is clearly problematic and needs to be fixed to ensure consistent behavior regardless of whether an object is freshly created or retrieved from the database.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-11999": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible example. The user is reporting that in Django 2.2+, they cannot override the `get_FOO_display()` method for model fields with choices, whereas this was possible in Django 2.1.\n\nThe issue includes:\n1. A specific version range where the behavior changed (works in 2.1, broken in 2.2+)\n2. A complete code example showing the model definition with a choices field\n3. The expected behavior (custom method should return \"something\")\n4. The actual behavior (method returns the choice display value instead of the overridden value)\n\nThe problem is clearly defined and would be straightforward for a developer to investigate by examining the changes in Django's model field handling between versions 2.1 and 2.2. The solution would likely involve understanding why Django is bypassing the overridden method in newer versions and determining if this is a bug or an intentional design change.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12009": {
        "explanation": "This issue is well-specified. It clearly identifies a redundancy in Django's installation process where two scripts with the same functionality are being installed:\n\n1. `/usr/bin/django-admin.py` - installed via the `scripts` parameter in setup.py\n2. `/usr/bin/django-admin` - installed via the `entry_points` parameter in setup.py\n\nThe issue provides:\n- The exact file paths of the redundant scripts\n- The content of both scripts showing they perform the same function\n- The specific setup.py configuration causing each installation\n- A clear statement of the problem (redundancy)\n- A suggested solution (install only one of these scripts)\n\nThe issue provides all the necessary information to understand the problem and implement a solution. A developer would need to modify Django's setup.py file to either remove the script entry or the entry_point, deciding which one to keep based on best practices or backward compatibility considerations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12039": {
        "explanation": "This issue is well-specified with clear examples of the current behavior and expected behavior. The issue describes a whitespace formatting problem in Django's SQL generation for CREATE INDEX statements. There are two specific cases identified:\n\n1. Missing space between column name and DESC keyword: `(\"name\"DESC)` instead of `(\"name\" DESC)`\n2. Extra space after opclasses: `(\"name\" text_pattern_ops )` instead of `(\"name\" text_pattern_ops)`\n\nThe issue even points to the specific file and line number in the codebase where the problem occurs (django/db/backends/ddl_references.py, line 87), explaining that the bug is caused by the assumption that col_suffixes would be empty for ascending order, but it actually contains empty strings.\n\nWith this information, a developer would be able to understand the problem, locate the relevant code, and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12050": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: Query.resolve_lookup_value is coercing list values to tuples\n2. When it was introduced: In PR #30687\n3. The impact: Breaking exact value queries for specific field types like PickledField\n4. The expected behavior: The return type should match the input iterable type\n\nThe issue provides enough context for a developer familiar with the codebase to understand what needs to be fixed. The developer would need to:\n1. Look at the changes in PR #30687\n2. Find where Query.resolve_lookup_value is handling iterables\n3. Modify the code to preserve the original type of the iterable instead of converting to tuple\n\nWhile the issue doesn't provide code examples or specific test cases, it gives enough information for someone familiar with Django's ORM to understand and address the problem. A developer would need some knowledge of Django's query resolution system, but the issue itself is clear about what's wrong and what needs to be fixed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12062": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The goal is to add a new setting `DATABASE['TEST']['MIGRATE'] = False` that would disable all migrations on a particular database during tests.\n2. The implementation approach is even suggested: hook into `django.db.migrations.loader.MigrationLoader.migrations_module()` and return `None`.\n3. It references a previous issue (#24919) which provides context and indicates this is an extension of existing functionality.\n\nThe issue provides both the what (disable migrations during tests via a configuration setting) and the how (modify a specific method to return None). This gives a clear direction for implementation and what would constitute a successful solution. A developer familiar with Django's database migration system would have enough information to implement this feature without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12073": {
        "explanation": "This issue is well-specified with clear context and a proposed solution. The description:\n\n1. Clearly identifies the problem: The `InvalidQuery` exception is barely documented and only mentioned once in the documentation.\n\n2. Explains current usage: It's used for documented `QuerySet.raw` usage and for field deferring select related misuse.\n\n3. Proposes a specific solution: Replace documented usage with `FieldDoesNotExist`, replace undocumented usage with `FieldError`, and provide a deprecation shim to warn on `except InvalidQuery` usages.\n\nA developer would be able to:\n- Locate the `InvalidQuery` exception in the codebase\n- Find its current usages\n- Implement the proposed replacements\n- Add a deprecation warning mechanism\n\nThe issue provides enough context and direction to implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12091": {
        "explanation": "This issue is well-specified for implementation. It clearly states:\n\n1. The specific feature to be deprecated: `HttpRequest.is_ajax()`\n2. The rationale for deprecation:\n   - It relies on a non-standard header (X-Requested-With)\n   - jQuery's popularity is decreasing\n   - Better alternatives exist (using the standard Accept header)\n3. Precedent from another framework (Flask deprecated a similar feature)\n\nThe task is straightforward: implement a deprecation for this method following Django's standard deprecation policy. This would typically involve:\n- Adding a deprecation warning to the method\n- Documenting the deprecation in release notes\n- Suggesting alternatives in the warning message\n\nThere's no ambiguity about what needs to be done, and Django has established patterns for deprecating features.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12113": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific error: \"sqlite3.OperationalError: database is locked\" occurring when using persistent SQLite databases with the --keepdb option.\n\n2. It provides the exact test that fails: admin_views.test_multidb\n\n3. It includes a complete code diff showing the changes made to enable persistent test databases.\n\n4. It provides the full command used to run the test: \"tests/runtests.py admin_views.test_multidb -v 3 --keepdb --parallel 1\"\n\n5. It includes a complete stack trace showing exactly where and how the error occurs.\n\nThe issue is clearly about a database locking problem that happens specifically when using persistent SQLite test databases with the --keepdb option in Django's test suite. The goal would be to fix the test or the framework to allow persistent SQLite test databases to work properly with the admin_views.test_multidb test.\n\nA developer would have all the necessary information to reproduce the issue and begin investigating potential solutions, such as examining how database connections are managed in the test or implementing proper locking mechanisms.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12121": {
        "explanation": "This issue is well-specified with clear context and a specific problem to solve. The user describes:\n\n1. The exact scenario: Using multiple instances of a named route where the correct one is chosen based on URL converter matching\n2. The current behavior: `ValueError` in `to_url` is not caught, unlike in `to_python` where it indicates no match\n3. The workaround they discovered: Returning an empty string from `to_url` causes the regex check to fail\n4. Two specific proposed solutions:\n   - Update `_reverse_with_prefix` to catch `ValueError` from `to_url` similar to how `to_python` works\n   - Update documentation to clarify that `to_url` should return a non-matching string to decline a match\n\nThe issue includes enough technical details about the Django URL routing system and converter behavior to understand the problem. The attached file (though not visible in this description) is referenced as demonstrating the issue. The request is specific enough that a developer could implement either of the proposed solutions without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12125": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is precisely described: when using an inner class that inherits from django.db.models.Field within a Django model, the migration file incorrectly references the inner class as if it were a top-level class in the module.\n\nThe issue provides:\n1. A minimal code example to reproduce the issue\n2. The actual output generated by makemigrations\n3. The expected output (what the path should be)\n4. A real-world example showing the practical impact\n\nThe bug is clearly in Django's migration system, which is not correctly handling the path to inner classes when generating migration files. The solution would involve modifying how Django's migration system resolves and serializes references to inner field classes.\n\nA developer would have enough information to locate the relevant code in Django's migration system and fix the path resolution for inner classes without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12143": {
        "explanation": "This issue is well-specified with clear details about the problem and its solution:\n\n1. The issue identifies a specific bug: using string formatting to create regex patterns without escaping special characters in the `prefix` parameter can lead to data loss in the admin changeform view.\n\n2. The exact location of the problematic code is provided with a GitHub link to the specific line in `admin/options.py`.\n\n3. The issue explains why this is a problem - the `prefix` parameter has no restrictions and could contain regex special characters, which would break the regex pattern.\n\n4. A clear solution is proposed: use `re.escape()` to properly escape special characters.\n\n5. The reporter has already done preliminary investigation to determine this is likely an isolated issue (by grepping the codebase).\n\n6. The issue notes this is a regression and identifies the specific commit that introduced it.\n\nWith this information, a developer could easily locate the problem, understand the issue, and implement the suggested fix (using `re.escape()` on the `prefix` parameter). The scope is well-defined and the solution approach is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12148": {
        "explanation": "The issue describes a discrepancy between the `get_absolute_url()` method in the FlatPage model and the `reverse()` function in Django. Specifically:\n\n1. The FlatPage model implements `get_absolute_url()` without using `reverse()`\n2. There's a comment suggesting this approach handles SCRIPT_NAME issues\n3. The link in the admin interface doesn't work when using a prefix for flatpages URLs\n4. The templatetag for resolving a flatpage works correctly\n\nThe issue is reasonably clear about the problem: there's inconsistent behavior between two methods of generating URLs for FlatPages, causing admin interface links to break when URL prefixes are used.\n\nHowever, the issue lacks some specifics that would make it well-specified:\n- No code examples showing the current implementation\n- No specific error messages or traceback\n- No clear indication of the expected behavior or proposed solution\n- No mention of which Django version this affects\n\nA developer familiar with Django's FlatPage model would likely understand the issue and be able to investigate, but would need to examine the codebase to fully understand the problem and determine an appropriate solution. The solution would likely involve modifying the `get_absolute_url()` method to use `reverse()` or ensuring both methods handle URL prefixes consistently.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12153": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue occurs in a multi-database environment when the default database is not set.\n2. The specific migration file causing the problem is identified (0011_update_proxy_permissions).\n3. The exact line of code causing the issue is provided (line 42 in django/contrib/auth/migrations/0011_update_proxy_permissions.py).\n4. The error message is included: \"django.core.exceptions.ImproperlyConfigured: settings.DATABASES is improperly configured...\"\n5. There's a reference to Django documentation stating that omitting the default database should be allowed.\n\nThe issue is that the migration is trying to use the default database connection even when it's not configured, which contradicts Django's documentation saying the default database can be omitted in multi-database setups.\n\nA solution would need to modify the migration to respect multi-database configurations where the default database is not set. This is a clear, specific problem with enough context to implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12155": {
        "explanation": "This issue is well-specified with clear information about:\n\n1. The problem: docutils reports an error when rendering view docstrings where the first line is not empty\n2. The current behavior: admindoc works correctly only with docstrings where the first line is empty\n3. A specific example of the problematic code pattern\n4. The exact error message that occurs\n5. The root cause: in the `trim_docstring` function, the indentation calculation includes the first line which has 0 indentation\n6. A proposed solution: skip the first line when calculating the minimum indentation\n\nThe issue provides enough context to understand the problem, locate the relevant code, and implement the suggested fix. The solution is straightforward - modify the `trim_docstring` function to exclude the first line when calculating indentation. This is a well-defined task with a clear success criterion: docstrings with content on the first line should render correctly without errors.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12161": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: `through_defaults` in Django doesn't support callable values, unlike other similar features like `defaults` in `get_or_create` or the `default` argument in model fields.\n\n2. The current behavior is explained: callable values are passed through without being evaluated, resulting in unexpected behavior (e.g., storing the function's repr in a CharField).\n\n3. The solution approach is indicated: The issue mentions that fixing this is \"pretty straightforward\" and references an attached pull request, suggesting the implementation pattern should follow other similar Django features that already support callable defaults.\n\n4. The context is provided: Reference to the original ticket (#9475) that introduced the feature and a link to the documentation for the similar `get_or_create` method that already supports callable defaults.\n\nA developer familiar with Django would understand exactly what needs to be done: modify the `through_defaults` functionality to evaluate callable values before using them, similar to how Django handles callable defaults in other parts of the framework.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12172": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem: Django's \"async unsafe\" checks are causing issues for users in Jupyter notebook environments, where apparently-synchronous code is executed in an async environment. The issue also proposes a solution: adding a flag (either a setting or environment variable) to disable these async-unsafe checks.\n\nThe core requirements are clear:\n1. Add a way to disable Django's async-unsafe checks\n2. Preferably implement this as an environment variable (though a setting is also mentioned as an option)\n3. Ensure the solution works well with Jupyter notebooks\n\nWhat's missing are some implementation details:\n- The exact name of the environment variable/setting\n- The specific behavior when the flag is enabled (complete disabling vs. warning)\n- How this interacts with Django's existing async safety mechanisms\n- Any potential security implications of disabling these checks\n\nHowever, these details can reasonably be determined during implementation by following Django's conventions and best practices. The issue provides enough context about the problem and desired solution for a developer familiar with Django to make a meaningful attempt at solving it.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12187": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly articulated: the Django ManifestStaticFilesStorage creates a manifest file that lists all static files, which is publicly accessible by default, potentially exposing sensitive information. The issue description provides:\n\n1. A clear explanation of the security concern\n2. Specific risks associated with the problem\n3. The current workaround the reporter is using\n4. Two potential solution approaches\n\nWhat makes this not fully \"well-specified\" is that it doesn't provide a concrete implementation proposal or specific technical requirements for the solution. The suggested solutions are conceptual rather than detailed technical specifications. \n\nThe core request is to \"Allow configuration of where to save staticfiles manifest\" which is clear enough, but the exact implementation details would need to be determined during development. A developer would need to:\n1. Determine how to modify ManifestStaticFilesStorage to accept a configurable path for the manifest\n2. Decide how this would integrate with different storage backends (local filesystem, S3, etc.)\n3. Consider backward compatibility\n\nDespite these gaps, there's enough information to understand the problem and begin working on a solution based on the suggested approaches or by developing an alternative approach that addresses the core concern.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12193": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific bug in Django's SplitArrayField when used with BooleanField.\n2. It clearly explains the behavior: after the first True value in the initial data array, all subsequent widgets are incorrectly checked regardless of their actual values.\n3. It identifies the root cause: the CheckboxInput widget's get_context() method modifies the attrs dictionary by adding 'checked': True, and this modification persists for subsequent widgets.\n4. It even provides a link to the specific line in the Django codebase that causes the issue.\n\nA developer would have enough information to:\n1. Reproduce the issue using SplitArrayField with BooleanField\n2. Understand that the problem is in how the attrs dictionary is being modified\n3. Know exactly where in the code to look (the CheckboxInput widget's get_context method)\n4. Develop a solution that likely involves creating a fresh attrs dictionary for each widget or otherwise preventing the 'checked' attribute from persisting between widgets\n\nThe issue description provides all the necessary technical details to understand and fix the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12198": {
        "explanation": "The issue description provides some context but lacks clarity in several areas:\n\n1. It mentions a problem with `sensitive_variables()` decorator affecting the signature of decorated functions, specifically in the context of Django's authentication backends.\n\n2. It states that when the `authenticate` method of a custom `AuthenticationBackend` is decorated with `sensitive_variables`, `inspect.getcallargs` will \"always match\" - but it's not clear what this means exactly.\n\n3. It mentions that calling the authenticate function will attempt to call the backend with \"any set of credentials\" and raise an uncaught TypeError for an \"unmatching backend\" - but doesn't explain what constitutes an \"unmatching backend\".\n\n4. It states this worked in version 1.6 (presumably Django 1.6), implying it's a regression.\n\nHowever, the issue lacks:\n- Clear steps to reproduce the problem\n- Expected vs. actual behavior examples\n- Code samples showing the issue\n- Details about what \"preserve the signature\" means in this context\n- Explanation of what the `sensitive_variables()` decorator is supposed to do\n\nWhile there's enough information to understand the general area of the problem (Django authentication backends and a decorator that's affecting function signatures), a developer would need to make assumptions about the exact nature of the issue and how to fix it. They would likely need to research Django's authentication system and the `sensitive_variables()` decorator to fully understand the problem.",
        "label": "VAGUE"
    },
    "django__django-12209": {
        "explanation": "This issue is well-specified with clear details about a regression in Django's behavior between versions 2.2 and 3.0. The reporter provides:\n\n1. A complete code example showing the model definition\n2. The exact commands that demonstrate the issue\n3. The expected behavior (INSERT followed by UPDATE in Django 2.2)\n4. The new behavior (two INSERTs in Django 3.0, with the second one failing)\n5. Additional context about when the behavior changes (when default=uuid4 is removed)\n6. A related ticket that might explain the cause\n7. A real-world impact (affects loaddata management command)\n8. A suggestion for what the intended behavior might have been\n\nThe issue clearly identifies a backward compatibility problem with saving model instances that have an explicit primary key value when that field has a default value. A developer would have enough information to reproduce the issue, investigate the changes between Django 2.2 and 3.0 (particularly around the referenced ticket #29260), and implement a fix that restores the expected behavior or provides a more backward-compatible solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12212": {
        "explanation": "This issue is well-specified with clear details about the problem and a potential solution. The description includes:\n\n1. Complete error tracebacks showing exactly where the error occurs\n2. A clear explanation of the root cause - a variable 'pk' being referenced before assignment, which masks the real error of 'NoneType' object not being iterable\n3. The specific file and line number where the problem occurs (django/core/serializers/python.py line 139)\n4. A proposed solution: changing `for pk in field_value:` to `for pk in field_value or []:`\n5. Context about the likely cause - a field that was migrated from ForeignKey to ManyToMany\n\nThe issue provides enough information for a developer to understand the problem, locate the affected code, and implement the suggested fix. The reporter has done thorough debugging work and even provided a working solution. A developer could confidently create a PR to fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12225": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: When an inline admin model has multiple foreign keys to the same parent model, Django raises admin.E202 error with a message that doesn't guide users on how to fix it.\n\n2. The current behavior is explicitly described with an example error message: \"(admin.E202) 'account.PaymentApplication' has more than one ForeignKey to 'account.Invoice'.\"\n\n3. The requested solution is specific: The error message should be improved to recommend specifying the `fk_name` parameter, which is the standard way to resolve this issue in Django admin.\n\nThe issue provides enough context for a developer familiar with Django admin to understand the problem and implement a solution. The fix would involve modifying the error message generation for admin.E202 to include advice about using the `fk_name` parameter. This is a straightforward enhancement to an existing error message that would improve developer experience.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12231": {
        "explanation": "This issue is reasonably well-specified. The user has:\n\n1. Clearly identified the problem: Django's related manager `set()` method is not properly handling string values for foreign keys, causing unnecessary database operations (removing and re-adding all foreign keys).\n\n2. Pointed to the specific code location in Django where the issue occurs: https://github.com/django/django/blob/master/django/db/models/fields/related_descriptors.py#L1004\n\n3. Explained their current workaround: converting strings to integers before calling `set()`.\n\nHowever, there are some aspects that could be clearer:\n- The issue doesn't provide a minimal reproducible example showing the problem\n- It doesn't specify the Django version where this occurs\n- It doesn't clearly state what the expected behavior should be (though it's implied)\n\nThe user is asking several questions about how to handle this issue, which gives flexibility in how to approach a solution. The core problem is clear enough that a developer could investigate the code at the specified location and understand how string values are being compared to integer primary keys, leading to the described behavior.\n\nA solution would likely involve modifying the `set()` method to properly handle type conversion when comparing foreign key values, or to provide clearer documentation/warnings about the expected types.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12237": {
        "explanation": "This issue is very well-specified with a clear problem description, reproduction steps, and analysis of the root cause. The reporter has:\n\n1. Clearly identified the problem: slugify() with allow_unicode=True doesn't properly handle the Turkish/Azerbaijani letter \"\u0130\" (capital I with dot above), resulting in validation errors.\n\n2. Provided a minimal reproducible example with code snippets showing the issue.\n\n3. Included detailed analysis of why the problem occurs: Python's .lower() converts \"\u0130\" to a combination of \"i\" plus a \"COMBINING DOT ABOVE\" character, which fails the slug validation regex.\n\n4. Proposed a potential solution: changing the order of operations in slugify() to apply .lower() before the regex substitution.\n\n5. Included additional context about Unicode standards and character mappings.\n\n6. Provided a GitHub repository with further analysis and test cases.\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and implement a solution. The reporter has done thorough research and even proposed a reasonable fix. This is an exemplary bug report that leaves little room for ambiguity.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12262": {
        "explanation": "This issue is well-specified with clear examples of the problem and expected behavior. The issue describes two specific bugs in Django's template tag system:\n\n1. When creating a simple tag with keyword-only arguments that have default values, attempting to use those keyword arguments in a template raises an incorrect error: \"'hello' received unexpected keyword argument 'greeting'\" - even though 'greeting' is a valid parameter.\n\n2. When supplying the same keyword argument multiple times, it raises the wrong error message: \"'hi' received unexpected keyword argument 'greeting'\" instead of the expected \"'hi' received multiple values for keyword argument 'greeting'\".\n\nThe issue includes:\n- Clear code examples showing the template tag definitions\n- Template usage examples that trigger the bugs\n- The incorrect error messages that are currently shown\n- An indication of what the correct behavior should be\n- Information about which versions are affected\n\nThe reporter even mentions they already have a fix ready, which suggests they understand the problem thoroughly. This is a well-defined bug report with reproducible examples and clear expectations for the solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12273": {
        "explanation": "This issue is reasonably well-specified. The description clearly identifies a bug where resetting the primary key of a child model (by setting it to None) doesn't work as expected. The issue includes:\n\n1. A clear code example showing the model structure (Item as parent, Derived as child)\n2. The specific method being tested (reset() which sets uid=None)\n3. The expected behavior (overwriting the existing object on save())\n4. A test case that demonstrates the failure\n5. The Django version (2.1.2)\n\nThe issue is that when a child model (Derived) inherits from a parent model (Item), setting the primary key to None in the reset() method doesn't cause the save() operation to create a new object as expected.\n\nWhat makes this \"reasonably specified\" rather than \"well specified\" is that:\n1. It doesn't explicitly state what the expected behavior should be (though it's implied)\n2. It doesn't explain why the current behavior is wrong according to Django's documentation\n3. It doesn't mention if this worked in previous Django versions\n\nHowever, there's enough information for a developer familiar with Django's ORM to understand the issue and attempt a solution. The code example is complete enough to reproduce the problem.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12276": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes that:\n\n1. The `use_required_attribute()` method currently exists in `ClearableFileInput` but should be moved to the parent class `FileInput`.\n2. The purpose is to prevent the \"required\" attribute from appearing in HTML when initial data (an existing file) is already present.\n3. The issue includes a specific link to the relevant code in the Django codebase.\n4. The rationale is provided - when editing a model instance that already has a file, the field shouldn't be marked as required in HTML.\n\nThe issue author even considers potential edge cases by asking if there are use cases where this change might not be desirable.\n\nA developer could implement this solution by:\n1. Moving the `use_required_attribute()` method from `ClearableFileInput` to its parent class `FileInput`\n2. Ensuring the behavior remains the same (not displaying required attribute when initial data exists)\n3. Testing that this works for both `FileInput` and `ClearableFileInput` widgets\n\nThe issue is specific, includes the location of the code to be modified, explains the rationale, and proposes a clear solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12286": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly states the expected behavior according to Django documentation: when a sublanguage is specified but not available, Django should fall back to the base language.\n\n2. It provides a specific example of the problem: setting LANGUAGE_CODE = \"de-at\" raises a SystemCheckError (translation.E004) even though according to documentation it should fall back to \"de\".\n\n3. It includes the exact error message being received.\n\n4. It provides a contrasting example (es-ar) that works correctly, helping to isolate the issue.\n\n5. The Django version (3.0.2) is specified, which is important for reproducing the issue.\n\nThe issue is describing a discrepancy between documented behavior and actual behavior in Django's language handling system. The problem is that Django is raising an error for sublanguages when it should be falling back to base languages. This is specific enough to investigate and fix without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12299": {
        "explanation": "This issue is reasonably well-specified. It describes a problem where QuerySet operations like `union()`, `intersection()`, and `difference()` don't properly work with subsequent `update()`/`delete()` operations. \n\nThe example provided shows that when using `union()` to combine QuerySets, the subsequent `update()` operation only applies to the first part of the union (only applying the first filter condition) rather than to the entire combined QuerySet.\n\nThe issue is asking for a descriptive error to be raised when these operations are attempted, rather than silently failing or producing incorrect results. This makes sense as a solution approach since these operations are not working as expected.\n\nWhat's missing is:\n1. The exact error message text that should be displayed\n2. Whether this should be implemented for all three operations or if there are different behaviors for each\n3. The specific Django version this applies to\n\nHowever, these details can be reasonably inferred during implementation. The core requirement - to raise a descriptive error when these operations are incorrectly combined - is clear enough to proceed with a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12304": {
        "explanation": "This issue is well-specified with a clear problem and solution:\n\n1. Problem: Django's enumeration types (Choices classes) cannot be used in templates for comparison because they are callable, and Django templates automatically call callables without arguments, causing errors.\n\n2. Example: The template code `{% if student.year_in_school == YearInSchool.FRESHMAN %}` fails because Django tries to call `YearInSchool` without the required value argument.\n\n3. Proposed solution: Add `do_not_call_in_templates = True` to the Choices classes to prevent Django from automatically calling them in templates.\n\nThe issue provides enough context to understand the problem, includes a specific example of the failing code, and proposes a straightforward solution. A developer familiar with Django would understand exactly what needs to be fixed and how to implement the solution. The fix would involve modifying the enumeration/Choices classes in Django to include the `do_not_call_in_templates = True` attribute.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12308": {
        "explanation": "This issue is well-specified with clear information about:\n\n1. The problem: JSONField values are displayed incorrectly in the admin interface when they are readonly. They appear as Python dictionary representation (with single quotes) rather than valid JSON (with double quotes).\n\n2. A specific example: `{\"foo\": \"bar\"}` is displayed as `{'foo': 'bar'}` which is not valid JSON.\n\n3. A proposed solution path: Add a special case in `django.contrib.admin.utils.display_for_field` to call the `prepare_value` method of the JSONField instead of directly using `json.dumps`.\n\nThe issue provides enough context to understand what's happening, where the problem is occurring, and even suggests a reasonable approach to fix it. A developer could take this information and implement a solution without needing additional clarification. The file path and function that needs modification are explicitly mentioned, making it straightforward to locate the code that needs changing.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12313": {
        "explanation": "This issue is well-specified with clear reproduction steps and detailed error information. The problem is clearly defined: Django's makemigrations command doesn't properly detect model name case changes (e.g., changing \"Rubrictype\" to \"RubricType\"), resulting in:\n\n1. No RenameModel operation being generated for the case change\n2. Repeated AlterField operations being generated on subsequent makemigrations runs\n3. Errors when trying to manually add a RenameModel operation\n\nThe issue includes:\n- Complete code examples showing the model definitions before and after changes\n- Exact command outputs showing the problem\n- Error messages and stack traces\n- A workaround that was discovered (using an intermediate name change)\n\nThe expected behavior is implied: Django should detect case-only model name changes and generate proper RenameModel operations, just as it does for other name changes. The issue provides enough information for a developer to reproduce the problem and understand what needs to be fixed in Django's migration system.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12325": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue describes a specific problem with Django's Multi-Table Inheritance (MTI) implementation where the order of OneToOneField declarations matters when one field is a parent_link and another is a regular OneToOneField to the same model.\n\nThe issue demonstrates:\n1. A concrete code example showing the problem\n2. The exact error message received\n3. A working solution (changing the order of field declarations)\n4. A clear question about whether this is expected behavior\n\nThe issue is asking whether this ordering requirement is by design or a bug, and if it's by design, why the parent_link marker isn't sufficient to identify the inheritance relationship regardless of declaration order.\n\nA developer could reproduce this issue and investigate the Django codebase to understand why the order matters in this specific case, and potentially fix it so that the explicit parent_link marker is respected regardless of field declaration order.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12360": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: when model names approach the 100-character limit, the auto-generated permission codenames (add_*, change_*, delete_*, view_*) can exceed the 100-character limit in the Permission model, causing database errors.\n\n2. A concrete example is provided with:\n   - A sample model with a 98-character name\n   - The exact error message received: \"django.db.utils.DataError: value too long for type character varying(100)\"\n\n3. The context is clear: this happens when using Django's contrib.auth with models that have very long names.\n\n4. The reporter even mentions their workaround (setting Meta.default_permissions), showing they understand the issue.\n\n5. The request is specific: add validation to check for this edge case to prevent the confusing error.\n\nThe solution direction is implied: implement a system check that warns users when model names are long enough that they would generate permission codenames exceeding the 100-character limit. This is a straightforward validation task with clear acceptance criteria.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12364": {
        "explanation": "This issue is well-specified with clear context and a specific problem to solve. The user identifies that:\n\n1. The `_get_deterministic_ordering()` function in Django's admin changelist looks for `unique_together` in a model's Meta class to determine if a model is already totally ordered.\n\n2. The function should also check for `UniqueConstraint` objects, which are not currently considered by the algorithm.\n\n3. Django's documentation recommends using `UniqueConstraint` over `unique_together`, suggesting the latter may be deprecated in the future.\n\nThe solution path is clear: modify the `_get_deterministic_ordering()` function to also check for `UniqueConstraint` objects when determining if a model is already totally ordered. The user has provided context with a link to the relevant PR (#10692) that implemented the original functionality, which gives a good starting point for understanding the code that needs to be modified.\n\nThe issue has a well-defined scope, a clear problem statement, and a straightforward solution approach. There are no ambiguities about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12394": {
        "explanation": "This issue is reasonably well-specified. It describes a specific error that occurs when raising an error about protected related objects during deletion. The issue includes:\n\n1. A clear error message and stack trace showing the TypeError: 'itertools.chain' object is not subscriptable\n2. The specific location of the error (in django/db/models/deletion.py line 300)\n3. The context in which it occurs (when trying to delete an object with protected relations)\n4. A test case that reproduces the issue (test_protect_via in delete.tests.OnDeleteTests)\n\nThe issue is that when trying to access error.protected_objects[0], the code crashes because protected_objects is an itertools.chain object which doesn't support indexing.\n\nWhile the issue doesn't explicitly state the fix, it's clear that the solution would involve modifying how protected_objects are accessed or ensuring they're in a subscriptable format before attempting to index them. A developer familiar with Django's deletion mechanism would likely understand what needs to be fixed.\n\nThe only minor ambiguity is that the issue mentions \"Pull request follows\" but doesn't include the actual PR, which might have provided additional context. However, there's enough information to understand and address the problem.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12396": {
        "explanation": "This issue is well-specified with clear reproduction steps and a detailed error message. The problem is clearly described: when running a subset of tests, Django is trying to access a database without the \"test_\" prefix that should be added to test databases.\n\nThe issue includes:\n1. A clear description of the problem: Django tries to access the original database name instead of the test database when running a subset of tests\n2. A hypothesis about the cause: when running a subset of tests, only the default database is set up properly, but the check command still tries to access all databases\n3. Complete reproduction steps with:\n   - Exact code for the test settings file\n   - The exact command to run the tests\n   - The full error message showing the access denied error\n4. The specific Django version/commit being used (c33eb6dcd0c211f8f02b2976fe3b3463f0a54498)\n\nThe issue is in the Django test runner's handling of multiple databases when running a subset of tests. The test runner correctly sets up the test database for the 'default' database but fails to properly handle the 'other' database when running checks.\n\nA developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12406": {
        "explanation": "This issue is well-specified with clear details about the problem and expected behavior. The issue describes:\n\n1. The exact problem: When using a RadioSelect widget for a ForeignKey field with blank=False, it still shows a blank option (\"--------\") that appears as a valid choice.\n\n2. A concrete code example showing the model definition and form setup that produces the issue.\n\n3. The actual HTML output that demonstrates the problem, showing how the blank option is rendered and even pre-selected with \"checked='checked'\".\n\n4. The expected behavior: \"there should be no checked option for RadioSelect's <input> tags when rendering a new form from a model if blank is not a valid selection.\"\n\nThe issue provides all the necessary context to understand the problem and what a solution should accomplish. A developer would need to modify how Django's ModelForm handles RadioSelect widgets for ForeignKey fields when blank=False, ensuring no blank option is presented. This is a specific, well-defined task with clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12407": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The user wants to enhance the `{% include %}` template tag to use `select_template` instead of `get_template` when the template variable contains a list of templates, allowing for fallbacks.\n\nThe issue includes:\n1. A clear description of the problem: the `{% include %}` tag doesn't support template fallbacks\n2. Concrete examples showing the current behavior (error when passing a list of templates)\n3. A demonstration of the expected behavior using `select_template`\n4. The specific line of code that needs to be changed\n5. A note about a potential complication due to recent code changes\n\nThe solution path is straightforward - modify the `include` template tag to use `select_template` when the template name is a list, similar to how other Django rendering functions work. This would allow the tag to try each template in the list until it finds one that exists.\n\nThe issue provides enough context and examples for a developer to implement and test the solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12419": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific feature to modify: The SECURE_REFERRER_POLICY setting that controls the Referrer-Policy header\n2. The exact change requested: Change the default value from its current setting to \"same-origin\"\n3. The rationale: To make Django applications leak less information to third-party sites\n4. Potential risks: Linked websites that depend on verification through the Referer header might break\n5. Context: References to documentation and browser support information\n\nThe implementation would be straightforward - locate where the default value for SECURE_REFERRER_POLICY is defined in Django's settings module and change it to \"same-origin\". The issue also provides enough context about the security implications and potential risks of making this change.\n\nThis is a focused, specific change request with clear parameters and justification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12453": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is precisely identified: `TransactionTestCase.serialized_rollback` fails to restore objects due to foreign key ordering constraints.\n\n2. The root cause is explained in detail: When deserializing database content in `deserialize_db_from_string`, objects are saved without a transaction wrapper, which can cause integrity errors if an object with a foreign key is saved before the referenced object.\n\n3. The solution is clearly proposed: Wrap the deserialization in a transaction, similar to how `loaddata` handles it.\n\n4. A specific code diff is provided showing exactly what changes are needed.\n\n5. The submitter explains why this is likely a bug (the transaction was \"forgotten\" in the implementation) and draws a parallel to the `loaddata` command which correctly uses transactions.\n\n6. There's even a suggestion about how to create a deterministic test for this issue.\n\nThe issue contains all the necessary information to understand the problem, verify it, and implement the proposed solution. No additional clarification is needed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12464": {
        "explanation": "This issue is well-specified with clear technical details about a specific problem in Django's SQLite backend. The issue describes:\n\n1. The exact problem: Django raises NotSupportedError when using DISTINCT with GROUP_CONCAT() on SQLite, even though SQLite does support this functionality.\n2. The specific location of the issue in the codebase: django.db.backends.sqlite3.operations.py, lines 60-64.\n3. A clear explanation of why this is incorrect: SQLite does support DISTINCT on aggregate functions like GROUP_CONCAT.\n4. A reference to documentation about GROUP_CONCAT functionality.\n5. A code example showing a working implementation of GroupConcat that the issue reporter has created.\n6. A specific limitation (GROUP_CONCAT only allows DISTINCT when a delimiter isn't specified).\n7. A proposed solution: skip the check entirely to allow users to extend the ORM as needed.\n\nThe issue provides enough context and technical details to understand the problem and implement a solution. A developer could locate the relevant code, understand the issue, and make the necessary changes to fix the problem without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12469": {
        "explanation": "The issue description identifies a specific bug in the admin date_hierarchy filter functionality, but lacks some important details:\n\n1. It clearly states the problem: When a user with a non-UTC timezone (specifically America/Los_Angeles) uses the date filter by month in the admin interface, they see an extra day displayed that belongs to the previous month.\n\n2. However, the description is missing several important details:\n   - No screenshots or specific examples showing the incorrect behavior\n   - No information about the expected behavior\n   - No steps to reproduce the issue\n   - No details about which Django version this occurs in\n   - No information about the model or admin configuration being used\n\nDespite these missing details, the core issue is reasonably clear: there's a timezone-related bug in the date_hierarchy filter that causes incorrect date display at month boundaries. A developer familiar with Django's admin date filtering could likely reproduce and investigate this issue based on the timezone information provided, even though additional details would be helpful.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12470": {
        "explanation": "This issue is well-specified with a clear problem description. The user has provided:\n\n1. A complete code example showing the model inheritance structure\n2. The exact query being executed\n3. The actual output (SQL query with ASC ordering)\n4. The expected output (SQL query with DESC ordering)\n\nThe issue is that when a Child model inherits from a Parent model that has Meta.ordering = [\"-pk\"], the ordering is not correctly applied - it's using ASC instead of DESC (ignoring the minus sign). This is a specific bug with a clear reproduction case and expected behavior.\n\nThe solution would need to fix how Django handles the ordering attribute inheritance in model inheritance scenarios, specifically ensuring that the minus sign for descending order is properly respected when generating SQL queries for child models.\n\nThere's no ambiguity about what's happening or what should happen instead.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12477": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The user is trying to use `UniqueConstraint` in a model's Meta options instead of the older `unique_together` approach.\n2. When using `UniqueConstraint`, Django's validation check (fields.E310) is still reporting that no subset of fields is unique, despite the constraint being properly defined.\n3. When switching to the older `unique_together` syntax, the error disappears.\n4. The documentation indicates that `unique_together` may be deprecated in the future, so the user wants to use the newer `UniqueConstraint` approach.\n\nThe issue is clearly a bug in Django's validation system - it's not recognizing `UniqueConstraint` as satisfying the uniqueness requirement that fields.E310 is checking for. The solution would involve modifying Django's field validation logic to recognize `UniqueConstraint` objects in the `constraints` list when checking for field uniqueness.\n\nThe issue provides:\n- Clear code examples showing both approaches\n- The exact error message\n- Links to relevant documentation\n- A clear explanation of the expected behavior\n\nThis is sufficient information to locate the validation code in Django and modify it to recognize `UniqueConstraint` objects when checking for field uniqueness.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12484": {
        "explanation": "This issue is well-specified. The user is requesting an enhancement to Django's system check admin.E002 to provide a more helpful hint message when a developer accidentally includes the USERNAME_FIELD in REQUIRED_FIELDS for a custom user model.\n\nThe issue clearly:\n1. Describes the current behavior (error message without a hint)\n2. Provides a concrete example of the problematic code\n3. Explains why this is confusing for developers\n4. Suggests a specific improvement (adding a hint that explicitly mentions which field needs to be removed)\n5. Even provides sample text for the hint message\n\nThe solution would involve modifying Django's system check for custom user models to add this hint when the USERNAME_FIELD is found in REQUIRED_FIELDS. This is a straightforward enhancement to an existing error message that would improve developer experience.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12496": {
        "explanation": "This issue is well-specified with clear examples demonstrating both the expected behavior and the problematic behavior. The issue describes a specific bug in Django's ORM when dealing with multi-inheritance models where the parent model has a custom primary key (specifically a UUID field).\n\nThe description provides:\n1. Clear code examples showing the model definitions\n2. The exact SQL queries being executed in both scenarios\n3. The specific conditions under which the bug occurs (multi-inheritance + custom PK in parent model)\n4. The unexpected behavior (an extra UPDATE query before the INSERT)\n5. The environment details (Python 3.6, PostgreSQL 9.6)\n\nA developer would be able to reproduce this issue and understand what needs to be fixed - specifically, Django is incorrectly generating an extra UPDATE query when creating a child model instance when the parent model has a custom primary key. The solution would involve modifying Django's ORM code to handle this specific case correctly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12503": {
        "explanation": "This issue is well-specified with clear details about the problem and solution:\n\n1. The problem is clearly identified: The `makemessages` command doesn't show errors when no locale is specified, even though it should raise a `CommandError`.\n\n2. The cause is identified: There's a bug in an if statement that checks if a locale has been specified.\n\n3. The expected behavior is clear: The command should raise a `CommandError` when none of the locale-specifying flags (--locale, --exclude, or --all) are present.\n\n4. A solution has already been implemented: The reporter has fixed this in their fork and submitted a PR (#12503).\n\n5. The PR link is provided for reference: https://github.com/django/django/pull/12503\n\nThe issue provides all the necessary information to understand the problem, verify it, and evaluate the proposed solution. The reporter has even done the work of fixing it and is just asking about next steps to move the PR forward.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12504": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies the problem: the logout functionality in the admin app is implemented as a link (GET request) rather than a form (POST request), which means it lacks CSRF protection. The issue also explains why this is a concern:\n\n1. It's a matter of best practice (\"purity\") - GET requests should not change system state\n2. While not critical from a security perspective (as noted by the submitter), it's still a deviation from proper web security practices\n\nTo solve this issue, I would need to:\n1. Find the logout link in the admin app\n2. Convert it from a simple link (GET request) to a form submission (POST request) with CSRF protection\n3. Ensure the user experience remains similar\n\nThe only minor ambiguity is which specific CSRF protection mechanism should be used, but this would likely follow the existing patterns in the codebase for other form submissions. The issue provides enough context to implement a proper solution without needing additional clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12508": {
        "explanation": "This issue is well-specified. It clearly describes:\n\n1. The current limitation: Django's `dbshell` management command doesn't support running SQL directly with a `-c` flag\n2. The current workaround: Using pipes to feed SQL into the command\n3. The proposed solution: Add a `-c` argument to the `dbshell` command that would allow direct SQL execution\n4. The rationale: It would be consistent with the `shell` command which already has this capability\n\nThe request is straightforward - add a command-line option `-c` to the `dbshell` management command that accepts SQL as an argument and executes it directly. The implementation would involve modifying the `dbshell` command to check for this flag, extract the SQL statement, and pass it to the database client instead of opening an interactive shell.\n\nThis is a common pattern in command-line tools (like bash's `-c` flag), and the issue provides a clear example of the desired behavior. A developer could implement this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12513": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: The `providing_args` argument in the Signal class is purely documentational and provides no actual functionality or checking.\n2. The rationale: It's consuming memory unnecessarily, causes cognitive distraction for developers, and has led to confusion.\n3. The proposed solution: Deprecate the `providing_args` argument from Signal.\n\nThe issue provides enough context about what needs to be done (deprecate a specific argument from a specific class), why it should be done (it's unused and confusing), and even references a related issue (#19579) for additional context. \n\nA developer would need to:\n1. Locate the Signal class implementation\n2. Add deprecation warnings for the `providing_args` argument\n3. Update relevant documentation to reflect this change\n4. Ensure backward compatibility\n\nThis is a straightforward deprecation task with clear boundaries and objectives.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12517": {
        "explanation": "This issue is well-specified with clear technical details. It describes:\n\n1. The specific problem: inconsistent datetime formats in Django's runserver logging\n2. Examples of the inconsistency: [16/Apr/2018 13:32:35] vs [2018-04-16 13:32:35,745]\n3. The root cause: WSGIRequestHandler uses server_time with BaseHTTPServer.log_date_time_string format, while WSGIServer uses a different format via ServerFormatter.format\n4. Current workaround: providing datefmt parameter when constructing the logger\n5. Suggested solution direction: making the formats consistent and possibly configurable\n6. Additional context about using %(asctime)s vs %(server_time)s\n\nA developer would have enough information to locate the relevant code in Django's logging system, understand the inconsistency, and implement a solution that makes the datetime formats consistent. The issue provides sufficient technical context and a clear goal for what a successful solution would look like.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12518": {
        "explanation": "This issue is well-specified with clear reproduction steps and a defined problem. The issue describes a specific bug in Django's `sqlmigrate` command when dealing with squashed migrations:\n\n1. The problem is clearly demonstrated with exact commands and error outputs\n2. There's a reproducible test case with a link to a GitHub repository\n3. The expected behavior is explicitly stated with two specific requirements:\n   A) Improve error handling with a better error message\n   B) Allow inspection of individual migrations that have been squashed\n\nThe issue provides enough context to understand the problem domain (Django migrations), the specific command affected (`sqlmigrate`), the exact error conditions, and what a successful solution would look like. A developer could begin implementing a solution without needing additional clarification.\n\nThe solution would involve modifying Django's migration system to better handle squashed migrations in the `sqlmigrate` command, with specific focus on error handling and allowing inspection of individual migrations within a squash.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12519": {
        "explanation": "This issue is well-specified with clear details about the problem. The reporter has:\n\n1. Identified a specific regression in Django 3.0.2 compared to 2.2.9\n2. Provided the exact SQL queries from both versions showing the difference\n3. Pointed out the key difference: in the GROUP BY clause, the subquery annotation \"in_package\" is missing in the 3.0.2 version\n4. Explained the impact: duplicate results (519 vs 490) when the query should be distinct\n5. Referenced a related issue (#31094) for context\n\nThe core problem is clearly visible in the SQL diff - the GROUP BY clause in Django 3.0.2 is missing the subquery annotation that was present in 2.2.9. This is causing the query to produce duplicate results.\n\nA developer can understand exactly what needs to be fixed: ensure that subquery annotations are properly included in the GROUP BY clause when multiple annotations are declared. The issue provides sufficient context and examples to reproduce and verify a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12532": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a discrepancy between two Django form field classes:\n\n1. `MultipleChoiceField` uses \"invalid_list\" as the error message key\n2. `ModelMultipleChoiceField` uses \"list\" as the error message key\n\nThe implied solution is to make `ModelMultipleChoiceField` consistent with `MultipleChoiceField` by changing its error message key from \"list\" to \"invalid_list\".\n\nWhile the issue doesn't provide specific code examples or file locations, a Django developer familiar with the codebase would be able to locate these classes in the forms module and make the necessary change. The issue is straightforward and the solution path is clear - find where `ModelMultipleChoiceField` defines its error messages and update the key.\n\nThe only minor ambiguity is whether there might be backward compatibility concerns with changing an error message key, but this is something that could be addressed during implementation by following Django's deprecation policy if needed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12553": {
        "explanation": "This issue is well-specified with clear requirements and rationale. The submitter wants to increase the default password salt size in Django's BasePasswordHasher from approximately 71 bits to approximately 131 bits to comply with modern security standards. They have:\n\n1. Already created a patch (with a link to the PR)\n2. Provided a clear rationale for the change\n3. Included references to security standards from OWASP, Python documentation, and NIST that recommend a minimum of 128-bit salt sizes\n4. Indicated that NIST considers this a hard requirement\n\nThe change is specific, the motivation is clear, and the solution approach is already demonstrated in the linked PR. A developer would have all the information needed to understand what needs to be changed and why.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12588": {
        "explanation": "The issue description is somewhat brief but provides enough context to understand what is being requested. It asks for adding an option to the `remove_stale_contenttypes` command that would allow removing content type entries for apps that no longer exist in the system. The description mentions this option should be disabled by default.\n\nThe issue references a discussion and a PR, which suggests there's additional context available, but the links appear to be placeholders or broken in the provided description (shown as \"\u200bdiscussion\" and \"\u200bPR\" without actual URLs). This is a limitation, as those resources might contain important details about implementation requirements or edge cases.\n\nHowever, even without those additional resources, the core request is clear: modify the existing `remove_stale_contenttypes` command to add a new option for removing content type entries for nonexistent apps, with this option being disabled by default. This is a specific, actionable feature request that a developer familiar with the codebase could implement.\n\nThe task involves:\n1. Understanding the current `remove_stale_contenttypes` command functionality\n2. Adding a new option parameter (likely a boolean flag)\n3. Implementing logic to identify and remove content types for nonexistent apps when the option is enabled\n4. Ensuring the option is disabled by default\n\nWhile some implementation details might require judgment calls, the core requirement is reasonably clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12591": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: A specific commit (f9ff1df1) made it impossible to replace a generic site-wide admin action with a new one, resulting in admin.E130 system check error.\n\n2. A concrete example: The issue with the qsessions app that needs to delete session objects in non-bulk mode to clear caches, with a link to the specific code.\n\n3. A potential workaround: Overriding modeladmin.delete_queryset instead of replacing the action.\n\nThe issue provides enough context about what's happening and even suggests a potential solution path. While it doesn't explicitly state what the desired behavior should be, it's reasonably clear that the goal is to allow specialized admin actions to replace global ones in specific ModelAdmin classes without triggering system check errors.\n\nA developer familiar with Django's admin system would have enough information to investigate and implement a solution that allows replacing global admin actions with specialized ones per-admin without triggering the admin.E130 check.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12613": {
        "explanation": "This issue is well-specified with a clear problem description. The user is experiencing an error when trying to serialize Django models with JSONField to XML format. The error occurs specifically when using Django's serializers.serialize(\"xml\", queryset, fields=fields) function and including a JSONField in the fields parameter.\n\nThe error stack trace is provided, showing exactly where the error occurs:\n1. In the XML serializer's handle_field method\n2. When calling characters() in xmlutils.py\n3. The TypeError indicates it \"expected string or bytes-like object\"\n\nFrom this information, we can understand that:\n1. The XML serializer doesn't properly handle JSONField data types\n2. The specific error is that the JSONField value is not being properly converted to a string before being passed to the XML serializer\n3. The solution would involve modifying the XML serializer to properly handle JSONField values\n\nThis is a concrete bug with a clear reproduction path and error message, making it well-specified for a developer to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12627": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: The `make_password` function currently accepts any Python object as an argument, which is considered problematic.\n\n2. The expected solution is explicitly stated: Modify `make_password` to throw a TypeError when anything other than bytes or string is passed as an argument.\n\n3. The rationale is thoroughly explained with multiple reasons:\n   - Security concerns (weak passwords)\n   - Consistency with other libraries\n   - Consistency with documentation\n   - Consistency with Django's own `validate_password` behavior\n\n4. There's even a reference to a discussion where this issue was likely debated.\n\nA developer would know exactly what code needs to be modified, what the change should be, and how to test that the change works correctly. The issue provides enough context to understand the importance of the change and its alignment with Django's existing patterns.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12663": {
        "explanation": "This issue is well-specified with clear information to understand and reproduce the problem:\n\n1. The issue describes a regression in Django where using SimpleLazyObject with a nested subquery annotation fails, which previously worked before a specific commit (35431298226165986ad07e91f9d3aca721ff38ec).\n\n2. Complete code is provided:\n   - The models involved (A, B, C)\n   - A test case that reproduces the issue\n   - The exact error traceback\n\n3. The error is clearly identified: When using a SimpleLazyObject in a filter condition with a subquery annotation, Django fails with a TypeError because it's trying to convert the SimpleLazyObject to an integer.\n\n4. The expected behavior is implied: the code should work as it did before the regression, allowing SimpleLazyObject to be used in this context.\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided test case\n2. Understand what's happening by examining the traceback\n3. Investigate the specific commit mentioned to see what changed\n4. Implement a fix that allows SimpleLazyObject to work properly in this context again\n\nThe issue provides all necessary context and code to understand, reproduce, and fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12669": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem: when annotating a QuerySet with a UUID key, the order_by functionality breaks because UUID values are not considered valid order_by arguments. The issue also proposes a specific solution: modifying the ORDER_PATTERN regex in django.db.models.sql.constants to allow hyphens in field names.\n\nThe issue provides:\n1. The exact file and constant that needs modification\n2. The current regex pattern\n3. The proposed new regex pattern\n4. Confirmation that the fix works with PostgreSQL\n\nThe only uncertainty is whether this change would be compatible with other database backends, which the reporter explicitly asks about. This is a reasonable question that would need investigation during implementation, but doesn't prevent understanding what needs to be done.\n\nTo implement a solution, I would need to:\n1. Verify the issue with UUID fields in order_by\n2. Test the proposed regex change\n3. Check compatibility with all supported database backends\n4. Add appropriate tests\n5. Update documentation if needed\n\nWhile there are some details to work out, the core problem and a potential solution path are clearly defined.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12671": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with Django management commands' OutputWrapper class, which currently adds a newline character to the text provided as the 'out' argument. The issue states that calling self.stdout.write() or self.stderr.write() without arguments fails because 'msg' is a positional argument.\n\nThe implied solution is to modify these methods to allow empty messages, which would enable calling these methods without arguments while still adding a newline character.\n\nWhile the issue doesn't provide code examples or error messages, the problem is clear enough for someone familiar with Django's management command system. The issue mentions that a PR exists, suggesting that someone has already implemented a solution, though the PR details aren't provided.\n\nA developer with knowledge of Django's management commands would be able to:\n1. Locate the OutputWrapper class in Django's codebase\n2. Identify how the write() method is implemented\n3. Modify it to handle empty messages appropriately\n\nThe only minor ambiguity is whether the solution should make the 'msg' parameter optional with a default empty string, or implement some other approach, but this is a reasonable implementation detail that can be determined during development.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12708": {
        "explanation": "This issue is well-specified with clear steps to reproduce and a detailed description of the problem. The reporter has:\n\n1. Provided the specific Django version (1.11.10)\n2. Given clear reproduction steps (creating models with fields in both unique_together and index_together, then deleting index_together)\n3. Identified the exact failure point (django/db/backends/base/schema.py, line 378, in _delete_composed_index())\n4. Explained the error message and why it occurs (finding both _uniq and _idx constraints)\n5. Described their use case (refactoring to use the Options.indexes feature)\n6. Clearly articulated two specific issues that need to be addressed\n\nThe problem is a technical bug in Django's migration system where it can't properly handle the deletion of an index_together when the same fields are also in unique_together. This is specific enough that a developer could investigate the code path, understand the issue, and develop a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12713": {
        "explanation": "The issue description is somewhat vague but provides enough context to understand the problem. The user is reporting that setting the widget parameter in the `formfield_for_manytomany()` method doesn't work, while noting that it does work in the similar `formfield_for_foreignkey()` function. \n\nFrom this, we can infer:\n1. There's an inconsistency between how these two similar methods handle widget overrides\n2. The user expects `formfield_for_manytomany()` to accept a widget parameter like `formfield_for_foreignkey()` does\n3. This is likely in the context of Django's admin interface customization\n\nWhile the description doesn't provide code examples or the exact expected behavior, a developer familiar with Django's admin customization would understand what needs to be fixed: making the `formfield_for_manytomany()` method respect the widget parameter similar to how `formfield_for_foreignkey()` does.\n\nThe issue could be more clearly specified with code examples showing the current behavior and expected behavior, but there is enough information to make a reasonable attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12733": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: The current implementation of `sql_flush()` executes multiple queries (one to truncate tables and additional ones to reset sequences).\n2. The proposed solution: Use PostgreSQL's `TRUNCATE ... RESTART IDENTITY` syntax to combine these operations into a single query.\n3. The motivation: Performance improvement by reducing the number of queries.\n\nWhat makes it not fully well-specified:\n- It doesn't specify which codebase or project this is for (though it's likely Django based on the function name).\n- It doesn't provide the current implementation of `sql_flush()` or show exactly where in the code the change needs to be made.\n- It doesn't specify if there are any compatibility concerns or edge cases to consider.\n\nHowever, an experienced developer familiar with the codebase should be able to locate the `sql_flush()` function, understand its current implementation, and modify it to use the `RESTART IDENTITY` syntax for PostgreSQL backends. The change is focused and the intent is clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12734": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user explains that when changing the max_length of a field that is used as a primary key (cve_id) in a model (Vulnerability) that has a ManyToManyField relationship with another model (AppVersion), the migration correctly updates the field in the main table but fails to update the corresponding foreign key field in the many-to-many join table (vulnerability_app).\n\nThe issue includes:\n1. The specific model definition showing the ManyToManyField relationship\n2. The exact change made (increasing max_length from 15 to 100)\n3. The migration code that was generated\n4. The observed behavior (main table field updated, but join table field not updated)\n\nThis is a clear bug report with enough information to reproduce the issue and understand what the expected behavior should be - when changing the max_length of a primary key field that is referenced in a many-to-many relationship, Django should update the field length in both the main table and the join table.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12741": {
        "explanation": "This issue is well-specified with a clear problem statement and solution. The issue describes:\n\n1. The current signature of the `DatabaseOperations.execute_sql_flush()` method: `def execute_sql_flush(self, using, sql_list)`\n2. The proposed change: remove the `using` parameter and infer it from `self.connection.alias` instead\n3. The rationale: some internal uses are already passing `connection.alias` as the `using` parameter, making it redundant\n\nThe issue provides enough context to understand what needs to be changed, why it should be changed, and how to implement the change. A developer would need to:\n1. Find the `DatabaseOperations.execute_sql_flush()` method\n2. Change its signature to remove the `using` parameter\n3. Update the method implementation to use `self.connection.alias` instead\n4. Update all callers of this method to no longer pass the `using` parameter\n\nThis is a straightforward refactoring task with clear requirements and implementation path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12747": {
        "explanation": "This issue is well-specified. It clearly describes:\n\n1. The current behavior of QuerySet.Delete method, including its return format (a tuple with total count and a dictionary of model-specific counts)\n2. The inconsistency being reported: when zero objects are deleted, models with foreign keys return (0, {}) while \"simple\" models return (0, {'model_label': 0})\n3. The expected behavior: consistent return format in both cases - either both should return an empty dictionary or both should include the model label with zero count\n\nThe issue provides concrete examples of the current behavior and clearly states what the reporter expects as the correct behavior. A developer would have enough information to reproduce the issue and implement a fix to make the behavior consistent.\n\nThe fix would likely involve modifying the QuerySet.Delete method to ensure consistent return values when zero objects are deleted, regardless of whether the model has foreign keys or not.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12748": {
        "explanation": "The issue is asking to add support for resetting sequences in SQLite databases. It provides a specific reference to the internal `sqlite_sequence` table with a link to the SQLite documentation that explains how sequence tables work in SQLite.\n\nFrom this information, I can understand:\n1. The feature request is to add functionality to reset sequences in SQLite databases\n2. The implementation should use the `sqlite_sequence` table, which is a special internal table in SQLite\n3. The documentation link provides technical details about how sequences work in SQLite\n\nWhat's missing:\n- Specific details about the API or interface for this feature\n- How this should integrate with the existing codebase\n- Any specific edge cases to handle\n\nHowever, for an experienced developer familiar with the codebase, this is likely enough information to implement a solution. The task is clear - add support for resetting sequences in SQLite by manipulating the sqlite_sequence table as documented in the SQLite specification. The implementation details would follow the project's existing patterns for database operations.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12754": {
        "explanation": "This issue is well-specified with a clear description of the problem and steps to reproduce it. The issue describes a specific error that occurs during Django migrations when:\n\n1. A field is moved from a parent model to a child model subclass in the same migration\n2. The auto-generated migration operations are in the wrong order (CreateModel for the subclass followed by RemoveField from the parent)\n3. The error occurs because when creating the subclass, the field still exists in the parent, causing a name clash\n\nThe issue also includes:\n- Clear code examples showing the before and after model states\n- The exact error message received: \"django.core.exceptions.FieldError: Local field 'title' in class 'Book' clashes with field of the same name from base class 'Readable'\"\n- A proposed solution: reverse the order of migration operations (remove field from parent first, then create the subclass)\n\nA developer would have enough information to reproduce the issue and implement a fix by modifying Django's migration auto-detector to generate operations in the correct order.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12771": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. The current implementation: ModeState stores fields in a List[Tuple[str, models.Field]] to preserve ordering\n2. The rationale for changing: \n   - Auto-detector doesn't consider field re-ordering as a state change\n   - Django doesn't support table column reordering\n   - Field ordering is only relevant for model forms (unlikely during migrations)\n   - Current implementation causes \"awkward and inefficient lookup by name\"\n3. The proposed solution: Switch ModelState.fields to Dict[str, models.Field]\n4. Additional suggestion: Apply the same change to ModelState.indexes and .constraints\n\nThe task is straightforward - change the data structure from a list of tuples to a dictionary while maintaining the same functionality. The issue provides sufficient context about why this change is beneficial and what needs to be changed. A developer familiar with the Django codebase would be able to implement this change without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12774": {
        "explanation": "This issue is well-specified with a clear problem statement and reproduction steps. The issue describes that Django's QuerySet.in_bulk() method currently doesn't work with fields that are made unique via UniqueConstraint in the model's Meta class, but only works with fields that have unique=True directly on the field definition.\n\nThe issue includes:\n1. A clear code example showing the model definition with a UniqueConstraint\n2. The exact error message that occurs when trying to use in_bulk() with such a field\n3. A statement that the fix should be \"pretty simple\" and that the reporter already has a patch\n\nThe expected behavior is implicit but clear: in_bulk() should recognize fields with UniqueConstraint as valid unique fields, just as it does for fields with unique=True. The solution would involve modifying the in_bulk() method to check for UniqueConstraint definitions in addition to the current check for unique=True.\n\nThis is a well-defined, narrow scope issue with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12796": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. The specific problem: `makemigrations` requires an active database connection to check migration consistency, which causes failures when working on machines without database access.\n2. The exact location in the code causing the issue (with a GitHub link to the relevant code).\n3. Current workarounds that are inconvenient (adding a dummy backend or starting a local database).\n4. A clear proposed solution: add a flag to skip the consistency check or fail gracefully when a database connection isn't available.\n\nThe issue provides enough context about the problem, why it's problematic, and what a solution might look like. A developer could implement this feature without needing additional clarification. The solution would involve modifying the `makemigrations` command to include an option to bypass the database consistency check when specified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12821": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue:\n\n1. Clearly identifies the current state: only 4 out of 17 admin JavaScript files are minified, creating inconsistency\n2. Provides complete data: lists all files with their sizes\n3. Explains why this is problematic: manual minification is an easy-to-forget step for contributors\n4. Proposes a specific solution: stop minifying admin files altogether while continuing to distribute vendored libraries minified\n5. Provides rationale for the solution: files are small, minification gains are minimal especially when compression is used, and third-party apps can handle optimization\n\nThe implementation would involve:\n- Removing the minified versions of the admin JS files\n- Updating any references to use the non-minified versions\n- Updating documentation to remove the minification step for admin files\n- Ensuring tests still pass with this change\n\nThere are no ambiguities about what needs to be done or why. The scope is well-defined and limited to the admin static assets.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12830": {
        "explanation": "This issue is reasonably well-specified. The user is requesting a new parameter called `absolute_max` for the `formset_factory` function in Django. The issue clearly explains the current behavior of the `max_num` parameter and why it doesn't meet the user's needs.\n\nThe issue describes that:\n1. The current `max_num` parameter (defaulting to 1000) limits the number of forms that can be accessed in a formset\n2. This causes problems when users have more than 1000 initial values or POST data\n3. The user expects to be able to access all forms regardless of the `max_num` parameter\n\nThe proposed solution is to add an `absolute_max` parameter to `formset_factory` that would presumably allow for a higher or separate limit on the total number of forms.\n\nWhile the exact implementation details of how the `absolute_max` parameter should work aren't fully specified, there's enough information to understand the problem and make a reasonable attempt at a solution. A developer familiar with Django's formset implementation would be able to understand where and how to add this parameter.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12851": {
        "explanation": "The issue is reasonably well-specified. It clearly states the goal: to remove the `ifequal` feature from the template language. The rationale is provided - it's not used in modern projects, not recommended, and is taking up space in the codebase.\n\nHowever, there are some details that would be helpful to know but aren't specified:\n1. Which template language is being referred to (though this would be clear from the repository context)\n2. Whether there are any backward compatibility concerns to address\n3. Whether documentation needs to be updated\n4. Whether there are tests that need to be modified\n\nDespite these missing details, the core task is clear: remove the `ifequal` functionality from the template language. A developer with access to the codebase should be able to identify the relevant code sections and remove them, along with any associated tests and documentation. The developer would need to make reasonable decisions about backward compatibility and deprecation notices based on the project's conventions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12855": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific function to be deprecated: `django.conf.urls.url()`\n2. The context: This function is already documented as \"likely to be deprecated in a future release\" and is an alias to `django.urls.re_path()`\n3. The history: The change was made in 2016 for Django 2.0\n4. The action requested: Formally deprecate this function\n\nThe task is straightforward - implement a formal deprecation for this function following Django's deprecation policy. This would typically involve:\n- Adding a deprecation warning to the function\n- Updating documentation to indicate the function is deprecated\n- Following Django's standard deprecation timeline\n\nThere's no ambiguity about what needs to be done, and a developer familiar with Django's codebase and deprecation practices would have all the information needed to implement this change.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12856": {
        "explanation": "The issue is well-specified and provides clear information about what needs to be fixed:\n\n1. It identifies a specific problem: When using UniqueConstraint in Django models, makemigrations doesn't validate that the fields specified in the constraint actually exist in the model.\n\n2. It contrasts this with the existing behavior of unique_together, which properly raises an error (models.E012) when non-existent fields are referenced.\n\n3. The issue mentions a demonstration that's attached (though we don't have access to it here), which would show the difference in behavior between UniqueConstraint and unique_together.\n\nThe solution would clearly involve adding validation to the UniqueConstraint implementation to check that the fields specified actually exist in the model, similar to how unique_together already does this validation. This is a straightforward enhancement to ensure consistent behavior between the two methods of specifying unique constraints.\n\nThe issue provides enough context for a developer familiar with Django's model system to understand and implement the required change.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12858": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific error (models.E015) that occurs during system checks when using certain lookups in model ordering.\n2. It provides the exact error message from the system check.\n3. It demonstrates that the ordering actually works at runtime with concrete examples of queries that succeed.\n4. It specifies the model relationships involved: Stock \u2192 Supply \u2192 Product \u2192 Product (self-reference).\n5. It even references a specific PR (#29408) that likely introduced the regression.\n\nThe issue is describing a discrepancy between the system check (which fails) and the actual runtime behavior (which works). The model relationships are clearly defined, and the specific lookup path that's causing the issue is explicitly stated. A developer would have enough information to reproduce the issue and understand what needs to be fixed - the system check is incorrectly flagging a valid ordering expression as invalid.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12869": {
        "explanation": "This issue is well-specified with clear technical details about a problem in Django's collectstatic command. The issue describes:\n\n1. The specific problem: collectstatic command doesn't run staticfiles checks because it has `requires_system_checks = False`\n2. The context: Users can make a common mistake with STATICFILES_DIRS (missing trailing comma) that would normally be caught by system checks\n3. The current behavior: The check_finders function that would catch this error isn't being run when collectstatic is executed directly\n4. A proposed solution approach: Run finder.check() for each finder and surface errors\n\nThe issue provides enough technical context about Django's internals to understand what's happening and even suggests a specific implementation approach. A developer familiar with Django would have enough information to locate the relevant code, understand the problem, and implement a solution that ensures staticfiles checks are performed during collectstatic operations.\n\nThe only minor detail missing is exactly how to surface the errors back to stderr, but the issue acknowledges this as an area needing further investigation. This doesn't prevent a meaningful attempt at a solution since the core problem and approach are clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12906": {
        "explanation": "This issue is well-specified with clear requirements. It describes a specific problem: admin templates now depend on the `django.template.context_processors.request` context processor, but this dependency is not documented. The issue clearly outlines what needs to be done:\n\n1. Document this change in the release notes with a deprecation period\n2. Document it in the admin documentation\n3. Add a system check framework warning (that will eventually become an error)\n\nThe issue provides the specific commit that introduced this dependency (d24ba1be7a53a113d19e2860c03aff9922efec24), explains why this is problematic (Django templates silence errors, so the missing context processor won't break templates immediately but will cause certain features not to work), and outlines exactly what documentation changes are needed. A developer would have enough information to implement the requested changes without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12910": {
        "explanation": "The issue is reasonably well-specified. It proposes replacing the boolean flag `Command.requires_system_checks = True` with a more flexible attribute `Command.required_system_checks = '__all__'` that would allow:\n\n1. Setting it to an empty list to skip all checks (equivalent to the current `False`)\n2. Setting it to specific tags (e.g., `['staticfiles']`) to run only a subset of checks\n3. Setting it to `'__all__'` to run all checks (equivalent to the current `True`)\n\nThe motivation is clear: to avoid the \"manual and error prone options['skip_checks'] dance\" and provide more flexibility for subclasses.\n\nWhile the issue doesn't provide the exact implementation details or file locations, a developer familiar with Django's command system would be able to locate the relevant code and implement this change. The issue also references a PR comment by Simon which provides context, though we don't have access to that specific comment.\n\nThe proposal is specific enough to understand what needs to be changed and why, though some details about backward compatibility and migration strategy might need to be determined during implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12915": {
        "explanation": "This issue is well-specified with clear information about what's missing and the problem it causes. The issue describes that the `StaticFilesHandlerMixin` is missing an async response function called `get_response_async`, which is needed for the `ASGIStaticFilesHandler` to work properly. The error traceback is provided, showing exactly where the failure occurs - when trying to call `self._middleware_chain(request)` which is `None`. \n\nThe solution would involve implementing the missing `get_response_async` method in the `StaticFilesHandlerMixin` class to properly handle async requests, similar to how it likely already has a synchronous version. The traceback provides enough context to locate the relevant files and understand the inheritance chain involved.\n\nA developer with access to the Django codebase would be able to:\n1. Locate the `StaticFilesHandlerMixin` class in the handlers.py file\n2. Understand how it interacts with the ASGI handler\n3. Implement the missing async method based on the existing pattern in the codebase\n4. Test that the ASGIStaticFilesHandler works correctly after the change",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12928": {
        "explanation": "This issue is well-specified with clear requirements and implementation details:\n\n1. The problem is clearly defined: The cached template loader provides speed benefits but requires server restarts when templates change during development.\n\n2. The solution approach is outlined: Implement an autoreload behavior for the cached template loader.\n\n3. The proposed implementation details are provided:\n   - A configuration example showing how it would work\n   - The specific settings needed (`cache_templates: True` and `autoreload: DEBUG`)\n   - A reference to a related issue (#25788) that this implementation would build upon\n\n4. The expected behavior is clear: When enabled, the cached template loader should automatically reload templates when they change, without requiring server restarts, while still maintaining the speed benefits of caching.\n\nThe issue provides enough information for a developer to understand what needs to be implemented and how it should work. The implementation would involve modifying the cached template loader to check for template changes when autoreload is enabled.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12933": {
        "explanation": "This issue is well-specified for the following reasons:\n\n1. It clearly identifies the problem: The `clearsessions` command currently writes to stderr directly when a backend doesn't implement `clear_expired()` method.\n\n2. It explicitly states the desired solution: The command should raise a `CommandError` instead of writing to stderr directly.\n\n3. It references a PR discussion that raised this issue, providing context and validation that this is a recognized problem.\n\n4. It follows a standard pattern in Django's management commands where errors should be raised as `CommandError` exceptions rather than writing directly to stderr.\n\nThe task is straightforward: modify the `clearsessions` command to check if the backend implements `clear_expired()` and raise a `CommandError` if it doesn't, rather than writing directly to stderr. This is a specific, actionable change with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12957": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The specific file that needs to be modified: contrib/gis/templates/gis/admin/openlayers.js\n2. The problem: float values in the JavaScript template are not using the |safe filter\n3. The consequence: when DECIMAL_SEPARATOR is set to something other than a dot (.), it breaks JavaScript syntax\n\nThe issue could be more specific about exactly which float values in the template need the |safe filter applied, but this can be determined by examining the file and identifying all float values that might be affected by the DECIMAL_SEPARATOR setting.\n\nA developer with knowledge of Django templates and GeoDjango would understand that:\n- Django templates can apply filters like |safe to values\n- DECIMAL_SEPARATOR is a Django setting that affects how decimal numbers are displayed\n- JavaScript requires decimal points to be represented with dots (.)\n- The solution involves adding the |safe filter to float values in the template\n\nThe solution approach is clear: examine the openlayers.js template, identify float values that could be affected by DECIMAL_SEPARATOR, and add the |safe filter to ensure they're rendered correctly regardless of locale settings.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-12961": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter provides:\n\n1. Complete model definitions for EntityA and EntityB\n2. Sample data creation code\n3. The exact query that works (simple order_by)\n4. The exact query that fails (order_by with F expression and nulls_last)\n5. The specific error message received\n\nThe issue is about using order_by() with F expressions on union() querysets, specifically when trying to use nulls_last=True on a descending order. The reporter demonstrates that simple ordering works, but expression-based ordering fails with a specific database error.\n\nThe reporter also references related tickets to distinguish this issue from existing ones, showing they've done research. The expected outcome is either a fix to make the expression-based ordering work with union querysets, or a more explicit error message if this limitation is by design.\n\nThis provides all the necessary information for a developer to reproduce the issue and understand what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12965": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific regression in Django 3.1 where `Model.objects.all().delete()` now generates SQL with a subquery instead of a simple DELETE statement.\n\n2. It clearly explains two specific problems this causes:\n   - Incompatibility with MySQL's LOCK TABLES feature\n   - Significant performance regression (with benchmarks showing 0.2s vs 7.5s)\n\n3. The issue includes:\n   - Exact SQL queries generated by both Django 3.0 and 3.1\n   - Reference to MySQL documentation explaining the lock tables limitation\n   - Detailed benchmark results demonstrating the performance impact\n   - A link to a related PR where the issue was discovered\n\nThe solution direction is also implied: revert to the previous SQL generation behavior for `all().delete()` operations to avoid the subquery. A developer would have enough information to locate the relevant code in Django that changed between 3.0 and 3.1 and modify it to generate the simpler DELETE statement again.\n\nThis issue provides all the necessary context, reproduction steps, and clear criteria for what would constitute a successful fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12973": {
        "explanation": "This issue is well-specified with a clear problem and solution path. The issue describes a specific scenario where a user follows Django's system check errors to add middleware components, but encounters an AssertionError because the order of middleware matters (SessionMiddleware must come before AuthenticationMiddleware). \n\nThe request is to update the E410 error message to include a hint about the required ordering of middleware components, specifically that SessionMiddleware must be placed before AuthenticationMiddleware. This would prevent users from encountering the AssertionError after following the system check instructions.\n\nThe solution would involve modifying the error message for the E410 check to include information about the required ordering. The code that generates these system check messages is clearly identified, and the change needed is straightforward - enhance an existing error message to be more informative about dependencies between middleware components.\n\nAll the necessary context is provided, including the exact error messages and the specific problem that needs to be addressed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-12983": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific function that needs to be modified: `django.utils.text.slugify()`\n2. The current behavior: The function doesn't strip dashes and underscores from the beginning and end of the string\n3. The expected behavior: The function should strip these characters\n4. A concrete example showing both the current output and the desired output after the fix\n\nThe issue provides enough information for a developer to understand what needs to be changed and how to verify the solution works correctly. The modification would involve updating the `slugify()` function to strip leading and trailing dashes and underscores from the final slug. This is a straightforward task with a clear acceptance criterion.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13012": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug where constant expressions wrapped in ExpressionWrapper are incorrectly included in GROUP BY clauses in PostgreSQL queries.\n\n2. The description provides complete code examples showing both the problematic case and a working alternative.\n\n3. The exact error message is included: \"django.db.utils.ProgrammingError: aggregate functions are not allowed in GROUP BY\".\n\n4. The expected behavior is clearly stated - Django should identify constant expressions (even when wrapped in ExpressionWrapper) and omit them from GROUP BY clauses.\n\n5. The SQL queries generated in both cases are shown, making it easy to understand the difference between the problematic and working scenarios.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed: modify how Django handles ExpressionWrapper objects containing constant expressions so they're not included in GROUP BY clauses. The solution would likely involve examining how Django's query compiler identifies constants and extending that logic to handle ExpressionWrapper objects properly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13023": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue clearly identifies the bug: DecimalField.to_python() raises TypeError instead of ValidationError when a dictionary is passed as the value parameter.\n\n2. The impact is explained: When saving a model with a decimal field mistakenly set to a dictionary, the TypeError makes it difficult to identify which field is causing the problem, especially in models with many fields.\n\n3. A solution is already proposed: The user has created a pull request (#13023) to fix the issue.\n\nThe problem description provides enough context to understand what's happening, why it's a problem, and what needs to be fixed. A developer could easily reproduce this issue by creating a model with a DecimalField and attempting to save it with a dictionary value. The expected behavior (raising ValidationError instead of TypeError) is clearly implied.\n\nThe only minor detail missing is the exact error message or stack trace, but this doesn't prevent understanding or solving the issue since the problem is straightforward and the expected behavior is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13028": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific error: `NotSupportedError` when filtering a queryset where the right-hand side has a field named `filterable=False`.\n\n2. Complete code examples are provided showing:\n   - The model definitions (ProductMetaDataType and ProductMetaData)\n   - The exact query that causes the error (ProductMetaData.objects.filter(value=\"Dark Vador\", metadata_type=self.brand_metadata))\n   - The full error traceback\n\n3. The user has identified a workaround: renaming the field from \"filterable\" to \"filterable_test\" resolves the issue.\n\n4. The issue appears to be that Django's query builder is confusing the field name \"filterable\" with some internal property or method related to filtering functionality.\n\nThe problem is clearly reproducible and has a specific cause that can be investigated in Django's codebase. The solution would likely involve either fixing Django's query builder to handle field names that match internal attributes, or documenting this limitation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13030": {
        "explanation": "This issue is well-specified with a clear problem description and reproduction steps. The issue describes how Django's `prefetch_related` on a foreign key currently includes NULL values in the IN clause of SQL queries, which is unnecessary for foreign keys since NULL values don't match any records. The issue provides:\n\n1. A complete code example with model definitions\n2. Step-by-step reproduction instructions with shell commands\n3. The actual SQL queries being generated, showing the problem (NULL in the IN clause)\n4. A clear rationale for why this is problematic (NULL's behavior in SQL could lead to incorrect results)\n\nThe solution would involve modifying Django's prefetch_related implementation to filter out NULL values before constructing the IN clause for foreign key lookups. This is a well-defined task with a clear success criterion: when a Book with author=None exists, the query should not include NULL in the IN clause.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13033": {
        "explanation": "This issue is well-specified with clear details about the problem. The user has:\n\n1. Provided complete model definitions showing the self-referencing foreign key relationship\n2. Included multiple example querysets demonstrating the unexpected behavior\n3. Shown the actual SQL queries generated in each case\n4. Identified specific unexpected behaviors:\n   - The order_by(\"record__root_id\") produces a DESC sort when ASC is expected\n   - An unnecessary extra JOIN is being performed\n   - Different but related queries produce more optimal SQL\n5. Included version information (initially on 2.2.10, verified on 3.0.6)\n6. Provided workarounds they've discovered\n\nThe issue appears to be a bug in Django's query generation when dealing with self-referencing foreign keys and ordering. The problem is clearly reproducible from the information provided, and the expected behavior is well articulated. A developer could implement a solution based on this description without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13066": {
        "explanation": "This issue is well-specified with clear requirements. It describes:\n\n1. The specific problem: Django's Argon2PasswordHasher needs to be updated to support the argon2id variety and make it the default\n2. The historical context: When Django 1.10 was released, only argon2d and argon2i were available\n3. The specific upstream changes that need to be incorporated:\n   - Add support for argon2id\n   - Make argon2id the default type\n   - Update default parameters (memory_cost from 512 to 102400, parallelism from 2 to 8)\n4. References to specific commits and changes in the upstream library\n\nThe issue provides all the necessary information to implement the solution, including the specific parameter values that need to be changed. A developer can understand exactly what needs to be done without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13077": {
        "explanation": "This issue is reasonably well-specified. The reporter describes a specific problem: \"django-admin runserver\" fails on Windows but works with \"--noreload\" flag. They've identified that the issue appears to be related to how the console scripts strip the .exe extension from sys.argv[0], and they've found a workaround by modifying utils/autoreload.py. \n\nThe issue provides:\n1. Clear reproduction steps (running django-admin runserver on Windows)\n2. The exact error message received\n3. A workaround that resolves the issue (using --noreload flag)\n4. A hypothesis about the root cause (extension stripping in sys.argv[0])\n5. A potential fix location (utils/autoreload.py)\n\nWhat's missing is:\n1. The exact Django version affected\n2. Complete environment details (Windows version, Python version)\n3. Consistent reproduction steps (reporter mentions there are \"other conditions\" but isn't sure what they are)\n\nDespite these gaps, there's enough information to investigate the issue in Django's autoreload functionality and how it handles file paths on Windows, particularly with respect to .exe extensions. A developer could reasonably attempt to reproduce and fix this issue based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13085": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. Problem: The `compilemessages` command in Django runs `msgfmt` on all `.po` files, even when the corresponding `.mo` files are already up-to-date, causing unnecessary processing time.\n\n2. Proposed solution: Modify `compilemessages.py` to check the modification times of `.po` and `.mo` files, and only run `msgfmt` when the `.po` file is newer than the `.mo` file.\n\n3. Implementation details:\n   - Compare mtimes of `.po` and `.mo` files\n   - Only process files where `.po` is newer than `.mo`\n   - Handle the case where the `.mo` file doesn't exist\n   - Consider the interaction with the `is_writable` function\n\n4. Evidence of improvement: The submitter has already implemented this change and observed a performance improvement from 1.75 seconds to 0.2 seconds.\n\nThe issue includes enough technical detail about the problem, the proposed solution, and the expected outcome for a developer to implement the change without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13089": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The error is clearly stated: \"'NoneType' object is not subscriptable\" occurring in the Django cache backend's _cull method\n2. A complete stack trace is provided showing exactly where the error occurs\n3. The specific Django version (2.2.11) is mentioned\n4. The exact code location is identified (with a GitHub link)\n5. The reporter has analyzed the issue and identified the likely cause: cursor.fetchone() is returning None, and then the code tries to access [0] on that None value\n6. A reasonable solution is suggested: add a check to see if cursor.fetchone() returns data before trying to access it\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a fix. The suggested solution (checking if data exists before accessing it) is straightforward and makes sense given the error. A developer could implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13109": {
        "explanation": "This issue is well-specified with clear details about the problem and a suggested solution. The issue describes:\n\n1. A specific bug in Django's ForeignKey.validate() method where it uses _default_manager instead of _base_manager for validation\n2. A complete code example demonstrating the problem with model definitions and form implementation\n3. The exact error message users encounter (\"article instance with id 123 does not exist\")\n4. The specific file and line numbers in the Django codebase where the issue occurs\n5. A clear explanation of why changing the model's default manager is not an acceptable workaround\n6. A specific suggested solution (use _base_manager instead of _default_manager for validation)\n\nThe issue provides all the necessary context to understand the problem, reproduce it, and implement the suggested fix. A developer could take this information and create a PR that modifies the validation logic in the specified file to use _base_manager instead of _default_manager.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13111": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: WeekArchiveView needs to support the \"%V\" format (ISO 8601 week number) which was added to Python's strptime in Python 3.6.\n\n2. The solution path is outlined:\n   - Add \"%V\" to the list of accepted week formatters in WeekArchiveView\n   - Handle the special case where \"%V\" requires \"%G\" (ISO year) instead of \"%Y\" (regular year)\n   - Alternatively, improve the ValueError message to indicate the correct format combination\n\n3. Context is provided with reference to a previous issue (#26217) that discussed a related problem with week formatters.\n\n4. The technical requirements are specific and actionable - it's a matter of updating the code to support a specific date format and handling the compatibility requirements between date format specifiers.\n\nA developer familiar with Django's WeekArchiveView would have enough information to implement this feature without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13112": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The error message is clearly stated: \"ValueError: The field DJ_RegLogin.Content.category was declared with a lazy reference to 'dj_reglogin.category', but app 'dj_reglogin' isn't installed.\"\n\n2. The relevant code is provided, including:\n   - The model definitions (Category and Content)\n   - The INSTALLED_APPS configuration\n   - The AppConfig class\n\n3. The issue is specifically about a ForeignKey with a mixed-case app name causing makemigrations to crash.\n\n4. There's a clear regression point: it worked in Django 3.0 but fails in Django 3.1b1.\n\nThe problem is evident: Django is looking for 'dj_reglogin' (lowercase) but the app is registered as 'DJ_RegLogin' (mixed-case) in INSTALLED_APPS. This case sensitivity issue is causing the migration to fail. The solution would involve ensuring the app name references are consistent, likely by modifying the AppConfig's name attribute or by using explicit app_label in model Meta classes.\n\nA developer can understand the issue and implement a solution without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13115": {
        "explanation": "This issue is well-specified for an experienced developer familiar with Django's database constraints system. It clearly identifies:\n\n1. The problem: UniqueConstraint with deferrable=True is incompatible with certain index-specific parameters (include and opclasses), but the system currently doesn't throw an error when these are combined.\n\n2. The required solution: Add error handling when these incompatible combinations are used.\n\n3. Additional task: Add documentation about this incompatibility.\n\nThe issue assumes knowledge of Django's database constraint system, particularly how UniqueConstraint works with deferrable constraints and index-specific parameters. For someone familiar with this codebase, it's clear what needs to be done:\n- Identify where UniqueConstraint validation happens\n- Add validation logic to check for incompatible combinations of deferrable with include/opclasses\n- Throw appropriate errors\n- Update documentation to explain this limitation\n\nThe issue doesn't specify the exact error message or documentation wording, but these are reasonable implementation details that can be determined during development.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13121": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with duration field expressions in Django when using SQLite and MySQL databases. The error occurs when trying to perform operations on DurationField values using Django's F() expressions.\n\nThe issue includes:\n1. A clear code example showing the model definition with a DurationField\n2. The problematic code that's causing the error (adding a timedelta to a DurationField using F())\n3. A detailed traceback showing the exact error (decimal.InvalidOperation)\n\nWhat makes this not fully \"well-specified\" is:\n- It doesn't explicitly state what the expected behavior should be, though it's implied that the operation should work\n- It doesn't specify the Django version being used\n- It doesn't provide details about the database configurations\n- It doesn't mention if this works on other database backends (like PostgreSQL)\n\nHowever, a developer familiar with Django would likely understand that this is a bug where duration field expressions are not properly handled in SQLite and MySQL backends. The solution would involve fixing how Django converts duration values in these specific database backends.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13128": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with temporal subtraction in Django's ORM when trying to calculate the difference between two DateTimeField fields. \n\nThe issue shows:\n1. A clear model definition with start and end DateTimeField fields\n2. The query that's failing (subtracting start from end)\n3. The exact error message received\n4. The current workaround being used (adding a zero timedelta with an explicit output_field)\n\nThe goal is to make temporal subtraction work without needing to use the ExpressionWrapper or the workaround shown. While the issue doesn't explicitly state what the desired solution should look like, it's reasonable to infer that the desired behavior is for Django to automatically determine that subtracting two DateTimeField objects should result in a DurationField without requiring explicit type specification.\n\nA developer familiar with Django's ORM and expression system would understand what needs to be fixed and could implement a solution based on this description.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13158": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user has:\n\n1. Provided complete code examples showing both the working case (using `|` operator) and the problematic case (using `.union()`)\n2. Clearly described the expected behavior (no objects should be added when form is submitted empty)\n3. Clearly described the actual behavior (all objects matching the queryset are added when using `.union()`)\n4. Provided context about the models and form setup\n\nThe issue appears to be a bug in Django's handling of `.union()` queries in ModelMultipleChoiceField when the field is submitted empty. The problem is reproducible with the provided code, and the expected behavior is clearly stated. A developer would have enough information to investigate why `.union()` behaves differently from the `|` operator in this specific context and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13162": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The author identifies that current merge migration filenames only contain timestamps (e.g., \"0003_merge_20160102_0304.py\") which don't indicate which migrations were merged. They propose a specific alternative naming scheme that includes the names of the merged migrations (e.g., \"0003_merge_0002_conflicting_second_0002_second.py\").\n\nThe issue provides:\n1. A clear description of the current behavior\n2. A concrete example of the current output\n3. A specific proposal for the improved naming scheme\n4. A concrete example of the desired output\n5. Acknowledgment of alternative approaches and flexibility\n\nThe implementation would involve modifying the code that generates merge migration filenames to include information about the merged migrations rather than just a timestamp. This is a straightforward enhancement with a well-defined scope and clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13170": {
        "explanation": "This issue is well-specified and clear about what needs to be implemented. The issue describes a current limitation in Django's FilteredRelation where nested relations are not supported in the condition parameter, and provides a concrete example of code that currently fails with a specific error message. \n\nThe request is straightforward: modify the FilteredRelation functionality to support nested relations in its condition parameter. The example clearly illustrates both the current behavior (raising a ValueError) and implies the desired behavior (allowing the nested relation 'pizzas__toppings__name__startswith' to work in the condition).\n\nA developer with knowledge of Django's ORM and query system would understand what needs to be changed - the validation logic that currently prevents nested relations in FilteredRelation conditions would need to be modified to support them instead. The error message in the example even points to the specific validation that's failing.\n\nWhile implementation details aren't provided, this is appropriate for an issue description as those would be determined during the solution phase. The scope is well-defined and the expected outcome is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13192": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies the exact location of the problem in Django's codebase (django/core/mail/message.py:98, in the sanitize_address function).\n\n2. It explains the technical cause: Django uses email.header.Header.encode which introduces newlines at 75 characters, but recent Python security updates no longer allow this behavior.\n\n3. It provides a link to the specific Python commit that caused the incompatibility: https://github.com/python/cpython/commit/f91a0b6df14d6c5133fe3d5889fad7d84fc0c046#diff-3c5a266cd05e7d4173bf110ee93edd16\n\n4. The issue clearly states the impact: Django can no longer send emails with names longer than 75 characters.\n\nA developer would have enough information to locate the affected code, understand the problem, and develop a solution that handles email names over 75 characters in a way that's compatible with the latest Python versions. The solution would likely involve modifying the sanitize_address function to handle long names differently or to work with the new Python behavior.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13195": {
        "explanation": "This issue is well-specified with clear details about the problem and a potential solution. The issue describes:\n\n1. The specific problem: HttpResponse.delete_cookie() method doesn't preserve the SameSite attribute when deleting cookies, causing browser warnings.\n\n2. Context and reproduction: The reporter provides the exact warning message from Firefox and shows the cookie headers before and after the deletion.\n\n3. Technical analysis: The reporter explains that the delete_cookie method doesn't set the SameSite attribute and only sets 'Secure' in specific cases, which will cause problems with Chrome and Firefox's upcoming security changes.\n\n4. Proposed solution: The reporter even provides a code snippet showing how they modified the delete_cookie method to fix the issue by preserving the SameSite attribute.\n\nThe issue provides enough information for a developer to understand the problem, verify it, and implement a solution. The reporter has done thorough investigation and provided a clear path forward. A developer could implement a fix based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13199": {
        "explanation": "This issue is well-specified with clear details about the problem and a potential solution. The issue describes:\n\n1. The specific problem: HttpResponse.delete_cookie() method doesn't preserve the SameSite attribute when deleting cookies, causing browser warnings.\n\n2. Concrete example: The reporter provides the exact warning message from Firefox and shows the cookie headers before and after the deletion.\n\n3. Technical context: The issue explains that Chrome and Firefox will soon ignore cookies with SameSite=None that aren't marked 'Secure', which could affect cookies deleted through HttpResponseBase's delete_cookie method.\n\n4. Root cause identification: The delete_cookie method doesn't set the SameSite attribute when deleting cookies.\n\n5. Proposed solution: The reporter even provides a code snippet showing how they modified the delete_cookie method to fix the issue by preserving the SameSite attribute.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The reporter has done thorough investigation and even provided a working solution that could be refined for the official fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13212": {
        "explanation": "This issue is well-specified with a clear problem statement and solution direction. The request is to modify Django's validators to include the provided value in ValidationError, allowing custom error messages to reference the invalid value using a %(value)s placeholder. \n\nThe issue provides:\n1. A clear example of the desired outcome: \"blah\" is not a valid email.\n2. The specific technical change needed: make built-in validators provide the value to ValidationError\n3. A reference to existing documentation that already shows this pattern\n4. The context of why this is useful (for custom error messages)\n\nA developer would understand exactly what needs to be changed: modify the validator classes in Django to pass the invalid value to the ValidationError constructor so it can be referenced in error messages. This is a straightforward enhancement that follows an existing pattern in the documentation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13218": {
        "explanation": "This issue is well-specified with clear technical details and context. It describes:\n\n1. The problem: The migrate command checks for a __file__ attribute on migrations packages, which prevents it from working with Python 3 namespace packages (which don't have __init__.py files and thus no __file__ attribute).\n\n2. The history: The check was added in PR #21015 because __file__ was used for migration file discovery.\n\n3. The current situation: PR #23406 changed migration file discovery to use pkgutil.iter_modules() which uses __path__ instead of __file__, making the __file__ check unnecessary.\n\n4. The solution direction: Remove the __file__ check since it's no longer needed and is preventing migrations from working with namespace packages.\n\nThe issue provides sufficient technical context, references to related PRs, and a clear understanding of what needs to be fixed. A developer would need to locate the __file__ check in the migrate command code and remove it, ensuring that namespace packages work correctly with migrations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13220": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly stated: ValidationErrors with identical messages don't equal each other, which is counter-intuitive and complicates testing.\n\n2. The solution is explicitly defined: Add an `__eq__` method to the ValidationError class to allow comparison.\n\n3. There are specific implementation details provided:\n   - The comparison should go beyond just comparing `self.messages == other.messages`\n   - The comparison should be independent of the order in which errors were raised in a field or in non_field_errors\n\nThe issue provides enough context to understand what needs to be done. A developer would need to:\n1. Locate the ValidationError class in Django's codebase\n2. Implement an `__eq__` method that compares ValidationError objects in an order-independent way\n3. Ensure the implementation works for both field errors and non-field errors\n\nThe only minor detail that might need interpretation is exactly how to implement the order-independence, but this is a standard programming task (likely involving sorting or using set operations) that a competent developer could implement without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13230": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The task is to add a \"comments\" parameter to the feed.add_item() method in the syndication.views module.\n2. The purpose is to allow direct definition of item_comments without having to use item_extra_kwargs.\n3. The issue mentions that comments is already explicitly mentioned in the feedparser but not implemented in the view.\n\nThe issue provides enough context to understand:\n- The specific function that needs modification (feed.add_item())\n- The specific module where the change is needed (syndication.views)\n- The specific parameter to add (comments)\n- The rationale for the change (simplifying the API by avoiding the detour via item_extra_kwargs)\n- A reference point (feedparser already mentions comments)\n\nA developer familiar with the syndication framework would be able to locate the relevant code, understand the current implementation, and make the necessary changes to implement this feature request without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13240": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. The current behavior: Django raises ImproperlyConfigured during settings initialization when SECRET_KEY is not provided\n2. The desired behavior: Django should raise ImproperlyConfigured only when Settings.SECRET_KEY is accessed\n3. A use case that motivates the change: management commands that don't need SECRET_KEY but are forced to provide one\n4. The current workaround: generating a random secret key for each run\n\nThe issue provides enough context to understand what needs to be changed in the Django codebase. A developer would need to:\n1. Locate where SECRET_KEY validation happens during initialization\n2. Move this validation to happen on access instead\n3. Ensure this change doesn't break existing functionality\n\nThis is a focused, specific change request with clear acceptance criteria - the error should be raised on access rather than initialization.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13250": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly defined: JSONField's `__contains` and `__contained_by` lookups don't work with nested values specifically on SQLite.\n\n2. The issue provides context about why this happens: SQLite lacks native support for JSON containment testing with nested structures.\n\n3. The issue includes specific code examples showing what currently doesn't work, with a diff showing test cases that should pass but currently fail.\n\n4. The examples demonstrate the expected behavior - that partial containment of nested structures should work (e.g., `{'baz': {'a': 'b'}}` should match `{'baz': {'a': 'b', 'c': 'd'}}` in containment tests).\n\n5. The issue references the general principle that should be followed for containment testing.\n\nThe only minor uncertainty is the feasibility of implementing a solution, as noted by \"I'm not sure if it's feasible to emulate it in Python.\" However, this doesn't affect the clarity of what needs to be fixed - it's just acknowledging that the solution might be complex.\n\nA developer with Django knowledge would understand exactly what needs to be fixed and could attempt a solution based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13251": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible steps. The user explains that:\n\n1. They have a model with a field named `negate`\n2. When trying to filter using `Foo.objects.filter(negate=True)`, they get a specific error: `TypeError: _filter_or_exclude() got multiple values for argument 'negate'`\n3. They've found a workaround using `negate__exact=True`\n4. They note that `negate` is not documented as a reserved argument name\n\nThe issue is clearly a bug where a field named `negate` conflicts with an internal parameter in Django's filtering mechanism. The error message is specific, and the workaround demonstrates that this is likely a parameter name collision in the Django ORM. \n\nA developer would have enough information to investigate the `_filter_or_exclude` method in Django's codebase to see why `negate` is being treated as a special parameter rather than a field name, and then implement a fix to either document this limitation or modify the code to handle this case properly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13265": {
        "explanation": "This issue is well-specified with clear details about the problem. The user is experiencing a crash when using Django's AlterOrderWithRespectTo() with a ForeignKey when the '_order' field is included in an Index. \n\nThe issue description provides:\n1. The exact model definition with Meta class configuration\n2. The generated migration operations in the correct order\n3. A clear explanation of the problem: the AddIndex operation for the '_order' field is being executed before AlterOrderWithRespectTo, but '_order' doesn't exist yet at that point\n4. A hypothesis about the solution: AlterOrderWithRespectTo needs to run before AddIndex operations that reference '_order'\n\nThe problem is reproducible based on the provided code, and the solution would involve ensuring the correct order of migration operations so that the '_order' field is created before any indexes that reference it are added. This is a specific Django ORM/migrations issue with a clear technical cause and potential solution path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13267": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific error that occurs when instantiating an abstract model with a string ForeignKey in Django 1.9.8, which worked in Django 1.8.\n\n2. It provides the exact code that reproduces the issue:\n```python\nclass UserProfile(Model):\n    user = models.ForeignKey('auth.user')\n    class Meta(object):\n        app_label = 'core'\n        abstract = True\nUserProfile()\n```\n\n3. It includes the complete error traceback showing the TypeError: \"isinstance() arg 2 must be a class, type, or tuple of classes and types\"\n\n4. It points to the specific line in Django's source code that's causing the issue.\n\n5. The reporter has done research, noting they couldn't find documentation about this limitation or any mention in the release notes.\n\nThe issue is clearly a regression in Django 1.9.8 where instantiating an abstract model with a string ForeignKey fails, while it worked in Django 1.8 and still works for abstract models without foreign keys. A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13279": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem: session data cannot be decoded during the transition to Django 3.1 due to a change in the format for session data (referenced in commit d4fff711d4c97356bd6ba1273d2a5e349326eb5f and PR #31274). \n\nThe issue explains that setting DEFAULT_HASHING_ALGORITHM to 'sha1' is not sufficient to support running multiple instances of the same project during the transition. It also suggests a potential solution: using the legacy encode() method when DEFAULT_HASHING_ALGORITHM is set to 'sha1'.\n\nWhile the issue doesn't provide detailed implementation steps or code examples, it clearly identifies the problem, references relevant code changes, and suggests a direction for the solution. A developer familiar with Django's session handling would likely understand what needs to be done.\n\nThe only missing details are the specific implementation of how to use the legacy encode() method and where exactly the changes need to be made, but these can be determined by examining the codebase, particularly the referenced commit and PR.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13281": {
        "explanation": "This issue is well-specified with clear details about a behavior change in Django's FK field caching between versions 1.11.x and 2.x. The description includes:\n\n1. A specific problem: FK field caching behavior changed when copying model instances\n2. A complete test case that reproduces the issue\n3. The exact commit that likely caused the change (bfb746f983aa741afa3709794e70f1e0ab6040b5)\n4. Clear questions about whether the behavior is correct and what the recommended approach should be\n5. A potential solution path suggested by Simon (making a deep-copy of self._state)\n\nThe test case clearly demonstrates the expected behavior (bar1 should still point to foo1 after copying and modifying bar2) and shows how it fails in Django 2.x. The issue is specific enough that a developer could understand the problem, reproduce it, and implement a solution without needing additional clarification. The suggested solution direction provides a good starting point for fixing the issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13287": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug: when an app has `default_app_config` defined in `__init__.py` but either doesn't have an `apps.py` file or has an empty `apps.py` file, the application crashes.\n\n2. The issue provides detailed error traces for both scenarios (missing apps.py and empty apps.py), showing the exact same error: `UnboundLocalError: local variable 'app_config_name' referenced before assignment`.\n\n3. The issue points to the likely source of the regression: ticket #31180.\n\n4. The error occurs in a specific location: `django/apps/config.py` line 157 or 160 in the `create` method of `AppConfig`.\n\nWith this information, a developer can:\n1. Reproduce the issue by creating an app with the described configuration\n2. Locate the problematic code in the Django codebase\n3. Understand the regression by reviewing the changes made in ticket #31180\n4. Fix the issue by ensuring `app_config_name` is properly initialized before being referenced\n\nThe issue provides all necessary details to understand, reproduce, and fix the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13295": {
        "explanation": "This issue is well-specified with clear details about the problem and potential solutions:\n\n1. The problem is clearly defined: When using `save(update_fields=('my_non_concrete_field',))` with a non-concrete field, Django silently fails to save the field without raising any error.\n\n2. The submitter has identified the likely cause: A discrepancy between checking field validity against `meta.fields` but later using `meta.local_concrete_fields` to determine which fields to save.\n\n3. The submitter has provided specific code references with links to the relevant parts of Django's codebase.\n\n4. Two potential solutions are proposed:\n   - Ideal solution: Allow non-concrete fields to specify which underlying concrete fields should be saved on their behalf\n   - Simpler solution: Raise an error when a non-concrete field is included in update_fields\n\nThe issue provides enough context for a developer to understand the problem, verify it, and implement one of the proposed solutions. The submitter has done good research to identify the root cause, making it easier to develop a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13297": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: In Django 3.1, `TemplateView.get_context_data()`'s kwargs returns SimpleLazyObjects that cause crashes when used in database filtering.\n\n2. It provides complete code examples showing:\n   - Code that worked in Django 3.0\n   - The same code failing in Django 3.1\n   - The working solution (explicitly converting to string)\n   - The exact error message\n\n3. It includes the URL configuration showing how the view is being called\n\n4. It specifies the environment: Python 3.7.8 with SQLite\n\n5. The root cause is identified: In Django 3.1, URL parameters are passed as SimpleLazyObject instances rather than strings, and these objects need explicit conversion before being used in database queries.\n\nA developer could easily reproduce this issue and implement a fix based on the information provided. The fix would likely involve ensuring that URL parameters are properly converted from SimpleLazyObject to string before being used in database queries.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13300": {
        "explanation": "This issue is well-specified with a clear problem statement and desired outcome. The user is requesting that Django's `Exists()` function for subqueries should use `SELECT 1` syntax (like the `.exists()` method does) instead of selecting all fields from the table. \n\nThe issue provides:\n1. Clear examples of current behavior (showing how `.exists()` uses `SELECT 1` but `Exists()` subqueries select all fields)\n2. A concrete use case where this change would be beneficial (reducing query size from 15KB to 8KB)\n3. A workaround they're currently using (`.annotate(_1=Value(1, output_field=IntegerField())).values_list('_1')`)\n4. The specific technical change requested (make `Exists()` use `SELECT 1` where possible)\n\nA developer could implement this feature with the information provided, as the technical requirements are clear and the expected behavior is well-defined by comparison to the existing `.exists()` method. The issue doesn't leave significant room for interpretation about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13301": {
        "explanation": "This issue is well-specified with a clear problem description. The issue explains that:\n\n1. The `createsuperuser` command in Django has a default username suggestion feature\n2. When validating if this default username is available, it always checks against the default database\n3. The command has a `--database` option, but the validation for the default username doesn't respect this option\n4. This causes problems when using multiple databases\n\nThe problem is clearly defined and the expected behavior is implied: the default username validation should respect the `--database` option passed to the command.\n\nA developer would be able to:\n1. Locate the `createsuperuser` command code in Django\n2. Find the validation logic for the default username\n3. Modify it to respect the `--database` option\n4. Test the fix with a multi-database setup\n\nThe issue provides enough context to understand the problem and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13315": {
        "explanation": "The issue description identifies a specific bug where using a Q object with limit_choices_to on a ForeignKey field that involves a join can result in duplicate options appearing in form fields. The description mentions that a regression test is included in the patch that demonstrates the problem.\n\nWhile the description is brief, it clearly identifies:\n1. The specific component affected (ForeignKey with limit_choices_to)\n2. The specific condition that triggers the bug (using a Q object with joins)\n3. The observable problem (duplicate options in form fields)\n4. A reference to a regression test that demonstrates the issue\n\nThe description lacks details about:\n- The exact Django version affected\n- Code examples showing the problematic behavior\n- The expected behavior\n\nHowever, since a regression test is mentioned, an experienced Django developer should be able to understand the issue and work on a solution by examining the test case. The problem domain is specific enough that a developer familiar with Django's ORM and form handling would understand what needs to be fixed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13321": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The error occurs after upgrading to Django 3.1 on a staging server\n2. The specific error is provided: decoding invalid session data causes a crash\n3. Complete stack trace is included showing the exact error chain\n4. The error is reproducible in a specific browser (Chrome) but not in Firefox\n5. The environment details are provided (Gunicorn behind nginx proxy)\n6. The error appears to be related to session handling with an invalid/corrupted session\n\nThe root cause is clear: there's an issue with session data decoding that results in a \"binascii.Error: Incorrect padding\" error when trying to decode base64 data from an invalid session. This is likely due to an old session from before the upgrade that has an incompatible format.\n\nA developer would have enough information to:\n1. Understand the session decoding failure\n2. Implement a solution to handle invalid session data gracefully\n3. Test the fix in the specific environment described\n\nThe issue references a similar problem, suggesting this might be a known issue with a potential solution path already available.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13325": {
        "explanation": "This issue is well-specified with clear details about what needs to be fixed. The description:\n\n1. Identifies the specific problem: inconsistent key validation checks in cache backends\n2. Provides context about a previous fix (CVE2020-13254) that this issue builds upon\n3. Lists exactly which methods in which classes are missing validation:\n   - LocMemcache.touch()\n   - BaseMemcachedCache.delete_many()\n   - MemcachedCache.touch()\n   - MemcachedCache.get()\n   - MemcachedCache.delete()\n   - PyLibMCCache.touch()\n4. Specifies what needs to be done: add self.validate_key(key) calls to these methods\n5. Requests a test to ensure validation is called for all operations\n6. Explains the security implications (potential data loss in unvalidated delete operations)\n\nA developer would have clear direction on what to implement and where to make the changes. The solution approach is straightforward - add validation calls to the specified methods and create tests to verify validation occurs across all cache operations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13341": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific inconsistency: TemplateView automatically passes kwargs to the context while ListView does not.\n\n2. It provides a concrete example of code that worked in Django 3.0 but fails in Django 3.1, showing the exact error message.\n\n3. It demonstrates the workaround (converting to string explicitly) that resolves the issue.\n\n4. It includes relevant technical details: Python version (3.7.8), database (SQLite), and the specific error message.\n\n5. The root cause is identified: kwargs in TemplateView.get_context_data() returns SimpleLazyObjects in Django 3.1, which causes crashes when used for filtering without explicit conversion.\n\nThe issue has a clear scope (TemplateView's behavior) and provides enough context for a developer to understand the problem, reproduce it, and implement a solution. The solution would likely involve either:\n1. Making TemplateView consistent with other views by not automatically passing kwargs to context, or\n2. Ensuring that the SimpleLazyObjects are properly handled/converted when passed to the context.\n\nAll necessary information is provided to understand and address the issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13343": {
        "explanation": "This issue is well-specified with clear details about the problem and expected solution:\n\n1. The problem is clearly identified: When a FileField has a callable storage parameter, the callable is being evaluated during deconstruction (which happens during makemigrations), which breaks the runtime flexibility that callable storage is meant to provide.\n\n2. The expected behavior is explicitly stated: The deconstruction should return a reference to the original callable rather than evaluating it, similar to how callable upload_to parameters are handled.\n\n3. Context is provided: The issue includes references to documentation, the PR that introduced the bug, and the original ticket that the PR was meant to fix.\n\n4. The technical details are specific: It explains that the current implementation evaluates the callable during deconstruction and inlines the result, rather than preserving the callable reference.\n\nA developer would have enough information to understand the issue, locate the relevant code in Django's FileField deconstruction logic, and implement a fix that preserves callable storage references during deconstruction.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13344": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is clearly defined: when using Django with ASGI (specifically with uvicorn), the first middleware in the middleware chain receives a coroutine object instead of an HttpResponse object in its process_response method, while all other middlewares receive the expected HttpResponse object.\n\nThe issue includes:\n1. A clear description of the problem\n2. Detailed reproduction steps\n3. Example code showing the middleware implementation\n4. Configuration details (settings.py)\n5. Expected vs. actual behavior with console output examples\n6. A reference to a related issue in another package (django-cors-headers) that's affected by this problem\n\nA developer would have all the necessary information to investigate and fix this issue. The problem appears to be in how Django's ASGI handler processes responses through the middleware chain, with the first middleware receiving a coroutine instead of the resolved HttpResponse. This is a specific technical issue with a clear scope and reproduction path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13346": {
        "explanation": "This issue is well-specified with a clear problem description. The user has identified that when using models.JSONField, the __in lookup doesn't work properly with key transforms on MySQL, Oracle, and SQLite databases. They've provided a concrete example showing the discrepancy:\n\n1. When filtering with `our_field__key__in: [0]`, they get 0 results\n2. When filtering with `our_field__key: 0`, they get 312 results\n\nThe expected behavior is that both queries should return the same results since the first is just asking for records where the key value is in the list [0], which should be equivalent to asking for records where the key value is 0.\n\nThe issue title clearly states the affected databases (MySQL, Oracle, and SQLite) and the specific problem (key transforms with __in lookup). The reproduction steps are clear, and the expected vs. actual behavior is well-defined. A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13347": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific component (SafeExceptionReporterFilter) and its exact limitation: it doesn't properly recurse into dictionaries that have non-string keys.\n\n2. A concrete example is provided showing the problematic case: `SOME_SETTING = {1: {'login': 'cooper', 'password': 'secret'}}` where the key is an integer (1) rather than a string.\n\n3. The root cause is clearly identified: the `cleanse_setting` method tries to apply a regex to the key before recursing into the value, which fails for non-string keys.\n\n4. A specific link to the relevant code in the GitHub repository is provided, pointing to the exact line where the issue occurs.\n\nThe solution would require modifying the `cleanse_setting` method to properly handle non-string keys when recursing through dictionaries. This is a straightforward bug fix with a clear path to resolution based on the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13354": {
        "explanation": "This issue is well-specified with clear reproduction steps and a detailed explanation of the problem. The issue describes:\n\n1. A specific bug: MigrationOptimizer incorrectly orders operations when app names contain uppercase letters\n2. Clear reproduction steps:\n   - Create a project and app with uppercase letters\n   - Set up specific models with inheritance relationships\n   - Run makemigrations and migrate commands\n3. The exact error that occurs\n4. The root cause is identified: MigrationOptimizer combines operations without considering model dependencies\n5. A clear condition when the bug occurs (uppercase letters in app name) and when it doesn't (all lowercase app name)\n\nThe issue provides all necessary information to understand the problem, reproduce it, and identify where in the codebase the fix would need to be made (MigrationOptimizer). A developer could work on this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13363": {
        "explanation": "This issue is well-specified with clear details about the problem and expected behavior:\n\n1. The issue clearly identifies the problem: TruncDate and TruncTime functions don't properly use the tzinfo parameter that's passed in, instead using get_current_timezone_name() unconditionally.\n\n2. The issue provides specific code references with links to the problematic code in the Django codebase.\n\n3. A concrete example is provided showing the expected usage pattern and explaining why the current behavior is incorrect.\n\n4. The reporter explains what they expected to happen and what actually happens.\n\n5. A workaround is described, which further clarifies the issue by showing how the reporter had to solve their problem differently.\n\n6. The issue mentions that a PR (pull request) has been created, suggesting the reporter has already worked on a solution.\n\nThe issue is focused on a specific functionality (timezone support in TruncDate/TruncTime functions), provides clear reproduction steps, and explains the expected behavior. A developer could understand and address this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13369": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The exact code that's failing is provided:\n   ```python\n   expr = Value(3) * F('id')\t\t\t\t\t\t\t\t\t\t\t\t \n   o = Model.objects.using('default')\t\t\t\t\t\t\t\t\t\t \n   res = o.values('field_name').annotate(expr=expr).values('expr')\n   print(res)\n   ```\n\n2. A complete error traceback is included, showing the exact error:\n   \"Expression contains mixed types: IntegerField, AutoField. You must set output_field.\"\n\n3. The issue identifies that this is a regression bug, as the same query worked in Django 3.1 but fails in Django 3.2.\n\nThe problem is clearly about Django's ORM not being able to automatically determine the output field type when multiplying a Value (IntegerField) by a model's id field (AutoField). The error message even suggests the solution - setting an output_field.\n\nA developer would be able to reproduce this issue and understand what needs to be fixed: either the automatic type resolution needs to be improved in Django 3.2 to handle this case (as it did in 3.1), or documentation needs to be updated to clarify that output_field must be explicitly set in these mixed-type scenarios.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13371": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue identifies a specific bug: `django.db.models.query.Row` objects (returned by `QuerySet.values_list()` with `named=True`) cannot be pickled.\n\n2. It provides a complete code example that reproduces the error, showing:\n   - The model definition\n   - The exact query that creates the problematic Row object\n   - The verification of the object type\n   - The exact error message when trying to pickle the object\n\n3. The issue mentions a specific context where this is problematic (with the cacheops package).\n\n4. The issue references related Django ticket (#15648) that introduced the feature.\n\nThe problem is clearly defined and reproducible. A developer would understand exactly what needs to be fixed: make the `django.db.models.query.Row` class pickleable. This would likely involve modifying the Row class implementation to support the pickle protocol.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13386": {
        "explanation": "This issue is well-specified with a clear problem statement and solution direction. The issue describes:\n\n1. The problem: The `max_age` parameter in `set_cookie()` should be an integer according to RFC6265, but currently accepts other numeric types like floats, which can cause compatibility issues with some cookie parsers.\n\n2. The proposed solution: Convert the `max_age` parameter to an integer before using it.\n\n3. Specific implementation questions that need decisions:\n   - How to handle floats with decimal parts (round, truncate, or only convert when decimal part is 0)\n   - Error handling approach (throw exception or use original value)\n\nThe issue provides enough context to understand the problem, the rationale for fixing it, and the specific decisions that need to be made during implementation. A developer could proceed with implementing a solution based on this description, making reasonable decisions about the open questions or following common practices in the codebase.\n\nThe only minor ambiguity is around the specific implementation details for handling non-integer values, but these are explicitly called out as decision points rather than missing information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13401": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The issue describes:\n\n1. A specific bug where fields inherited from an abstract model are considered equal across different concrete models (B and C)\n2. Clear reproduction steps with code examples showing how to trigger the issue\n3. Evidence of the problem: `len({B._meta.get_field('myfield'), C._meta.get_field('myfield')}) == 1`\n4. A proposed solution: adjust `__eq__`, `__hash__`, and `__lt__` methods to consider the field's model when comparing fields\n5. A specific implementation suggestion (ordering by creation_counter first)\n6. A note about potential impacts (a test that broke in an experimental branch)\n\nThe issue provides enough context to understand the problem, reproduce it, and implement a solution. The proposed approach is also reasonable and specific. A developer could implement this solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13406": {
        "explanation": "This issue is well-specified with a clear description of the problem and a complete minimal reproducible example. The user is experiencing a crash when trying to recreate a queryset with values()/values_list() from a pickled query object. The issue description includes:\n\n1. A clear explanation of what they're trying to do (pickle query objects for later re-evaluation)\n2. The expected behavior (result should be a list of dicts)\n3. The actual behavior (broken model instances are returned instead)\n4. A complete code example showing the models and the problematic code\n5. The full error traceback\n\nThe issue is specifically about Django's queryset pickling behavior when using values() and annotate() together. The problem occurs when the query object is pickled, then loaded into a new queryset, and the resulting objects are of the wrong type (model instances instead of dictionaries).\n\nThis is detailed enough that a developer could reproduce the issue and understand what needs to be fixed in Django's queryset pickling implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13410": {
        "explanation": "This issue is well-specified with clear information about the bug and its solution:\n\n1. The problem is clearly identified: The POSIX implementation of file locks in Django's locks.py incorrectly returns False even when locks are successfully acquired.\n\n2. The root cause is explained: Python's fcntl.flock() returns None on success (not 0) and raises OSError on failure, but the current code checks for a return value of 0.\n\n3. The impact is described: Users who need non-blocking locks can't tell if they've successfully acquired a lock.\n\n4. A specific solution is provided: A complete code diff showing how to properly handle the fcntl.flock() behavior by using try/except blocks instead of checking return values.\n\nThe issue includes all necessary context, the exact file location (django/core/files/locks.py), and a complete solution that can be directly implemented. There's no ambiguity about what needs to be fixed or how to fix it.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13417": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user explains that when using the `annotate()` method with an aggregation function like `Count()` on a queryset for a model that has default ordering defined in its Meta class, the `ordered` property incorrectly returns `True` even though the resulting SQL query does not include an ORDER BY clause.\n\nThe issue includes:\n1. A clear description of the expected vs. actual behavior\n2. A specific code example demonstrating the problem\n3. The SQL queries that are generated in each case\n4. The values of relevant properties (`qs.ordered` and `qs.query.default_ordering`)\n5. The database being used (PostgreSQL)\n\nThe problem is that GROUP BY queries with annotations are not preserving the model's default ordering, but the `ordered` property is not reflecting this fact. This is a specific, technical issue with a clear path to investigation and resolution. A developer would need to examine why the `ordered` property doesn't correctly reflect the actual state of the query when annotations with aggregations are used.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13431": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user provides a specific model definition with a clear structure\n2. The exact query that's causing the problem is shown\n3. The actual SQL being generated is provided\n4. The error message is included\n5. The expected SQL output is clearly stated\n\nThe issue is that when using QuerySet.aggregate() with annotated fields, Django is not correctly handling the field names in the generated SQL. Specifically, when using F() references to annotated fields in aggregate functions, Django is not properly translating the field names in the subquery.\n\nThe problem is reproducible with the given code example, and the expected behavior is clearly defined. A developer would be able to understand the issue, reproduce it, and work on a fix without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13447": {
        "explanation": "The issue is reasonably specified but could benefit from more details. The requester wants to:\n\n1. Add the model class to the app_list context in the Django admin view\n2. Make the _build_app_dict method public since it's used by both index and app_index views\n\nThe issue explains the motivation (needing to manipulate the app_list in a custom admin view) and identifies that currently the dictionary contains a serialized model rather than the model class itself.\n\nWhat's missing:\n- Specific details about which file(s) need to be modified\n- The exact implementation they're envisioning\n- Any potential backward compatibility concerns\n- Tests that would need to be updated\n\nHowever, for an experienced Django developer familiar with the admin interface, this is likely enough information to:\n1. Locate the relevant code in the Django admin that builds the app_list\n2. Modify it to include the model class in the context\n3. Make the _build_app_dict method public\n\nThe solution path is clear enough that a developer could make a reasonable attempt without further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13449": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: using Lag() function with a DecimalField crashes on SQLite databases.\n\n2. The reporter provides complete code to reproduce the issue:\n   - A complete model definition\n   - The exact query that causes the error\n   - The full error traceback\n   - The generated SQL that's causing the problem\n\n3. The reporter has already done analysis and identified the likely cause: the CAST statement is incorrectly placed in the generated SQL, wrapping only the LAG function instead of the entire window expression.\n\n4. The reporter has verified that the issue only occurs with DecimalField (not with FloatField).\n\n5. The reporter has even found a workaround (using output_field=FloatField()).\n\nThis issue contains all the necessary information to understand, reproduce, and fix the problem. The bug appears to be in how Django generates SQL for window functions with DecimalField in SQLite. A developer could immediately start working on a fix based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13454": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user has provided the exact model code showing their GenericForeignKey setup\n2. They've shown how they're trying to use EmptyFieldListFilter in the admin\n3. They've provided the specific error message: \"'GenericForeignKey' object has no attribute 'empty_strings_allowed'\"\n4. They've noted a workaround (using resource_contract_id works fine)\n5. They've suggested a potential solution direction (extending GenericForeignKey)\n\nThe issue is reproducible based on the information provided. The problem is that EmptyFieldListFilter expects fields to have an 'empty_strings_allowed' attribute, which GenericForeignKey objects don't have since they're not actual database fields but rather virtual fields that combine information from other fields.\n\nA developer could implement a solution by either:\n1. Modifying EmptyFieldListFilter to handle GenericForeignKey objects\n2. Extending GenericForeignKey to include the missing attribute\n3. Creating a custom filter that works with GenericForeignKey\n\nThe requirements for a successful solution are clear: make EmptyFieldListFilter work with GenericForeignKey fields in the Django admin.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13460": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The exact template code causing the issue is provided\n2. The translation strings are shown\n3. The specific error message is included: \"TypeError: not all arguments converted during string formatting\"\n4. The root cause is identified: gettext string formatting fails when a float is passed\n5. A workaround is mentioned: removing floatformat and casting to int works\n6. Two potential solutions are suggested:\n   - Swallow TypeError with a friendlier message\n   - Check type compatibility with string format\n\nThe issue provides enough context to understand the problem and suggests reasonable approaches to fix it. A developer familiar with Django's translation system would have sufficient information to implement a solution without needing further clarification. The issue is focused on a specific bug with reproducible conditions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13466": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible steps. The user has identified that when using Django's `call_command` function with a boolean flag argument that's part of a required mutually exclusive group, it fails with the error \"ignored explicit argument 'True'\". \n\nThe issue includes:\n1. A complete code example showing the management command definition\n2. The exact command that works from the command line (`./manage.py test --flag`)\n3. The exact code that fails (`call_command('test', flag=True)`)\n4. The complete error traceback\n\nThe problem is clearly related to how Django's `call_command` function handles boolean flag arguments that are part of a required mutually exclusive group. The expected behavior would be for `call_command('test', flag=True)` to work the same as running `./manage.py test --flag` from the command line.\n\nA developer would have enough information to investigate and fix this issue by examining how Django's `call_command` function processes boolean arguments and how it interacts with argparse's mutually exclusive groups.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13490": {
        "explanation": "This issue is well-specified with a clear problem description and a minimal reproducible example. The user is reporting that when using QuerySet.difference() (and by extension, likely union() and intersection() as well), subsequent filtering operations like get() with filters don't work as expected. \n\nThe example clearly demonstrates the issue:\n1. They create a QuerySet of all Test objects (qs1)\n2. They create a QuerySet with just one Test object (qs2)\n3. They use difference() to get a QuerySet with objects in qs1 but not in qs2 (qs3)\n4. When they try to use get(id=2) on qs3, they get a MultipleObjectsReturned error even though there should only be one object with id=2\n\nThe error message indicates that the filter is not being properly applied after the difference() operation. The issue title suggests that this problem also affects union() and intersection() operations.\n\nThe user has provided:\n- Clear steps to reproduce\n- Expected behavior (get one object)\n- Actual behavior (error about multiple objects)\n- Environment details (Django 3.1.2, Python 3.8.5)\n- Confirmation that it happens in other environments too\n\nThis is sufficient information to investigate and fix the issue without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13495": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user clearly describes what they're trying to do: use TruncDay() with DateField as output_field and a tzinfo parameter\n2. They show the exact SQL that's being generated, which is missing the timezone conversion\n3. They've investigated the code and identified the specific issue: timezone conversion is only applied when output_field is DateTimeField, not when it's DateField\n4. They've included the relevant code snippet showing where the problem occurs\n5. The question is clear: is this intentional behavior or a missing feature?\n\nThe issue provides enough context and code details for a developer to understand the problem and implement a solution. The solution would likely involve modifying the as_sql method to consider the tzinfo parameter when output_field is DateField, similar to how it's handled for DateTimeField.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13512": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a problem: Django admin doesn't display Unicode characters properly in JSONFields, instead showing them as ASCII escape sequences (e.g., Chinese characters appear as \"\\u4e2d\\u56fd\" instead of \"\u4e2d\u56fd\"). \n\nThe user has:\n1. Demonstrated the issue with a code example showing how json.dumps encodes Unicode characters by default\n2. Specified the context (Django admin when editing a JSONField containing Chinese characters)\n3. Mentioned they've already created a pull request that fixes the issue (PR #714)\n\nWhat makes this not fully \"well-specified\" is that it doesn't provide:\n- The exact version of Django or django-mysql being used\n- The specific component in the codebase that needs modification\n- A complete reproduction case showing the admin interface issue\n\nHowever, with the information provided and by examining the referenced PR, an experienced developer should be able to understand and address the issue. The solution would likely involve modifying how JSONFields are serialized in the admin interface to preserve Unicode characters.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13513": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The problem is clearly identified: Django's debug error view doesn't respect Python's `__suppress_context__` attribute (PEP 415) when displaying exception tracebacks.\n\n2. A specific test case is provided that demonstrates the issue: a view that raises a `RuntimeError` and then raises a `ValueError` with `from None` to suppress the context.\n\n3. The exact location of the bug in the codebase is identified: the `explicit_or_implicit_cause()` function inside `get_traceback_frames()` in `django/views/debug.py`.\n\n4. The current implementation is shown, explaining why it doesn't respect `__suppress_context__`.\n\n5. A specific solution is proposed with code that would fix the issue.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The fix appears straightforward - modify the `explicit_or_implicit_cause()` function to check for `__suppress_context__` before returning the implicit context.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13516": {
        "explanation": "This issue is well-specified with clear information about what's happening and what's expected:\n\n1. The problem is clearly defined: flush() on stdout/stderr in management commands doesn't work properly, particularly during migrations.\n\n2. There's a specific example provided: During migrations, output is not displayed incrementally as expected. Instead, all output is buffered and only displayed at the end of the migration process.\n\n3. The expected behavior is explicitly stated: Output should be displayed incrementally as the migration progresses, showing each step as it happens rather than all at once at the end.\n\n4. The issue identifies a specific function (flush()) that isn't working as expected in a specific context (management commands).\n\nA developer would be able to reproduce this issue by running a migration command and observing the buffered output behavior. They would then need to investigate why the flush() calls aren't properly forcing output to be displayed immediately, likely focusing on how stdout/stderr are handled in Django management commands.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13528": {
        "explanation": "This issue is reasonably well-specified, describing a problem with Django's template filters when combining `floatformat` and `intcomma` in non-English locales (specifically German). The issue clearly identifies:\n\n1. The expected behavior: In Germany, 1000.11 should display as \"1000,11\" (comma as decimal separator)\n2. The actual behavior: When using `Decimal(1000.11)|floatformat:\"2\"|intcomma`, it displays as \"1,000.11\" (English-style formatting with comma as thousands separator)\n\nThe issue demonstrates understanding of the problem by showing that direct use of `intcomma(float(1000.11))` gives \"1.000,11\" which is closer to the expected German format.\n\nWhile the issue provides enough information to understand the problem, there are some minor ambiguities:\n- The exact Django version is mentioned (1.5.1), but it's not clear if this is the only affected version\n- The locale settings being used aren't explicitly shown (though German is mentioned)\n- There's no code sample showing the complete template context and usage\n\nDespite these minor gaps, there's enough information to investigate and attempt a solution focused on ensuring proper locale-aware formatting when chaining these template filters.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13530": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: KeyTransform for JSONField produces invalid SQL when used in the ordering attribute of ArrayAgg function.\n2. The database context is provided (PostgreSQL).\n3. Complete sample code is provided showing:\n   - Model definitions\n   - Sample data creation\n   - The exact query that fails\n4. The actual invalid SQL output is shown: `ARRAY_AGG(DISTINCT (\"children\".\"data\" ->> 'default') ORDER BY None(\"children\".\"data\"))`\n5. Version information is provided: \"This was working fine before Django 3.1\"\n\nThe issue provides all the necessary information to reproduce the problem and understand what's happening. A developer would be able to:\n1. Set up the models as described\n2. Run the query\n3. Observe the invalid SQL\n4. Investigate why KeyTransform is being converted to \"None\" in the ORDER BY clause\n5. Fix the SQL generation for KeyTransform when used in ArrayAgg ordering\n\nThe expected behavior is implied - the SQL should properly include the KeyTransform in the ORDER BY clause rather than \"None\".",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13537": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that Django's `manage.py dbshell` command doesn't respect the charset settings defined in the DATABASES configuration when connecting to MySQL. They've provided:\n\n1. A clear problem statement: the dbshell command doesn't respect database_options, specifically charset settings\n2. Their use case: running SQL scripts through dbshell for cross-database compatibility\n3. The specific issue: encoding problems when using UTF-8\n4. A proposed solution: they've attached a patch (though the actual patch isn't visible in the description)\n\nWhat makes this not fully \"well-specified\" is:\n- The actual patch they're referring to isn't visible in the issue description\n- There's no specific error message or example of the \"weird encoding issues\" they encountered\n- No details about their Django/MySQL versions\n\nHowever, the core issue is clear enough that a developer familiar with Django's database connection handling could investigate and implement a solution. The problem is focused on making the MySQL backend's dbshell command respect the charset settings from the DATABASES configuration.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13551": {
        "explanation": "This issue is well-specified with a clear problem description and a suggested solution path. The issue describes a security vulnerability where password reset tokens remain valid even after a user changes their email address, which should invalidate them. The description includes:\n\n1. A clear reproduction sequence showing the steps to encounter the issue\n2. The expected behavior (token should be rejected) vs. actual behavior (token is allowed)\n3. A specific suggested fix (add user's email address to the hash value generation)\n4. A consideration about edge cases (users without email addresses)\n\nThe suggested solution is specific - modify the `_make_hash_value()` method in `PasswordResetTokenGenerator` to include the user's email address in the hash calculation. This would ensure that when a user's email changes, any existing password reset tokens become invalid.\n\nThe issue also raises a valid consideration about users without email addresses and suggests a potential approach (factoring out the token generation method onto the model).\n\nWith this information, a developer could implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13553": {
        "explanation": "This issue is well-specified with clear reproduction steps and a proposed solution. The problem occurs when using Django's QuerySet.order_by() on a union() queryset that contains only one non-empty query, resulting in invalid SQL with multiple ORDER BY clauses. The issue description provides:\n\n1. A clear reproduction scenario with code examples\n2. The exact SQL query that's being generated\n3. Specific error messages from different database backends (PostgreSQL, SQLite)\n4. A note about MySQL behavior\n5. A proposed solution: return the non-empty query and make the union a no-op in this case\n\nThe issue is related to Django's ORM query generation and has a well-defined scope. The solution path is reasonably clear - modify the union() operation to handle the special case where one of the querysets is empty. This would prevent the generation of invalid SQL with multiple ORDER BY clauses.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13556": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem: when `transaction.on_commit(None)` is called, it leads to a TypeError because None is not callable. The issue suggests two potential solutions: either prevent None from being registered in the first place, or handle it gracefully to avoid the error.\n\nFrom this description, I can understand:\n1. The specific function causing the issue (`transaction.on_commit()`)\n2. The problematic input (passing `None`)\n3. The resulting error (`TypeError: 'NoneType' object is not callable`)\n4. Two potential approaches to fix it\n\nWhile the issue doesn't provide the exact code location or implementation details, it's clear enough what needs to be fixed: the `on_commit()` method should be modified to either reject None values with a meaningful error or handle them gracefully. This is a straightforward validation issue that can be addressed without additional clarification.\n\nThe only minor ambiguity is which of the two suggested approaches is preferred, but both are reasonable solutions that could be implemented and then discussed in a PR.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13560": {
        "explanation": "The issue describes a problem with error reporting in Django where a ProtectedError/RestrictedError contains an iterator object (specifically an itertools.chain object) that doesn't display properly when the error is reported to Sentry.\n\nThe issue provides a specific example of the error message format:\n```\n(\"Cannot delete some instances of model 'A' because they are referenced through protected foreign keys: 'A.protect', 'B.protect'.\", <itertools.chain object at 0x7f96b5cb6c40>)\n```\n\nThe problem is clear: when these errors are raised from Collector.collect, they contain iterators rather than concrete collections, which makes the error representation in Sentry not informative because iterators don't show their contents in repr().\n\nWhile the issue doesn't explicitly state the desired solution, it's reasonably clear that the fix would involve ensuring that the error contains a concrete collection (like a list) instead of an iterator, so that when the error is reported to Sentry, it shows the actual objects that are causing the protection constraint rather than just showing an iterator object reference.\n\nThe issue provides enough context to locate the relevant code in Django's Collector.collect method and modify how the ProtectedError/RestrictedError is constructed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13568": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. A specific scenario: When defining a custom User model that uses a UniqueConstraint in Meta.constraints instead of unique=True on the USERNAME_FIELD\n2. The current behavior: Django raises auth.E003 system check error\n3. The reason for the alternative approach: Avoiding extra implicit *_like indexes on PostgreSQL for CharField/TextField\n4. The proposed solution: Extend the system check to recognize UniqueConstraint in Model._meta.constraints as satisfying the uniqueness requirement\n\nThe issue provides a concrete code example demonstrating the problem and clearly explains the technical rationale. The solution direction is also specified - modify the system check to recognize UniqueConstraint as an alternative way to enforce uniqueness.\n\nA developer could implement this solution without needing additional clarification. The task involves modifying Django's authentication system checks to recognize both field-level unique=True and model-level UniqueConstraint as valid ways to ensure USERNAME_FIELD uniqueness.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13569": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, observed behavior, and even a proposed solution. The reporter has:\n\n1. Provided a complete code example with model definitions and test data\n2. Shown three different query scenarios with their outputs\n3. Identified the specific problem: using `order_by('?')` (random ordering) unexpectedly breaks queryset aggregation\n4. Included the SQL query that's being generated, showing how the GROUP BY clause is being affected\n5. Traced the issue to a specific location in Django's codebase (the `get_group_by` method in the SQL compiler)\n6. Proposed a patch that fixes the issue by excluding Random expressions from being added to the GROUP BY clause\n7. Verified the patch doesn't break existing tests\n\nThe issue is clearly about a bug where random ordering (`order_by('?')`) is incorrectly affecting GROUP BY clauses in SQL queries, causing aggregations to be broken down when they shouldn't be. The expected behavior is that random ordering should not affect the aggregation results.\n\nA developer could implement and test a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13578": {
        "explanation": "This issue is well-specified with a clear problem description and a concrete solution suggestion. The issue describes:\n\n1. The exact error scenario: When a user incorrectly passes a prefix as a positional argument instead of a keyword argument to a FormSet\n2. The specific error message that appears: \"ManagementForm data is missing or has been tampered with\"\n3. The exact file and line number where the error occurs: django/forms/formsets.py, line 57\n4. A clear suggestion for improvement: Include the form._errors in the output to make debugging easier\n5. An example of what those errors look like, which would be helpful to include\n\nThe issue provides enough context to understand the problem, locate the code that needs to be modified, and implement the suggested improvement. The solution would involve enhancing the error message in the ValidationError to include the specific form errors that would help developers quickly identify the issue with their formset prefix usage.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13589": {
        "explanation": "This issue is well-specified with a clear description of the problem and expected behavior. The user provides:\n\n1. A complete code example demonstrating the issue\n2. The exact error message received\n3. A clear explanation of the current behavior vs. expected behavior\n4. A workaround they're currently using\n5. A specific question about why the behavior works this way and a suggestion for improvement\n\nThe issue describes a specific Django ORM behavior where setting a related object (ForeignKey) on a model instance and then saving the parent object doesn't automatically update the foreign key ID in the child object. This causes integrity errors when bulk creating objects.\n\nThe user is asking if there's a reason this behavior can't be improved so that when a parent object is saved, it would automatically update any child objects that reference it with the newly assigned primary key.\n\nThis is a concrete, reproducible issue with a clear path to investigate and potentially fix in Django's ORM implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13590": {
        "explanation": "This issue is well-specified with clear details about the problem and a suggested solution. The description provides:\n\n1. The context: Upgrading from Django 2.2 to 3.0\n2. The specific problem: Named tuples used as arguments to __range filters that worked in 2.2 now cause errors in 3.0\n3. The exact error message: \"TypeError: __new__() missing 1 required positional argument: 'far'\"\n4. The root cause: In Query.resolve_lookup_value, Django tries to reconstruct named tuples by passing an iterator to the constructor, but named tuples don't support this pattern\n5. A specific solution: Use the * operator to expand the iterator contents into the constructor\n\nThis provides all the necessary information to locate the issue in the codebase, understand the problem, and implement the suggested fix. The developer would need to find the resolve_lookup_value method in django.db.models.sql.query.Query and modify how it reconstructs named tuples.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13606": {
        "explanation": "This issue is well-specified for an experienced software engineer familiar with SQL query optimization. The description clearly identifies:\n\n1. The specific problem: The current implementation in `sql.Query.split_exclude` uses `NOT IN` with subqueries for filtering multi-valued relationships\n2. Why it's problematic: This approach is difficult for query planners to optimize and has issues with NULL value handling\n3. The proposed solution: Replace `NOT IN` with `NOT EXISTS` in the generated SQL\n\nThe issue provides enough technical context for someone familiar with the codebase to locate the relevant code in `sql.Query.split_exclude` and understand what needs to be changed. The solution direction is clear - replace the current `parent.id NOT IN (SELECT child.parent_id ...)` pattern with an equivalent `NOT EXISTS` pattern.\n\nThis is a common SQL optimization pattern that any experienced database developer would understand. The only minor detail missing is specific examples of the current and desired SQL output, but this isn't necessary as the transformation is a standard SQL pattern.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13615": {
        "explanation": "The issue is asking to add a warning in the makemessages command when an incorrect locale code is provided with the `-l` flag. The description suggests normalizing locale codes to a standard format (e.g., converting various forms like \"zh_cn\", \"ZH_CN\", \"ZH-CN\" to the standard \"zh_CN\").\n\nThe issue provides:\n1. Clear examples of the problem (various ways developers might input locale codes)\n2. A proposed solution (normalize the directory name)\n3. The specific command affected (makemessages)\n\nHowever, there are some details that aren't fully specified:\n- What constitutes a \"correct\" locale code? Is there a specific standard to follow?\n- Should the command just warn about incorrect codes or actually normalize them automatically?\n- What should happen if normalization isn't possible?\n\nDespite these questions, there's enough information to understand the problem and implement a reasonable solution based on locale code standards. A developer could implement normalization of locale codes in the makemessages command with appropriate warnings when non-standard formats are detected.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13616": {
        "explanation": "The issue describes a problem with Django's `makemessages` command when the `/locale` directory doesn't exist under an app directory. The error message \"Unable to find a locale path to store translations for file conftest.py\" is reported as unclear and not helpful for users to understand what's wrong or how to fix it.\n\nThe issue is reasonably specified because:\n1. It clearly identifies the problem: the error message from `makemessages` is not user-friendly\n2. It provides the exact error message that's problematic\n3. It suggests potential improvements (mentioning \"locale\" directory in the error message)\n4. It mentions that even after improving the error message, the command fails silently\n5. It proposes potential solutions (including LOCALE_PATHS by default or improving error messages)\n\nHowever, there are some details that would be helpful but are missing:\n1. The exact Django version being used\n2. The complete command that was run\n3. The project structure where this occurred\n4. Whether the user was trying to create translations for a specific app or the entire project\n\nDespite these missing details, there's enough information to understand the issue and work on improving the error message and handling of missing locale directories in the `makemessages` command.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13620": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is reporting that when using Django's `call_command` with a command that has a mutually exclusive group containing an argument with `nargs='+'` and `type=int`, passing a list of integers doesn't work properly. \n\nThe issue demonstrates two specific problems:\n1. When passing a list directly (`foo=[1, 2, 3]`), it raises an error because it's trying to parse the entire list as a single integer\n2. When passing multiple arguments (`'--foo=1', '--foo=2', '--foo=3'`), only the last value is kept\n\nThe user also mentions their current workaround (setting `type=str` and manually converting), which helps clarify the issue further.\n\nThe expected behavior is clearly implied: the `call_command` function should properly handle list arguments inside mutually exclusive groups, similar to how it would handle them outside such groups.\n\nThis issue contains all the necessary information to understand the problem, reproduce it, and work on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13658": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The problem is clearly identified: ManagementUtility computes a program name (prog_name) from the passed argv parameter, but when it instantiates CommandParser, it doesn't pass this computed value, causing CommandParser to use sys.argv[0] instead.\n\n2. The issue description includes relevant code snippets showing both the current implementation and the proposed fix.\n\n3. The specific change required is explicitly stated: add a 'prog=self.prog_name' parameter when instantiating CommandParser.\n\n4. The submitter explains the context and why this fix matters - they're using Django in an embedded environment where sys.argv[0] is None, causing exceptions.\n\nThe solution is straightforward and doesn't require additional information or clarification. A developer could implement this fix with the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13660": {
        "explanation": "This issue is well-specified with clear problem description, reproduction steps, and a suggested solution:\n\n1. The problem is clearly defined: When using `python -m django shell -c` with Python code that contains function definitions, variables defined at the module level are not accessible within those functions.\n\n2. Reproduction steps are provided with exact commands and expected vs. actual output.\n\n3. The root cause is identified: The issue is in the `shell.py` command's `handle` method where `exec(options['command'])` is called without providing a globals dictionary.\n\n4. A solution is suggested: Pass an empty dictionary as the second argument to `exec()` to create a proper scope for the executed code.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a fix. The fix would involve modifying the `exec()` call in Django's shell command to properly handle variable scoping when executing code with function definitions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13665": {
        "explanation": "This issue is well-specified with clear technical details about the problem. The issue describes:\n\n1. The specific problem: When using `serialized_rollback=True` in Django tests, the database serialization is using `_default_manager` which is a custom manager that filters out some records, causing test failures.\n\n2. A concrete example with code snippets showing:\n   - A `Service` model with a custom manager that excludes records with pk=1\n   - A test class using `serialized_rollback=True`\n   - A migration that loads fixtures\n\n3. The root cause: The serialization mechanism is using the custom manager which filters records, causing inconsistencies in the test database.\n\n4. A proposed solution: A link to a PR (#13150) that presumably fixes the issue by using `base_manager` instead of `_default_manager` for serialization.\n\nThe issue provides enough context to understand the problem, reproduce it, and evaluate the proposed solution. A developer would be able to implement a fix based on this description, which appears to involve modifying Django's serialization mechanism to use `base_manager` instead of `_default_manager` when serializing the database for tests.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13667": {
        "explanation": "This issue is well-specified with clear technical details about the problem and the desired solution:\n\n1. The problem is clearly defined: QuerySet.union().exists() doesn't apply the same optimizations that QuerySet.exists() does, resulting in inefficient SQL for some database engines (specifically mentioning MySQL).\n\n2. The current behavior is explicitly shown with an example SQL query that is generated.\n\n3. The desired behavior is explicitly shown with an example of the optimized SQL that should be generated.\n\n4. A current workaround is provided, demonstrating that the solution is possible but requires verbose code.\n\n5. The scope is well-defined - the optimization applies to union operations but not to intersection and difference operations, with a clear explanation of why.\n\nThe issue provides enough technical context for a developer familiar with Django's QuerySet implementation to understand what needs to be changed. The solution would involve extending the existing optimization logic in QuerySet.exists() to handle the union case appropriately.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13670": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies that Django's dateformat.y() function doesn't properly handle years less than 1000, specifically not adding leading zeros for the last two digits.\n\n2. The reporter provides concrete examples comparing Django's behavior with Python's datetime and PHP's date functions, showing the inconsistency.\n\n3. The expected behavior is clearly implied: Django's dateformat.y() should return the last two digits of the year with leading zeros if needed, matching Python's and PHP's behavior.\n\n4. The issue even includes a reference to a pull request that the reporter has already prepared to fix the issue.\n\nThe problem is specific, reproducible, and has a clear expected outcome. A developer would have all the information needed to understand and address the issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13682": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. The specific problem: When users forget to call `.as_view()` on a Class-Based View (CBV) in URL patterns, Django currently produces a confusing error message that only appears at runtime and doesn't clearly indicate what went wrong.\n\n2. A concrete example of the problematic code and the resulting error message.\n\n3. Why this is problematic: The error is difficult to debug, especially for beginners, as the traceback doesn't include the user's code.\n\n4. The proposed solution: Detect this error earlier (during URL pattern construction) and provide a more helpful error message that explicitly tells the user they need to call `.as_view()`.\n\n5. A link to a pull request that implements this solution.\n\nThe issue provides enough context to understand the problem, why it matters, and what a successful solution would look like. The PR link also gives a concrete implementation to reference. Anyone familiar with Django's URL routing and Class-Based Views would understand exactly what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13684": {
        "explanation": "This issue is well-specified with a clear problem description and a suggested solution. The issue describes a misleading error message that appears when loading a database backend fails with an ImportError. \n\nThe problem is clearly demonstrated with a specific example: when django_cockroachdb is installed but has an import error itself, Django reports \"isn't an available database backend\" and suggests using other backends, which is misleading because the real issue is that the backend couldn't be imported due to a dependency problem.\n\nThe issue reporter even suggests a specific improvement to the error message: \"'django_cockroachdb' isn't an available database backend or couldn't be imported. Check the above exception.\" They also suggest that the \"Try using...\" part of the message might only be appropriate when the backend name begins with \"django.\"\n\nWith this information, a developer could:\n1. Locate the error message in django/db/utils.py\n2. Modify the error handling to distinguish between \"backend doesn't exist\" and \"backend exists but failed to import\"\n3. Implement the suggested improved error message that points users to check the actual exception\n\nThe issue provides enough context, a clear reproduction case, and a reasonable suggestion for improvement.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13689": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user has identified that when using an ExpressionWrapper in a Django ORM query with a group by operation, the expression is being omitted from the GROUP BY clause in the generated SQL. \n\nThe issue includes:\n1. The specific Django version (3.1.3) and database (Postgres)\n2. A complete code example that demonstrates the problem\n3. The actual SQL query that is generated\n4. The specific issue - the GROUP BY clause only includes \"main_fred\".\"bob_id\" but should include \"main_fred\".\"bob_id\" IS NULL\n\nThe expected behavior would be for the GROUP BY clause to include the full expression \"main_fred\".\"bob_id\" IS NULL instead of just \"main_fred\".\"bob_id\", since the query is grouping by the \"bob_id__is_null\" annotation.\n\nThis is a clear bug with a specific technical issue that can be investigated and fixed without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13691": {
        "explanation": "The issue is reasonably well-specified with clear information about the problem:\n\n1. The issue occurs in Django 3.1.3 specifically with SQLite (not with MySQL or Postgres)\n2. The problem happens when using JSON path lookup with `values()` method: `Bob.objects.values(\"my_json_field__position\")`\n3. The error occurs only when the value at the \"position\" key is a numeric type (integer, float) or boolean\n4. The error doesn't occur with strings, None values, dictionaries, or lists\n5. The error message is clearly provided: \"TypeError: the JSON object must be str, bytes or bytearray, not float\"\n6. The stack trace is included showing where the error occurs in Django's internals\n\nThe issue appears to be a bug in Django's SQLite JSON field handling where numeric values in JSON path lookups are not properly processed. The error occurs in Django's JSON field handling code when trying to convert the database value.\n\nWhile the issue description doesn't include a minimal reproducible example with model definitions, the problem is specific enough that a developer familiar with Django's ORM and JSON fields should be able to understand and attempt to fix it. The stack trace provides good context for where to look in the code.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13693": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific error that occurs when running `django-admin runserver` on Windows with Python < 3.8\n2. It provides a complete error traceback showing exactly where the failure occurs\n3. It specifies the exact environment (Windows 10, Python 3.7.6, Django 3.1.3)\n4. It includes clear steps to reproduce the issue\n5. It identifies the root cause: `django.utils.autoreload.get_child_arguments` returns a `WindowsPath` object, but `subprocess.Popen` expects a string\n6. It even provides a working monkey-patch solution that demonstrates how to fix the issue\n\nThe solution would need to modify the `get_child_arguments` function in Django's autoreload module to convert any `Path` objects to strings before passing them to `subprocess.Popen`. The issue description provides all the necessary information to understand, reproduce, and fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13708": {
        "explanation": "This issue is well-specified and provides clear context for what needs to be implemented. The request is to add a 'durable' flag to Django's transaction.atomic() function that would ensure operations are not only atomic but also durable (committed to disk) when the block completes.\n\nThe issue description:\n1. Clearly explains the current behavior of transaction.atomic() (guarantees atomicity but not durability)\n2. Defines the problem: nested atomic blocks delay commits, preventing durability guarantees\n3. Describes the proposed solution: add a 'durable' flag that would check if an atomic block is already active and raise an error if so\n4. References existing implementations and discussions (David Seddon's blog post and Twitter conversation)\n5. Mentions that a core Django developer (Aymeric Augustin) has already indicated support for the idea\n\nThe implementation would involve:\n- Adding a 'durable' parameter to transaction.atomic()\n- When durable=True, checking if an atomic block is already active\n- Raising an error if a durable atomic block is nested inside another atomic block\n\nThis is a well-defined feature request with clear acceptance criteria and implementation guidance.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13710": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The author explains:\n\n1. The current behavior: Django's Admin Inline classes require specifying both `verbose_name` and `verbose_name_plural` separately, even when you want to override the default naming.\n\n2. The inconsistency: This differs from how Django's model Meta class works, where `verbose_name_plural` is automatically derived from `verbose_name` if only the latter is specified.\n\n3. The proposed solution: Make Admin Inline's `verbose_name_plural` automatically derive from `verbose_name` when only `verbose_name` is specified, consistent with how model Meta classes work.\n\n4. Implementation status: The author has already written a patch with tests and is ready to submit it.\n\nThe issue clearly identifies the problem, explains why it's confusing/inconsistent, proposes a specific solution that aligns with existing Django patterns, and indicates that implementation work is already complete. A developer could understand exactly what needs to be done and why from this description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13714": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific scenario: when using spatialite/sqlite backend with a non-existent database file path, Django raises a misleading error about JSONFields not being supported rather than a file path error.\n\n2. The issue provides:\n   - Exact configuration (DATABASES setting)\n   - The exact error message received\n   - The verification steps taken to check JSON extension support\n   - The actual solution (fixing the database path)\n   - The core problem (misleading error message)\n\n3. The environment is clearly specified (Django 3.1.3, Debian Buster Docker)\n\nThe issue is asking for the error handling to be improved so that when a database file path is invalid, the error message correctly indicates the file path problem rather than misleadingly suggesting JSON extension issues.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Understand the root cause\n3. Implement a fix that improves error handling for invalid SQLite database paths\n\nThe solution would likely involve modifying the error handling in Django's SQLite/SpatiaLite database backend to check for file path validity before attempting to validate JSON field support.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13722": {
        "explanation": "The issue is asking for a new feature to add a method on InlineModelAdmin that would allow customizing the initial data for inline formsets in the Django admin interface. \n\nThe description provides:\n1. Clear target: Add a method on InlineModelAdmin class\n2. Purpose: To provide initial data for inline formsets\n3. Context: Similar to the existing get_changeform_initial_data method, but adapted for formsets\n\nHowever, there are some details that aren't fully specified:\n- The exact name of the new method isn't provided\n- The exact signature/parameters of the method aren't specified\n- The implementation details are left open (intentionally)\n- No examples of use cases are provided\n\nDespite these gaps, a developer familiar with Django's admin system would understand what's being requested. The issue is asking for a hook method that would allow customizing formset initial data, similar to how get_changeform_initial_data works for regular forms. The implementation would need to account for the list nature of formset data.\n\nThis is reasonably specified because a developer could implement a solution based on this description, though they would need to make some decisions about the exact API design.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13741": {
        "explanation": "This issue is well-specified with a clear problem and proposed solution:\n\n1. Problem: The current `UserChangeForm` requires a `clean_password` method to prevent accidental changes to password values, and custom forms need to implement this method too.\n\n2. Proposed solution: Set the `disabled=True` property on the `ReadOnlyPasswordHashField` by default, which would make the `clean_password` method unnecessary since disabled fields ignore submitted values in favor of initial values.\n\nThe issue provides:\n- Clear context about the current implementation\n- Reference to documentation\n- Explanation of the `disabled` property and how it works\n- A specific proposal for how to fix the issue\n\nA developer could implement this solution without needing additional clarification. The change would involve modifying the `ReadOnlyPasswordHashField` class to set `disabled=True` by default, and potentially updating documentation to reflect that `clean_password` is no longer necessary when using this field.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13743": {
        "explanation": "The issue is reasonably specified, providing clear context about a problem with ConnectionAbortedError in Django's development server on Windows systems. The issue:\n\n1. Identifies the specific problem: ConnectionAbortedError is causing long stack traces in the development server on Windows\n2. Provides references to related issues in CPython and Python's bug tracker\n3. Mentions that this happens during normal interaction with Chrome\n4. Points to a specific file and function that needs modification: is_broken_pipe_error in django/core/servers/basehttp.py\n5. Indicates that ConnectionAbortedError should be treated as a broken pipe error\n\nWhat's missing:\n- The exact code change needed (though this can be determined by examining the file)\n- Specific test cases or reproduction steps\n- The exact error messages being displayed\n\nHowever, an experienced developer could locate the is_broken_pipe_error function in the specified file, understand its purpose, and modify it to also catch ConnectionAbortedError exceptions. The solution direction is clear even if all implementation details aren't provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13744": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly stated: python-memcached is unmaintained and causing difficulties in fixing issues.\n2. The context is provided: A new cache backend for pymemcache has been added as an alternative.\n3. The proposed solution is explicit: Deprecate django.core.cache.backends.memcached.MemcachedCache in Django 3.2 and remove it in Django 4.1.\n4. The issue references specific GitHub issues for context (#29867, #29887, python-memcached#95).\n\nThe implementation would involve:\n1. Adding deprecation warnings to the MemcachedCache backend in Django 3.2\n2. Updating documentation to indicate the deprecation and recommend alternatives\n3. Planning for removal in Django 4.1\n\nThis is a standard deprecation process in software development with clear steps and timeline, making it well-specified for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13768": {
        "explanation": "This issue is well-specified with a clear problem and solution:\n\n1. The problem is clearly identified: Django's `Signal.send_robust()` method silently catches exceptions from signal receivers without logging them, making it difficult for developers to be aware of these issues.\n\n2. The proposed solution is specific: Add a `logger.exception()` call in the `except` clause of `send_robust()` to log these exceptions.\n\n3. The rationale is provided: Django already logs exceptions in similar situations (like missing template variables), and logging would help users see these exceptions in their error handling tools like Sentry.\n\n4. The context is clear: This is about improving developer experience by making errors more visible, while noting that expected exceptions should still be caught in the receiver function.\n\nThe implementation would be straightforward - locate the `send_robust()` method in Django's signal handling code, identify the exception handling block, and add appropriate logging. There's no ambiguity about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13773": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific scenario: when renaming fields in a circular pattern (a->c; b->a; c->b) and then using squashmigrations with an optimization barrier.\n\n2. The exact failure is described: the migration optimizer tries to create a CreateModel operation with duplicate field names, which fails.\n\n3. The submitter has identified a likely root cause: RenameField operations are being incorrectly optimized when they reference old_name.\n\n4. The submitter mentions they'll attach a migration file that reproduces the issue (though we don't see it in this excerpt, it suggests they have a concrete reproduction case).\n\nThe issue provides enough context for a developer to understand the problem, reproduce it, and investigate the suspected area of code. The description of the circular renaming pattern and the optimization barrier makes it clear what conditions trigger the bug. A developer could work on this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13774": {
        "explanation": "This issue is well-specified with clear details about the problem. The reporter has:\n\n1. Clearly identified the function with the issue: `prefetch_related_objects()`\n2. Described the specific scenario where the bug occurs: when processing repeated model instances in a stream\n3. Explained the exact behavior: the `to_attr` is not set on all instances if the first instance already has it set\n4. Provided the technical reason for the issue: Django checks if the first instance `is_fetched` and skips calling `prefetch_one_level()` which is responsible for setting the `to_attr`\n5. Included specific links to the relevant code in Django with line numbers, showing where the issue occurs\n\nThe issue provides enough context to understand the problem and the underlying cause. A developer could reproduce this issue and implement a fix based on this information. The expected behavior is clear: `prefetch_related_objects()` should set the `to_attr` on all instances, even if some instances already have it set.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13786": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is precisely identified: when squashing migrations, an `AlterModelOptions` operation with empty options (`{}`) doesn't properly clear model options when combined with a `CreateModel` operation.\n\n2. The specific code location is provided: `django/db/migrations/operations/models.py` line 144 on commit 991dce4f.\n\n3. The technical cause is explained: `CreateModel.reduce()` uses `options={**self.options, **operation.options}` which merges options but doesn't handle the case where options should be removed.\n\n4. The issue contrasts this with `AlterModelOptions.state_forwards()` which does have logic to remove options.\n\n5. The reporter has tested against Django 2.2 and believes the issue persists in the master branch.\n\nA developer could reproduce this issue and understand what needs to be fixed: the `CreateModel.reduce()` method needs to be updated to properly handle the case where `AlterModelOptions` with empty options should clear existing options rather than just merging them.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13791": {
        "explanation": "This issue is well-specified with clear evidence of a bug in Django's `parse_duration()` function. The reporter has:\n\n1. Clearly identified the problem: negative signs in ISO duration strings are ignored when the timedelta only contains days\n2. Provided specific examples demonstrating the issue with input/output pairs\n3. Included the expected vs. actual behavior\n4. Pointed to a specific line in the code (django/utils/dateparse.py line 147) that likely contains the bug\n5. Offered a reasonable hypothesis about why the bug occurs (sign multiplication with days)\n\nThe issue includes enough information for a developer to reproduce the problem and understand what needs to be fixed. The solution would involve ensuring that the sign is properly applied to the days component in the parse_duration function.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13794": {
        "explanation": "This issue is reasonably well-specified. It clearly describes a bug where the `add` template filter fails when attempting to concatenate a regular string with a lazy string, resulting in an empty string due to a TypeError. The error message is provided: \"can only concatenate str (not \"__proxy__\") to str\".\n\nThe issue identifies:\n1. The specific component affected (the `add` template filter)\n2. The specific operation that fails (concatenating a string with a lazy string)\n3. The observed behavior (empty string result)\n4. The underlying error (TypeError with specific message)\n\nWhat's missing is:\n1. Steps to reproduce the issue (a code example would be helpful)\n2. The expected behavior (though it's implied that string concatenation should work)\n3. Information about the environment or version where this occurs\n\nDespite these missing details, a developer familiar with the codebase should be able to:\n1. Understand what a lazy string is in this context\n2. Locate the `add` filter implementation\n3. Reproduce the issue\n4. Implement a fix that properly handles string concatenation with lazy strings\n\nThe issue is specific enough to guide a solution approach without requiring significant guesswork.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13797": {
        "explanation": "The issue is reasonably specified. It clearly states the goal: to add compression support to the dumpdata command, similar to how the loaddata command already supports compressed fixtures. The issue includes a reference to the Django documentation about compressed fixtures for loaddata, which provides context about the existing functionality that should be mirrored.\n\nWhat makes this reasonably specified rather than well-specified:\n1. It doesn't specify which compression formats should be supported (though we can infer from the linked documentation that at minimum, gzip, bzip2, and zip should be supported as these are supported by loaddata).\n2. It doesn't detail the exact command-line interface for this feature (e.g., what flags or options should be added).\n3. It doesn't specify implementation details or edge cases to handle.\n\nHowever, these gaps can be reasonably filled by:\n1. Following the same compression formats supported by loaddata\n2. Using a similar command-line interface pattern as other Django commands\n3. Following Django's coding conventions and patterns\n\nA developer familiar with Django could implement this feature by examining how compression works in loaddata and applying similar patterns to dumpdata. The goal is clear enough to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13807": {
        "explanation": "This issue is well-specified with clear reproduction steps, error messages, and root cause identification. The problem occurs when Django's loaddata command tries to work with tables named after SQL keywords (like \"order\") in SQLite databases. The issue description:\n\n1. Provides exact steps to reproduce the bug\n2. Shows the specific error message that occurs\n3. Identifies the root cause in the Django codebase (missing backticks around table names in SQL statements)\n4. Points to the exact file, function, and line numbers where the issue occurs\n5. Shows the problematic code in context\n6. Specifies the Django versions where the issue is confirmed\n\nA developer would have a clear understanding of what needs to be fixed: the SQL statements in the check_constraints function need to properly quote table names to handle SQL keywords. The solution would involve modifying the identified code to properly escape table names in the PRAGMA statements.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13808": {
        "explanation": "This issue is well-specified with clear requirements. The user wants to enhance Django's PostgreSQL database backend to better support Postgres service configurations. \n\nThe issue:\n1. Clearly explains what Postgres services are and how they work\n2. Shows the current workaround (using OPTIONS with 'service' parameter but requiring redundant NAME field)\n3. Proposes two specific implementation alternatives\n4. Provides context on why this would be useful (avoiding redundancy, making configuration more portable)\n\nThe technical requirements are straightforward - modify Django's PostgreSQL backend to properly handle service configurations without requiring redundant information. The user even offers to create a PR themselves.\n\nThe only minor ambiguity is which of the two proposed solutions would be preferred, but this is a design choice that could be made during implementation with appropriate justification. The core requirement is clear enough to proceed with a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13809": {
        "explanation": "This issue is well-specified with a clear request: add a `--skip-checks` option to Django's `runserver` command. The rationale is clearly explained - it would make the `runserver` command consistent with other management commands that already have this option, and it would improve development workflow by reducing server startup time (15-20 seconds mentioned specifically). \n\nThe issue provides:\n1. The exact feature needed (a `--skip-checks` flag for runserver)\n2. The motivation behind it (consistency and development speed)\n3. A reference to a related StackOverflow question for additional context\n\nA developer would know exactly what to implement - they need to modify the `runserver` command to accept a `--skip-checks` option that, when provided, would bypass the system checks that normally run when starting the development server. This is a straightforward feature request with a clear implementation path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13810": {
        "explanation": "This issue is well-specified with clear technical details and a reproducible example. The user has:\n\n1. Provided a specific GitHub repository with a minimal reproduction case\n2. Clearly described the error: \"TypeError: object HttpResponse can't be used in 'await' expression\"\n3. Identified the specific conditions that trigger the issue (using ASGI with a middleware that raises MiddlewareNotUsed)\n4. Pointed to the exact code in Django that they believe is causing the problem (with a link to the specific line)\n5. Explained their understanding of why the issue occurs - the middleware chain being \"poisoned\" when MiddlewareNotUsed is raised\n6. Described a workaround (commenting out the middleware)\n7. Referenced relevant documentation\n\nThe issue describes a specific bug in Django's handling of middleware in ASGI context when MiddlewareNotUsed is raised. The expected behavior is that raising MiddlewareNotUsed should skip the middleware without affecting the rest of the middleware chain, but it appears to be causing type incompatibility issues in async contexts.\n\nA developer could take this information and investigate the middleware handling in Django's ASGI implementation, particularly focusing on how MiddlewareNotUsed affects the handler variable and subsequent middleware processing.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13814": {
        "explanation": "This issue is well-specified with clear details about the problem and how to reproduce it. The issue describes:\n\n1. The exact problem: In django/template/backends/django.py, the get_package_libraries function catches ImportError and raises InvalidTemplateLibrary instead, which masks the original error and makes debugging difficult.\n\n2. Detailed reproduction steps:\n   - Create two apps (form_utils and reports)\n   - Write a template tag in reports\n   - Add a module in form_utils that imports a non-existent module\n   - Import the form_utils module in the reports template tag\n   - Access the admin page to trigger the error\n\n3. Expected vs. actual behavior:\n   - Current behavior: Shows InvalidTemplateLibrary error with limited information\n   - Desired behavior: Show the original ImportError with full stack trace for easier debugging\n\n4. The issue even includes the exact file path, line number, and code context where the problem occurs.\n\nThis is a clear bug report with all the necessary information to understand and fix the issue. A developer could implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13820": {
        "explanation": "This issue is well-specified with clear technical details and a specific solution proposal. The author:\n\n1. Clearly identifies the problem: Django's migration loader rejects modules without `__file__` attributes, which affects frozen Python environments even when they're not namespace packages.\n\n2. Provides specific technical context: The issue explains how Django currently checks for namespace packages (using `getattr(m, '__file__', None)`) and why this is problematic (regular packages in frozen environments may also lack `__file__`).\n\n3. Proposes a specific solution: Augment the check with `isinstance(m.__path__, list)` to properly identify namespace packages based on Python's documented behavior.\n\n4. Sets clear scope boundaries: The author explicitly states they are NOT asking to allow namespace packages for migrations, just making the check more compliant with Python's import API.\n\n5. References relevant documentation and related issues (#30950, #32177).\n\n6. Indicates they will submit a PR for this change.\n\nThe issue provides enough technical detail that a developer could implement the solution without needing additional clarification. The change is narrowly scoped and the expected behavior is clearly defined.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13821": {
        "explanation": "This issue is well-specified. It clearly states the intention to drop support for SQLite versions older than 3.9.0 in Django. The description provides:\n\n1. A clear technical requirement: Drop support for SQLite < 3.9.0\n2. Justification: SQLite 3.9.0+ supports indexes on expressions and the SQLITE_ENABLE_JSON1 compile-time option\n3. Context about OS distributions: Ubuntu Xenial (3.11.0) and Debian Jessie (3.8.7) with their EOL dates\n4. Timeline reference: SQLite 3.9.0 was released in October 2015, and there's a precedent of supporting libraries for about 5 years after release (similar to GEOS libraries)\n\nThe task is straightforward - update Django's codebase to require SQLite 3.9.0 as the minimum supported version. This would involve updating version checks, documentation, and potentially removing any code specifically designed to handle older SQLite versions. The issue provides sufficient context and reasoning for this change.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13822": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that Django raises an error (fields.E305) when two models with the same name from different apps have ManyToManyFields pointing to the same model, even when using related_name='+' (which should suppress the automatic backwards relation and avoid name conflicts).\n\nThe issue includes:\n1. Clear error identification (fields.E305)\n2. Specific scenario where it occurs (ManyToManyFields with related_name='+' in models with same name but different apps)\n3. A link to a test project that reproduces the issue (https://github.com/rafick1983/django_related_name_bug)\n\nWhat makes it not perfectly specified:\n1. The exact Django version is not mentioned\n2. The full error message is not provided\n3. The expected behavior is implied but not explicitly stated (user expects related_name='+' to prevent the error)\n\nHowever, with the test repository and the clear description of the problem, a developer should be able to reproduce the issue and understand what needs to be fixed. The expected behavior is that related_name='+' should prevent the need for unique related names across models with the same name in different apps.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-13837": {
        "explanation": "This issue is well-specified with clear technical details and a proposed solution. The issue describes a limitation in Django's autoreload functionality where it only detects when Python is launched with `python -m django` but not with other packages. The description explains:\n\n1. The specific problem: `django.utils.autoreload.get_child_arguments` only detects when `-m django` is used, not other packages\n2. The context: This limits creating Django-based command-line utilities with their own `__main__` sub-module\n3. The proposed solution: Use Python's documented approach to determine if `-m` was used by checking `__main__.__spec__` properties\n4. The technical reasoning: Detailed explanation of how `__main__.__spec__.parent` can be used to determine if Python was started with `-m pkg`\n5. The benefits: The solution would work reliably and avoid using `__file__` which isn't available in all Python environments\n\nThe issue includes references to Python documentation and explains the algorithm clearly. The submitter has already created a PR with the fix. This provides all the necessary information for a developer to understand the problem and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13841": {
        "explanation": "This issue is well-specified with clear objectives and proposed solutions. The author:\n\n1. Clearly identifies the problem: Django modules using `__file__` at the module level causes issues in frozen Python environments\n2. Lists the specific modules affected (django.forms.renderers, django.views.debug, django.utils.version, django.contrib.auth.password_validation)\n3. Provides detailed context about each problematic module\n4. Explicitly states what the ticket is NOT about (removing `__file__` entirely)\n5. Proposes specific fixes with links to commits that address each module\n6. Explains the implementation approach for each module with rationale\n\nThe author has already created a PR (GH-13841) with separate commits for each module fix, making it easy to understand the proposed changes. The issue includes sufficient technical details about how `__file__` is currently used and how it should be accessed lazily instead. A developer could implement the solution based on this description without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13884": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in Django's i18n.set_language function where it incorrectly unquotes URL parameters containing encoded ampersands (%26).\n\n2. The expected behavior is clearly stated: when redirecting after changing the language, the URL parameters should remain properly encoded.\n\n3. A concrete example is provided showing:\n   - Input URL with encoded ampersand: `?paramter=some%20%26%20thing`\n   - Current incorrect output: `?paramter=some%20&%20thing`\n   - Expected output: `?paramter=some%20%26%20thing` (maintaining the encoding)\n\n4. A test case is included that reproduces the issue, showing exactly how to trigger the bug and what the expected behavior should be.\n\nThe solution would need to modify the set_language view to ensure it preserves URL encoding for special characters in parameters. This is a well-defined task with clear acceptance criteria (the test should pass).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13915": {
        "explanation": "This issue is well-specified. It clearly states the task: drop support for Python 3.6 and 3.7 in Django. The issue provides context that Django 3.2 LTS (supported until April 2024) is the last version to support these Python versions, and it mentions their end-of-life dates (December 2021 for Python 3.6 and June 2023 for Python 3.7). The issue also references Django's policy about Python version support and includes links to relevant discussions. \n\nTo implement this, a developer would need to:\n1. Update the Python version requirements in setup files\n2. Remove any compatibility code specific to Python 3.6 and 3.7\n3. Update documentation to reflect the new minimum Python version\n4. Update CI/testing configurations to remove these Python versions\n\nThe task has a clear objective and the necessary context is provided to understand why this change is needed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13924": {
        "explanation": "The issue description provides specific information about a bug in the migration system. It identifies:\n\n1. A specific commit (c86a3d80a25acd1887319198ca21a84c451014ad) that introduced the problem\n2. The issue it was trying to address (#29721)\n3. The specific technical problem: migrations are being marked as applied even when deferred SQL fails to execute\n4. The root cause: the code doesn't account for schema editor's accumulated deferred SQL that runs at SchemaEditor.__exit__ time\n\nFor an experienced developer with access to the codebase, this information is sufficient to:\n- Locate the specific commit that introduced the issue\n- Understand the original problem it was trying to solve (by looking at #29721)\n- Identify where in the code the deferred SQL execution happens (SchemaEditor.__exit__)\n- Understand the logical flaw (migrations being marked as applied despite deferred SQL failures)\n\nThe issue is technical and specific enough that a developer could reasonably implement a fix without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13925": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The user is experiencing a specific warning (models.W042) when upgrading from Django 3.1 to Django 3.2 alpha.\n2. The warning occurs on models that inherit from parent models which already define a primary key.\n3. The user provides specific error messages and examples of affected models.\n4. The user explains that these models should not be using auto-created primary keys since they inherit the primary key from their parent models.\n5. The issue is reproducible - it happens when running make_migrations after upgrading to Django 3.2 alpha.\n\nThe problem is clearly a regression in Django 3.2 alpha where the framework is not correctly recognizing inherited primary keys and is incorrectly suggesting that DEFAULT_AUTO_FIELD should be configured.\n\nA developer would have enough information to:\n1. Reproduce the issue by creating models with inherited primary keys\n2. Verify the behavior difference between Django 3.1 and 3.2 alpha\n3. Investigate why Django 3.2 is not correctly recognizing inherited primary keys\n4. Implement a fix to properly detect and handle inherited primary keys\n\nThe issue description provides all necessary context to understand and address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13933": {
        "explanation": "This issue is well-specified with a clear problem and solution path. The issue identifies that ModelChoiceField does not include the invalid value in its validation error message, unlike other field types like ChoiceField. The description points out the exact inconsistency in the error messages between ModelMultipleChoiceField (which includes \"%(value)s is not one of the available choices\") and ModelChoiceField (which only says \"That choice is not one of the available choices\" without showing the invalid value).\n\nThe solution approach is also clearly outlined: pass parameters with the invalid value and modify the default error message for the 'invalid_choice' code to include the value placeholder, similar to how it's done in ModelMultipleChoiceField.\n\nThe issue provides the relevant code snippets showing the current implementation, making it easy to understand what needs to be changed. A developer would know exactly what to modify: update the default_error_messages dictionary in ModelChoiceField to include the %(value)s placeholder in the 'invalid_choice' message, and ensure the validation code passes this value when raising the error.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13952": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly identified: migrate command's verbose output from signal emissions (emit_pre_migrate_signal and emit_post_migrate_signal) is always sent to sys.stdout rather than being directed to the provided stdout parameter.\n\n2. The context is clear: When using call_command to invoke migrate with verbosity flags, the output pollutes sys.stdout instead of being captured by the provided stdout parameter.\n\n3. There's a specific reference to a pull request review that discovered the issue, providing additional context.\n\n4. The solution direction is implied: modify the signal emission functions to respect the provided stdout parameter rather than always using sys.stdout.\n\nA developer would understand exactly what needs to be fixed - the signal emission functions need to be updated to accept and use a stdout parameter rather than writing directly to sys.stdout. This would allow proper output capturing during testing.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13964": {
        "explanation": "This issue is well-specified with a clear description of the problem, including:\n\n1. A precise code example demonstrating the problematic behavior\n2. The expected behavior versus the actual behavior\n3. A working alternative that shows the correct functionality\n4. The specific conditions under which the bug occurs (parent object with foreign key to a model with non-numeric primary key)\n5. A hypothesis about the root cause (how Order.product_id field is handled at assignment)\n\nThe issue describes a specific bug where setting a foreign key relationship to a newly created object that doesn't have its primary key set at assignment time, but is set afterwards, results in the parent object not recognizing the primary key change. This leads to data loss as the foreign key field gets set to an empty string instead of the actual primary key value.\n\nThe reproduction steps are clear and complete, showing both the failing case and a working alternative. The expected behavior is also clearly stated - the foreign key should reflect the primary key of the related object even if that primary key is set after the relationship is established.\n\nA developer would have all the information needed to investigate and fix this issue without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-13992": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user clearly describes what they were trying to do: delete the first Comment of each Post using `Comment.objects.order_by('post_id', 'created_at').distinct('post_id').delete()`\n\n2. They explain the expected behavior versus the actual behavior: they expected only the first comment of each post to be deleted, but instead all comments were deleted.\n\n3. They verified the issue by checking the SQL generated: `DELETE FROM comments;` without any filtering.\n\n4. They identified the inconsistency between `.count()` (which respects the distinct) and `.delete()` (which ignores it).\n\n5. They even suggest potential solutions: either raise an error or produce the expected SQL.\n\nThe issue describes a specific bug in Django's ORM where using `.distinct()` with `.delete()` doesn't work as expected, and instead deletes all records. This is a well-defined problem with clear reproduction steps and expected behavior. A developer could implement a solution based on this description without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14007": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user has:\n\n1. Clearly identified the issue: database converters (specifically `from_db_value`) are not being called for returning fields on insert operations.\n\n2. Provided a complete code example showing the custom field definition (`MyAutoField`), model implementation, and the expected vs. actual behavior.\n\n3. Demonstrated the issue with concrete examples showing:\n   - Normal query behavior (where the converter works correctly)\n   - Insert behavior with `create()` (where the converter is not applied)\n   - Bulk create behavior (also showing the issue)\n\n4. Included the specific technical context (Django ORM field customization)\n\nThe issue is reproducible based on the information provided, and the expected behavior is clear: the `from_db_value` converter should be applied to returned primary key values after insert operations, just as it is for query operations. This would ensure consistent behavior where `MyIntWrapper` is always used instead of raw integers.\n\nA developer could implement a solution without needing additional clarification, as the problem scope is well-defined and the expected behavior is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14011": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly defined: LiveServerTestCase's ThreadedWSGIServer doesn't close database connections properly after each thread, causing \"database is being accessed by other users\" errors during test teardown.\n\n2. The issue provides specific context:\n   - Django version (2.2.17)\n   - Reference to a previous issue (#22414) that was fixed in 1.11 but reappeared\n   - The exact error message being encountered\n   - The race condition nature (happens ~50% of the time)\n\n3. The reporter has done investigation:\n   - Identified the likely cause (ThreadedWSGIServer vs WSGIServer)\n   - Provided a working workaround with code example\n   - Referenced relevant documentation about ThreadingMixIn\n   - Included a quote from a developer who anticipated this type of issue\n\nA developer could confidently implement a solution based on this information - likely modifying the ThreadedWSGIServer implementation to properly close database connections when threads terminate. The issue has enough detail to reproduce the problem and understand the root cause.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14014": {
        "explanation": "The issue description states that introspection doesn't return column order for unique constraints on SQLite. While this is a clear statement of a problem, it lacks some important details:\n\n1. It doesn't specify which introspection API or method is being used\n2. It doesn't provide any code examples showing the current behavior\n3. It doesn't explain the expected behavior or why column order matters for unique constraints\n4. It doesn't mention if this is a regression or a new feature request\n\nHowever, for someone familiar with database introspection in the codebase, this might be enough information to understand the problem. The issue is likely referring to the database introspection functionality that retrieves metadata about database objects, and specifically that when retrieving information about unique constraints in SQLite databases, the order of columns in those constraints is not being preserved or returned.\n\nA developer with knowledge of the codebase would likely be able to:\n1. Locate the SQLite introspection code\n2. Identify how unique constraints are currently handled\n3. Compare with how column order is handled for other database backends\n4. Implement a solution that preserves column order information\n\nWhile more details would be helpful, this issue is specific enough about the problem to be actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14016": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue describes a specific bug where using the `|` (OR) operator with Q objects fails with a \"TypeError: cannot pickle\" when one of the Q objects contains a `dict_keys` object (from using `.keys()`). \n\nThe description includes:\n1. A clear code example that reproduces the error\n2. The exact error message\n3. A potential workaround (using `Q(x__in={})` instead)\n4. The expected behavior (that using `.keys()` should work)\n\nThe issue is related to Django's Q objects and their inability to handle certain non-pickleable types like `dict_keys`. The solution would likely involve modifying how Q objects handle these types during the OR operation, possibly by converting `dict_keys` objects to lists or other pickleable types.\n\nA developer could work on this issue without needing additional clarification, as the problem is clearly defined and reproducible.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14017": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user has identified that there's an asymmetry in how the `&` operator works between `Q` and `Exists` objects in Django:\n\n1. `Exists(...) & Q(...)` works correctly\n2. `Q(...) & Exists(...)` raises a TypeError\n\nThe error message and traceback are provided, showing exactly where the problem occurs in Django's code. The user has even suggested a likely cause - a missing `__rand__` method implementation.\n\nThe issue has:\n- A clear description of the problem\n- A minimal reproducible example with input and output\n- The exact error message and traceback\n- A reasonable hypothesis about the cause\n\nA developer would have enough information to locate the issue in Django's codebase and implement a fix that makes the `&` operator work commutatively between `Q` and `Exists` objects.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14019": {
        "explanation": "The issue is asking to make the `__repr__` methods consistent across several database-related classes: Index, UniqueConstraint, ExclusionConstraint, and CheckConstraint. \n\nWhile the issue is brief, it clearly identifies:\n1. The specific classes that need modification (Index, UniqueConstraint, ExclusionConstraint, CheckConstraint)\n2. The specific method to be modified (`__repr__`)\n3. The goal: to make the formatting consistent across these classes\n\nWhat's missing is:\n- Specific details about how the current implementations differ\n- Which format should be considered the \"correct\" one that others should match\n- Any specific requirements for the new consistent format\n\nHowever, these details can be determined by examining the current code for these classes and making a reasonable judgment about the best approach to standardize them. A developer with access to the codebase could review the current implementations, identify the inconsistencies, and propose a standardized format.\n\nThis is a straightforward task for someone familiar with the codebase, as it involves reviewing and standardizing string representation methods across related classes.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14030": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific problem: Django admin's login and logout views use the `never_cache` decorator directly on instance methods, but they should be using `method_decorator` instead.\n\n2. It explains why this is problematic: While `never_cache` works because it only operates on the response, other decorators that use the request (like `required_http_methods`) will fail with a specific error: `AttributeError: 'CustomAdminSite' object has no attribute 'method'`.\n\n3. It provides context about why this happens: The first argument to the decorator is supposed to be the request, but it's actually the admin_site instance (self).\n\n4. It references a related historical issue for additional context.\n\nThe solution is clearly implied: Apply `method_decorator` to the `never_cache` decorators in the admin login and logout views. This is a straightforward fix that involves modifying specific code in Django's admin implementation.\n\nA developer with Django knowledge would understand exactly what needs to be done without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14034": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The issue describes a problem with Django's MultiValueField where a required sub-field is being ignored when the parent MultiValueField is set as not required.\n\nThe issue includes:\n1. Complete code to reproduce the problem\n2. Clear expected vs. actual behavior\n3. Specific test cases demonstrating the issue\n4. A clear description of what's wrong: when a MultiValueField has require_all_fields=False and required=False, but contains a sub-field with required=True, validation is not working as expected\n\nThe issue author expects that even if the parent MultiValueField is not required, a required sub-field should still be validated when the form is submitted. This is a reasonable expectation and the issue provides enough information to investigate and fix the problem in Django's form validation logic.\n\nThe only minor ambiguity is whether this is a bug or intended behavior in Django, but the reproduction code and expected behavior are clear enough to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14043": {
        "explanation": "This issue is well-specified with clear objectives and context:\n\n1. The problem is clearly defined: Django's dbshell management commands risk leaking passwords through process information, and the current solution (using environment variables) is still not fully secure.\n\n2. The specific enhancement requested is explicit: Add support for PostgreSQL passfile to dbshell, which is a more secure way to handle database credentials.\n\n3. The issue provides context about the current implementation and its limitations, referencing specific code locations and Django versions.\n\n4. The submitter has already implemented a similar solution in another project (django-dbbackup) and provides a link to the implementation for reference.\n\n5. A patch is attached as a starting point for discussion, with the submitter indicating willingness to complete the implementation.\n\nThe solution approach is outlined clearly: use a temporary file in .pgpass format for PostgreSQL (and potentially options files for MySQL), wrapped in a context manager. This gives a clear direction for implementation.\n\nThe only minor detail not specified is exactly how the patch should be integrated with the existing codebase, but this is a reasonable omission since the submitter has provided a starting implementation and is seeking consensus on the approach before finalizing.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14053": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies the component (HashedFilesMixin's post_process() method) and the specific bug (yielding multiple times for the same file).\n\n2. It provides concrete examples showing the current behavior vs. expected behavior with command outputs.\n\n3. It explains three specific negative consequences of the bug:\n   - Incorrect statistics in collectstatic output\n   - Duplicate work for subclasses handling yielded files\n   - Inconsistent yielding of intermediate files\n\n4. It distinguishes this issue from a related one (#28604) with clear reasoning.\n\n5. It references the PR that introduced the issue and even links to a comment that previously mentioned this problem.\n\nThe issue provides enough context and detail for a developer to understand the problem, locate the relevant code, and implement a solution that ensures post_process() only yields each original file once with its final processed version.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14071": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The problem is clearly identified: Django's admin.E002 check doesn't catch when a field is referenced by its \"attname\" (e.g., 'author_id') in raw_id_fields instead of its name (e.g., 'author').\n\n2. A concrete example is provided showing how the current implementation allows invalid configurations to pass validation: `raw_id_fields = ['author_id']` passes the check but doesn't actually apply the raw_id feature to the author field.\n\n3. The issue explains the technical reason for the problem: models.Options.get_field allows retrieval by both name and attname, but only the name should be used in raw_id_fields.\n\n4. A specific solution is proposed: The _check_raw_id_fields_item method should verify that field.name == field_name after successful field retrieval and return an error when that's not the case.\n\nThe issue provides enough context for a developer to understand the problem, verify it, and implement the suggested fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14077": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug where SQLite returns integers (1/0) instead of booleans (True/False) when querying a JSONField using values() or values_list().\n\n2. The reporter provides a complete code example showing:\n   - The model definition with a JSONField\n   - The exact query being executed (PredictionData.objects.values_list('data', 'data__value'))\n   - The actual results from SQLite ({'value': True}, 1)\n   - The expected results from PostgreSQL ({'value': True}, True)\n\n3. The environment is clearly specified:\n   - Python 3.9.1\n   - SQLite 3.33.0\n   - Django 3.1.7\n\nThe issue is reproducible and the expected behavior is clear - SQLite should return boolean values like PostgreSQL does when querying JSONField data. This is a well-defined bug with a clear path to verification and resolution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14089": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: OrderedSet doesn't support Python's built-in reversed() function\n2. The reason why it should: OrderedSet is ordered, so it would be natural to support reversing\n3. The solution: Add a __reversed__() method to OrderedSet\n\nThe implementation details are straightforward for anyone familiar with Python's protocols. The __reversed__() method needs to return an iterator that yields items in reverse order. Since OrderedSet is ordered, implementing this method should be relatively simple by iterating through the underlying ordered data structure in reverse.\n\nA developer with access to the codebase would be able to locate the OrderedSet class, understand its current implementation, and add the required __reversed__() method without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14122": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a bug where Meta.ordering fields are incorrectly included in GROUP BY clauses, which leads to incorrect aggregation results. The issue provides:\n\n1. Context by referencing a previous ticket (#14357) that was closed but apparently not fully fixed\n2. Identifies the specific commit (0ddb4ebf) that attempted to fix the issue but didn't completely solve it\n3. Explains the specific problem: while ORDER BY was removed when Meta.ordering is used, the GROUP BY clause still includes Meta.ordering fields\n4. Provides a link to a PR (#14122) that includes a test case demonstrating the issue\n\nThe only elements that could make this more specific would be:\n- A code example directly in the issue description showing the problem\n- More details about the exact impact of the incorrect GROUP BY clause\n- Specific Django versions affected\n\nHowever, since there's a PR with a test case linked, a developer should be able to understand the issue and work on a solution by examining that PR and the referenced commit.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14140": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a bug in Django's Q object deconstruction process where Q objects with a single child are treated differently than those with multiple children.\n\n2. The description provides specific code examples demonstrating the problem, including the exact error message and traceback.\n\n3. The root cause is identified: single-child Q objects are deconstructed into kwargs instead of args, which fails when the child is not subscriptable (like an Exists object).\n\n4. A potential solution is already proposed (removing the special case for single-child Q objects) with a link to an existing pull request.\n\n5. The description even considers backward compatibility concerns, noting that while a more compatible approach would be to check if the child is a length-2 tuple, it's unlikely anyone depends on this undocumented behavior.\n\nWith this information, a developer could understand the issue, reproduce it, and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14151": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. It identifies the specific component (CsrfViewMiddleware) and the exact issue (not handling ValueError from urlparse).\n2. It provides a direct link to the problematic line of code in the GitHub repository.\n3. It gives a concrete example of input that would cause the problem ('https://[').\n4. The issue is straightforward to understand: the middleware doesn't handle the case where the referer header contains an invalid URL that causes urlparse() to raise a ValueError.\n\nA developer would be able to:\n1. Locate the exact line of code causing the issue\n2. Understand the specific error case (invalid referer URL)\n3. Implement proper error handling for the ValueError exception\n4. Test with the provided example ('https://[')\n\nThe solution would clearly involve adding try/except handling around the urlparse() call to gracefully handle malformed referer URLs.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14155": {
        "explanation": "This issue is well-specified with a clear problem statement and suggested solution. The issue describes that:\n\n1. When a `functools.partial()` function is used as a view in Django, the `ResolverMatch.__repr__()` method doesn't display useful information.\n2. Instead of showing the underlying function and arguments, it just shows \"functools.partial\".\n3. The suggested solution is to unwrap `functools.partial` objects in `ResolverMatch.__init__()` to expose the underlying function and arguments.\n\nThe issue provides enough context to understand the problem and a clear direction for the solution. A developer familiar with Django's URL resolver system would be able to locate the `ResolverMatch` class, understand how it currently handles function representations, and implement the suggested unwrapping of partial functions to improve the debugging experience.\n\nThe solution would involve modifying the `ResolverMatch.__init__()` method to detect when a view is a `functools.partial` object and extract both the original function and the pre-applied arguments for better representation in the `__repr__()` method.\n\nThis is a focused, specific enhancement with a clear path to implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14164": {
        "explanation": "The issue is well-specified with a clear problem statement and example. The function `to_locale` in Django's translation utilities is not idempotent - calling it twice on the same input produces an unexpected result. The example shows that `to_locale('en-us')` correctly returns 'en_US', but `to_locale(to_locale('en-us'))` incorrectly returns 'en_us' instead of remaining 'en_US'. \n\nThe issue also explains a practical impact: it breaks functionality when using `django.utils.translation.override` with locale codes. The desired solution is clear - make `to_locale` idempotent so that calling it multiple times on the same input produces the same result as calling it once.\n\nThe issue provides enough context to understand the problem, reproduce it, and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14170": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a specific bug: the query optimization in YearLookup incorrectly applies to \"__iso_year\" lookups, breaking the functionality of ExtractIsoYear.\n\n2. It provides concrete examples showing the problem:\n   - Shows that annotations with ExtractIsoYear work correctly\n   - Shows that filters using these annotations incorrectly use BETWEEN\n   - Shows that implicit lookups with __iso_year also incorrectly use BETWEEN\n\n3. It explains the impact: filters using iso_year return incorrect data.\n\n4. It even references a PR that fixes the issue, providing additional context.\n\nThe issue includes specific code examples with SQL output that clearly demonstrate the problem. A developer would have enough information to understand the issue, reproduce it, and implement a solution (likely by ensuring the optimization for BETWEEN is not applied to \"__iso_year\" lookups). The PR reference also provides a potential solution path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14182": {
        "explanation": "This issue is well-specified and clear. The request is to add support for a precision argument to Django's Round function, which currently only supports rounding to the nearest integer. The issue provides:\n\n1. Clear problem statement: Django's Round function lacks precision argument support\n2. Evidence that all major database backends supported by Django have this capability\n3. Links to documentation for each database backend showing the functionality\n4. A working example of the user's current workaround\n5. Implicit requirements for the solution (extend the existing Round function to accept a precision parameter)\n\nThe solution would involve modifying Django's Round function implementation to accept an optional second argument for precision, similar to how the underlying database functions work. This is a straightforward enhancement request with clear acceptance criteria - the Round function should work with a precision argument across all supported database backends.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14199": {
        "explanation": "The issue description provides a specific problem: using Value() with DecimalField in annotations crashes on SQLite. The description includes:\n\n1. A clear problem statement: \"Value() with DecimalField crashes on SQLite\"\n2. A specific scenario: When annotating with a decimal field, Django passes a string to create_decimal_from_float function\n3. A concrete code example: SomeModel.objects.all().annotate(shear_length=Value(1, output_field=DecimalField()))\n\nHowever, the description is missing some important details:\n- The exact error message that occurs\n- The Django version where this happens\n- The expected behavior\n- Any traceback information\n\nDespite these missing details, the issue is specific enough that a developer could reproduce the problem using the provided code example and investigate the root cause. The mention of \"create_decimal_from_float function\" also provides a hint about where in Django's codebase to look for the issue. A developer could reasonably attempt to fix this issue by examining how Django handles decimal values in SQLite annotations.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14238": {
        "explanation": "This issue is well-specified with clear information about the problem and a suggested solution:\n\n1. The problem is clearly described: When setting DEFAULT_AUTO_FIELD to a custom field that subclasses BigAutoField or SmallAutoField, Django crashes with a specific error message.\n\n2. A complete reproduction case is provided:\n   - The custom field definition (MyBigAutoField subclassing models.BigAutoField)\n   - The configuration (DEFAULT_AUTO_FIELD = \"example.core.models.MyBigAutoField\")\n   - The exact error message with full traceback\n\n3. The root cause is identified: The error occurs in AutoFieldMeta.__subclasscheck__ where it's not properly recognizing subclasses of BigAutoField and SmallAutoField as valid auto fields.\n\n4. A specific solution is suggested: Modify the _subclasses property in AutoFieldMeta.__subclasscheck__ to allow subclasses of BigAutoField and SmallAutoField.\n\nWith this information, a developer can locate the relevant code in Django, understand the issue, and implement the suggested fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14266": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue occurs after upgrading to Django 3.2\n2. A detailed error traceback is provided showing exactly where the crash happens\n3. The error message is clear: \"Invalid base64-encoded string: number of data characters (369) cannot be 1 more than a multiple of 4\"\n4. The contents of the problematic 'messages' cookie are provided\n5. The specific file and line number where the crash occurs is linked (django/contrib/messages/storage/cookie.py line 175)\n\nThe problem is clearly related to Django's message framework's cookie storage, where previously stored cookies are now causing crashes after the upgrade to Django 3.2. The error suggests there's an issue with base64 encoding/decoding of the cookie data.\n\nA developer would have enough information to:\n1. Understand the issue is with the cookie storage for messages\n2. See that the problem is related to base64 decoding\n3. Identify that the upgrade to Django 3.2 changed something in how cookies are handled\n4. Have the actual cookie data to test with\n\nThis is sufficient information to investigate and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14267": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a bug in Django's Q object deconstruction process where Q objects with a single child are treated differently than those with multiple children.\n\n2. The description provides specific code examples demonstrating the problem, including the exact error message and traceback.\n\n3. The root cause is identified: single-child Q objects are deconstructed into kwargs instead of args, which fails when the child is not subscriptable (like an Exists object).\n\n4. A potential solution is already proposed (removing the special case for single-child Q objects) with a link to an existing pull request.\n\n5. The description even considers backward compatibility concerns, noting that while a more backward-compatible approach might be possible, it's unlikely anyone relies on this undocumented behavior.\n\nWith this information, a developer could understand the issue, reproduce it, and implement a solution without needing additional clarification. The issue provides all necessary context about what's happening, why it's happening, and a suggested approach to fix it.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14271": {
        "explanation": "This issue is well-specified with clear details about the problem. The user has:\n\n1. Provided a complete code example showing the problematic query\n2. Included the exact error message: \"no such column: V0.user_id\"\n3. Demonstrated that the query works in Django 3.1.8 but fails in Django 3.2\n4. Included the SQL generated by both versions, highlighting the difference\n5. Provided a sample project repository (https://github.com/rsalmaso/django32-subquery-test) to reproduce the issue\n6. Specified the exact versions tested (3.1.8, 3.2, and even future commits)\n\nThe issue is clearly about a regression in Django 3.2 where combining two querysets in a subquery (using the `|` operator in the `active()` method) fails with a specific SQL error. The problem appears to be related to how Django 3.2 generates SQL for the EXISTS clause, incorrectly referencing \"V0.user_id\" which doesn't exist in the generated query.\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14311": {
        "explanation": "This issue is reasonably well-specified. It describes a problem with Django's autoreloader functionality when using the `python -m custom_module runserver` pattern. The issue explains that:\n\n1. There was a previous fix (with a link to the commit) that addressed autoreloading for package modules with `__main__.py`\n2. The current problem is that when using a dotted module path like `-m foo.bar.baz` (where baz.py is under foo/bar), the arguments incorrectly become `-m foo.bar`\n3. This is causing issues with the autoreloader for non-Django modules started with \"python -m\"\n\nThe issue provides enough context to understand what's happening and what needs to be fixed. A developer familiar with Django's autoreloader would likely understand the problem and be able to implement a solution.\n\nWhat's missing is:\n- Specific examples of the incorrect behavior\n- Expected behavior details\n- Steps to reproduce the issue\n\nHowever, given the technical nature of the description and the reference to the previous fix, a developer could reasonably infer what needs to be done - fix the module path handling in the autoreloader to properly handle dotted module paths.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14313": {
        "explanation": "This issue is well-specified with clear reproduction steps and error details. The problem occurs when:\n\n1. A Django admin page is set up with a Post model that has a ManyToMany relationship with User\n2. The PostAdmin has search_fields that include a related field (authors__email)\n3. When searching for posts, then selecting and trying to delete them, a 500 error occurs\n4. The specific error is \"TypeError: Cannot call delete() after .distinct()\"\n\nThe issue includes:\n- Complete model definitions\n- Admin configuration\n- Exact steps to reproduce\n- Full error traceback\n- Information about database (Postgres)\n- Version information (works in Django 3.1.8, broken in 3.2.x)\n- A hypothesis about the cause (related to preserving filters on delete)\n\nThis provides all the necessary information to investigate and fix the issue. A developer can reproduce the problem, understand the context, and has a starting point for where to look in the codebase (the changes between Django 3.1.8 and 3.2.x related to preserving filters on delete).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14315": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: The PostgreSQL database client's `runshell` function doesn't respect environment variables in some cases\n2. The specific behavior: It returns an empty dict instead of None for the environment, which prevents the use of `os.environ`\n3. The root cause: A specific commit is identified as introducing the bug (with a link)\n4. A reference PR is provided for additional context\n\nThe issue provides enough information to understand what's happening and where to look. A developer would need to:\n1. Examine the identified commit that introduced the bug\n2. Understand how environment variables should be handled in the PostgreSQL client's `runshell` function\n3. Fix the code to properly respect `os.environ` when appropriate\n\nWhile the issue doesn't explicitly outline the expected behavior or provide a code sample of the fix, it gives enough context for a developer familiar with Django's database client implementation to understand and address the problem. The reference to the specific commit and PR provides additional context that would help in resolving the issue.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14324": {
        "explanation": "This issue is well-specified with clear details about what's wrong and how to reproduce it. The description:\n\n1. Clearly identifies the problem: The migration autodetector is generating a different related_name for self-referential ManyToManyField than it used to.\n2. Provides a specific example: Shows exactly what code triggers the issue (a self-referential ManyToManyField).\n3. Shows the expected vs. actual behavior: It should create related_name='_mymodel2_field_3_+' but instead creates related_name='field_3_rel_+'.\n4. Identifies when the regression was introduced: Points to a specific commit (aa4acc164d1247c0de515c959f7b09648b57dc42) and references another issue (#29899).\n\nA developer would be able to:\n1. Reproduce the issue using the provided example\n2. Locate the problematic code by examining the referenced commit\n3. Understand what the correct behavior should be\n4. Implement a fix to restore the previous behavior\n\nThe issue provides all the necessary information to understand and fix the problem without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14334": {
        "explanation": "This issue is reasonably well-specified. It describes a specific problem with Django's LocaleMiddleware not correctly handling the zh-Hant-HK locale from the accept-language header when both zh-hans and zh-hant are defined in settings.LANGUAGES. The issue points to a specific component (LocaleMiddleware) and even identifies the likely problematic function (django.utils.translation.trans_real.get_supported_language_variant).\n\nWhat makes it not fully well-specified:\n1. It doesn't provide a reproducible test case or code example\n2. It doesn't specify the Django version where this issue occurs\n3. It doesn't clarify the expected behavior in detail (though it implies zh-hant should be chosen)\n4. It doesn't explain why zh-hant should be chosen over zh-hans when zh-Hant-HK is in the accept-language header\n\nHowever, a developer familiar with Django's internationalization system would likely understand the issue and be able to investigate the language variant matching logic in the specified function to determine why zh-Hant-HK is being incorrectly matched to zh-hans instead of zh-hant.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14336": {
        "explanation": "This issue is well-specified with a clear problem description and a concrete example. The issue identifies that Django is inconsistently using two different case formats for subquery aliases: 'Col%d' in one place and 'col%d' in another. The description provides:\n\n1. Specific links to the relevant code in the Django codebase\n2. A concrete example query that demonstrates the problem\n3. An explanation of why this is problematic (it would fail on databases with case-sensitive column names)\n4. A statement that the fix would be trivial\n\nThe issue is focused on a specific bug with clear reproduction steps. A developer would be able to understand what needs to be fixed: the inconsistent casing of column aliases needs to be standardized to prevent failures on case-sensitive databases. The solution would involve making the casing consistent across the codebase, likely by choosing one format and updating all occurrences.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14349": {
        "explanation": "This issue is well-specified with clear information about what's happening:\n\n1. The problem is clearly identified: URLValidator tests are failing on Python versions patched for bpo-43882 (Python 3.10.0b1 and 3.9 git branch).\n\n2. Specific test failures are provided with exact error messages and stack traces.\n\n3. The root cause is explained: Python's fix for bpo-43882 changes URL splitting behavior to strip LF, CR, and tab characters before splitting, which means these characters never reach Django's validator.\n\n4. Two potential solutions are suggested: either reject URLs with forbidden characters early, or follow Python's new recommendation to silently strip these characters.\n\nA developer would have enough information to:\n1. Understand the issue and reproduce it\n2. Locate the affected code in Django's URLValidator\n3. Implement one of the suggested approaches or devise another solution\n4. Test the fix against the failing test cases\n\nThe issue provides all necessary context to make an informed decision about how to handle these special characters in URLs according to Django's validation philosophy.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14351": {
        "explanation": "This issue is reasonably well-specified with detailed information about the problem. The user has:\n\n1. Clearly identified the issue: A difference in behavior between Django 2.2.5 and 3.2 when using Q objects with OR operations, specifically with `agent__property_groups__in` vs `agent__property_groups__id__in`.\n\n2. Provided working and non-working code examples that demonstrate the issue.\n\n3. Included the specific error message: \"subquery must return only one column\".\n\n4. Shared debugging information including SQL queries, object inspection, and a temporary hack that fixed the issue.\n\n5. Demonstrated their understanding of the root cause: when using `__in` with a queryset directly, all fields are being included in the subquery rather than just the ID field.\n\nThe issue is technical and complex, but there's enough information for an experienced Django developer to understand the problem and attempt a solution. The core issue appears to be in how Django 3.2 handles the generation of subqueries in Q objects with OR operations.\n\nWhile the description is somewhat dense with technical details, it provides sufficient context to understand what's happening and what needs to be fixed. A developer would need to investigate how Django's query compiler handles Q objects with OR operations and why it's including all fields in the subquery in one case but not the other.\n\nThe solution would likely involve modifying how Django's query compiler handles these specific types of queries to ensure only the necessary columns are included in the subquery.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14368": {
        "explanation": "This issue is well-specified with clear examples and a specific problem to solve. The issue describes:\n\n1. The exact problem: Django's datetime parser doesn't support spaces between time and timezone in ISO-8601 datetime strings\n2. Concrete examples of what works and what doesn't work\n3. The specific location of the code that needs to be modified (django.utils.dateparse.py)\n4. Two potential solutions (depend on python-dateutil or inline its functions)\n5. A reference to an existing pull request that likely contains a proposed solution\n\nThe issue provides enough context about the ISO-8601 standard, shows the specific parsing failure case, and points to the exact regex in the codebase that needs modification. A developer could implement a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14372": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is precisely identified: Saving a FileField raises a SuspiciousFileOperation exception after upgrading to Django 3.2.1.\n\n2. The root cause is clearly explained: Django 3.2.1 introduced a security fix (CVE-2021-31542) that requires only basenames to be passed to FieldFile.save, but FileField.pre_save is passing full paths.\n\n3. The issue includes specific steps to reproduce:\n   - model_instance.file_attribute = File(open(path, 'rb'))\n   - model_instance.save()\n\n4. The reporter has even identified the specific code causing the problem and created a PR with a proposed fix.\n\n5. The issue references the specific Django documentation that explains the change that caused this problem.\n\nWith this information, a developer would have a clear understanding of what's happening, why it's happening, and what needs to be fixed. The issue provides enough context to implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14373": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific component with the bug: `django.utils.dateformat.DateFormat`\n2. The specific method with the issue: `Y()` \n3. The expected behavior: \"always return a four-digit year padded with zeros\"\n4. The actual behavior: \"doesn't seem to be the case for year < 1000\"\n\nThe problem is clearly defined - the `Y()` method in the DateFormat class is not properly zero-padding years that are less than 1000, which contradicts the expected behavior of always returning a four-digit year.\n\nA developer would know exactly what to look for in the codebase (the DateFormat class and its Y method), understand what's wrong (insufficient zero-padding for years < 1000), and know what the fix should accomplish (ensure years are always returned as four digits with zero-padding when needed).\n\nThe solution would likely involve modifying the Y() method to ensure proper zero-padding for all years, particularly those less than 1000.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14376": {
        "explanation": "This issue is well-specified with clear details about what needs to be fixed. The problem is that Django's MySQL backend is using deprecated keyword arguments \"db\" and \"passwd\" which should be replaced with \"database\" and \"password\" respectively. The issue provides:\n\n1. Exact location of the code that needs to be changed (with a GitHub link to the specific lines)\n2. Reference to the mysqlclient library's deprecation of these parameters (with a commit link)\n3. Information about when the new parameters were added to mysqlclient (version 1.3.8)\n4. Confirmation that all supported Django versions require a mysqlclient version that supports the new parameters\n5. Clear indication that the fix could be backported to all currently supported Django versions\n\nA developer would know exactly what code to change, what to change it to, and which versions of Django should receive the fix. This is a straightforward parameter name update that can be implemented with minimal risk.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14382": {
        "explanation": "This issue is well-specified with clear information about:\n\n1. The problem: When using `django-admin startapp` with a directory path that has a trailing slash (which often happens with bash tab-completion), it results in an error.\n\n2. The exact error message: \"CommandError: '' is not a valid app directory. Please make sure the directory is a valid identifier.\"\n\n3. The root cause: The issue points to the specific file and line number (line 77 of django/core/management/templates.py) where the problem occurs. It explains that `os.path.basename()` on a path with a trailing slash returns an empty string.\n\n4. A proposed solution: The issue suggests a specific code change to fix the problem by using `target.rstrip(os.sep)` to remove any trailing slashes before calling `os.path.basename()`.\n\nWith this information, a developer can easily reproduce the issue, understand the cause, and implement the suggested fix or a similar solution. The issue provides all necessary context to create a meaningful PR without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14387": {
        "explanation": "This issue is well-specified with clear details about the problem. The user has:\n\n1. Provided the relevant model code\n2. Shared the function that's causing the issue\n3. Demonstrated the expected vs. actual behavior with specific examples\n4. Included the SQL queries being generated\n5. Shown that filtering with 'US' works but filtering with 'GB' doesn't\n6. Identified the specific problem: when filtering an OR-combined queryset with the same value as the original filter, the filter is not being applied correctly\n\nThe issue appears to be a bug in Django's query generation when applying additional filters to OR-combined querysets. The user has provided enough context and examples to reproduce the issue, understand the expected behavior, and attempt a solution. The problem is clearly in how Django handles the SQL generation for these specific types of combined queries.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14395": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that a change in Django's CacheHandler.all() method is causing performance problems by initializing all cache connections unnecessarily, even when they're not being used.\n\nThe issue includes:\n1. Clear identification of the problematic commit with a link\n2. Comparison of the old vs new implementation\n3. Description of the problem (unnecessary initialization causing IO lags)\n4. A proposed solution/workaround\n\nWhat makes this reasonably (rather than well) specified:\n- It doesn't explicitly state which Django version this affects\n- It doesn't provide a complete reproduction case with settings.py configuration\n- It doesn't include performance metrics showing the impact\n- It asks \"which version seems to be right?\" which suggests some uncertainty about the correct approach\n\nHowever, there's enough information for a developer to understand the issue and work on a solution. The core problem is clear: the new implementation of CacheHandler.all() initializes all cache connections defined in settings, whereas the old implementation only returned connections that were already initialized. The user has provided a working workaround that could serve as the basis for a proper fix.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14396": {
        "explanation": "This issue is well-specified. It clearly requests changing the default protocol in Django's Sitemap class from 'http' to 'https'. The issue mentions the specific component (Sitemap), the specific attribute (protocol), the current default value (implicitly 'http'), and the desired new default value ('https'). It also links to the relevant documentation and suggests implementing this with a proper deprecation timeline to ensure backward compatibility. \n\nThe task is straightforward: modify the default value of the protocol attribute in the Sitemap class from 'http' to 'https', and implement appropriate deprecation warnings for users who might be relying on the current default. This is a common type of change in web frameworks as HTTPS has become the standard for websites.\n\nA developer would know exactly what code needs to be changed and what the expected outcome should be.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14399": {
        "explanation": "This issue is reasonably well-specified. The user is describing a common problem in Django deployment: static files work in development but not when deploying with ASGI servers like Hypercorn. The issue specifically points out that the Django documentation for Hypercorn deployment doesn't address static file serving, and suggests that integrating WhiteNoise (a solution for serving static files in production) into the Django documentation would be helpful.\n\nThe request is clear: update the Django documentation for Hypercorn deployment to include guidance on serving static files, possibly recommending WhiteNoise. The user has provided context, links to relevant documentation, and alternative solutions they've tried.\n\nWhile there are some implementation details that would need to be worked out (exactly what content to add to the docs, where to place it, etc.), the core request is understandable and actionable. A developer could create a PR that adds information about static file serving to the Hypercorn deployment documentation without needing further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14404": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a specific problem: the `catch_all_view()` function in Django is not properly handling the `FORCE_SCRIPT_NAME` setting because it uses `request.path_info` (which excludes the script name) instead of `request.path` (which includes it) when constructing a redirect URL.\n\nThe issue includes:\n1. A clear identification of the problematic function (`catch_all_view()`)\n2. The specific issue (not supporting `FORCE_SCRIPT_NAME`)\n3. A technical explanation of what's happening (using `path_info` instead of `path`)\n4. A link to a pull request that presumably fixes the issue\n\nWhat makes this not fully \"well-specified\" is that it doesn't explicitly explain:\n- The exact context in which this occurs\n- The expected vs. actual behavior with a concrete example\n- The impact of this issue on users\n\nHowever, for an experienced Django developer, this information is likely sufficient to understand and address the problem. The pull request link also provides additional context for implementing a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14411": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific component with the problem: ReadOnlyPasswordHashWidget in the admin interface\n2. The exact issue: The label element has a 'for' attribute pointing to a non-labelable element\n3. The current behavior: The widget renders text, not an input element\n4. The expected solution: The label shouldn't have a 'for' attribute since there's no labelable element\n\nA developer would need to:\n1. Locate the ReadOnlyPasswordHashWidget implementation\n2. Identify how it renders its label with the 'for' attribute\n3. Modify the code to remove the 'for' attribute when rendering the label\n\nThis is a straightforward HTML/accessibility issue with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14416": {
        "explanation": "This issue is well-specified with a clear problem description and solution. The issue describes a caching problem with Django's LocaleMiddleware where redirects based on Accept-Language headers are being inappropriately cached by HTTP proxies/CDNs, causing users to be redirected to pages in the wrong language. The problem scenario is clearly outlined with specific steps to reproduce:\n\n1. A user with German language preferences visits a page and gets redirected to the German version\n2. Another user with French language preferences visits the same page but gets incorrectly redirected to the German version due to cached redirects\n\nThe issue identifies the root cause: LocaleMiddleware is not adding appropriate Vary headers to indicate that the response depends on the Accept-Language header or language cookie.\n\nThe solution is also clearly specified: adding a Vary header to inform caches that the response varies based on these headers.\n\nThis provides enough information for a developer to:\n1. Understand the problem\n2. Locate the relevant code in LocaleMiddleware\n3. Implement a fix by adding appropriate Vary headers to the redirect responses\n\nThe issue contains all necessary context including the technical details of how the caching problem occurs and what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14430": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes a bug where using an empty list with `__in=` lookup returns an `EmptyResultSet` exception that prevents `Coalesce` from working properly in aggregations.\n\nThe description includes:\n1. Three test cases showing the expected vs. actual behavior\n2. A technical explanation from a Django developer (Simon Charette) identifying the root cause\n3. Specific links to the relevant code in Django's codebase\n4. A clear indication of what's happening: the ORM doesn't perform any query when an empty list is used with `__in=`, causing the `EmptyResultSet` exception to be caught and all aggregates to be converted to `None`\n5. A suggestion for how to fix it: alter the `except EmptyResultSet` clause to account for `Coalesce()` instances used with `Value()`\n\nThe issue provides enough context and technical details for a developer to understand the problem and implement a solution. The expected behavior is clear: when using `Coalesce(Sum('quantity'), Value(0))` with an empty queryset, it should return 0 (the default value) rather than None.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14434": {
        "explanation": "The issue description provides some specific information but lacks important context:\n\n1. It identifies a specific problem: The `_create_unique_sql` function creates a statement where `references_column` is always false.\n2. It provides a cause: An instance of `Table` is being passed to `Columns` when a string is expected.\n\nHowever, the description is missing several critical details:\n- No code examples or stack traces\n- No information about the codebase or project this relates to\n- No expected behavior vs. actual behavior\n- No steps to reproduce the issue\n- No information about what `_create_unique_sql`, `references_column`, or `Columns` are\n\nDespite these gaps, the issue appears to be a type mismatch bug with a clear cause. A developer familiar with the codebase would likely understand what needs to be fixed - the function needs to be modified to pass a string instead of a Table object to the Columns constructor/function. The solution would involve finding where this happens in the `_create_unique_sql` function and correcting the parameter type.\n\nFor someone with access to the full codebase, this would be reasonably actionable, though they might need to spend some time understanding the context first.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14441": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a problem: the `get_image_dimensions()` function crashes when given a path to a non-existent image file. The expected behavior is implied - the function should handle non-existent files gracefully rather than crashing.\n\nWhile the issue doesn't provide specific details like:\n1. The exact error message or stack trace\n2. The expected return value when the file doesn't exist (e.g., return None, return a default value, or raise a specific exception)\n3. Code examples showing the current behavior\n\nA competent developer could still implement a solution by:\n1. Locating the `get_image_dimensions()` function in the codebase\n2. Adding appropriate file existence checks before attempting to process the image\n3. Implementing proper error handling for non-existent files\n4. Ensuring the function returns a sensible value or raises an appropriate exception when the file doesn't exist\n\nThe core problem and the need for a fix are clear, even if some implementation details need to be determined during development.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14447": {
        "explanation": "This issue is well-specified with clear technical details and proposed solutions. The author:\n\n1. Clearly identifies the problem: The database cache is running a slow COUNT(*) query twice during cache updates, causing performance issues.\n\n2. Provides specific code references with links to the relevant GitHub files.\n\n3. Offers a concrete solution for the first problem: Pass the count from the first query to the _cull method to avoid running the second COUNT(*) query.\n\n4. Proposes a second optimization: Not running the culling code on every cache update, but at some interval.\n\n5. Asks specific questions about implementation details, such as how to determine when to run culling and whether a new setting would be appropriate.\n\nThe issue includes enough context about the current implementation, the performance problem, and proposed solutions that a developer could implement the changes without needing additional clarification. The author has even thought through naming conventions for potential new settings.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14451": {
        "explanation": "This issue is well-specified with clear context and a solution path. The problem is clearly identified: Django's syndication feeds code doesn't support self-closing XML tags for empty elements, which is now standard practice. The issue explains:\n\n1. The specific problem: Django's feed generation doesn't use self-closing tags for empty elements\n2. The historical context: The code was written before self-closing tags were common\n3. The technical solution: Pass a specific argument to SimplerXMLGenerator (which inherits from Python's XMLGenerator)\n4. The implementation status: Python added this functionality in 3.2\n5. A reference implementation: The user has already created a PR (#14451) attempting to fix this\n\nThe issue provides enough technical detail for a developer to understand what needs to be done and even includes a reference implementation. The solution appears to be straightforward - updating Django's feed generation code to use the self-closing tag functionality that already exists in Python's XMLGenerator.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14453": {
        "explanation": "This issue is well-specified. The user clearly describes:\n\n1. The problem: When 'urlpatterns' is misspelled in a urls.py file, Django throws a misleading error message suggesting a circular import problem rather than identifying the misspelling.\n\n2. The specific error message that appears: \"The included URLconf does not appear to have any patterns in it. If you see valid patterns in the file then the issue is probably caused by a circular import.\"\n\n3. The actual cause: The variable name 'urlpatterns' was misspelled (as 'urlspattern' in their example).\n\n4. The desired solution: Add a more helpful error message that would identify the misspelling of 'urlpatterns' as a potential cause.\n\nThe user has even created a pull request addressing this issue, which provides additional context. This is a clear, specific enhancement request to improve Django's error messaging for a common mistake. A developer would have enough information to implement a solution that checks for misspellings of 'urlpatterns' and provides a more helpful error message.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14463": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The goal is to add functionality to Django to allow users to specify comments for tables and columns in the database.\n2. The issue provides a concrete implementation proposal with code examples showing:\n   - A new `db_column_comment` parameter for model fields to specify column-level comments\n   - A new `db_table_comment` attribute in the Meta class to specify table-level comments\n3. The purpose is clearly stated: to provide valuable information for database administrators, data analysts, and data scientists.\n4. It's mentioned that most Django-supported databases already support table and column comments, so this is about exposing that functionality through Django's ORM.\n\nThe implementation approach is straightforward and follows Django's existing patterns for database configuration. A developer with knowledge of Django's ORM and database layer would have enough information to implement this feature without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14471": {
        "explanation": "This issue is well-specified with clear technical details and a proposed solution. The issue describes:\n\n1. A specific problem: The CSRF middleware does unnecessary work when the CSRF cookie token has the wrong format\n2. The exact location of the problem: CsrfViewMiddleware.process_view() and _get_token() functions\n3. A detailed explanation of what happens currently: generating new tokens that will never match\n4. A clear proposed solution: reject the request earlier if the cookie token has the wrong format\n5. Implementation details: allow InvalidTokenFormat to bubble up from _get_token() and handle it differently in process_request() vs process_view()\n6. References to a similar issue (#32795) for context\n\nThe issue provides enough technical context for a developer familiar with Django's CSRF protection to understand the problem and implement a solution without needing additional clarification. The proposed implementation approach is specific and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14480": {
        "explanation": "This issue is reasonably well-specified. The request is to add logical XOR support to Django's Q() objects and QuerySet, similar to the existing AND and OR operations. The issue description:\n\n1. Clearly states what feature is needed (XOR support for Q objects)\n2. Provides context that XOR is available in major databases (PostgreSQL, MySQL, SQL Server, Oracle) but not SQLite\n3. Links to relevant StackOverflow discussions that provide implementation ideas\n4. References a specific answer that could serve as a starting point for implementation\n\nWhat makes it not fully \"well-specified\" is that it lacks some technical details:\n- No specific implementation approach is mandated (though an example is linked)\n- No discussion of how to handle SQLite which doesn't support XOR\n- No mention of test cases or documentation requirements\n- No discussion of backward compatibility concerns\n\nHowever, these gaps can be reasonably filled by an experienced Django developer who understands the codebase and how similar logical operators (AND, OR) are implemented. The linked StackOverflow answer provides a good starting point for implementation strategy.\n\nA successful solution would involve adding XOR support to Q objects in a way consistent with existing logical operators, with appropriate database-specific implementations and fallbacks for SQLite.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14493": {
        "explanation": "This issue is well-specified with clear reproduction steps and a detailed error message. The problem occurs when setting `max_post_process_passes = 0` in a custom class derived from `ManifestStaticFilesStorage`, which causes an `UnboundLocalError` because the variable 'substitutions' is referenced before assignment. The issue includes:\n\n1. Exact code to reproduce the problem\n2. The complete error traceback\n3. A link to the specific code in Django where the error occurs\n4. An explanation of why the error happens (substitutions is only set if a loop is entered at least once)\n5. The motivation for setting max_post_process_passes to 0\n\nThe solution would need to fix the `post_process` method in Django's `staticfiles/storage.py` to handle the case where `max_post_process_passes = 0` properly, likely by initializing the `substitutions` variable before the loop that might not execute. This is a clear bug with a well-defined scope.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14495": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Clearly described the problem: Django has inconsistent behavior with replaced/squashed migrations - it warns about ambiguity when using a short name, but crashes with a KeyError when trying to use the full name of a replaced migration.\n\n2. Provided specific reproduction steps with command examples and error outputs.\n\n3. Identified the core issue: Django should either allow migrating to replaced migrations that still exist on disk, or it should handle the error case more gracefully.\n\n4. Suggested potential solutions: either make it possible to migrate to replaced migrations, or improve the error handling and ambiguity resolution.\n\nThe issue contains enough technical detail about Django's migration system to understand the problem and begin implementing a solution. A developer could work on this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14500": {
        "explanation": "The issue description provides a clear problem statement about a bug in the MigrationExecutor's behavior when unapplying a squashed migration. Specifically:\n\n1. The issue occurs when unapplying a squashed migration\n2. It happens when the replaced migration files are still present\n3. The current behavior: MigrationExecutor marks both the squashed migration AND the replaced migrations as unapplied\n4. The expected behavior (implied): Only the replaced migrations should be marked as unapplied, not the squashed migration itself\n\nWhile the description is concise, it clearly identifies a specific bug in the migration system's behavior. For someone familiar with Django's migration system, this would be enough information to locate the relevant code in the MigrationExecutor and fix the issue. The expected behavior is reasonably clear from the description of the problem.\n\nThe issue could have been more detailed by including steps to reproduce, code examples, or error messages, but the core problem is still understandable for someone with knowledge of Django's migration system.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14508": {
        "explanation": "This issue is well-specified with a clear problem description and reproduction steps. The user has identified a conflict between Django's model inheritance system and Python properties. Specifically, when a child model tries to define a field with the same name as a property in the parent model, Django fails with an unclear error message.\n\nThe issue includes:\n1. A clear code example that reproduces the problem\n2. The exact error message received\n3. The Django versions tested (1.2 and 1.4 pre-alpha)\n4. A suggestion for improvement (better error messages)\n\nThe problem is technically well-defined: Django should either prevent this conflict with a clearer error message or handle the property overriding in a more graceful way. The documentation states that field name hiding is not permitted, but it doesn't explicitly address the interaction between properties and fields with the same name.\n\nA developer could implement a solution by either:\n1. Improving the error message to clearly indicate the property-field conflict\n2. Modifying Django's model initialization to handle this case properly\n3. Updating the documentation to explicitly warn about property-field conflicts\n\nThe reproduction steps are complete and the expected behavior is reasonably implied (either better error messages or allowing the override).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14513": {
        "explanation": "This issue is well-specified with clear context and requirements. It describes a specific problem with Django's `showmigrations` command regarding how it displays squashed migrations. The issue explains:\n\n1. The current behavior: When all original migrations have been applied, a squashed migration shows as applied [X] even if it hasn't been explicitly recorded in the migration recorder.\n\n2. The problem: This can confuse deployment processes that look for empty brackets [ ] to determine if a migration needs to be run.\n\n3. The requested solution: Add an indication in the `showmigrations` output to show when a squashed migration has been \"soft\" applied (meaning the original migrations are applied but the squashed migration hasn't been recorded yet).\n\nThe issue provides sufficient context by referencing related issues (#25231, #24900) and explaining the disconnect between the current output and the actual state. It's clear what needs to be changed (the output format of `showmigrations`) and why (to better indicate the true state of squashed migrations). A developer familiar with Django's migration system would have enough information to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14534": {
        "explanation": "This issue is well-specified with clear technical details about a bug in Django's form widget system. The issue describes:\n\n1. The exact problem: `BoundWidget.id_for_label()` ignores the ID set by `ChoiceWidget.options` and instead calculates its own ID\n2. The current implementation (with code snippets)\n3. The expected behavior\n4. A proposed solution (changing `id_for_label()` to return `self.data['attrs']['id']`)\n5. The specific context where this occurs (when rendering subwidgets of a `CheckboxSelectMultiple` widget)\n\nThe issue provides enough context about the Django form system, the specific classes involved, and the exact methods that need modification. A developer familiar with Django's form system would be able to understand the problem and implement a solution based on this description without needing additional clarification.\n\nThe proposed solution is straightforward, and the issue reporter has clearly identified the root cause of the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14539": {
        "explanation": "This issue is well-specified with a clear example that demonstrates the problem. The issue shows:\n\n1. The exact input string: 'Search for google.com/?q=1&lt! and see.'\n2. The expected output: 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>! and see.'\n3. The actual output: 'Search for <a href=\"http://google.com/?q=1%3C\">google.com/?q=1&lt</a>lt! and see.'\n\nThe problem is clearly identified: the urlize() function is not correctly handling HTML escaped characters (specifically &lt which represents '<') and trailing punctuation. The function is incorrectly including \"lt\" as part of the text outside the link when it should recognize \"&lt\" as a single HTML entity.\n\nA developer can understand exactly what needs to be fixed: the urlize() function needs to be modified to properly recognize HTML entities and handle trailing punctuation correctly. The solution would involve ensuring that HTML entities like \"&lt\" are kept intact and that punctuation following the URL is properly separated from the URL itself.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14559": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue identifies that:\n\n1. The `bulk_update()` method currently returns `None`\n2. The `update()` method returns the number of rows matched\n3. The request is to make `bulk_update()` also return the number of rows matched\n4. The issue includes a link to the relevant code showing that `bulk_update()` calls `update()` internally\n5. A specific implementation approach is suggested: add up the return values from the `update()` calls and return that sum\n\nThe issue provides enough context to understand what needs to be changed, where the change needs to be made, and how the change should be implemented. A developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14580": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a complete code example showing the model definitions\n2. Shown the exact migration file that gets generated\n3. Included the specific error message with line numbers\n4. Identified the root cause: a missing import statement for `models` in the generated migration file\n5. Specified the Django version (3.2.4) where the issue occurs\n6. Clearly stated the expected behavior (valid Python in generated migrations) vs. actual behavior (missing import)\n\nThe problem is straightforward: when Django generates a migration file that references a model from the base classes, it's not properly importing the `models` module that's referenced in the `bases` tuple. This is causing a NameError when the migration is executed.\n\nA developer could easily reproduce this issue and understand what needs to be fixed - the migration writer needs to ensure that when `models.Model` is referenced in the `bases` parameter, the appropriate import statement is included in the generated file.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14584": {
        "explanation": "The issue is reasonably specified, though it could benefit from more details. Here's what we can understand:\n\n1. The issue is about enhancing the Django database backend logging to include the database alias.\n2. The rationale is provided: this information is essential when working with database routing.\n3. A PR link is provided, which suggests there's already a proposed solution.\n\nWhat makes this reasonably specified rather than well-specified:\n- It doesn't explicitly state what logging output should be modified (though we can infer it's the SQL query logging)\n- It doesn't specify the exact format of how the alias should be included\n- It doesn't mention any specific files that need to be modified\n\nHowever, with knowledge of Django's codebase and by examining the linked PR, an experienced developer could understand what needs to be done. The PR link provides additional context that would help in implementing a solution. The core request is clear: include the database alias in the logging output to help with database routing scenarios.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14602": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: The current regex in `parse_time` accepts some inputs that might be considered invalid\n2. The specific component: The `time_re` regular expression in Django's dateparse utility\n3. An example of problematic behavior: `parse_time('0:5: ')` returning `datetime.time(0, 5)`\n4. The current regex pattern: `(?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})(?::(?P<second>\\d{1,2})(?:[\\.,](?P<microsecond>\\d{1,6})\\d{0,6})?)?`\n5. Potential solutions: Either tighten the regex or document the current behavior as accepted\n\nWhat makes it not fully specified is that it doesn't explicitly state which specific patterns should be rejected - it gives one example but suggests there may be others. A developer would need to analyze the current regex, identify problematic patterns, and make judgment calls about which ones to reject.\n\nHowever, there's enough information to make a meaningful attempt at a solution by:\n1. Analyzing the current regex to identify potential \"invalid\" inputs it accepts\n2. Testing edge cases\n3. Proposing a tighter regex that rejects problematic patterns while maintaining compatibility with valid inputs\n4. Documenting the changes",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14608": {
        "explanation": "The issue is well-specified and clear. It describes:\n\n1. The current behavior: Forms add a \"nonfield\" CSS class for non-field errors in ErrorList instances\n2. The expected behavior: FormSets should similarly add a \"nonform\" CSS class for non-form errors\n3. The rationale: This would allow custom ErrorList implementations to distinguish between form field errors, non-field errors (forms), and non-form errors (FormSets)\n4. The proposed solution: Add the \"nonform\" CSS class and document it\n\nThe issue provides enough context about the current implementation and clearly states what needs to be changed. A developer familiar with Django's form system would understand what needs to be done: modify the FormSet class to add a \"nonform\" CSS class to non-form errors, similar to how the Form class adds a \"nonfield\" class to non-field errors, and then document this feature. The implementation would likely involve modifying the FormSet class's error handling code.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14631": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem: BaseForm's _clean_fields() and changed_data methods don't access values via BoundField objects, which leads to inconsistency and duplicated code paths. The issue also proposes a specific solution: modify these methods to access values through BoundField objects and move the per-field logic of changed_data() into a BoundField method called did_change().\n\nThe issue provides enough context to understand what needs to be changed and why. It references specific methods (_clean_fields() and changed_data), identifies a specific inconsistency (form._clean_fields() can return a different value from form[name].initial), and even suggests a code implementation approach with a sample of how the changed_data method could be simplified.\n\nThere are some minor details that could be clarified (such as the exact implementation of the BoundField._did_change() method), but these are implementation details that a developer familiar with the codebase could reasonably determine. The issue also references another ticket (#32917) and a test case that could be adjusted to cover the issue, providing additional context.\n\nOverall, a developer with knowledge of Django's form system would have enough information to implement a solution based on this description.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14634": {
        "explanation": "The issue description is quite brief and lacks specific details about implementation requirements. While it clearly states the goal - to create a mixin that shows a message when an object is successfully deleted - it doesn't provide important context such as:\n\n1. What framework or system this is for (Django, Flask, etc.)\n2. How the current deletion process works\n3. What the message should contain or how it should be displayed\n4. How the mixin should be integrated with existing code\n5. Any specific requirements for the message format or display mechanism\n\nWithout this context, a developer would need to make significant assumptions about the implementation details. They would likely need to explore the codebase to understand the current deletion process and how mixins are typically implemented in the system.\n\nHowever, the core functionality request is clear enough that an experienced developer familiar with the codebase could likely implement a solution with reasonable assumptions. The concept of a mixin that provides success messaging for deletion operations is a straightforward pattern in many frameworks.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14641": {
        "explanation": "This issue is well-specified with clear instructions on what needs to be done. The issue describes:\n\n1. The exact problem: Special-case logic for time and datetime objects exists in BoundField.initial() that should be moved to BaseForm.get_initial_for_field()\n2. Provides specific file paths and line numbers for both the source and destination of the code to be moved\n3. Links to the relevant code in the GitHub repository\n4. Explains where exactly in the destination method the code should be placed (under the \"if callable(value)\" block)\n5. Provides context by referencing a previous ticket (#32920)\n6. Even suggests a potential future improvement (moving logic to Field class)\n\nA developer can understand exactly what needs to be done without needing additional clarification. The task is a straightforward code refactoring with specific locations identified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14667": {
        "explanation": "This issue is well-specified with clear examples demonstrating the unexpected behavior. The reporter provides:\n\n1. A clear model definition (Company with id, name, trade_number, country fields)\n2. Specific code examples showing the problematic behavior\n3. The actual SQL queries generated\n4. The expected SQL queries\n\nThe issue is about how Django's QuerySet methods `.only()` and `.defer()` interact when chained together. Specifically, when `.defer()` is called after `.only()`, it doesn't properly exclude fields as expected. The reporter shows that:\n\n- `Company.objects.only(\"name\").defer(\"name\")` should select only the \"id\" field but instead selects all fields\n- `Company.objects.only(\"name\").defer(\"name\").defer(\"country\")` should also select only the \"id\" field but selects all fields\n- `Company.objects.only(\"name\", \"country\").defer(\"name\")` works as expected, selecting only \"id\" and \"country\"\n\nThis is a clear bug in Django's QuerySet implementation where the deferred fields aren't properly handled when chained after `.only()`. A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14672": {
        "explanation": "This issue is well-specified with clear information about the problem and solution:\n\n1. The problem is clearly identified: In Django 3.2, the `ManyToManyRel` class is missing a call to `make_hashable` on the `through_fields` attribute when it's a list, causing a \"TypeError: unhashable type: 'list'\" when checking models.\n\n2. The issue provides detailed context: It explains that in Django 3.2, an identity property was added to `ForeignObjectRel` to enable comparison, and this property is used to generate a hash. While `limit_choices_to` is made hashable with `make_hashable`, the same treatment is missing for `through_fields` when it's a list.\n\n3. A complete minimal reproduction case is provided with all the necessary model definitions.\n\n4. The full error traceback is included, showing exactly where the error occurs.\n\n5. The solution is explicitly stated: \"Add missing make_hashable call on self.through_fields in ManyToManyRel.\"\n\nWith this information, a developer could locate the relevant code in Django's codebase, understand the issue, and implement the fix by adding the missing `make_hashable` call to the `through_fields` attribute in the `ManyToManyRel` class.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14681": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a minimal reproduction app with specific code changes\n2. Detailed the setup requirements (migrate, create superuser)\n3. Outlined three specific test scenarios with expected outcomes\n4. Included the exact error message being received\n5. Clearly stated the expected behavior (filesystem error instead of CSRF error)\n\nThe issue describes a bug where Django incorrectly reports a CSRF verification failure when the actual problem is with file storage (missing directories). The reporter has identified that when the media directory or its tmp subdirectory is missing, instead of getting a filesystem error, the user gets a misleading CSRF error.\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided steps\n2. Understand the expected behavior\n3. Investigate why a storage issue is being misreported as a CSRF issue\n4. Implement a fix that provides the correct error message\n\nThe issue is specific, reproducible, and has a clear definition of success (proper error reporting).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14725": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a limitation in Django's model formsets: they don't provide a way to create an \"edit only\" view of objects, where users can only edit existing objects but not create new ones. The issue explains that users currently try to use `extra=0` to accomplish this, but this approach is unreliable because:\n\n1. The `extra` parameter only controls the initial number of extra forms displayed\n2. Users can still add more forms via JavaScript or by manually sending additional POST data\n\nWhat's missing from the issue description is:\n1. A specific proposed solution or implementation approach\n2. Detailed requirements for how the \"edit only\" functionality should behave\n3. Edge cases that need to be handled\n\nHowever, the core problem is clear enough that an experienced Django developer could propose a reasonable solution, likely involving adding a new parameter or option to model formsets that explicitly disallows the creation of new objects during form processing. The solution would need to validate submitted data to ensure it only contains existing objects.\n\nGiven the Django framework's patterns, this is a reasonable feature request that fits within the existing architecture and has a clear use case.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14730": {
        "explanation": "This issue is well-specified. It clearly identifies a problem: developers can currently define a `related_name` on symmetrical `ManyToManyFields`, but this related name is never used because no related field is created in symmetrical relationships. This can lead to developer confusion.\n\nThe solution is also clear: raise an error when a developer attempts to define a `related_name` on a symmetrical `ManyToManyField` to prevent this confusion.\n\nThe issue provides:\n1. The specific component affected (`ManyToManyField`)\n2. The specific condition to check for (when `symmetrical=True` or when it's a self-referential relationship)\n3. The specific parameter that should trigger an error (`related_name`)\n4. The reason for the change (to prevent developer confusion)\n5. The expected behavior (raise an error)\n\nA developer with knowledge of Django's ORM system would have enough information to implement this change without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14733": {
        "explanation": "The issue is reasonably well-specified. It clearly describes a feature request to add `deletion_widget` and `get_deletion_widget()` to Django formsets, similar to the existing `ordering_widget` and `get_ordering_widget()` that were introduced in Django 3.0. The issue provides context by referencing ticket #29956 and mentions that this would be useful for the same use case - overriding fields in formsets that are updated in the frontend.\n\nThe request is straightforward: implement for deletion widgets the same pattern that already exists for ordering widgets. A developer familiar with Django formsets would understand what needs to be done - create a deletion_widget attribute and corresponding get_deletion_widget() method that would allow customizing how the DELETE checkbox is rendered in formsets.\n\nWhile the issue doesn't provide implementation details or specific code examples, the parallel to the existing ordering widget functionality provides enough context for a developer to understand what's needed. The referenced discussion link would likely provide additional context if needed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14751": {
        "explanation": "This issue is well-specified with a clear problem statement and desired outcome. The user explains:\n\n1. The current problem: The `makemigrations` command isn't script-friendly because it:\n   - Writes progress output to stdout instead of stderr\n   - Doesn't provide a structured way to identify created files\n\n2. The specific use case: Running `makemigrations` in a Docker container and needing to copy newly created migration files to a development machine for source control\n\n3. The current workaround: Manually parsing output to find affected apps and inspecting directories for new files\n\n4. A proposed solution: Having `makemigrations` write the paths of created files to stdout\n\nThis issue provides enough context to understand the problem and implement a solution. The request is for a specific enhancement to the `makemigrations` command to make it more scriptable by providing structured output of created files. A developer could implement this by modifying the command to output file paths in a structured format and redirect progress messages to stderr.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14752": {
        "explanation": "This issue is very well-specified with clear problem statement and proposed solution:\n\n1. The problem is clearly defined: Currently, customizing autocomplete field results requires extending AutocompleteJsonView and overriding the entire get() method, which creates maintenance overhead as the original method evolves.\n\n2. A specific solution is proposed: Refactor AutocompleteJsonView to extract the result serialization logic into a separate serialize_result() method that can be easily overridden.\n\n3. Code examples are provided for both the current approach (showing the problem) and the proposed solution (showing how it would simplify customization).\n\n4. The expected outcome is clear: Allow adding extra fields to autocomplete responses without having to override the entire get() method.\n\n5. The submitter has even offered to provide the patch for implementation.\n\nThe issue provides all the necessary context, rationale, and implementation details needed to understand what needs to be done and why. A developer could immediately begin implementing this solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14765": {
        "explanation": "This issue is well-specified with a clear problem statement and solution. The issue explains that:\n\n1. PR #14760 changed all calls to ProjectState.__init__() to pass real_apps as a set\n2. Currently, ProjectState.__init__() checks if real_apps is a set and converts it if not\n3. The proposed change is to replace this conversion logic with an assertion that real_apps is a set when non-None\n4. The rationale is that ProjectState construction is part of Django's internal API, so we can enforce this constraint\n\nThe issue provides enough context about the current implementation, the desired change, and the reasoning behind it. The solution would involve modifying the ProjectState.__init__() method to replace the type checking and conversion with an assertion. This is a straightforward code change with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14771": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that when Django's auto-reloader is active, Python's `-X` options (specifically `-X utf8` in this case) are not being passed to the reloaded process.\n\nThe evidence provided shows:\n1. When running with `-X utf8`, the initial process shows \"=== UTF-8\" (correct encoding)\n2. After auto-reload, it shows \"=== cp936\" (reverting to the default Windows encoding)\n3. When running with `--noreload`, the encoding remains \"UTF-8\" throughout\n\nThe issue is clear: Django's auto-reloader is not preserving the `-X` command-line options when it spawns a new process. This is particularly important for options like UTF-8 mode that affect how Python handles encodings.\n\nThe solution would involve modifying Django's auto-reloader to properly pass through any `-X` options from the parent process to the child processes it spawns during reloading.\n\nWhile the issue doesn't explicitly state which files need to be modified, an experienced Django developer would know to look at the auto-reloader implementation in Django's codebase, likely in the `django.utils.autoreload` module.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14779": {
        "explanation": "This issue is well-specified with clear details about the bug. The description explains:\n\n1. The exact problem: When creating a custom template tag with `takes_context=True` but no other parameters, and forgetting to include the context parameter in the function definition, Django throws an unhelpful IndexError instead of a more informative TemplateSyntaxError.\n\n2. The specific scenario: The issue occurs only when the tag has no other parameters besides the missing context parameter.\n\n3. A code example showing both the working case (where other parameters exist) and the failing case.\n\n4. The exact error traceback showing where the problem occurs in the Django codebase (in the `parse_bits` function when it tries to check `params[0]`).\n\n5. The issue mentions that a PR will follow, indicating the reporter has a clear understanding of the problem and likely a solution in mind.\n\nThe issue provides enough information to understand the bug, reproduce it, and identify where in the code the fix would need to be applied (in the `parse_bits` function to handle the case of an empty params list when `takes_context=True`). A developer could implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14785": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is clearly defined: when a NaN value is stored in a DecimalField using SQLite3, attempting to retrieve the object from the database raises a TypeError. The issue description includes:\n\n1. Precise environment details (Python 3.8.10, Django 3.2.6, SQLite3)\n2. Complete code to reproduce the issue\n3. The exact error message and stack trace\n4. Multiple scenarios where the issue occurs (direct refresh_from_db() and admin interface)\n\nThe issue is specific to how Django's DecimalField handles NaN values when using SQLite3. The error occurs in the conversion process when retrieving data from the database. The full traceback points to the specific location in Django's code where the error occurs (in the sqlite3 operations converter function).\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14787": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue states that Django's `method_decorator()` doesn't properly preserve function attributes when used with decorators that expect these attributes (like those using `@wraps`). \n\nThe example shows:\n1. A custom `logger` decorator that uses `@wraps` and tries to access `func.__name__`\n2. When this decorator is applied to a class method using Django's `method_decorator`\n3. The specific error that occurs: `AttributeError: 'functools.partial' object has no attribute '__name__'`\n\nThe issue is that `method_decorator()` is creating a `functools.partial` object but not preserving the original function's attributes like `__name__`, `__module__`, etc., which causes problems with decorators that expect these attributes.\n\nA solution would need to modify `method_decorator()` to ensure that when it creates a partial object, it copies over the relevant attributes from the original function, similar to how `functools.wraps` works. This is a clear, specific problem with a well-defined scope for a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14792": {
        "explanation": "This issue is well-specified with clear technical details about a regression in Django 3.2 regarding time zone conversion in database functions. The issue:\n\n1. Clearly identifies the problem: When using \"Etc/GMT-10\" timezone with Trunc() or Extract() database functions, Django 3.2 behaves differently than Django 3.1.\n\n2. Provides specific technical details:\n   - In Django 3.1, timezone._get_timezone_name() returns \"+10\" for \"Etc/GMT-10\"\n   - In Django 3.2, it returns \"Etc/GMT-10\" instead\n   - Shows how this affects the SQL query generation with specific examples\n   - Demonstrates the issue is reproducible with both pytz and zoneinfo.ZoneInfo\n\n3. Identifies the likely cause: A change in the return value of timezone._get_timezone_name() that affects the TimezoneMixin.\n\n4. Shows the actual SQL queries generated in both versions, making it clear what changed.\n\nThe issue provides enough context and technical details for a developer to understand the problem, reproduce it, and implement a solution without needing additional clarification. The root cause is identified, and the expected behavior is implied (Django 3.2 should maintain the same behavior as 3.1 for these timezone conversions).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14802": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue identifies a repetitive pattern in cache backends where `make_key()` and `validate_key()` are called sequentially. It proposes creating a helper function called `make_and_validate_key()` that combines these operations to reduce repetition and ensure consistent validation. The issue even provides the exact implementation of the proposed helper function, including its signature, parameters, and body. It also discusses an alternative approach (adding a validate flag to `make_key()`) and explains why that might be problematic due to backward compatibility concerns. With this information, a developer could implement the requested helper function without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14832": {
        "explanation": "This issue is reasonably well-specified. It describes a specific problem with the Django `floatformat` template filter, which currently doesn't respect the localization settings when used within a `{% localize %}` block that contradicts the global `USE_L10N` setting.\n\nThe issue clearly identifies:\n1. The specific component affected (floatformat template filter)\n2. The exact problem (it doesn't respect localization settings from the context)\n3. The root cause (formats.number_format() is called without the use_l10n parameter)\n4. Links to the specific code locations in the Django codebase\n\nThe only ambiguity is in the solution approach. The issue reporter mentions they \"do not see any easy solution to this, as filters do not take context...\" This indicates some uncertainty about how to implement the fix, but doesn't prevent understanding what needs to be fixed.\n\nA developer familiar with Django's template system would likely understand how to approach this issue - they would need to find a way to make the floatformat filter aware of the current localization context, possibly by modifying how template filters access context information or by adding a parameter to force unlocalized format.\n\nThe issue is specific enough that a developer could begin working on a solution without needing significant clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14855": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The problem is clearly defined: When using a custom Admin Site, ForeignKey fields marked as readonly generate incorrect URLs with \"/admin/\" instead of \"/custom-admin/\".\n\n2. The root cause is identified: In django.contrib.admin.helpers, the get_admin_url function doesn't use the current_app parameter when generating URLs.\n\n3. A specific solution is proposed: Adding the current_app parameter to the reverse function call, using the admin_site.name from the model_admin.\n\n4. The submitter has even provided the exact code change they believe would fix the issue.\n\nThe only uncertainty expressed is about potential side effects of the proposed solution, which is a reasonable concern but doesn't detract from the clarity of the issue description. The submitter is asking for feedback on their approach, which indicates they've done thorough analysis.\n\nWith this information, a developer could easily reproduce the issue, understand the proposed fix, and evaluate whether it's the correct approach. The issue provides enough context to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14861": {
        "explanation": "This issue is well-specified with a clear description of the problem and a minimal reproducible example. The issue describes a bug in Django's admin interface where conditionally showing/hiding inline forms based on a model field's value works in one direction but not the other.\n\nSpecifically:\n1. The code example clearly shows the models and admin setup\n2. The expected behavior is described: inline forms should appear/disappear based on the `show_inlines` field\n3. The actual behavior is clearly described: \n   - Works when creating new objects\n   - Works when changing from show_inlines=True to False\n   - Fails when changing from show_inlines=False to True with specific error messages\n4. The error messages are provided, pointing to missing management form data\n\nThis is a technical issue with a clear reproduction path and error messages that point to the likely cause (management form data not being properly initialized when dynamically adding inlines). A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14871": {
        "explanation": "This issue is well-specified with a clear problem description. The issue states that Select2 doesn't load translations when using language codes with subtags (like \"pt-BR\"). The problem is specifically identified: when using such language codes, the translation files are not found. The issue even points to the root cause in the Django codebase (line 366 in django/contrib/admin/widgets.py) where some languages are converted to lowercase, causing the mismatch.\n\nFrom this description, I can understand:\n1. The specific problem: Select2 translations don't work with language codes containing subtags\n2. A concrete example: \"pt-BR\" doesn't load the expected translation\n3. The likely cause: language codes being converted to lowercase in the Django admin widgets code\n4. The location in the codebase where the fix would likely need to be applied\n\nThis provides enough information to investigate and implement a solution that would properly handle language codes with subtags for Select2 translations.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14880": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: RelatedField._check_clashes() doesn't mention the clashing name in error messages, making debugging difficult\n2. The location: The specific method in Django's codebase that needs modification\n3. A reference to an existing PR (#14880) that attempted to solve this issue\n4. The goal: Improve error messages to include the clashing name\n\nThe issue provides enough context for a developer to understand what needs to be fixed. The only minor gaps are:\n- Specific examples of the current error message vs. desired error message aren't provided\n- The exact nature of the unit test impacts isn't detailed\n\nHowever, these gaps can be filled by examining the referenced PR and the existing code. A developer with access to the codebase could reasonably implement a solution based on this description.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-14894": {
        "explanation": "This issue is well-specified with a clear description of the problem. The reporter has provided:\n\n1. Complete model definitions for Article and Comment\n2. Test data creation code\n3. The exact query code that demonstrates the issue\n4. Console output showing both the expected (correct) behavior and the incorrect behavior\n5. SQL queries generated in both cases\n6. A clear explanation of why this is problematic in real-world applications\n\nThe issue describes a specific bug where using an empty list in a filter condition (`author_name__in=[]`) causes Django ORM to generate incorrect SQL for annotations that use Subquery and Coalesce. Instead of properly applying the Coalesce fallback value (101), it's generating a direct \"0 AS A\" in the SQL and similarly affecting calculations that depend on that value.\n\nThe reporter has isolated the issue to a minimal reproducible example and explained the context where this becomes problematic in real applications. They've also attached a minimal reproduction project (though we don't have access to it in this context).\n\nThis is a well-defined bug with clear steps to reproduce, expected vs. actual behavior, and enough context to understand the impact.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14915": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue occurs after migrating from Django 3.0 to Django 3.1\n2. The specific error is clearly stated: `TypeError: unhashable type: 'ModelChoiceIteratorValue'`\n3. The context is provided - it happens when trying to use a ModelChoiceIteratorValue as a dictionary key\n4. Code examples are provided showing both the failing case (using a dict lookup) and a working case (using an array)\n5. The root cause is identifiable: In Django 3.1, the ModelChoiceIteratorValue class was introduced to wrap values in ModelChoiceField, and this class is not hashable by default, breaking code that uses these values as dictionary keys\n\nThe issue is specific enough that a developer could:\n1. Understand the problem (ModelChoiceIteratorValue is not hashable)\n2. Identify where it's happening (when using the value as a dictionary key)\n3. Develop a solution (likely by extracting the actual value from the ModelChoiceIteratorValue object before using it as a key)\n\nThis is a concrete, reproducible issue with sufficient context to implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14919": {
        "explanation": "This issue is well-specified with clear context, problem description, and a proposed solution:\n\n1. The problem is clearly defined: There's a discrepancy in how durable atomic blocks are handled between TransactionTestCase and TestCase. In TestCase, durability errors are ignored because durability checks are turned off.\n\n2. The impact is explained: Durability errors don't surface until code reaches staging/production environments, making it difficult to catch these issues during testing.\n\n3. A specific solution is proposed: Add a marker to transactions created by TestCase, maintain a stack of active transactions, and modify the durability check to consider these markers. This would allow skipping durability checks only when a durable atomic block is directly within a TestCase transaction.\n\n4. The submitter has even provided a proof-of-concept patch with a pull request link, which gives a concrete implementation to review.\n\nThe issue provides enough technical detail about the problem and a clear direction for the solution. A developer familiar with Django's transaction handling in tests would have sufficient information to implement a solution based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14935": {
        "explanation": "This issue is well-specified and clearly describes a problem with Django migrations when using callable defaults for model fields. The issue is that when a migration is generated for a field with a callable default (in this case, a function that generates a random password), the migration only calls the function once during the migration process, resulting in all existing records getting the same value instead of unique values.\n\nThe issue includes:\n1. A clear description of the problem\n2. A code example showing the model definition with the callable default\n3. The generated migration code that demonstrates the issue\n4. The expected behavior (that each record should get a unique random password)\n\nThe title suggests a solution approach: adding a warning during the `makemigrations` process to alert developers when they're using a callable default on a field that should have unique values. This would help prevent the issue where all records end up with the same value.\n\nA developer would have enough information to implement a solution that detects this pattern during migration generation and provides an appropriate warning message.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14960": {
        "explanation": "The issue is well-specified with clear technical details about a bug in the Django `createsuperuser` command. The problem is precisely described:\n\n1. The issue identifies a specific component: the `createsuperuser` command in Django\n2. It clearly explains the bug: field validation is not performed for REQUIRED_FIELDS when values are passed via command-line arguments in interactive mode\n3. It explains how validation works in different modes (non-interactive and interactive)\n4. It points to specific code locations where validation happens in other scenarios\n5. It describes the consequence: invalid data (like strings instead of integers for foreign keys) gets passed to `UserManager.create_superuser`\n\nThe issue provides enough context for a developer to understand the problem, locate the relevant code, and implement a fix that would validate command-line provided values for REQUIRED_FIELDS in interactive mode. The solution would likely involve adding validation for these command-line provided values before they're used to create a superuser.\n\nA developer with access to the Django codebase would be able to identify the specific files and functions that need modification based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14996": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific scenario: when renaming a field in a model and setting the old name as db_column to avoid database operations.\n\n2. It provides concrete code examples showing the model before and after the change:\n   - Original: `core = models.BooleanField()`\n   - Changed: `core_renamed = models.BooleanField(db_column='core')`\n\n3. It shows the exact SQL that Django currently generates, which includes unnecessary operations:\n   - First renames the column from \"core\" to \"core_renamed\"\n   - Then renames it back from \"core_renamed\" to \"core\"\n\n4. It compares this to a working scenario (referenced as #31826) where not renaming the field but using db_column correctly produces no SQL operations.\n\n5. The expected behavior is clearly stated: this should be a \"SQL noop\" (no operation) since the database column name remains the same.\n\nThe issue provides all the necessary context, reproduction steps, current behavior, and expected behavior. A developer could implement a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14997": {
        "explanation": "This issue is well-specified with clear details about the problem. The user has provided:\n\n1. A complete model definition showing the Tag class with a UniqueConstraint\n2. The exact migration files that are causing the problem\n3. The specific error message: \"the '.' operator prohibited in index expressions\" when trying to alter a field in a model with a unique constraint on SQLite\n4. The Django version (4.0a1) and database backend (SQLite)\n5. The full traceback showing exactly where the error occurs in Django's SQLite schema handling\n\nThe issue appears to be that when altering a field (changing max_length from 200 to 150) on a model that has a UniqueConstraint, Django's SQLite backend fails with an error about the \".\" operator in index expressions. This happens during the table recreation process that SQLite uses for schema alterations.\n\nThe problem is specifically related to how Django handles F() expressions in UniqueConstraint when remaking tables in SQLite. The error message and traceback provide enough context to understand and reproduce the issue.\n\nA developer would have all the necessary information to investigate and fix this bug in Django's SQLite schema handling code.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-14999": {
        "explanation": "The issue description is somewhat brief but provides enough information to understand the problem:\n\n1. The issue is about the `RenameModel` operation in Django's migration system.\n2. When a model already has `db_table` defined, renaming the model should be a \"no operation\" (noop) since the database table name is explicitly set and wouldn't change.\n3. Currently, this is not working as expected:\n   - In PostgreSQL, it unnecessarily drops and recreates foreign key constraints\n   - In SQLite, it unnecessarily recreates the table\n\nThe issue is clear about what's happening and what should happen instead. The expected behavior is that when a model with an explicit `db_table` is renamed, the migration should recognize this as a situation where no database changes are needed.\n\nWhile the description doesn't provide code examples or specific implementation details, the problem is well-defined enough that a developer familiar with Django's migration system should be able to investigate and implement a solution. The issue is describing a specific bug with clear expected behavior.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15018": {
        "explanation": "This issue is well-specified with clear technical details about the problem. The user has:\n\n1. Clearly described the command structure with mutually exclusive arguments (--for and --until) that share the same 'dest' parameter\n2. Provided code examples showing how the command is defined\n3. Explained that the command works fine via command line but fails when using call_command()\n4. Specified two specific failure modes:\n   - No mechanism to pass a 'for' value through call_command\n   - The 'for' value is always required and attempts to parse the string 'until' value\n\nThe issue is describing a specific technical limitation in Django's call_command() function when dealing with mutually exclusive arguments that share the same destination parameter. The solution would need to address how call_command() handles these types of argument configurations.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - they would need to modify how call_command() handles mutually exclusive arguments with the same 'dest' parameter.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15022": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific performance problem in Django's admin changelist query when searching for clients with multiple words.\n2. The relevant models (Client and ClientOffice) are clearly defined with their relationships.\n3. The admin configuration (ClientAdmin) shows how the search_fields are set up, including fields from related models.\n4. The problem is clearly stated: when searching with multiple words (>3), the admin interface stalls.\n5. The root cause is identified: each search word creates an additional JOIN in the SQL query due to the qs = qs.filter(...) pattern.\n6. The issue mentions that a patch is attached (though not visible in this description), and notes it's for Django 1.2.5 but can be adapted for the current trunk.\n\nThe issue provides enough context about the models, relationships, and the specific performance problem to understand what needs to be fixed. The solution would involve optimizing how multiple search terms are handled in admin queries to prevent excessive JOINs.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15031": {
        "explanation": "This issue is well-specified with a clear problem statement and enough context to understand what needs to be fixed. The issue describes:\n\n1. The specific functionality that's broken: filtering in Django admin using the `__in` lookup with comma-separated values\n2. The exact limitation: inability to include values that contain commas themselves\n3. The specific function that needs to be modified: `prepare_lookup_value` in `contrib.admin.util`\n4. A concrete example of the current behavior: `/admin/auth/user/?username__in=johnny,viola,gordon`\n\nThe issue clearly identifies the problem, where it occurs in the codebase, and what needs to be fixed. A developer would need to modify the `prepare_lookup_value` function to properly handle values containing commas when they're part of the `__in` lookup. This would likely involve implementing a more sophisticated parsing mechanism that can distinguish between commas used as separators and commas that are part of the actual values.\n\nThe solution path is clear: modify the parsing logic in the identified function to handle comma-containing values correctly while maintaining backward compatibility.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15037": {
        "explanation": "This issue is well-specified with a clear problem description and even includes a proposed solution. The issue describes a specific bug in Django's `inspectdb` command, which is used to generate Django models from an existing database schema.\n\nThe problem is clearly articulated:\n1. When a foreign key constraint references a non-primary key field (in this case, a UNIQUE field), `inspectdb` incorrectly generates a foreign key to the primary key instead of to the specified field.\n2. A concrete example is provided with SQL schema that reproduces the issue.\n3. The expected behavior is that the generated model should have a foreign key to the specific field referenced in the constraint (foo.other_id), not to the primary key (foo.id).\n\nThe submitter mentions they've attached a potential fix, which indicates they have a clear understanding of what needs to be changed in the code.\n\nThis issue provides enough information for a developer to:\n1. Reproduce the problem using the provided SQL schema\n2. Understand what the correct behavior should be\n3. Implement a fix based on the understanding of the issue\n\nThe only minor detail missing is the actual attached fix that was mentioned, but this doesn't prevent understanding or solving the issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15038": {
        "explanation": "This issue is well-specified with a clear problem description and expected solution. The issue describes that:\n\n1. When using the `squashmigrations` command with the `--squashed-name` parameter set to a name that matches an existing migration file (e.g., \"initial\"), it overwrites the existing file instead of raising an error.\n\n2. The issue explains the specific consequences of this behavior:\n   - It can create inconsistency between code and database if the overwritten migration was already applied\n   - It can introduce a CircularDependencyError when the squashed migration includes dependencies on the file it's overwriting\n\n3. The expected solution is clearly stated: \"squashmigrations should exit with an error instead of overwriting existing migration files.\"\n\nThe issue provides a specific example command that reproduces the problem: `squashmigrations --squashed-name initial myapp 0002`, which would overwrite `0001_initial.py`.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Understand the problem\n3. Implement the requested solution (adding validation to prevent overwriting existing files)\n\nThe scope is well-defined and the fix appears straightforward - add a check before writing the squashed migration file to see if a file with that name already exists, and if so, exit with an error message.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15044": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue identifies specific components (CacheMiddleware and FetchFromCacheMiddleware) that are not thread-safe.\n2. It points to the exact line of code causing the problem: https://github.com/django/django/blob/main/django/middleware/cache.py#L186\n3. It provides a detailed error traceback showing the failure path.\n4. The reporter mentions they've created a reproducible test case with specific components (pylibmc cache and a @cache_page cached view).\n5. The issue occurs with concurrent requests even in the development server.\n6. Historical context is provided, noting the issue has existed since 2010 with a specific commit reference.\n\nThe problem is clearly that the cache middleware is storing a cache instance as an instance variable, which is not thread-safe when used with certain backends like pylibmc. A developer would have enough information to understand and fix this issue by modifying how the cache instance is accessed or stored to ensure thread safety.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15061": {
        "explanation": "The issue is reasonably specified, but there are some details that could be clearer. \n\nThe issue describes a problem with the MultiWidget class in Django, specifically that it generates an id_for_label with an index (f'{id_}0') which the reporter claims \"has not sense\" (meaning it doesn't make sense). They compare this to ChoiceWidget which has an add_id_index attribute that allows control over whether the index is included.\n\nThe proposed solution is to \"remove completely id_for_label method from MultiWidget Class\" or to \"Remove 'for = ...' from MultiWidget's <label>\".\n\nWhat's clear:\n1. There's an issue with how MultiWidget generates id_for_label values\n2. The reporter wants to change this behavior\n3. The specific class (MultiWidget) and method (id_for_label) are identified\n\nWhat could be clearer:\n1. The exact problem with the current behavior isn't fully explained\n2. The desired outcome is somewhat ambiguous - should the method be removed entirely or just modified?\n3. No code examples are provided showing the current problematic behavior\n\nDespite these gaps, a developer familiar with Django's form widget system would likely understand the issue and be able to implement a solution. The MultiWidget class and its id_for_label method can be located in the codebase, and the behavior can be modified or removed as appropriate.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15062": {
        "explanation": "This issue is well-specified with clear details about a bug in Django's ORM when using Case() and ExpressionWrapper() with DecimalField on SQLite. The reporter has:\n\n1. Clearly identified the problem: Case() and ExpressionWrapper() don't work properly with DecimalField on SQLite due to type conversion inconsistencies\n2. Provided detailed test cases that demonstrate the issue\n3. Included the SQL queries generated by each test case\n4. Shown the error messages that occur\n5. Explained the root cause: the issue appears to be related to how SQLite handles type conversions (string vs numeric literals)\n6. Noted that the issue only occurs on SQLite (works on MySQL)\n7. Referenced a similar issue (#18247) that might provide insight for a solution\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and begin working on a solution. The reporter has done thorough investigation work and provided a clear explanation of what's happening. A developer would have enough information to start debugging and fixing the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15087": {
        "explanation": "The issue is well-specified and provides clear information about what needs to be fixed:\n\n1. It identifies a specific component: the SQLite backend's DatabaseIntrospection.get_relations method\n2. It describes the current implementation: using regex to extract foreign key constraints from DDL\n3. It suggests a specific solution: use PRAGMA foreign_key_list instead, similar to how _get_foreign_key_constraints works\n4. It implies that the current implementation is unnecessarily complex and that a simpler, more reliable solution exists\n\nAn experienced developer familiar with Django's database layer would understand that:\n- The SQLite backend should be modified to use SQLite's built-in PRAGMA commands for introspection\n- The existing regex-based approach should be replaced\n- There's already a similar implementation (_get_foreign_key_constraints) that can serve as a reference\n\nThis is sufficient information to implement a solution without needing further clarification. The task involves refactoring a specific method to use a more direct approach that's already demonstrated elsewhere in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15098": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Clearly described the problem: Django's i18n_patterns doesn't work with locale codes that contain both script and region (like en-latn-us or en-Latn-US)\n2. Provided complete reproduction steps with code examples\n3. Shown the actual behavior (404 errors) vs. expected behavior (200 responses)\n4. Included relevant context about RFC 5646 standards for language tags\n5. Confirmed the issue can be reproduced on a fresh Django project\n6. Provided environment details (Python 3.7.5, Django 3.2.7)\n\nThe issue is specific enough that a developer could understand the problem, reproduce it, and implement a fix without needing additional clarification. The root cause appears to be that Django's internationalization system isn't properly handling language tags that include script components according to RFC 5646 standards.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15102": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies that `startproject` is not honoring the user's umask when creating new files.\n2. It provides a specific reproduction case with commands and expected vs. actual results.\n3. The root cause is identified: the fix for ticket #1651 is copying all permission bits rather than just the executable flags.\n4. There's a concrete example showing how this affects users (the Zero Install package case).\n\nThe solution would need to modify the `startproject` command to respect the user's umask when creating new files, rather than copying all permission bits from the template files. This is a straightforward task with a clear success criterion: after the fix, newly created project files should have permissions that respect the user's umask setting.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15103": {
        "explanation": "This issue is well-specified. The user is requesting a specific change to the `json_script` function - to make the `element_id` parameter optional instead of required. \n\nThe issue clearly explains:\n1. The specific function that needs modification (`json_script`)\n2. The exact change requested (make the `element_id` parameter optional)\n3. The rationale for the change (user wants to use it inside a template where ID isn't needed)\n4. The user has even verified that making this change doesn't break existing tests\n\nA developer would understand exactly what needs to be done: modify the `json_script` function to accept cases where no ID is provided. This is a straightforward feature request with a clear scope and implementation path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15104": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The issue is clearly described: The Django migration autodetector fails with a KeyError when encountering a custom ForeignKey field that hardcodes its 'to' parameter and removes it from its deconstructed kwargs.\n\n2. A complete error traceback is provided, showing exactly where the error occurs in Django's code.\n\n3. A self-contained reproduction test case is included that demonstrates the issue.\n\n4. The root cause is identified: In django/db/migrations/autodetector.py, the line `del deconstruction[2]['to']` fails when the 'to' key doesn't exist.\n\n5. A specific fix is proposed: Change `del deconstruction[2]['to']` to `deconstruction[2].pop('to', None)` to handle cases where the 'to' key is not present.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement the suggested fix. The solution appears straightforward - modify the autodetector to safely handle missing 'to' keys in field deconstructions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15108": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a specific inconsistency in the Django ORM API: the `Window.order_by` method doesn't support the leading dash syntax for descending order (e.g., \"-column_name\") that is supported by `QuerySet.order_by` and some aggregates.\n\nThe issue provides:\n1. The specific component affected (Window.order_by)\n2. The current behavior (doesn't support descending string syntax)\n3. The expected behavior (should support descending string syntax like other similar APIs)\n4. A suggested implementation approach (reuse logic from OrderableAggMixin.__init__)\n\nThe issue also includes some additional context about potential refactoring opportunities and API design considerations.\n\nWhile the issue doesn't provide specific code examples of the current behavior vs. expected behavior, a developer familiar with Django's ORM would understand what's being requested. The issue is written by someone who appears to have a good understanding of the codebase and has a clear plan for implementation.\n\nThe only minor ambiguity is around the scope of changes - whether this should be a minimal fix to support the syntax in Window.order_by or if it should include the broader refactoring suggestions mentioned. However, the core requirement is clear enough to proceed with a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15111": {
        "explanation": "This issue is well-specified with clear information about:\n\n1. The problem: django-admin start[project|app] commands don't send a proper user agent header when fetching remote templates via HTTP(S) URLs\n2. The consequence: Some sites (specifically GitLab using Cloudflare) block these requests due to the default urllib User Agent\n3. The proposed solution: Set the 'User Agent' header to 'Django/DJANGO_VERSION'\n4. A reference to a related GitLab issue for context\n\nThe issue provides enough context to understand what's happening, why it's a problem, and a clear direction for the solution. A developer would be able to locate the relevant code in Django that handles template fetching during project/app creation and modify the HTTP request headers accordingly.\n\nThe solution approach is straightforward - find where Django makes HTTP requests for templates and add the appropriate User-Agent header.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15127": {
        "explanation": "The issue description provides some context about a problem with Django's message tags in tests when using `@override_settings`, but lacks several important details:\n\n1. It doesn't provide any code examples showing the problem\n2. It doesn't specify what specific settings are being overridden\n3. It doesn't include expected vs. actual behavior with concrete examples\n4. It doesn't mention which version of Django is affected\n5. There's no information about how to reproduce the issue\n\nWhile I can infer that this is related to Django's messaging framework and specifically how message level tags aren't being properly updated when overridden in test settings, the lack of reproduction steps, code examples, and specific details about what settings are being changed makes this issue difficult to address without making assumptions.\n\nA developer would need to investigate how Django's message framework handles level tags, how the `@override_settings` decorator interacts with them, and make educated guesses about what might be causing the issue.",
        "label": "VAGUE"
    },
    "django__django-15128": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and a detailed analysis of the root cause. The reporter has:\n\n1. Provided a complete minimal code example that reproduces the issue\n2. Specified the exact Django versions where the issue occurs (2.2.24 and 3.2.9)\n3. Identified the specific error (AssertionError in Query.change_aliases)\n4. Explained their understanding of the root cause (alias collision in the change_map)\n5. Suggested a potential solution approach (preventing intersection between change_map keys and values)\n6. Provided additional context about the assertion's purpose and documentation improvements\n\nThe issue is about a non-commutative behavior in Django's QuerySet OR operation, where `qs2 | qs1` works but `qs1 | qs2` fails with an AssertionError. The problem occurs in the Query.change_aliases method when the change_map contains overlapping keys and values.\n\nA developer would have enough information to investigate and fix this issue without needing further clarification. The suggested solution direction is reasonable and provides a good starting point for implementing a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15135": {
        "explanation": "This issue is well-specified with a clear description of the problem. The issue provides:\n\n1. A complete code example showing the model definitions and the sequence of operations that leads to the problem\n2. The expected behavior: parent model should be updated with the ID of the child in the database\n3. The actual behavior: parent model still references Null\n4. A suggested solution: adding a check for ForeignKeys in bulk_update logic to copy the ID from the child model to the child_id field\n\nThe issue is describing a specific bug in Django's bulk_update() method where when a parent object has a newly created child object assigned to it (but not saved via the parent), the bulk_update() operation doesn't properly update the foreign key reference.\n\nThe problem is clear and reproducible based on the provided steps. A developer would be able to understand what's happening, verify the issue, and implement a fix based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15139": {
        "explanation": "This issue is well-specified with a clear objective. It asks to:\n\n1. Deprecate the PickleSerializer class\n2. Move it out of the core codebase into a third-party package\n\nThe rationale is clearly explained - the Pickle serializer is known to be dangerous and can lead to remote code execution vulnerabilities, as evidenced by the linked example. While the issue doesn't specify implementation details like deprecation warnings or migration paths, the core task is unambiguous.\n\nA developer would need to:\n- Add deprecation warnings to the PickleSerializer\n- Create a separate package for it\n- Update documentation to discourage its use\n- Ensure backward compatibility during the transition\n\nThese are standard steps for deprecating and moving functionality that any experienced developer would understand without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15161": {
        "explanation": "This issue is reasonably well-specified. It describes a previous change (PR #14047) that simplified the import path for the F() expression in Django's ORM from `django.db.models.expressions.F()` to `django.db.models.F()` to make generated migration code cleaner. The issue suggests applying the same technique to other expressions to further simplify migrations.\n\nWhile the issue doesn't explicitly list which other expressions should be modified, it provides a clear pattern to follow based on the previous PR. A developer familiar with Django would know that there are other similar expression classes (like Q(), Value(), Func(), etc.) that could benefit from the same treatment. The goal is clear: modify the deconstruct method of these expression classes to use simpler import paths.\n\nThe issue could have been more specific by listing all the expression classes that need modification, but the pattern is clear enough that a developer could identify candidates by examining the codebase and following the example of PR #14047.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15166": {
        "explanation": "The issue description provides a clear problem statement: the DatabaseCache backend doesn't quote all fields in queries, which is problematic specifically for Snowflake databases because Snowflake treats unquoted fields as uppercase. \n\nThe description also mentions that there is an attached patch that works with Django stable/3.2.x, indicating that a solution has already been proposed but needs to be reviewed and potentially adapted for the main branch once django-snowflake development catches up.\n\nWhile the description doesn't provide detailed technical specifics about which fields need quoting or exactly how the patch implements the solution, the core problem is clearly defined. A developer familiar with Django's database layer and SQL quoting conventions would understand what needs to be fixed. The mention of an existing patch also provides a reference point for the solution.\n\nThe issue is reasonably specified because:\n1. The problem is clearly identified (lack of field quoting in DatabaseCache backend)\n2. The specific database affected is mentioned (Snowflake)\n3. The consequence of the problem is stated (unquoted fields treated as uppercase)\n4. A patch exists as a reference point\n\nA developer would need to:\n1. Understand how field quoting works in Django's DatabaseCache backend\n2. Review the attached patch\n3. Implement similar changes for the main branch\n4. Test with Snowflake database\n\nThis is enough information to make a meaningful attempt at a solution, though some investigation into the specifics of the DatabaseCache backend and Snowflake's requirements would be needed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15180": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: when a string is passed as the `kwargs` parameter to `path()`/`re_path()` instead of a dictionary, Django fails with confusing error messages rather than raising a clear TypeError.\n\n2. The submitter provides concrete examples of the incorrect code:\n   ```python\n   path(\"path/to/thing\", MyView.as_view(), \"my_view\")\n   ```\n\n3. The exact error messages that occur are included:\n   - `ValueError: dictionary update sequence element #0 has length 1; 2 is required`\n   - `AttributeError: 'str' object has no attribute 'items'`\n\n4. The submitter suggests specific solutions:\n   - Add a type-guard in `_path` to assert that `kwargs` is dict-like or None\n   - Add a system check on URLPattern to raise a friendly message\n   \n5. The issue is reproducible with the steps provided.\n\nThe problem is clearly a usability issue where the API allows incorrect parameter types to be passed, leading to confusing errors later. The solution would involve adding type checking to prevent this common mistake, which is a straightforward enhancement to implement.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15202": {
        "explanation": "This issue is well-specified with a clear problem description. The issue states that when using Django's URLField with an invalid URL ('////]@N.AN'), it raises a ValueError with the message \"Invalid IPv6 URL\" instead of a ValidationError. The traceback is provided, showing the exact path through the code where the error occurs.\n\nThe problem is clear: Django's form validation system typically uses ValidationError to handle invalid input, but in this specific case, a ValueError is being raised instead. This breaks the expected behavior pattern of Django's form validation system, which should catch and convert all validation issues into ValidationError instances.\n\nA successful solution would involve modifying the URLField implementation to catch the ValueError from urllib.parse.urlsplit and convert it to a ValidationError, maintaining consistency with Django's form validation pattern.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15213": {
        "explanation": "This issue is well-specified with clear symptoms and reproducible examples. The problem is clearly defined: when using ExpressionWrapper with a negated Q object that has an empty list (~Q(pk__in=[])), the generated SQL query is malformed (missing the value after SELECT). The issue provides:\n\n1. A clear problem statement - ExpressionWrapper with ~Q(pk__in=[]) crashes\n2. Specific code examples showing both working and non-working cases\n3. The exact SQL output showing the issue (missing value in the SELECT statement)\n4. Context about how this relates to Q object reduction\n\nA developer would be able to reproduce this issue and understand what needs to be fixed - the SQL generation for ExpressionWrapper when handling negated Q objects with empty lists. The solution would need to ensure that a proper SQL expression is generated in this specific case.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15240": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. A specific problem: When using Django's squashmigrations feature, the django_migrations table can contain entries for migrations that no longer exist, which can cause naming conflicts with new migrations.\n\n2. A concrete example: The description provides a step-by-step scenario demonstrating how this problem occurs.\n\n3. A proposed solution: Truncate the django_migrations table to include only migrations that actually exist in the project.\n\n4. Implementation options: Either automatic (when executor runs or inside migrate command) or as a separate command.\n\n5. Pros and cons of the solution.\n\n6. A potential edge case to be careful about: When users delete replaced migrations before running squashed migrations.\n\nThe issue provides enough context and detail for a developer to understand the problem, the desired outcome, and potential implementation approaches. It even highlights edge cases to consider during implementation. This is sufficient information to begin working on a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15248": {
        "explanation": "This issue is reasonably well-specified. The request is to enhance pre/post delete signals in Django to include information about the origin of the delete operation - whether it came from a model.delete() call on a specific instance or from a queryset.delete() call.\n\nThe issue clearly states the problem: currently, when objects are deleted (which can cascade to related objects), signal listeners don't have context about what initiated the deletion. The desired enhancement would provide this context to signal handlers.\n\nWhat makes this reasonably specified rather than well-specified:\n1. It doesn't specify exactly how this information should be passed to the signal handlers (e.g., as additional parameters to the signal or through some other mechanism)\n2. It doesn't detail the exact format of the information to be provided\n3. It doesn't specify if this should be backward compatible or how existing code might be affected\n\nHowever, there's enough information to understand what needs to be done: modify the delete signal system to track and provide the origin of deletion operations. A developer familiar with Django's signal system would be able to implement a solution based on this description.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15252": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user has a multi-database setup with a router that should prevent migrations in non-default databases\n2. The specific problem is identified: MigrationRecorder is creating migration tables in all databases despite router settings\n3. The exact code path is described in detail:\n   - Test runner calls migrate on each connection\n   - Migration executor calls recorder.record_applied\n   - record_applied calls ensure_schema\n   - ensure_schema creates tables without respecting router settings\n4. The expected behavior is clearly stated: migration tables should only be created in the default database when a router specifies allow_migrate=False for other databases\n5. The user provides the specific files and line numbers where the issue occurs\n\nThe issue provides enough context about the Django multi-database architecture, the router implementation, and the specific component (MigrationRecorder) that needs to be fixed. A developer could locate the problematic code and understand what changes are needed to make MigrationRecorder respect the router's allow_migrate rules.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15268": {
        "explanation": "This issue is well-specified with clear requirements. The problem is clearly defined: when Django migrations include operations to remove constraints followed by operations to add new constraints, in some cases these can be optimized into a single operation.\n\nThe issue provides:\n1. A specific context with reference to a previous PR (#31503)\n2. A clear example of the current behavior (4 operations)\n3. A clear example of the desired behavior (2 operations)\n4. A specific test case reference for further context\n\nThe optimization goal is straightforward: when there's a pattern of \"remove all constraints\" followed by \"add specific constraints\" for the same model and same constraint type, these should be combined into a single \"set constraints to these values\" operation.\n\nThe solution would involve modifying Django's migration optimizer to detect this pattern and collapse the operations appropriately. This is a well-defined task with clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15272": {
        "explanation": "The issue is well-specified with a clear problem statement and proposed solution. It describes:\n\n1. The problem: The migration optimizer has trouble reducing operations, and developers often need to manually edit squashed migrations and re-run them through the optimizer.\n\n2. The proposed solution: A new management command called `optimizemigration` that:\n   - Takes two parameters: app_name and migration_name\n   - Reads a single migration\n   - Passes it through the optimizer\n   - Rewrites the result to disk\n   - Unlike squashmigrations, ignores the \"squashed-ness\" of the migration\n\n3. The implementation: The issue mentions an \"attached patch\" that contains an implementation, suggesting there's already code available to reference.\n\nThe requirements are specific enough that a developer could implement this feature without needing additional clarification. The command's behavior, parameters, and purpose are all clearly defined. The only minor detail missing is the exact optimization algorithm to use, but it's implied that it should use Django's existing migration optimizer.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15277": {
        "explanation": "This issue is well-specified with clear technical details and a proposed solution. The author:\n\n1. Clearly identifies the problem: When using `Value('test')`, Django creates a `CharField` with a `MaxLengthValidator` that has a null `max_length`, which is problematic and unnecessary since validators aren't used for the `output_field` of `Value`.\n\n2. Provides evidence with code examples and timing measurements showing the current behavior and the performance impact.\n\n3. Proposes a specific solution: Modify `CharField.__init__` to only add the `MaxLengthValidator` when `max_length` is not None, similar to how `BinaryField` handles it.\n\n4. Mentions they have a working branch with the change and all tests pass.\n\nThe issue includes specific code snippets, performance measurements, and a clear path to implementation. The author has already done the work and is ready to submit it for review. This is a micro-optimization with a well-defined scope and implementation strategy.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15278": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly stated: Adding a nullable OneToOneField crashes on SQLite with a specific error message \"Cannot add a UNIQUE column\"\n\n2. The issue provides:\n   - The exact error message and traceback\n   - The specific migration code causing the problem\n   - The context (happening between Django 4.0 and main branch)\n   - A reference to a related issue in another project (django-oauth-toolkit)\n   - A hint about potential causes (sqlite3 changes in PR #33355)\n\n3. The error is reproducible with the given migration code, which attempts to add a OneToOneField that is both nullable and unique.\n\n4. The issue is specific to SQLite, as mentioned in the title and description.\n\nA developer can understand the problem, reproduce it, and investigate the SQLite-specific handling of UNIQUE constraints on nullable fields in Django's migration system. The reference to PR #33355 provides a starting point for investigation into recent SQLite-related changes that might have caused this regression.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15280": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue provides complete model definitions showing the relationship between User and Profile models.\n2. It includes a complete test case that demonstrates the bug, with expected behavior clearly stated.\n3. The actual behavior is precisely described, including the exact query that's being unexpectedly executed.\n4. The issue reporter has done thorough investigation, showing the queries that are executed, checking deferred fields, and noting that the same issue occurs with ForeignKey relationships.\n5. The reporter has a hypothesis about what's happening: the User instances from the inner prefetch are incorrectly inheriting deferred field information from the outer User queryset.\n\nThe problem is clearly defined: when following prefetches back to a \"parent\" object, the deferred fields are not being handled correctly, causing unexpected database queries. The expected behavior would be that accessing user.profile.user.kind should not trigger any additional queries since that field was explicitly included in the inner prefetch's only() clause.\n\nThis is a technical issue with a specific Django ORM behavior that can be investigated and fixed with the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15292": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: Technical 404 debug page shows incorrect view name for Class-Based Views (CBVs)\n2. The specific issue: It's showing the full class representation (with `<class '...'>` wrapper) instead of just the class name\n3. An example of the incorrect output vs. expected output\n4. The regression commit that introduced the issue (0c0b87725bbcffca3bc3a7a2c649995695a5ae3b)\n\nThe issue provides enough context to understand what's wrong and what needs to be fixed. A developer would need to:\n1. Look at the regression commit to understand what changed\n2. Modify the code that generates the view name in the 404 debug page to properly handle CBVs\n3. Ensure it displays just the class name (view_tests.views.Http404View) rather than the full class representation\n\nWhile the issue doesn't explicitly state which files need to be modified, an experienced developer familiar with the codebase should be able to trace this through the regression commit and the 404 debug page rendering code. The expected behavior is clearly defined by the example.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15297": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly identified: The `ResolverMatch` representation for Class Based Views is incorrect in Django 4.0+, showing \"urlpatterns_reverse.views.view\" instead of the fully qualified class name.\n\n2. The cause is identified: Changes in #32260 that removed `update_wrapper` and stopped setting `__name__` and `__qualname__` in favor of using the `view_class` attribute.\n\n3. A test case is provided that demonstrates the issue.\n\n4. A specific solution is proposed: Modify the `ResolverMatch` class to check for the `view_class` attribute and use it when available.\n\n5. The submitter even provides the exact code change needed to fix the issue.\n\nThe issue includes all necessary context, a reproduction case, the root cause analysis, and a proposed solution. A developer could implement the fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15315": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible test case. The issue explains:\n\n1. The specific bug: Field.__hash__() changes value when a field is assigned to a model class, breaking the immutability expectation for hash functions\n2. A minimal code example that demonstrates the issue\n3. The root cause: the bug was introduced in PR #31750\n4. The impact: objects with changing hash values break when used in dictionaries\n5. A proposed solution: revert the __hash__ change from #31750\n\nThe issue provides enough context to understand the problem and implement a solution. A developer would need to:\n1. Examine the changes made to Field.__hash__() in PR #31750\n2. Understand why the hash value is changing when a field is assigned to a model\n3. Revert or modify the hash implementation to maintain immutability while preserving the equality fix from #31750\n\nThis is a well-defined task with clear acceptance criteria - the hash value should remain consistent before and after field assignment to a model class, allowing the assert statement in the example to pass.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15316": {
        "explanation": "The issue description identifies a specific problem with the `simplify_regex()` function in Django: it doesn't handle non-capturing groups correctly. The reporter mentions they discovered this while using Django REST Framework's Schema generator.\n\nHowever, the description lacks several important details:\n1. No examples of the problematic regex patterns\n2. No description of the current behavior vs. expected behavior\n3. No error messages or stack traces\n4. No information about what \"broken\" means specifically (does it throw an error, produce incorrect output, etc.)\n5. No suggested fix or approach\n\nWhile we can understand the general issue (the function doesn't properly handle non-capturing groups in regular expressions), we would need more specific information about how it fails and what the expected behavior should be to implement a proper solution. A developer would likely need to investigate the current implementation of `simplify_regex()` and understand how it currently processes regex groups before determining how to add support for non-capturing groups.",
        "label": "VAGUE"
    },
    "django__django-15318": {
        "explanation": "This issue is well-specified with clear examples, code snippets, and a detailed explanation of the current inconsistent behavior between reverse foreign key and many-to-many relationships when querying unsaved models. \n\nThe issue describes:\n1. The specific problem: inconsistent behavior between reverse FK and M2M fields on unsaved models\n2. A concrete code example demonstrating the issue\n3. The current behavior: reverse FK fields return empty queryset, while M2M fields raise an error\n4. A proposed solution with specific changes to make behavior consistent\n5. A link to an existing patch that implements the proposed changes\n6. Consideration of backward compatibility concerns\n\nThe issue provides enough technical detail for a developer to understand the problem, verify it, and implement a solution. The proposed changes are clearly outlined, and there's even a reference implementation available. A developer could proceed with implementing a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15320": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a bug in the `Subquery.as_sql()` method that generates invalid SQL by incorrectly removing the first and last characters of the SQL statement.\n\n2. The reporter provides a complete reproduction case with code examples showing:\n   - The expected behavior\n   - The current incorrect behavior\n   - A workaround by setting `q.query.subquery = True`\n\n3. The reporter suggests a specific fix: setting the `subquery` attribute to `True` in the `Subquery` constructor.\n\n4. The issue includes the exact SQL output showing the problem (missing 'S' at the beginning and missing '\"' at the end).\n\nThe problem is clearly defined, reproducible with the provided code, and has a suggested solution path. A developer could implement and test a fix based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15324": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: Django crashes with a 500 error when a file with a null byte in its extension is uploaded.\n\n2. The exact error is identified: `ValueError: embedded null byte` when Django tries to create a tempfile with the problematic extension.\n\n3. A complete reproduction script is provided that demonstrates how to trigger the bug.\n\n4. A sample Django view that exhibits the problem is included.\n\n5. The issue suggests a potential solution direction: escaping the parsed-out extension if preserving the extension is important.\n\nThe problem scope is clear - handling null bytes in file extensions during file uploads - and the expected behavior is also clear: Django should handle this case gracefully rather than crashing with a 500 error. The issue provides all the necessary context for a developer to understand, reproduce, and fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15334": {
        "explanation": "This issue is reasonably well-specified. The user is requesting that Django's QuerySet.iterator() method should support prefetch_related() functionality. \n\nThe issue clearly identifies:\n1. The current behavior: prefetch_related() calls are ignored when using QuerySet.iterator()\n2. The documentation that confirms this behavior\n3. A rationale for why this should be changed: iterator() now supports chunking, and it would be beneficial to prefetch related objects for each chunk\n\nWhat makes this reasonably (rather than well) specified:\n- It doesn't provide specific code examples showing the current behavior\n- It doesn't outline a specific implementation approach\n- It doesn't address potential performance implications or edge cases\n\nHowever, a developer familiar with Django's ORM would understand what's being requested and could implement a solution that makes prefetch_related() work with iterator(). The chunking mechanism mentioned provides a clear direction for how this might be implemented (applying prefetch_related() to each chunk rather than trying to prefetch everything at once).\n\nA successful solution would modify Django's ORM to allow prefetch_related() to work when using iterator(), likely by applying the prefetch operation to each chunk of results.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15352": {
        "explanation": "This issue is well-specified with clear requirements. The problem is that Django's error reporting system is not cleansing session IDs, which can lead to security vulnerabilities when these IDs appear in public bug reports. The issue:\n\n1. Clearly identifies the problem: session IDs are being exposed in error reports\n2. Provides evidence: links to multiple GitHub issues where this has happened\n3. Explains the security risk: potential for account hijacking\n4. References a solution: points to an existing PR (#15352) that addresses this issue\n5. Provides context: references related issue #29714 and a discussion thread\n\nA developer would understand exactly what needs to be done: modify Django's error reporting system to cleanse session IDs similar to how it already handles other credentials. The existing PR even provides a starting point for the implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15368": {
        "explanation": "This issue is well-specified with a clear problem description, reproduction steps, and expected behavior. The user has:\n\n1. Clearly identified the bug: `bulk_update()` doesn't work with plain `F('...')` expressions, instead treating them as strings.\n2. Provided a complete reproduction example with code and output.\n3. Shown the SQL query that demonstrates the problem.\n4. Identified the likely source of the bug in the Django codebase (a narrow type check).\n5. Suggested two potential solutions to fix the issue.\n\nThe issue includes all the necessary information to understand the problem and implement a solution. The fix would involve modifying the type check in the Django codebase to properly handle `F()` expressions in `bulk_update()` operations, and the user has even suggested specific approaches to solve it.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15375": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Identified a specific bug: Using `aggregate()` with the `default` parameter after `annotate()` causes an SQL error\n2. Provided the exact Django version (4.0.1)\n3. Included complete reproduction steps with code examples\n4. Shown the error message and stack trace\n5. Revealed the problematic SQL query being generated\n6. Demonstrated a workaround (using Coalesce explicitly)\n\nThe issue appears to be that when using the `default` parameter with an aggregate function after an `annotate()` call, Django generates invalid SQL missing a column list after the first SELECT statement. The SQL should be something like \"SELECT column_list FROM (SELECT...)\" but instead it's \"SELECT FROM (SELECT...)\".\n\nA developer would have enough information to locate the bug in Django's query generation code and fix it without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15380": {
        "explanation": "This issue is well-specified with clear information about the bug. It provides:\n\n1. A specific error scenario: The migration autodetector crashes when renaming both a model and a field in a single step\n2. The exact command that triggers the issue: `python manage.py makemigrations`\n3. A complete stack trace showing the error path and the specific exception (KeyError)\n4. The exact line where the error occurs: in `autodetector.py` line 823, when trying to access `self.to_state.models[app_label, old_model_name]`\n5. The specific key that's causing the KeyError: `('test_one', 'mymodel')`\n6. Information that this is a regression introduced by a specific commit: aa4acc164d1247c0de515c959f7b09648b57dc42\n\nWith this information, a developer can:\n1. Reproduce the issue by creating a test case that renames both a model and field\n2. Understand the root cause (the autodetector is looking for the old model name in lowercase, but it might be stored differently)\n3. Trace the regression to the specific commit that introduced it\n4. Develop a fix by examining how the model renaming and field renaming interact in the autodetector\n\nThe issue provides all necessary details to understand, reproduce, and fix the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15382": {
        "explanation": "This issue is well-specified with a clear problem description. The reporter provides:\n\n1. A specific code example that reproduces the issue\n2. The expected behavior (implicitly that the WHERE clause should be preserved)\n3. The actual behavior (the WHERE block is missing completely)\n4. A reference to a similar issue (#33018) which provides context\n\nThe issue describes a specific bug where using a negated Exists subquery with an empty queryset causes the entire WHERE clause to be removed from the query, rather than preserving other conditions (like `name='test'`). This is clearly unintended behavior since it would cause queries to return incorrect results.\n\nA developer could reproduce this issue using the provided code example and verify the behavior. The solution would likely involve fixing how Django handles negated Exists subqueries with empty querysets to ensure other WHERE conditions are preserved.\n\nThe reference to a similar issue (#33018) provides additional context that would help in understanding and solving the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15388": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided exact steps to reproduce the issue\n2. Clearly described the problem: adding BASE_DIR to TEMPLATES[0]['DIRS'] in settings.py prevents the dev server from auto-restarting when files are changed\n3. Demonstrated that removing BASE_DIR from TEMPLATES[0]['DIRS'] resolves the issue\n4. Noted that this affects all file changes, not just changes to settings.py\n\nThe issue appears to be a bug in Django's development server auto-reload functionality that occurs specifically when BASE_DIR is added to the template directories configuration. The reproduction steps are straightforward and should allow a developer to consistently reproduce the issue, diagnose the root cause, and implement a fix. The expected behavior (dev server should restart on file changes) is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15401": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a specific behavior change between Django 1.11 and Django 2.0-2.2 regarding how filters handle unsaved related objects. The issue describes:\n\n1. The specific scenario: Using an unsaved object in a related filter (Foo.objects.filter(related_obj=bar))\n2. The behavior in Django 1.11: Returns an empty QuerySet\n3. The behavior in Django 2.0-2.2: Equivalent to filtering with None, which can return non-empty results\n\nThe issue also references a related ticket and asks two clear questions:\n1. What is the intended behavior?\n2. Is this documented?\n\nThe issue suggests a direction for the solution (preventing unsaved objects from being used in related filters), which aligns with a comment from a Django maintainer.\n\nWhile the issue doesn't explicitly state what code changes are needed, an experienced Django developer would understand that this likely requires:\n1. Adding validation to prevent unsaved objects in related filters\n2. Adding appropriate deprecation warnings\n3. Updating documentation\n\nThe only minor ambiguity is around the exact implementation details and deprecation timeline, but these follow standard Django practices and can be reasonably inferred.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15413": {
        "explanation": "This issue is well-specified with clear details about the bug, its context, and a proposed solution:\n\n1. The issue clearly identifies a bug in Django's logging messages for synchronous middleware adaptation.\n\n2. It provides specific context by referencing the Django documentation that describes the expected behavior.\n\n3. It includes the relevant test case that demonstrates the issue.\n\n4. It explains the current implementation by showing the relevant code snippets from Django's codebase.\n\n5. It identifies the root cause: BaseHandler.adapt_method_mode treats the middleware name as a method name, resulting in incorrect log messages.\n\n6. It provides a detailed proposed fix with code changes, showing exactly what needs to be modified.\n\nThe issue contains all the necessary information to understand the problem and implement a solution. The proposed fix is comprehensive and addresses the identified issue. A developer could implement this solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15414": {
        "explanation": "This issue is well-specified for an experienced Django developer. It clearly identifies:\n\n1. The specific method with the issue: `QuerySet.bulk_update()`\n2. The exact problem: The method doesn't set `self._for_write = True` before accessing `self.db`\n3. The consequence: It uses `db_for_read` instead of `db_for_write`, affecting transaction atomicity\n4. A reference to a ticket where this was discovered for additional context\n\nThe solution path is clear: modify the `bulk_update()` method to set `self._for_write = True` before accessing `self.db`, similar to how other write operations in Django's ORM work. This would ensure that the proper database router is used for write operations and that transactions are handled correctly.\n\nA developer familiar with Django's ORM internals would have enough information to locate the issue in the codebase and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15421": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly identified: Python 3.8 on macOS changed the default multiprocessing start method from 'fork' to 'spawn', causing Django's parallel test runner to fail.\n\n2. Specific error messages are provided:\n   - \"django.core.exceptions.AppRegistryNotReady: Apps aren't loaded yet\"\n   - \"django.db.utils.OperationalError: FATAL: database 'xxx_1' does not exist\"\n\n3. The root cause is explained: With 'spawn' method, worker processes don't inherit parent memory state, so they need explicit initialization.\n\n4. The solution approach is outlined: Modify django.test.runner._init_worker to run django.setup() and set the proper database name prefix.\n\n5. The submitter has even attached a patch with their proposed solution.\n\nThis provides all the necessary information to understand the issue and implement a solution. The specific file and function to modify are identified, and the general approach for the fix is clear.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15423": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the LazyObject class where it incorrectly defines magic methods (__getitem__, __iter__) that may not exist on the wrapped object. The issue provides specific examples of the problem, showing how checking for these attributes with hasattr() returns True, but then attempting to use them raises TypeError exceptions because the underlying wrapped object (User in this case) doesn't actually implement these methods.\n\nThe issue description includes:\n1. The specific class that has the bug (LazyObject)\n2. The specific methods affected (__getitem__, __iter__)\n3. Clear code examples demonstrating the problem\n4. The exact error messages that occur\n\nA developer would be able to locate the LazyObject class in the codebase, understand how it's incorrectly implementing these magic methods, and modify it to properly check if the wrapped object supports these operations before exposing them. The solution would likely involve modifying LazyObject to only expose magic methods that are actually implemented by the wrapped object.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15433": {
        "explanation": "This issue is well-specified with clear reproduction steps and evidence. The problem is clearly defined: when creating a custom user model that extends AbstractUser and adding a ManyToManyField that references this custom User model via settings.AUTH_USER_MODEL, Django keeps generating the same AlterField migration repeatedly.\n\nThe issue description includes:\n1. A precise description of the problem\n2. A link to a GitHub repository that reproduces the issue\n3. The relevant code snippets showing the model definitions\n4. An example of the erroneous migration file\n5. The observation that using the built-in User model doesn't cause the issue\n\nThe problem appears to be related to how Django handles references to swappable settings (AUTH_USER_MODEL) in ManyToManyField when the referenced model extends AbstractUser. This is a specific technical issue with clear reproduction steps that would allow a developer to investigate and fix the problem in Django's migration system.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15438": {
        "explanation": "This issue is well-specified with a clear problem and solution:\n\n1. Problem: When using a specific language variant (e.g., \"de-ch\" for Swiss German), Select2 translations fall back to English instead of using the more generic language variant (\"de\" for German) that would be more appropriate.\n\n2. Proposed solution: Modify the code to fall back to the language without region (e.g., \"de\" from \"de-ch\") when an exact match for the specific regional variant cannot be found.\n\nThe issue provides:\n- A specific example (LANGUAGE_CODE=\"de-ch\")\n- The current behavior (falls back to English)\n- The expected behavior (should fall back to \"de\")\n- A clear suggestion for implementation (use the language without region as fallback)\n\nThe issue also clarifies that it's not a duplicate of another ticket, showing the reporter has done due diligence.\n\nA developer would have enough information to implement this feature without needing further clarification. The solution would involve modifying how Django handles language codes for Select2 translations to implement this fallback mechanism.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15467": {
        "explanation": "This issue is well-specified with a clear problem statement and a proposed solution. The issue describes:\n\n1. The specific problem: ModelAdmin with radio_fields is overriding the custom empty_label set in formfield_for_foreignkey\n2. A concrete code example showing the issue\n3. The exact location of the problem in the Django codebase (django/contrib/admin/options.py, line 234)\n4. A specific code change that would fix the issue\n\nThe issue provides enough context to understand what's happening: when using radio_fields in ModelAdmin, any custom empty_label set in formfield_for_foreignkey is being overridden by Django's default empty label. The proposed solution is to check if an empty_label is already set in kwargs before applying the default.\n\nThis is a straightforward bug fix with a clear path to resolution. A developer could implement and test this solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15474": {
        "explanation": "This issue is reasonably well-specified. It describes a specific bug with the Django ContentType migration system when working with non-default databases. The issue clearly states:\n\n1. The problem: When unapplying ContentType migration 0002 on a non-default database (using the --database flag), the ContentType rows have null values instead of their expected values.\n2. Steps to reproduce: Apply all contenttypes migrations to a non-default database, then roll back to migration 0001 using the command provided.\n3. Expected vs actual behavior: The ContentType rows should have their intended values, but they are null instead.\n\nWhat makes this not fully \"well-specified\" is that it doesn't provide complete context about:\n1. The exact Django version where this occurs\n2. The full database setup details\n3. What the \"legacy name field\" specifically refers to in the ContentType model\n4. The exact expected values that should be populated\n\nHowever, a developer familiar with Django's ContentType system would likely understand what's happening - when rolling back migrations, the system is failing to properly restore data in non-default databases. This is a specific enough issue that someone with Django knowledge could investigate and fix without needing much additional information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15481": {
        "explanation": "This issue is well-specified with a clear problem statement and expected behavior. The issue describes that:\n\n1. The `get_signed_cookie` method accepts a `timedelta` object for the `max_age` parameter\n2. The `set_cookie` and `set_signed_cookie` methods do not accept a `timedelta` object, throwing a TypeError\n3. There's a reference to a previous fix (#21363) that implemented similar functionality for TimestampSigner\n\nThe issue provides concrete examples showing both the working case (get_signed_cookie) and the failing case (set_cookie) with the exact error message. The solution would clearly involve modifying the `set_cookie` and `set_signed_cookie` methods to accept `timedelta` objects by converting them to seconds (integers) internally, similar to how `get_signed_cookie` already works.\n\nThe scope is well-defined, the expected behavior is clear, and there's even a reference to a similar fix that could guide the implementation. A developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15492": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: When using RemoteUserBackend, there's no easy way to sync changes from a remote system to Django user records during authentication.\n\n2. The proposed solution is specific:\n   - Add a new method called `synchronize_user` to the `django.contrib.auth.backends.RemoteUserBackend` class\n   - The method should have the same signature as `configure_user`\n   - It should be called in the `authenticate` method after fetching the user from the database\n   - It should be called regardless of whether the user was newly created or already existed\n\n3. The purpose is clear: Allow implementors to override this method to implement data synchronization between remote user data and Django user data on every authentication attempt.\n\nThe issue provides enough technical detail about where the change should be made, what it should do, and why it's needed. A developer familiar with Django's authentication system would have sufficient information to implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15498": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: Empty strings in the If-Modified-Since header now raise an exception\n2. When the problem started: Since commit d6aff369ad3\n3. The expected behavior: Empty strings should be ignored (as they were before)\n\nThe issue provides enough context to understand what needs to be fixed. A developer would need to:\n1. Look at commit d6aff369ad3 to understand what changed in the handling of If-Modified-Since headers\n2. Modify the code to restore the previous behavior of ignoring empty strings instead of raising exceptions\n\nWhile the issue doesn't provide specific code examples or error messages, the problem is narrow and focused enough that a developer familiar with the codebase should be able to identify and fix the issue without additional clarification. The commit reference provides a clear starting point for investigation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15499": {
        "explanation": "The issue is reasonably well-specified for someone familiar with Django's migration system. It describes a specific optimization opportunity: when a migration contains both a CreateModel operation and an AlterModelManagers operation, these can be combined into a single CreateModel operation (similar to how CreateModel + AlterModelOptions are already optimized).\n\nThe issue provides:\n1. A clear problem statement: There's an opportunity to optimize migrations by combining operations\n2. A specific example of what's already implemented (CreateModel + AlterModelOptions optimization)\n3. A clear request for what needs to be implemented (CreateModel + AlterModelManagers optimization)\n\nWhat's missing:\n1. Technical details about how the optimization should be implemented\n2. Specific code references or file locations\n3. Test cases or examples of the expected behavior\n\nHowever, for someone familiar with Django's migration system and the existing optimization for AlterModelOptions, these gaps could be filled by examining the codebase. The solution would likely follow a similar pattern to the existing optimization.\n\nTherefore, while not perfectly detailed, the issue provides enough information for an experienced Django developer to make a meaningful attempt at implementing the requested optimization.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15503": {
        "explanation": "This issue is well-specified with clear details about the problem. The description includes:\n\n1. A precise problem statement: JSONField's has_key lookup doesn't work with numeric keys on SQLite (and mentions MySQL and Oracle in the title)\n2. Version information for Django, Python, and SQLite\n3. Complete database configuration\n4. A minimal reproducible model definition\n5. A complete test case that demonstrates the issue\n6. The exact error output\n7. Additional information confirming it works on PostgreSQL but fails on SQLite\n\nThe issue provides everything needed to understand, reproduce, and potentially fix the problem. The root cause appears to be that the has_key lookup (and by extension has_keys and has_any_keys as mentioned in the title) doesn't properly handle numeric keys in JSON fields when using SQLite as the database backend. A developer would have enough information to investigate how Django's JSONField lookups are implemented for different database backends and fix the handling of numeric keys specifically for SQLite.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15521": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a specific bug: the `makemessages` command skips the `nl-nl-x-informal` folder.\n\n2. It explains the expected behavior: `DjangoTranslation` correctly translates `nl-nl-x-informal` to `nl_NL-x-informal` folder using the `to_locale` method.\n\n3. It describes the actual behavior: `makemessages` skips the folder and displays an error message suggesting `nl_NL_x_informal` instead.\n\n4. It provides the specific commit that introduced this behavior: https://github.com/django/django/commit/f63f3cdf0969c23fd0c05de0f4a2a1df0cd5112e\n\n5. It suggests a solution direction: \"The check for - in the locale should only be for the first section a.k.a. nl_NL\"\n\nWith this information, a developer can:\n- Understand the problem\n- Locate the relevant code (using the commit reference)\n- Understand what needs to be fixed (the validation logic for locale names)\n- Implement a solution that allows hyphens in the latter parts of the locale name\n\nThis is sufficient information to create a meaningful PR without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15525": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: loaddata fails on non-default database when natural keys use foreign keys\n2. The user provides a complete test case with:\n   - Clear error messages and stack trace\n   - Complete model definitions showing the relationship between Book and Author\n   - The exact JSON data being loaded\n   - The exact command that fails\n   - A link to a GitHub repository with a reproducible test case\n\n3. The expected behavior is clearly implied: loaddata should work the same way on non-default databases as it does on the default database when using natural keys with foreign key relationships.\n\n4. The user has identified the specific conditions where the bug occurs:\n   - Works in default database\n   - Fails in secondary database\n   - The problem doesn't appear if data is already present in the default DB\n\nThe issue contains all the necessary information to understand, reproduce, and potentially fix the problem. The error is specifically about Django's serializer not being able to find the Author object when building a Book instance using natural keys in a non-default database.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15526": {
        "explanation": "This issue is well-specified with clear details about the problem and solution:\n\n1. The problem is clearly identified: In Django's admin, the `view_on_site` redirect doesn't work correctly for custom admin sites because the `reverse` call in `get_view_on_site_url` is missing the `current_app` keyword argument.\n\n2. The impact is explained: When using custom admin sites, the \"view on site\" link incorrectly routes through the default admin site, which can cause access issues if the default admin has additional security protections.\n\n3. The solution is already identified: A patch has been created in PR #15526 to add the missing `current_app` parameter to the `reverse` call in `get_view_on_site_url`.\n\n4. The submitter even mentions they'll provide a regression test.\n\nThe issue provides enough context about the specific code location (contrib/admin/options.py), the function affected (get_view_on_site_url), the missing parameter (current_app), and links to a proposed solution. A developer could easily understand and implement the fix based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15554": {
        "explanation": "This issue is well-specified with clear details about the problem. The user is trying to use multiple FilteredRelation instances on the same relation but with different filters, and has discovered that Django only generates a single SQL JOIN for the first FilteredRelation while ignoring the second one.\n\nThe issue includes:\n1. A clear code example showing what they're trying to accomplish\n2. The specific Django feature being used (FilteredRelation)\n3. The expected behavior (two separate JOINs for the two different filtered relations)\n4. The actual behavior (only one JOIN is generated)\n5. Evidence of investigation (they inspected the raw SQL)\n6. A clear question about whether this is intended behavior or a bug\n\nThe issue provides enough context to understand the problem and attempt a solution. A developer would need to investigate Django's ORM implementation to determine if this is expected behavior or if there's a way to achieve what the user wants. The solution would either involve explaining why this is the expected behavior and suggesting an alternative approach, or fixing a bug in Django's ORM if this is indeed unintended behavior.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15560": {
        "explanation": "This issue is well-specified with a clear problem description. The issue explains:\n\n1. The exact scenario: Using a custom User model with a UniqueConstraint instead of unique=True\n2. The specific field affected: custom_username which is set as USERNAME_FIELD\n3. The command that fails: manage.py createsuperuser\n4. The exact error that occurs: IntegrityError when trying to create a user with a username that already exists\n5. The expected behavior (implied): The createsuperuser command should validate usernames that use a UniqueConstraint, just as it would for fields with unique=True\n\nThe solution would need to modify the createsuperuser command to check for uniqueness using the UniqueConstraint before attempting to create the user, similar to how it handles fields with unique=True.\n\nThe issue provides enough code context and a specific error message that would allow a developer to reproduce the problem and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15561": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem: when adding or changing choices in a field on SQLite, Django generates unnecessary SQL operations (creating a new table, inserting data, dropping the old table, and renaming the new one), whereas on PostgreSQL, the same migration generates no SQL.\n\nThe issue refers to a related issue (#33470) which provides context, and it clearly identifies the expected behavior: adding/changing choices should be a no-operation (noop) on SQLite, just as it is on PostgreSQL.\n\nWhile the issue doesn't provide code examples or the exact implementation details needed, an experienced Django developer familiar with the migration system would understand:\n1. The problem is in the SQLite backend's handling of AlterField operations\n2. The specific case is when only the 'choices' attribute is being modified\n3. The expected behavior is that no SQL should be generated in this case\n\nThe solution would involve modifying Django's migration system to detect when an AlterField operation is only changing choices on SQLite and skip generating the SQL operations in that case.\n\nThere are some details that would be helpful but aren't provided (like specific code examples or test cases), but there's enough information to understand the problem and start working on a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15563": {
        "explanation": "This issue is well-specified with a clear description of the problem and a reproducible example. The issue describes a bug in Django's queryset update functionality when dealing with multiple inheritance. \n\nThe problem is clearly demonstrated:\n1. When a model (Child) inherits from multiple base models (Base and OtherBase)\n2. Attempting to update fields from the second parent (OtherBase) via Child.objects.update()\n3. The update incorrectly modifies instances of the second parent class (OtherBase) rather than the Child instances\n4. This is shown through SQL logs and verification queries\n\nThe issue provides:\n- Complete model definitions\n- Step-by-step shell commands showing the issue\n- SQL queries that are executed\n- Verification queries showing the incorrect behavior\n\nA developer would have all the information needed to reproduce the issue and understand what the correct behavior should be: updates to Child.objects should modify the Child instances, not unrelated OtherBase instances. The solution would involve fixing how Django handles queryset updates in multiple inheritance scenarios.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15569": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed:\n\n1. The problem is clearly identified: `_unregister_lookup()` method doesn't clear the lookup cache, while `register_lookup()` does.\n2. The exact file and line numbers are provided: https://github.com/django/django/blame/main/django/db/models/query_utils.py#L212\n3. The solution is clearly described: add code to clear the cache in `_unregister_lookup()` similar to what's done in `register_lookup()` at line 202\n4. The issue even mentions that corresponding changes will be needed in a specific test: `schema.tests.SchemaTests.test_func_unique_constraint_lookups`\n5. A PR has already been created (PR #15569) which can serve as a reference\n\nWith this information, a developer would have a clear understanding of what needs to be fixed, where to find the relevant code, and what other parts of the codebase might be affected by the change.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15572": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific bug: Django 3.2.4+ autoreload breaks when there's an empty string in TEMPLATES DIRS.\n2. It explains the root cause: Django changed how template dirs are normalized using pathlib.Path, which transforms empty strings into the project root.\n3. It provides a specific code example showing the problematic pattern and how to fix it.\n4. It explains the technical mechanism of the bug: template_changed() always returns True when an empty string is in DIRS, preventing autoreload from working.\n5. It references the specific change that produced the regression with links to the relevant ticket and commits.\n6. It links to previous related reports.\n\nThe issue provides enough context and technical details for a developer to understand the problem and implement a solution. The fix would likely involve modifying the template_changed function in django/template/autoreload.py to properly handle empty strings in TEMPLATES DIRS.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15576": {
        "explanation": "This issue is reasonably well-specified. The description clearly identifies:\n\n1. The problem: A performance inefficiency in `.exists()` on distinct querysets where all fields are being selected unnecessarily\n2. The specific commit that introduced the issue (93cc6dcdac6fc3e506640fa38dd1798c3cd61cff)\n3. The bug number (#18414) related to the original fix\n4. A proposed solution: Only select all fields if the queryset is distinct AND has a slice applied (high_mark or low_mark is not None)\n\nThe developer has even identified that the change would be a \"one-line-change\" patch.\n\nThe only ambiguity is around testing - the developer is asking for guidance on how to test this change without inspecting the query directly, which they consider brittle. However, this doesn't affect understanding what needs to be fixed - it's more about how to verify the fix works correctly.\n\nA developer with access to the codebase could locate the relevant code based on the commit hash and bug number, understand the current implementation, and implement the suggested change. The testing approach could be determined after examining how similar functionality is tested in the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15586": {
        "explanation": "The issue description is somewhat vague and lacks specific details about implementation requirements. Here's what we can understand:\n\n1. There was a previous change (#25791) that allowed auto-reloading of cached templates.\n2. There was a discussion on PR 15140 about enabling the cached template loader by default in development environments.\n3. The implication is that users who don't want this behavior would need to explicitly specify different loaders.\n\nHowever, the issue lacks several important details:\n- It doesn't specify which codebase this is for (though context suggests it's likely Django)\n- It doesn't provide clear implementation requirements or acceptance criteria\n- It doesn't explain the current behavior in detail\n- It doesn't specify which files need to be modified\n- It doesn't mention any potential side effects or considerations\n\nWithout these details, a developer would need to:\n1. Research the referenced PRs and issues to understand the context\n2. Make assumptions about the implementation details\n3. Determine which files to modify based on their understanding of the codebase\n\nWhile a developer familiar with the codebase might be able to make progress on this issue, it requires significant background knowledge and research to fill in the gaps.",
        "label": "VAGUE"
    },
    "django__django-15607": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual behavior. The problem involves an unnecessary redirect in Django's LogoutView when a \"next\" parameter with an unsafe URL is provided. \n\nThe issue description provides:\n1. Detailed reproduction steps (setting LOGOUT_REDIRECT_URL, configuring URLs, adding a specific form)\n2. Expected behavior (next parameter ignored, redirect to LOGOUT_REDIRECT_URL)\n3. Actual behavior (an intermediary redirect occurs, with server logs showing the extra redirect)\n4. The root cause (identified in the LogoutView.get_next_page method)\n5. A link to the specific code in GitHub that's causing the issue\n6. Clarification that the end-user experience is correct, but there's an unnecessary redirect and code complexity\n\nA developer would have all the information needed to understand, reproduce, and fix this issue. The solution would involve modifying the LogoutView.get_next_page method to avoid the unnecessary redirect when an unsafe URL is provided in the \"next\" parameter.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15620": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. The current behavior: When a field migration is a no-op, the operation description appears in SQL comments but without any actual SQL statements, which can be confusing.\n\n2. The desired behavior: Add an explicit SQL comment like \"-- (no-op)\" or \"-- no SQL to execute\" to clearly indicate when an operation deliberately has no SQL statements.\n\n3. Examples of both the current output and the desired output, which clearly illustrate the problem and the proposed solution.\n\n4. The rationale for the change: To improve clarity in migration scripts, especially when there are consecutive no-op operations.\n\nThe issue provides enough context for a developer to understand what needs to be changed, where the change should be made (in the SQL migration generation code), and what the expected output should look like. The solution would involve finding where migration operation headers are generated and adding logic to output an additional comment when no SQL statements are generated for an operation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15629": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The issue describes a specific bug in Django's migration system where setting `db_collation` on a primary key field doesn't propagate that collation to foreign key fields that reference it, causing MySQL foreign key constraint errors.\n\nThe description includes:\n1. Clear example models showing the problematic code structure\n2. The actual SQL generated by Django's migration system\n3. The corrected SQL that would fix the issue\n4. A clear explanation of why the error occurs: \"The collation of the pk must match the collation of the fk otherwise an error will occur\"\n\nThe issue is specific to MySQL databases and occurs when using `db_collation` with primary keys that are referenced by foreign keys in other models. The solution would involve modifying Django's migration system to propagate the collation setting from primary keys to their corresponding foreign keys.\n\nA developer could implement a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15643": {
        "explanation": "This issue is not well-specified enough for a meaningful attempt at a solution. The description is quite vague and lacks critical details:\n\n1. It doesn't provide any code examples showing the problem\n2. It doesn't explain what the expected behavior is versus the actual behavior\n3. It doesn't specify what \"altered\" means in this context - is it renaming the primary key, changing its type, or something else?\n4. It doesn't provide steps to reproduce the issue\n5. It doesn't mention which version of the software this occurs in\n6. It doesn't explain the context of \"migrations\" - is this Django migrations, some other framework, or a custom system?\n\nWithout these details, it would be difficult to understand the exact nature of the problem, reproduce it, and implement a proper solution. A developer would need to ask for clarification about the specific scenario, expected behavior, and steps to reproduce before being able to work on this issue effectively.",
        "label": "VAGUE"
    },
    "django__django-15648": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly stated: decorators on methods called by `__get_dynamic_attr` cause a TypeError because of argument counting.\n\n2. The specific error is provided: \"TypeError: item_link() takes exactly 2 arguments (1 given)\"\n\n3. The root cause is identified: `__get_dynamic_attr` checks `code.co_argcount` to determine how to call the method, but decorators with `*args, **kwargs` break this logic.\n\n4. The relevant code snippet is included, showing the problematic conditional logic.\n\n5. A potential solution approach is suggested: remove one of the two methods and standardize on either `attr(item)` or `attr()`.\n\nWith this information, a developer could locate the `__get_dynamic_attr` method in the codebase, understand how it interacts with decorated methods, and implement a fix that addresses the argument counting issue when decorators are used. The issue provides enough context to reproduce the problem and points toward a reasonable solution path.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15651": {
        "explanation": "This issue is well-specified with clear requirements for implementing a RenameIndex operation in a backwards compatible way. The description provides:\n\n1. A clear purpose: To support deprecation of index_together by handling unnamed indexes in old projects\n2. Detailed API specification: RenameIndex(model, new_name, old_name=None, old_fields=None) with parameter requirements\n3. Implementation requirements:\n   - Use RENAME INDEX when old_name is given and the feature is available\n   - Alternative approach for databases without RENAME INDEX support\n   - Error handling for ambiguous cases\n   - Autodetector behavior\n   - Backwards operation behavior\n\nThe issue provides sufficient technical details about the expected functionality, edge cases to handle, and how the feature should behave across different database systems. A developer would have enough information to implement this feature without needing additional clarification.\n\nThe only minor detail that might need interpretation is the exact SQL query for looking up index names from information_schema, but this is a standard database operation that a developer working on this codebase would be expected to know how to implement.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15669": {
        "explanation": "This issue is well-specified. The request is to add a `--update` flag to Django's `makemigrations` management command that would mimic the functionality of South's `--update` flag for its `schemamigration` command. \n\nThe issue clearly:\n1. Identifies what feature is needed (an `--update` flag for `makemigrations`)\n2. Provides context by referencing the equivalent feature in South (a migration framework that Django replaced)\n3. Includes a link to South's documentation explaining how the feature worked\n4. Explains the use case (refining the latest migration during iterative development)\n\nA developer familiar with Django's migration system would understand what needs to be implemented - a flag that allows updating/replacing the most recent migration file rather than creating a new one. This is a common workflow need when iteratively developing database schema changes.\n\nThe implementation would involve modifying Django's migration commands to detect when the flag is used and update the most recent migration file instead of creating a new one, similar to how South's implementation worked.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15671": {
        "explanation": "The issue is reasonably well-specified. It describes a clear enhancement request to allow overriding FormSet's error messages for \"too few\" and \"too many\" forms by moving these messages to `default_error_messages`. This would enable users to override them via the `error_messages` argument when instantiating a FormSet.\n\nThe issue provides:\n1. A clear goal: Allow overriding FormSet's error messages for specific validation cases\n2. A specific implementation approach: Move messages to `default_error_messages`\n3. The expected outcome: Users can override messages via the `error_messages` argument\n\nWhile the issue doesn't provide the exact code that needs to be modified or the specific error messages that need to be moved, a developer familiar with Django's FormSet implementation would be able to locate these messages in the codebase and implement the requested change. The issue also mentions that extending this functionality to `inlineformset_factory` is out of scope for this particular issue.\n\nA developer would need to:\n1. Find where the \"too few\" and \"too many\" form error messages are defined\n2. Move them to the `default_error_messages` dictionary\n3. Ensure the validation code uses these messages from the dictionary\n4. Verify that they can be overridden via the `error_messages` parameter\n\nThis is a focused, achievable task with a clear success criterion.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15678": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The task is to deprecate the CryptPasswordHasher class in Django.\n2. The rationale is clearly explained: it's an old component (15 years), poorly documented, not recommended, UNIX-only, and the underlying Python crypt module was deprecated in Python 3.11.\n3. There's a clear timeline: deprecate in Django 4.1 and remove in Django 5.0.\n\nThe implementation would involve:\n1. Adding deprecation warnings to the CryptPasswordHasher class\n2. Updating documentation to indicate the deprecation\n3. Following Django's standard deprecation policy\n\nThis is a straightforward deprecation task with all the necessary context provided. A developer familiar with Django's codebase and deprecation process would have all the information needed to implement this change correctly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15682": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. A specific problem with the current implementation of OrderBy's nulls_first and nulls_last parameters, where setting nulls_first=False doesn't behave as expected (it still puts nulls first).\n\n2. The cause of the problem: both parameters default to False, and when both are False, the database decides the ordering.\n\n3. A clear example demonstrating the unexpected behavior with code snippets and their outputs.\n\n4. A specific proposed solution: change the default values of these parameters from False to None, let the database decide when both are None, and respect the explicit setting when one is not None.\n\nThe issue provides enough context and detail for a developer to understand the problem, locate the relevant code, and implement the suggested change. The solution would involve modifying the default parameter values in the OrderBy class and updating the logic to handle the new default values appropriately.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15689": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly identified: APPEND_SLASH setting adds significant latency to all requests not ending in \"/\" due to a change in Django 1.9.5/1.10 (commit 9390da7fb6e251eaa9a785692f987296cb14523f).\n\n2. The specific technical cause is explained: The implementation was changed to check should_redirect_with_slash() on every request rather than only on 404 responses, causing an extra urlconf lookup for every request not ending with \"/\".\n\n3. Evidence is provided: The issue includes benchmark results showing the performance impact (42.56 vs 71.32 requests/sec) with a minimal test project.\n\n4. The expected solution is implied: Modify the implementation to avoid the unnecessary urlconf lookup for successful requests, returning to the original performance-conscious behavior.\n\nA developer could easily reproduce this issue using the provided test project and implement a solution based on the clear technical explanation of what changed and why it's problematic.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15695": {
        "explanation": "This issue is reasonably well-specified. It describes a specific bug in Django's RenameIndex() operation when dealing with unnamed indexes that are moved backward and then forward again. The issue includes:\n\n1. A clear description of the problem: RenameIndex() crashes when an unnamed index is moved backward and then forward again\n2. The specific error message: \"django.db.utils.ProgrammingError: relation 'new_pony_test_idx' already exists\"\n3. A code snippet showing where to add a test that reproduces the issue\n4. The context that this happens specifically on PostgreSQL\n\nWhat makes this not fully \"well-specified\" is:\n- It doesn't explicitly explain the root cause of the issue (though it hints that it's related to not properly restoring auto-generated names)\n- It doesn't provide a complete reproduction case, just a test snippet to add\n- It doesn't specify which file(s) need to be modified to fix the issue\n\nHowever, an experienced Django developer would likely be able to:\n1. Add the test code to reproduce the issue\n2. Debug the RenameIndex operation to understand why it's not handling the backward/forward case correctly\n3. Fix the implementation to properly track and restore auto-generated names\n\nThe issue provides enough information to start working on a solution without requiring additional clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15698": {
        "explanation": "This issue is well-specified with clear information about the problem and a proposed solution:\n\n1. The issue describes a specific bug: templates crash when calling methods for built-in types in Django 3.2, whereas in Django 2.2 they would return empty strings.\n\n2. The reporter provides a clear reproduction case:\n   - A template with `{{ foo.count }}` where foo is non-existent\n   - An include statement passing this non-existent variable to another template\n   - The exact error message and stack trace\n\n3. The root cause is identified: when trying to call a method on a non-existent variable that would be a built-in method (like str.count), Django attempts to inspect the signature of the method but fails with a ValueError.\n\n4. A specific fix is proposed with a code diff that catches the ValueError exception when trying to get the signature of built-in methods.\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and implement a solution. The proposed fix is also reasonable and addresses the identified issue by handling the case where inspect.signature() raises a ValueError for built-in methods.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15703": {
        "explanation": "The issue is reasonably well-specified. It clearly states the goal: to deprecate the `Model.Meta.index_together` option in favor of the more flexible `Model.Meta.indexes` option with the `Index` class. \n\nThe rationale is provided: anything that can be done with `index_together` can be accomplished using `indexes` and the `Index` class, making `index_together` redundant.\n\nWhat's missing from the issue description are:\n1. Specific implementation details about how the deprecation should be handled\n2. Timeline for deprecation (when it should be marked deprecated, when it should be removed)\n3. Whether backward compatibility needs to be maintained\n4. Whether documentation needs to be updated to reflect this change\n\nHowever, these are standard considerations for any deprecation process in a software project, and an experienced developer would know to address these aspects. The core requirement - deprecating `index_together` in favor of `indexes` - is clear enough to begin implementation work.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15731": {
        "explanation": "This issue is well-specified with clear information about:\n\n1. The problem: `inspect.signature()` returns incorrect signature information for Django queryset methods, showing only `(*args, **kwargs)` instead of the actual parameters.\n\n2. A reproducible example: The issue provides code that demonstrates the problem using `Person.objects.bulk_create`.\n\n3. The root cause: The issue identifies the specific location in Django's codebase (manager.py line 84) where the problem occurs - the decorator only copies the name and docstring but not the complete function metadata.\n\n4. A proposed solution: Use `functools.wraps` instead of manually assigning name and docstring to properly preserve all function metadata.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The fix appears straightforward - replacing manual metadata copying with `functools.wraps` in the identified location.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15732": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user has a model with a primary key field 'id' that already has an implicit unique constraint\n2. They also have an erroneous explicit unique_together constraint on just that single field: unique_together = (('id',),)\n3. When trying to create a migration to drop this redundant constraint, the migration fails\n4. The error appears to be because Django's migration system expects only one unique constraint on the column but finds two (the PK constraint and the unique_together constraint)\n5. The database being used is PostgreSQL\n\nThe issue provides specific details about the database schema showing both constraints:\n- \"foo_bar_pkey\" PRIMARY KEY, btree (id)\n- \"foo_bar_id_1c3b3088c74c3b17_uniq\" UNIQUE CONSTRAINT, btree (id)\n\nThis is a specific Django ORM/migration issue with clear technical details that would allow a developer to reproduce the problem and work on a solution. The solution would involve finding a way to properly drop the redundant unique_together constraint without affecting the primary key constraint.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15737": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly defined: When saving a child object with a reference to an unsaved parent object, Django clears the cached reference to the parent after the parent is saved, causing unnecessary database queries when the parent is accessed later.\n\n2. The issue provides a concrete example with code that demonstrates the problem.\n\n3. The issue explains the underlying cause: In Model._prepare_related_fields_for_save(), setting the foreign key ID clears the cached reference to the parent object.\n\n4. A workaround is provided (child.parent = child.parent), which helps validate the issue.\n\n5. A specific solution is proposed: Replace setattr(self, field.attname, obj.pk) with setattr(self, field.name, obj) in Model._prepare_related_fields_for_save().\n\nThe only minor limitation is that the proposed solution hasn't been tested, but this is explicitly noted. The issue provides enough information for a developer to understand the problem, reproduce it, and implement a solution. The specific Django internals that need modification are identified.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15738": {
        "explanation": "This issue is well-specified with clear details about the problem and what the user is trying to accomplish. The user wants to change a ForeignKey field to a ManyToManyField in their Django model while also removing a unique_together constraint that can't be applied to a ManyToManyField.\n\nThe issue includes:\n1. Complete model definitions showing the current state\n2. The desired end state of the models\n3. The specific error message encountered\n4. The current workaround (using two separate migrations)\n5. The goal (to accomplish this in a single migration)\n\nThe user has even provided a reproducible example as an attachment. The problem is a common Django migration challenge when changing field relationships while maintaining database integrity.\n\nThe issue is asking for a way to perform this change in a single migration rather than the two-step process they're currently using. This is a clear, technical request with sufficient context to understand and attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15741": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies:\n\n1. The specific function with the problem: `django.utils.formats.get_format`\n2. The exact error message: `TypeError: getattr(): attribute name must be string`\n3. The context where it occurs: when using the date template filter with a lazy string like `some_date|date:_('Y-m-d')`\n4. The regression was introduced by a specific commit: `659d2421c7adb` (fixing issue #20296)\n\nFrom this information, I can understand that:\n- The `get_format` function doesn't properly handle lazy strings as parameters\n- This is causing template filters like `date` to fail when used with lazy strings (which are commonly used for internationalization)\n- The solution would involve modifying `django.utils.formats.get_format` to properly handle lazy string parameters\n\nThe only minor detail missing is the exact implementation of the fix, but that's expected in an issue description. The problem is clear enough that a developer familiar with Django could investigate the code and implement a solution that allows lazy parameters to work with the `get_format` function.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15742": {
        "explanation": "This issue is well-specified with a clear problem description and reproduction steps. The issue describes a bug in Django's `blocktranslate` template tag when used with the `asvar` parameter. The problem is that variables within the translation block are escaped, but the result is stored as a regular string (not a SafeString), causing double escaping when the stored variable is used in the template.\n\nThe issue includes:\n1. A clear description of the problem\n2. A reference to the documentation\n3. A minimal code example to reproduce the issue\n4. An assertion that demonstrates the bug\n5. Two potential solutions (either make the result a SafeString or don't escape variables within the block)\n\nThe expected behavior is clearly stated: prevent double escaping of HTML special characters. The reproduction code is complete and can be used to verify the issue and test a solution. The suggested solutions provide a good starting point for fixing the issue.\n\nA developer with access to the Django codebase would be able to locate the `blocktranslate` implementation, understand how it handles variable escaping, and implement one of the suggested solutions without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15744": {
        "explanation": "The issue description identifies a specific problem with a clear reference to a pull request (PR #15675) that introduces a bug. The bug is described as \"I/O operation on closed file\" which occurs because the ASGI request body is being closed before getting the response, specifically when accessing `request.body` in a view.\n\nThe issue provides enough context to understand:\n1. The specific component affected (ASGI handler)\n2. The nature of the bug (premature closing of request body)\n3. When it occurs (when accessing request.body in a view)\n4. A reference to the PR that introduced the issue\n\nWhile the description is brief, it points to a specific PR which would allow a developer to examine the code changes that introduced the bug. This provides sufficient information to investigate the root cause and develop a fix. The issue is technical but clearly defined for someone familiar with Django's ASGI implementation.\n\nA developer would need to:\n1. Review PR #15675 to understand the changes that introduced the bug\n2. Identify where in the ASGI handler the request body is being closed prematurely\n3. Modify the code to ensure the request body remains open until after the response is generated\n\nThis is a reasonably well-specified issue for an experienced Django developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15747": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific error scenario: When performing a QuerySet.update() operation on a queryset that is ordered by an annotated field, a FieldError is raised.\n\n2. The user provides complete code examples showing:\n   - Their admin model implementation\n   - The get_queryset method with annotations\n   - The custom action method that encounters the error\n   - The error handling they implemented as a workaround\n\n3. The expected behavior is clearly stated: The update operation should work regardless of how the queryset is sorted (by core model fields or annotated fields).\n\n4. The current behavior is clearly described: The update works when sorted by core model fields but fails with a FieldError when sorted by annotated fields.\n\n5. The error message is provided: \"Cannot resolve keyword 'field_that_data_is_currently_sorted_by' into field.\"\n\nThis is a specific Django ORM issue with a reproducible scenario. A developer could understand the problem, reproduce it, and attempt to fix it based on the information provided. The issue appears to be that when a queryset is ordered by an annotated field and then update() is called, Django is incorrectly trying to use the annotated field in the UPDATE SQL statement.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15752": {
        "explanation": "The issue is reasonably specified. It clearly states that the function `make_random_password()` should be deprecated, and provides context that it's unused since a specific commit (fcd837cd0f9b2c706bc49af509628778d442bb3f). It also references a comment by Carlton for additional context.\n\nWhile the issue doesn't explicitly outline the steps for deprecation (like adding deprecation warnings, documentation updates, etc.), the task itself is straightforward for an experienced developer familiar with standard deprecation practices. The commit reference provides a clear point to investigate when the function became unused.\n\nA developer would need to:\n1. Locate the `make_random_password()` function in the codebase\n2. Add appropriate deprecation warnings/notices\n3. Update documentation to indicate the function is deprecated\n4. Possibly add information about alternatives if they exist\n\nThese are standard steps for deprecation that don't need explicit specification for an experienced developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15766": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The author:\n\n1. Clearly identifies the problem: on_commit handlers don't execute after one handler raises an exception, which can cause important operations to be skipped.\n\n2. Provides context by comparing to Django's signal system which has a `send_robust` option.\n\n3. Gives concrete examples of the desired functionality with code snippets showing how the proposed solution would work.\n\n4. Proposes a specific implementation approach: adding a `robust` keyword argument to `transaction.on_commit()` that defaults to `False` for backward compatibility.\n\nThe issue provides enough information for a developer to understand what needs to be implemented, why it's needed, and how it should behave. The solution would involve modifying the transaction management code to handle exceptions in on_commit handlers differently when the robust flag is set.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15774": {
        "explanation": "This issue is well-specified with clear details about the problem and the required solution. The issue describes:\n\n1. The specific problem: Django processes HTTP Accept-Language headers in a case-sensitive manner, which contradicts standards (w3c, RFC2616, BCP47) that specify language tags should be parsed case-insensitively.\n\n2. A concrete example demonstrating the issue: Chrome's \"zh-TW\" works correctly while Firefox's \"zh-tw\" doesn't because of the lowercase \"tw\".\n\n3. The specific components that need fixing:\n   - Code changes: Make the language tag parsing case-insensitive, specifically mentioning the `parse_accept_lang_header()` function\n   - Documentation updates: Correct the documentation that incorrectly states language and country parts are always lowercase\n\nThe issue provides sufficient context, references to standards, specific examples of the problem, and clear direction on what needs to be fixed. A developer could implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15781": {
        "explanation": "This issue is well-specified. The problem is clearly described: when multi-line help text is provided for a Django management command, the formatting is lost in the output. The issue provides:\n\n1. A clear example of the code being used (defining a Command class with multi-line help text)\n2. The current output that shows the problem (help text being displayed without proper line breaks)\n3. The expected output (help text with proper formatting preserved)\n\nThe issue is about implementing customizable formatters for Django management commands so that multi-line help text is displayed with the intended formatting. The solution would involve modifying how Django's command system processes and displays help text to preserve the original formatting.\n\nThe requirements are clear, and a developer would be able to understand what needs to be fixed without additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15799": {
        "explanation": "This issue is reasonably well-specified. The reporter is describing a bug where help text for a SelectMultiple widget is being displayed even when the allow_multiple_selected property is set to False. \n\nThe issue includes:\n1. Clear identification of the problem: SelectMultiple widget in ModelAdminForm displays help text when allow_multiple_selected is False\n2. Specific code references: Points to django.forms.widgets rows 684-685 and django.contrib.admin.options.py rows 280-281\n3. The current behavior: The help text is displayed based only on isinstance checks, not considering the allow_multiple_selected property\n\nWhat's missing but can be reasonably inferred:\n1. The expected behavior: The help text should only be displayed when allow_multiple_selected is True\n2. The exact help text message that's being displayed incorrectly\n3. A complete reproduction case\n\nDespite these minor gaps, there's enough information to understand the issue and implement a solution - likely by modifying the condition in django.contrib.admin.options.py to also check the allow_multiple_selected property before displaying the help text.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15814": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific error that occurs when using `select_related()` followed by `only()` on proxy models in Django.\n2. The complete error traceback is provided, showing exactly where the error occurs: `ValueError: 'id' is not in list`.\n3. The relevant model definitions are included, clearly showing the proxy model relationship.\n4. A minimal reproduction case is provided via a Django management command.\n5. The environment details are specified (Windows 10, Python 3.10, Django 4.0.5).\n6. The issue reporter even identified the problematic code in Django's codebase (django/db/models/sql/query.py line 745) and suggested a potential fix.\n\nThe issue is describing a bug in Django's ORM where it doesn't properly handle the case when `only()` is used after `select_related()` specifically with proxy models. The error occurs because the code is trying to find the primary key in a list where it doesn't exist due to how proxy models are handled.\n\nA developer would have all the necessary information to reproduce the issue and implement a fix based on the suggested approach or an alternative solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15819": {
        "explanation": "The issue is reasonably well-specified. It describes a concrete problem with Django's `inspectdb` command, which is used to generate Django models from an existing database schema. \n\nThe problem is that when `inspectdb` generates models with multiple foreign key relationships to the same model, it doesn't automatically add `related_name` attributes to these fields. This causes Django to raise an error (fields.E304) about clashing reverse accessors.\n\nThe proposed solution is clear: modify the `inspectdb` command to automatically generate unique `related_name` attributes for fields when multiple relationships to the same model are detected.\n\nWhile the issue doesn't provide specific implementation details or test cases, the Django framework has well-established patterns for handling model relationships, and the `inspectdb` command's code would be accessible in the codebase. A developer familiar with Django would understand what needs to be done.\n\nThe only minor ambiguity is in the suggested solution - \"related_name='attribute_name'\" - it's not entirely clear what \"attribute_name\" should be, but a reasonable interpretation would be to use the field name itself as part of the related_name to ensure uniqueness.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15828": {
        "explanation": "The issue description is quite minimal and lacks specific details about the problem. It mentions that there's a regression in BaseConstraint.deconstruct() and __eq__ operators related to violation_error_message, but doesn't provide:\n\n1. What exactly is not working correctly\n2. What the expected behavior should be\n3. Any code examples demonstrating the issue\n4. Steps to reproduce the problem\n5. The impact of this issue\n\nThe only context provided is that it's a regression introduced in a specific commit (667105877e6723c6985399803a364848891513cc). While an experienced developer could look at this commit to understand what changed and infer the issue, the description itself doesn't provide enough information to understand the problem scope or requirements for a solution without additional investigation.\n\nA developer would need to:\n1. Examine the referenced commit\n2. Understand how BaseConstraint.deconstruct() and __eq__ should handle violation_error_message\n3. Determine how to fix the regression\n\nThis makes the issue vague as presented, requiring significant additional context gathering before implementation.",
        "label": "VAGUE"
    },
    "django__django-15851": {
        "explanation": "This issue is well-specified. It clearly identifies a problem with the Django `dbshell` command when used with PostgreSQL: when additional parameters are passed to the PostgreSQL client (psql), they are being ignored because they are placed after the database name in the command line arguments. \n\nThe issue provides:\n1. A clear example of the problem (`./manage.py dbshell -- -c \"select * from some_table;\"`)\n2. The actual output showing the error messages from psql\n3. A specific explanation of what's happening: psql expects all options to precede the database name\n4. A suggested solution: construct the args list in the proper order, with the database name at the end\n\nA developer would be able to locate the relevant code in Django that constructs the command-line arguments for PostgreSQL's psql client and modify it to ensure that any additional parameters are placed before the database name. This is a straightforward fix with a clear success criterion: the command shown in the example should work without the warnings and execute the SQL query as expected.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15863": {
        "explanation": "This issue is well-specified with a clear problem description and reproduction steps. The user has:\n\n1. Clearly identified the bug: the `floatformat` template filter drops precision when used with Decimal numbers\n2. Provided a minimal working example (MWE) that demonstrates the issue\n3. Included the expected vs actual output (showing precision loss)\n4. Identified the root cause: incorrect conversion to float within the floatformat implementation\n5. Even mentioned they've prepared a pull request to fix the issue\n\nThe problem is technically well-defined - when Decimal values with high precision are passed through the floatformat filter, precision is lost because the implementation converts Decimal to float internally. This is a specific, reproducible issue with a clear path to resolution (avoid the float conversion for Decimal inputs).\n\nA developer could take this information and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15869": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: The `length_is` template filter is outdated and less flexible than using the `length` filter with comparison operators in `{% if %}` tags.\n\n2. The proposed solution: Deprecate the `length_is` filter in favor of using the `length` filter with comparison operators.\n\n3. Examples showing both the current approach and the recommended approach, which makes it clear what changes are needed.\n\nThe task would involve:\n- Marking the `length_is` filter as deprecated in the codebase\n- Updating documentation to recommend using `length` with comparison operators instead\n- Potentially adding a deprecation warning when `length_is` is used\n\nThis is a straightforward deprecation task with clear reasoning and examples, making it well-specified for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15902": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that they're receiving template deprecation warnings related to \"default.html\" when using Django's ManagementForm, even though they never explicitly render forms using the {{ form }} expression. They've identified that the warning occurs during formset management form production, where the template choice doesn't matter since only hidden inputs are generated.\n\nThe issue provides:\n1. Clear context about what's happening (deprecation warnings for ManagementForm)\n2. The specific scenario (formset management form production)\n3. The user's understanding of why this might be unnecessary (only hidden inputs are produced)\n4. A clear question about whether this case should be special-cased to avoid the warning\n\nWhat's missing is:\n1. The exact deprecation warning text\n2. The Django version being used\n3. Code examples showing their implementation\n4. The specific templates involved\n\nHowever, for someone familiar with Django's form system, there's enough information to understand the issue and investigate a solution. The request is to determine if ManagementForm rendering should be exempted from the template deprecation warning since the template choice is irrelevant in this case.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15916": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly describes the current behavior: when using `modelform_factory` with a form that has a `formfield_callback` defined, the callback is not being used in the generated form.\n\n2. It provides a complete code example that demonstrates the issue, showing both the model definition and form class with the callback.\n\n3. It explains the expected behavior: that the `formfield_callback` specified in the Meta class of the form should be used when creating a new form via `modelform_factory`.\n\n4. It references a previous related ticket (#18573) and explains why a previous test for this functionality was incorrect.\n\n5. The issue is specific to a particular Django function (`modelform_factory`) and how it handles the `formfield_callback` parameter.\n\nA developer could reproduce this issue using the provided example and understand what needs to be fixed: the `modelform_factory` function should respect the `formfield_callback` defined in the Meta class of the form passed to it, rather than overwriting it with the default value of None.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15925": {
        "explanation": "This issue is well-specified with clear reproduction steps and error details. The reporter has:\n\n1. Provided a specific error message and stack trace\n2. Included detailed reproduction steps (both automated via Makefile and manual steps)\n3. Identified the exact conditions that trigger the bug (removing a field with db_index=True in SQLite 3.35.5+)\n4. Mentioned that the issue occurs when upgrading from Django 4.0.7 to 4.1\n5. Specified which field types are affected (TextField, IntegerField, SlugField, CharField, URLField)\n6. Noted that setting db_index=False avoids the error\n7. Mentioned they've attached a minimal example (though we don't have access to it)\n\nThe issue is clearly a bug in Django's migration system when removing indexed fields in SQLite 3.35.5+. A developer would have enough information to reproduce the issue and work on a fix by examining how Django handles index removal during migrations in SQLite.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15930": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The exact code that's causing the error is provided\n2. The specific error message is included\n3. The generated SQL that's failing is shown\n4. The expected behavior is clearly stated\n\nThe issue describes a bug where using `~Q(pk__in=[])` in a Django `Case` statement causes a SQL syntax error. The error occurs because the SQL being generated is missing a condition before the `THEN` keyword. This is a specific, reproducible issue with Django's ORM that can be investigated and fixed.\n\nThe context about `~Q(pk__in=[])` being a sentinel value in application code provides useful background on why this edge case matters.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Understand what's happening (the SQL generation is incorrect)\n3. Identify where in Django's ORM code the problem might be occurring\n4. Develop a fix that properly handles this edge case\n\nNo additional information is needed to begin working on a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15957": {
        "explanation": "This issue is well-specified with a clear problem statement and use case. The issue describes that Django's `Prefetch()` objects do not work with sliced querysets, providing a specific code example that results in an `AssertionError`. The reporter also explains their use case: displaying a list of categories with a few example objects from each category, which is a common pattern in web applications.\n\nThe issue identifies:\n1. The exact problem: `Prefetch()` objects don't work with sliced querysets\n2. A concrete code example showing the error\n3. The error message received: `AssertionError: Cannot filter a query once a slice has been taken`\n4. A reference to external documentation (StackOverflow) confirming the issue\n5. The use case and why this functionality would be valuable\n6. The current limitation (having to prefetch all objects instead of just a few)\n\nA developer would have enough information to:\n- Reproduce the issue\n- Understand the expected behavior\n- Investigate why sliced querysets are not supported in `Prefetch()`\n- Implement a solution that allows for efficient prefetching of a limited number of related objects\n\nThe issue is specific to Django's ORM functionality and provides all necessary context for a developer to work on a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15969": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user has provided their exact model configuration (Parent model with Child model having a foreign key with on_delete=SET_NULL)\n2. They've shared the specific performance issue (timeouts when deleting a parent with many children)\n3. They've included the actual SQL queries being generated that show the problem\n4. They've identified the specific unexpected behavior: Django is using \"WHERE id IN (list of all child IDs)\" instead of \"WHERE parent_id = 'parent123'\" for the SET_NULL operation\n5. They've even mentioned their workaround (switching to CASCADE)\n\nThe issue is asking for an explanation of why Django generates SQL this way for SET_NULL operations rather than using what appears to be a more efficient query pattern. This is a clear, technical question about Django's ORM behavior with sufficient context to investigate and provide a meaningful answer.\n\nA successful solution would explain why Django generates SQL this way for SET_NULL operations and possibly suggest optimizations or alternatives if appropriate.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15973": {
        "explanation": "This issue is well-specified with clear details about the problem. The user provides:\n\n1. A clear description of the error: \"AttributeError: 'str' object has no attribute '_meta'\" occurring during migration\n2. Complete code examples showing the three relevant models across different apps\n3. The generated migration file\n4. The full error traceback\n5. Information about attempted workarounds (putting models in the same file works)\n\nThe issue is specifically about Django's handling of many-to-many relationships with a custom \"through\" model when that model is defined in a separate app. The error occurs because Django is treating the string reference to the through model ('fonte_variavel.FonteVariavelModel') as a string rather than resolving it to the actual model class.\n\nThis is a well-documented bug report with all the necessary information to reproduce and understand the problem. A developer would have enough information to investigate why Django isn't properly resolving the string reference to the through model when it's in a separate app.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-15987": {
        "explanation": "The issue description is somewhat brief but provides enough information to understand the problem:\n\n1. There is a FIXTURE_DIRS setting in the system that can contain directories.\n2. There is a loaddata function that checks for duplicates in these directories.\n3. The issue is that when directories are specified as Path instances (likely from the pathlib module), the duplicate check fails to detect duplicates.\n\nThis is a specific bug with clear symptoms and a reproducible condition. While the description doesn't provide code examples or the exact implementation details of the duplicate detection logic, an experienced developer familiar with the codebase should be able to:\n\n1. Locate the loaddata function\n2. Identify the duplicate detection logic\n3. Understand how Path instances are being compared differently from other directory representations\n4. Fix the comparison logic to properly handle Path objects\n\nThe issue is specific enough that a developer could implement a solution without needing additional clarification. The root cause is likely that Path objects are being compared by identity rather than by their string representation or resolved path.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15993": {
        "explanation": "The issue describes a problem with the RenameModel operation in Django's migration system. It states that when a model already has a db_table defined, renaming the model should be a no-operation (noop) since the underlying database table name is explicitly set and shouldn't change. However, the issue reports that in Postgres, this operation incorrectly drops and recreates foreign key constraints, and in SQLite it recreates the table.\n\nWhile the issue clearly identifies the problem (RenameModel should be a noop when db_table is defined), it lacks some specifics:\n1. It doesn't provide code examples showing the issue\n2. It doesn't specify the expected behavior in detail\n3. It doesn't mention which versions of Django are affected\n\nHowever, for an experienced Django developer familiar with the migration system, this description provides enough information to:\n1. Understand the problem (RenameModel shouldn't do anything when db_table is explicitly set)\n2. Reproduce the issue (by creating a model with db_table, then renaming it)\n3. Identify where in the codebase to look (the RenameModel operation implementation)\n4. Develop a solution (modify the RenameModel operation to check for db_table and skip table renaming)\n\nThe issue is reasonably clear about what a successful solution would look like: the RenameModel operation should detect when db_table is defined and avoid making any database changes in that case.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-15996": {
        "explanation": "The issue is reasonably well-specified. It describes a problem with Django's serialization of Enum flags in migrations, specifically when using a combination of flags (like `re.UNICODE | re.IGNORECASE`). The issue explains that the current EnumSerializer tries to use the `.name` of an enum item, but when multiple flags are combined, there's no single name for the combined value.\n\nThe issue also proposes a solution: using `enum._decompose` to get a list of names for the combined flags, and then creating an expression that \"ORs\" these items together.\n\nWhat makes this reasonably specified rather than well-specified:\n1. It doesn't explicitly state which file or class needs to be modified\n2. It doesn't provide a complete code example showing the current behavior and expected behavior\n3. It doesn't specify if there are any edge cases to consider\n\nHowever, a developer familiar with Django's migration system and Python's enum module would likely understand what needs to be done. The issue provides enough context about the problem and a clear direction for the solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16002": {
        "explanation": "This issue is well-specified with clear steps to reproduce, expected behavior, and the actual error. The reporter has:\n\n1. Provided exact reproduction steps with specific versions (Python 3.10, Django 4.1, SQLite3)\n2. Included the complete model definition needed\n3. Shown the exact code that causes the issue (creating an object with value=\"nan\" and then retrieving it)\n4. Included the full traceback showing the error\n5. Explained the root cause: \"nan\" values skip validation in DecimalField.to_python but cause errors when retrieved\n\nThe issue is that Django's DecimalField allows storing \"nan\" (Not a Number) values in the database, but fails when trying to retrieve those values with a specific error. The reporter has identified that the validation in DecimalField.to_python doesn't catch \"nan\" strings, allowing them to be stored, but the retrieval process fails with a TypeError.\n\nThis is a clear bug with a complete reproduction case that would allow a developer to investigate and fix the issue without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16032": {
        "explanation": "This issue is well-specified with a clear description of the bug and a complete test case that reproduces the problem. The issue describes that when using `QuerySet.alias()` after `annotate()` in a Django ORM query, and then using that queryset with `__in` filter, it causes an error because the subquery returns more columns than expected.\n\nThe test case provided is complete and includes:\n1. The exact code that reproduces the issue\n2. The specific error message that occurs\n3. The expected result (what data should be returned)\n\nThe error message \"sub-select returns 10 columns - expected 1\" clearly indicates the nature of the problem - the subquery is including all annotated and aliased fields when it should only include the primary key for the `__in` comparison.\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided test case\n2. Understand the root cause (the alias fields not being cleared in the subquery)\n3. Implement a fix that ensures only the necessary fields are included in the subquery when using `__in` with a queryset that has both annotations and aliases\n\nNo additional information is needed to understand and fix this issue.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16037": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue is clearly defined: QuerySet.annotate() with Count aggregation is crashing after upgrading from Django 4.0.7 to 4.1.1.\n\n2. Detailed error messages are provided for both SQLite and PostgreSQL databases, showing the same underlying issue: \"sub-select returns 13 columns - expected 1\" and \"subquery must return only one column\".\n\n3. The exact code causing the issue is identified: `.annotate(likes=Count(\"liked_by\"))` in the `fetch_all_posts` method.\n\n4. Complete model definitions and manager code are provided, showing the relationships between models and how the query is constructed.\n\n5. The issue is reproducible: it works in Django 4.0.7 but fails in 4.1.1.\n\nA developer would have all the necessary information to investigate and fix this issue. The problem appears to be related to how Django 4.1.1 handles subqueries in annotations with Count aggregation compared to the previous version. The error messages clearly indicate that a subquery is returning multiple columns when only one is expected, which points to a specific area to investigate in the Django ORM's query generation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16041": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is clearly defined: when `empty_permitted` is passed in `form_kwargs` to a formset, rendering the formset's `empty_form` causes a KeyError. The issue includes:\n\n1. A complete code example showing how to reproduce the issue\n2. The exact error condition (KeyError when rendering empty_form)\n3. The expected behavior (empty_permitted should be ignored for empty_form)\n4. All necessary components: view, URL configuration, and template\n\nThe issue provides enough context to understand that this is a bug in Django's formset implementation where the empty_form handling doesn't properly account for the empty_permitted parameter. A developer would have all the information needed to investigate and fix this issue without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16046": {
        "explanation": "The issue describes a specific error that occurs in the `numberformat.py` file when a null value is encountered during formatting for the admin list_display. The error is clearly identified as an IndexError with the message \"string index out of range\" occurring when the code tries to check `if str_number[0] == \"-\"` on a null/empty string.\n\nThe issue provides:\n1. The specific file (numberformat.py)\n2. The exact line of code causing the problem (`if str_number[0] == \"-\"`)\n3. The context where it happens (when formatting for admin list_display)\n4. The exact error message (IndexError: string index out of range)\n5. The cause (when a number field is null)\n\nThe submitter even mentions they have a proposed fix ready.\n\nWhile the issue doesn't include the full code context or the exact fix, the problem is clear enough that an experienced developer familiar with the codebase could identify the location and implement a solution (likely checking if the string is empty before trying to access the first character). The issue is reasonably well-specified for someone to work on.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16053": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. It identifies the specific components affected: django.contrib.contenttypes.models.ContentType and django.contrib.auth.models.Permission\n2. It points to the exact commit (48c17807) that introduced the issue\n3. It clearly describes the problem: the __str__ method mixes non-localized app_label with localized model verbose_name, creating inconsistent strings\n4. It references a related ticket (#16027) and specific comment for context\n5. It suggests a potential solution path: either use all localized labels or none at all\n6. It even mentions a specific method (self._meta.apps.get_app_config(self.app_label).verbose_name) that could be used for a solution\n\nA developer would have enough information to locate the affected code, understand the issue, and implement a solution that makes the string representation consistently use either all localized or all non-localized text.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16067": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a specific bug: DecimalValidator in Django 2.0.13 rejects decimal values in scientific notation like '0E+1', which were previously accepted in Django 1.11.29.\n\n2. The description provides:\n   - Exact version numbers (1.11.29 vs 2.0.13)\n   - Precise code examples showing the behavior in both versions\n   - The specific commit that introduced the change\n   - A reproducible test case that demonstrates the issue\n\n3. The user has already identified that calling `Decimal('0E+1').normalize()` inside the `to_python` method solves the problem, but considers it a workaround.\n\n4. The user is asking if there's a way to prevent this validation error from happening in Django 2.0 through a flag or option.\n\nThe issue provides all the necessary context to understand the problem, reproduce it, and develop a solution. A developer would have a clear understanding of what needs to be fixed: either modify the DecimalValidator to accept zero values in scientific notation or provide a configuration option to control this behavior.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16076": {
        "explanation": "This issue is well-specified with a clear problem statement and a minimal, complete, and verifiable example (MCVE). The user is trying to perform a `startswith` lookup on a foreign key field (`parent_object_id`) but is encountering a `FieldError` stating \"Related Field got invalid lookup: startswith\". \n\nThe issue includes:\n1. Clear description of the models involved (Parent and Child)\n2. The exact query that's failing (`Child.objects.filter(parent_object_id__startswith='c')`)\n3. A workaround that works but is inefficient (`Child.objects.filter(parent_object__object_id__startswith='c')`)\n4. Complete code example that reproduces the issue\n5. The exact error message\n6. Version information (Python 3.9/3.10 with Django 4.0.7/4.1.1)\n\nThe user is requesting support for registering lookups on relation fields, specifically to allow filtering on a foreign key field using the `startswith` lookup without forcing a join. This is a specific feature request with a clear use case and reproduction steps, making it well-specified for a developer to understand and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16082": {
        "explanation": "The issue describes a specific problem with Django's MOD operator in expressions when combining different numeric types (Decimal and Integer). The issue states that unlike other mathematical operators, MOD doesn't properly resolve the output field type to Decimal when mixing these types.\n\nWhile the issue provides a clear description of the problem, it lacks some details that would make it more complete:\n1. No code example demonstrating the issue\n2. No information about the expected behavior vs. actual behavior\n3. No error messages or specific Django version information\n\nHowever, the core problem is clearly stated: the MOD operator doesn't handle type resolution between Decimal and Integer the same way other mathematical operators do. This is specific enough that a developer familiar with Django's ORM and expression system would understand what needs to be fixed - the MOD operator needs to follow the same type resolution rules as other mathematical operators when dealing with mixed numeric types.\n\nA developer could reasonably investigate this by examining how other mathematical operators handle type resolution and then applying the same pattern to the MOD operator implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16092": {
        "explanation": "This issue is not well-specified enough for a meaningful solution. There are several critical problems:\n\n1. The description is incomplete and confusing. It references a PR (#13709) but doesn't clearly state what the actual issue is.\n\n2. The title mentions adding \"Field.db_default\" for defining database defaults, but the code snippet doesn't show implementation of this feature - it's just modifying existing code to use field defaults.\n\n3. The code snippet is incomplete and lacks context. It's applying a diff to \"django/core/mamagement.py\" (which appears to be misspelled - should be \"management.py\"), but doesn't show the surrounding code or explain what function/method this is modifying.\n\n4. The syntax in the code snippet uses \"<>\" for inequality which is outdated Python syntax (should be \"!=\").\n\n5. There's no explanation of what problem this is solving, what the expected behavior should be, or how to test the implementation.\n\n6. It's unclear if this is implementing a new feature (Field.db_default) or just modifying how defaults are handled.\n\nWithout more context about what Field.db_default is supposed to do, how it differs from the existing Field.default, and a clearer specification of the implementation requirements, this issue cannot be meaningfully addressed.",
        "label": "VAGUE"
    },
    "django__django-16100": {
        "explanation": "The issue is reasonably specified but could benefit from more details. It clearly identifies a problem: the `changelist_view` in Django admin lacks transaction handling for `list_editable` processing, which could lead to inconsistent database states if errors occur during updates.\n\nThe core requirement is to add transaction handling to prevent partial updates when using the list_editable feature in the Django admin changelist view. This is a valid concern as database operations should generally be atomic when multiple related changes are being made.\n\nWhat's missing from the specification:\n1. No specific code references or file locations are provided\n2. No examples of the problematic behavior\n3. No details about what kind of transaction handling is preferred (savepoints, atomic blocks, etc.)\n4. No mention of test cases that should be added or modified\n\nHowever, for an experienced Django developer, this issue provides enough information to:\n1. Locate the relevant code in the Django admin module\n2. Understand the problem (lack of transaction handling in list_editable processing)\n3. Implement a solution using Django's transaction management tools\n\nA developer familiar with Django would know to look for the changelist_view method in the admin module and wrap the relevant database operations in transaction.atomic() blocks or similar transaction handling mechanisms.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16116": {
        "explanation": "This issue is well-specified and clearly describes a problem with the `makemigrations --check` command in Django. The issue points out an inconsistency in how the `--check` flag behaves across different Django migration commands:\n\n1. For `migrate --check` and `optimizemigration --check`, the commands just exit after checking\n2. For `makemigrations --check`, it still generates migrations, requiring an additional `--dry-run` flag to prevent actual file creation\n\nThe issue suggests that `makemigrations --check` should behave consistently with other commands and just exit without making migrations. The reporter provides context about when they noticed this issue and mentions that the current documentation doesn't address this behavior.\n\nThe problem is clearly defined, the expected behavior is specified (exit without making migrations), and the inconsistency with other similar commands is highlighted. A developer would have a clear understanding of what needs to be fixed: modify the `makemigrations` command to handle the `--check` flag consistently with other commands.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16117": {
        "explanation": "This issue is well-specified with a clear problem description and proposed solutions. The issue describes:\n\n1. The exact problem: Django migration files with dots in their names are not detected by Django's migrate/showmigrations commands\n2. A clear reproduction path:\n   - Create a model\n   - Add a constraint with dots in the name\n   - Run makemigrations (generates file with dots)\n   - Run migrate (fails to detect the file)\n3. Three potential solutions:\n   - Manually rename the migration file\n   - Change the constraint name to avoid dots\n   - Modify Django's source code to replace dots in migration filenames\n\nThe issue includes a GitHub repository with example code to reproduce the problem and even references a pull request with a proposed fix. The expected behavior is clear: Django should properly handle migration files even when constraint names contain dots.\n\nThis issue provides all the necessary information to understand the problem and implement a solution. The third approach (modifying Django's source code) is specifically mentioned as the preferred solution, which gives clear direction for what kind of PR would be expected.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16120": {
        "explanation": "This issue is well-specified and clear. It describes a specific problem with Django's `migrate --check` command:\n\n1. The issue: When running `migrate --check` and the database is already up-to-date, the command still emits `pre_migrate` and `post_migrate` signals, which it shouldn't since no actual migration is happening.\n\n2. A secondary issue: The output of the command looks like a real migration run, which can momentarily confuse users.\n\n3. The issue includes a clear example of the current behavior.\n\nThe problem is well-defined, and the expected behavior is implied: the `--check` flag should only check if migrations need to be applied without emitting signals or displaying output that suggests migrations are being run. This is a straightforward bug fix that would involve modifying the migration command's behavior when the `--check` flag is used to prevent signal emission when no migrations need to be applied, and potentially updating the output messaging to be clearer.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16136": {
        "explanation": "This issue is well-specified with clear reproduction steps and error details. The problem occurs when a Django View subclass has only an async \"post\" method defined, and a GET request is made to that view. Instead of returning a proper \"Method Not Allowed\" response, the server crashes with a TypeError: \"object HttpResponseNotAllowed can't be used in 'await' expression\".\n\nThe issue description provides:\n1. Complete error traceback\n2. Environment details (Django 4.1.1, Python 3.10.6)\n3. Minimal code to reproduce the issue\n4. Clear steps to reproduce (create view with only async post method, make GET request)\n5. Expected behavior is implied (should return 405 Method Not Allowed without crashing)\n\nThe root cause appears to be that Django's method dispatching mechanism isn't properly handling async views when the requested HTTP method isn't implemented. The solution would likely involve modifying how Django handles HTTP method dispatching for async views to ensure HttpResponseNotAllowed objects are properly returned without attempting to await them.\n\nThis is a well-defined bug with clear reproduction steps that can be addressed without additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16139": {
        "explanation": "This issue is well-specified with clear details about the problem and solution:\n\n1. The problem is clearly identified: When accessing UserAdmin via another model's Admin with to_field set, the link to the PasswordResetForm breaks with a 404 error.\n\n2. The cause is explained: The UserAdmin is accessed via a URL with a UUID instead of primary key, but the password reset link is hardcoded to use a relative path \"../password/\" which doesn't work in this context.\n\n3. The solution is proposed: Replace the hardcoded path with one that explicitly uses the instance's primary key: f\"../../{self.instance.pk}/password/\"\n\n4. The issue includes relevant code snippets showing the problematic code and where the fix needs to be applied.\n\n5. The submitter has even created a pull request with the proposed solution, which further clarifies the intended fix.\n\nA developer could implement this solution without needing additional clarification. The issue provides all necessary context about the bug, its cause, and how to fix it.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16142": {
        "explanation": "This issue is well-specified with clear context and a proposed solution. The problem is that the `get_language_from_request` function currently falls back to `settings.LANGUAGE_CODE` when it can't determine a language from the request, which creates ambiguity - when the function returns a language code, it's impossible to tell if that came from the request or from the default settings. The proposed solution is to modify the function to return `None` when no language can be determined from the request, allowing middleware to handle the fallback logic appropriately. The issue includes:\n\n1. Clear description of the current behavior\n2. Why this behavior is problematic (creating ambiguity and preventing custom middleware from working correctly)\n3. A specific proposed solution (return None instead of settings.LANGUAGE_CODE)\n4. The rationale for the change (separation of concerns - determining language from request vs. applying fallbacks)\n\nA developer could implement this solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16143": {
        "explanation": "This issue is well-specified with clear information about the problem and what needs to be improved. The user is experiencing a \"TransactionManagementError\" in Django that doesn't provide enough context about the root cause of the error. \n\nThe issue clearly identifies:\n1. The specific error message and where it's coming from (django/db/backends/init.py)\n2. The technical cause: self.needs_rollback is True, but the reason why it was set to True has been lost\n3. A clear proposed solution: transactions should track why needs_rollback was set and include that information in the error message\n4. Context about when the issue occurs (in Celery tasks but not when run manually)\n\nA developer would understand what needs to be fixed: modify Django's transaction management code to preserve and report the original cause when needs_rollback is set to True, making the error message more helpful for debugging. The solution would involve tracking the original exception or reason that caused needs_rollback to be set, and including that information when raising the TransactionManagementError.\n\nThis is a well-defined enhancement request with sufficient technical details to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16145": {
        "explanation": "This issue is well-specified. The problem is clearly described: when running Django's development server with `python manage.py runserver 0:8000`, the output message displays \"Starting development server at http://0:8000/\" instead of \"http://0.0.0.0:8000/\", which is inconsistent with the documentation and causes confusion. The expected behavior is that when a user provides \"0:8000\" as the address parameter, the output message should display \"0.0.0.0:8000\" to be consistent with the actual binding behavior (as \"0\" is treated as shorthand for \"0.0.0.0\" according to the documentation). The issue provides both the current behavior and the expected behavior, making it clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16208": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly stated: when debugging database calls using django.db.backends log, the BEGIN SQL calls are visible, but COMMIT and ROLLBACK operations are not logged. The requested solution is straightforward - to add logging for when database transactions are committed or rolled back.\n\nThe issue provides:\n1. Clear context (debugging with django.db.backends log)\n2. The specific problem (missing COMMIT and ROLLBACK logs)\n3. A suggested solution (log commits and rollbacks)\n\nWhat's missing is implementation details like:\n- Which specific files need to be modified\n- The exact format of the log messages\n- Whether this should be at the same log level as BEGIN statements\n- Whether this should be configurable\n\nHowever, these details can be reasonably inferred by examining the existing code that logs the BEGIN statements. A developer familiar with Django's database transaction handling would be able to locate where BEGIN is logged and implement similar logging for COMMIT and ROLLBACK operations following the same pattern.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16229": {
        "explanation": "This issue is reasonably well-specified with clear steps to reproduce and a specific technical problem. The issue describes:\n\n1. A clear bug: When creating an object via the admin with an ArrayField in an inline form, validation errors are bypassed on second submission\n2. Specific reproduction steps with code examples for models.py and admin.py\n3. A clear description of the expected vs actual behavior\n4. A hint at the root cause (related to hidden input with initial values)\n5. A potential workaround (forcing show_hidden_initial=False)\n\nThe issue provides enough context to understand the problem and attempt a solution. The only minor gaps are:\n- The exact Django version isn't specified\n- The complete traceback/error messages aren't provided\n- The exact implementation of the ArrayField isn't shown (though it's likely the standard Django PostgreSQL ArrayField)\n\nHowever, these gaps don't prevent understanding the core issue or attempting a solution. A developer could reproduce this issue with the provided code and work toward fixing the problem with the ModelForm's handling of callable defaults in ArrayFields within admin inlines.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16254": {
        "explanation": "This issue is well-specified with clear details about the problem. The reporter has:\n\n1. Clearly identified the issue: When adding a ManyToManyField to a model in Django 4.1 with SQLite, the table is being rebuilt unnecessarily, which didn't happen in Django 4.0.\n\n2. Provided complete steps to reproduce:\n   - Showed the initial models\n   - Showed the updated models with the ManyToManyField\n   - Showed the generated migration\n\n3. Included the exact SQL output from both Django 4.0 and 4.1, highlighting the difference (table rebuild in 4.1)\n\n4. Identified the specific commit that likely caused the regression (2f73e5406d54cb8945e187eff302a3a3373350be)\n\n5. Pointed to the relevant code change (removal of special-case handling for implicit M2M tables)\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and start working on a fix. The reporter has even provided a potential lead for fixing the issue by identifying the commit that introduced the regression and the specific code change that might need to be addressed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16255": {
        "explanation": "This issue is well-specified with clear information about the problem and a potential solution. The issue describes:\n\n1. The exact problem: When a sitemap contains no items but supports returning lastmod for an item, it fails with a ValueError because max() is called on an empty sequence.\n\n2. A complete stack trace showing where the error occurs.\n\n3. The specific file and function causing the issue: django/contrib/sitemaps/__init__.py in the get_latest_lastmod method.\n\n4. A proposed solution: Catch ValueError in addition to TypeError when calling max() on an empty sequence.\n\nThe issue provides all the necessary context to understand the problem and implement a fix. The proposed solution is straightforward - adding ValueError to the exception handling in the get_latest_lastmod method. This is a common pattern in Python to handle edge cases, and the fix appears to be minimal and focused.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16256": {
        "explanation": "This issue is well-specified with clear technical details about the problem and a suggested solution path. The issue describes that:\n\n1. The async-compatible interface added to QuerySet unintentionally added async methods (acreate(), aget_or_create(), and aupdate_or_create()) to related managers.\n2. These methods incorrectly call methods from QuerySet rather than from the related manager.\n3. The issue includes code snippets showing where the problem exists and even provides a partial implementation of the solution (showing the acreate() method implementation).\n\nThe issue clearly indicates what needs to be fixed: proper async versions of these methods need to be added to related managers that call the corresponding synchronous methods from the related manager rather than from QuerySet.\n\nThe description includes specific file paths (django/db/models/fields/related_descriptors.py) and shows code snippets with line numbers, making it easy to locate the problem. It also mentions that there are six combinations to consider (forward and reverse directions for three methods).\n\nA developer with Django knowledge would have enough information to implement a solution based on this description without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16260": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly defined: `refresh_from_db()` doesn't clear cached generic foreign keys, which leads to stale data in tests.\n\n2. The issue includes specific code examples showing:\n   - The model definition with a GenericForeignKey\n   - A test case that fails (showing the cached object isn't refreshed)\n   - A specific bug scenario with non-transitive equality assertions\n   - A workaround that was discovered\n\n3. The issue provides technical context:\n   - Django version (3.2.13)\n   - Confirmation that the code hasn't changed significantly in newer versions\n   - Initial investigation into the source of the problem in the `refresh_from_db()` method\n\n4. The issue points to a specific area in the code that might be causing the problem - the loop in `refresh_from_db()` that handles related objects but doesn't seem to consider generic relations as \"cached\".\n\nA developer could take this information and:\n1. Reproduce the issue using the provided examples\n2. Investigate why generic foreign keys aren't being cleared from cache\n3. Modify the `refresh_from_db()` method to properly handle generic foreign keys\n4. Test the fix using the provided test cases\n\nThe issue is specific, reproducible, and includes enough context to understand and address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16263": {
        "explanation": "This issue is well-specified with a clear problem statement and expected behavior. The issue describes:\n\n1. A specific problem: Django includes unused annotations in count queries, which is inefficient\n2. A concrete example: `Book.objects.annotate(Count('chapters')).count()` includes the Count annotation in the SQL even though it's not used\n3. The expected behavior: Django should strip out annotations that aren't referenced by filters, other annotations, or ordering\n4. A justification: This would improve performance for count() calls with complex annotations\n5. A precedent: select_related calls are already ignored in count() queries\n\nThe solution would involve modifying Django's ORM to detect and remove unused annotations when generating SQL for count() queries. This is a well-defined optimization task with clear success criteria - the count() method should generate simpler SQL when annotations aren't being used in filters, other annotations, or ordering.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16281": {
        "explanation": "This issue is well-specified with clear steps to reproduce the problem. The description provides:\n\n1. The exact model definitions before and after the change\n2. The specific migration commands to run\n3. The exact steps to reproduce the error (using the admin page)\n4. The expected error message (\"no such column: app_foo_bar.from_foo_id\")\n\nThe issue is describing a bug where changing a ManyToManyField's target from another model ('Bar') to 'self' (or in this case 'Foo', which is the same model) doesn't properly update the database schema during migration. This results in a missing column error when trying to use the relationship.\n\nA developer would have all the necessary information to reproduce this issue and understand what needs to be fixed - the migration system needs to properly handle the schema changes when a ManyToManyField's target is changed to 'self'.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16302": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The problem is clearly stated: Django's CharField requires a positive integer for max_length and doesn't support max_length=None, which would be useful for database backends like PostgreSQL that support unlimited length text fields.\n\n2. The issue includes specific code examples showing both the problematic code and the error message.\n\n3. The issue identifies the exact location of the validation code in Django's codebase (django/core/management/validation.py:40).\n\n4. A specific solution is proposed: modify the validation code to allow max_length=None.\n\n5. The issue also identifies a related concern with FileField that would need to be addressed.\n\nThe requirements for a successful solution are clear: modify Django to allow CharField with max_length=None, which would map to unlimited length varchar/text columns in databases that support them. The issue provides enough context and specificity for a developer to implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16306": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The description:\n\n1. Identifies the exact problem: BaseForm.__getitem__() does unnecessary work in the \"happy path\"\n2. Provides a specific link to the code in question with line numbers\n3. Explains the current behavior: the method checks self.fields and then checks if name is in self._bound_fields_cache\n4. Proposes a clear solution: return self._bound_fields_cache[name] at the beginning and handle KeyError instead of the current approach\n\nThe issue describes a straightforward optimization where the code can be restructured to avoid unnecessary checks in the common case. The solution approach is clearly outlined - use a try/except pattern with KeyError handling rather than explicit dictionary membership checking. This is a common Python pattern for performance optimization.\n\nA developer could implement this change without needing additional clarification, as both the problem and solution are well articulated.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16311": {
        "explanation": "This issue is well-specified with clear context and a proposed solution. The issue:\n\n1. Clearly identifies the problem: The BREACH attack vulnerability in Django applications using gzip compression\n2. Provides background information: Links to the attack details and Django's previous response\n3. References a specific solution: The \"Heal The Breach\" paper with a mitigation technique involving adding randomness to the gzip filename field\n4. States a clear objective: Implement this mitigation in Django with sensible defaults\n\nThe implementation details are reasonably clear - add random bytes to the gzip filename field of compressed responses to mitigate the BREACH attack while still allowing gzip compression to be used. This is a specific technical change with a defined purpose.\n\nA developer could proceed with implementing this solution based on the information provided, though they would likely need to read the referenced paper for implementation specifics. The overall goal and approach are well-defined.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16315": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in Django's QuerySet.bulk_create() method when used with models that have db_column attributes set.\n2. The user provides a concrete code example showing their model definition with db_column attributes.\n3. The user shows the exact method call that's failing (bulk_create with update_conflicts, update_fields, and unique_fields parameters).\n4. The user provides the actual SQL that's being generated, which shows the problem: the INSERT part correctly uses the db_column values (\"BlacklistID\"), but the ON CONFLICT part incorrectly uses the field names (\"blacklistid\").\n5. The user includes the exact error message from PostgreSQL: \"ERROR: column \"blacklistid\" does not exist at character 1508\"\n6. The user even suggests what the correct SQL should look like.\n\nThe issue is clearly a bug where bulk_create() is not properly handling db_column attributes in the ON CONFLICT clause of the SQL statement. The solution would involve modifying the bulk_create implementation to use the db_column values consistently throughout the generated SQL, including in the ON CONFLICT and UPDATE SET clauses.\n\nA developer could implement a solution without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16322": {
        "explanation": "This issue is well-specified with clear details about the problem. The user has:\n\n1. Provided the exact query that's causing the issue: `Model.objects.annotate(foo=F('column')).aggregate(foo=Sum(F('foo')))`\n2. Explained the different behavior between SQLite (produces a result) and MySQL (crashes with error)\n3. Included the full error traceback\n4. Identified the root cause: Django produces incorrect SQL when the same keyword appears in both `annotate()` and `aggregate()`\n5. Shown the SQL that Django actually produces vs. what it should produce\n6. Explained why this is problematic (SQLite silently produces incorrect results)\n\nThe issue is clearly a bug in Django's ORM where it fails to properly handle name collisions between annotations and aggregations. A developer would have enough information to reproduce the issue and understand what needs to be fixed - either by preventing the same name from being used in both methods or by correctly generating SQL that handles this case properly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16333": {
        "explanation": "This issue is well-specified with a clear problem statement and solution path. The issue describes that:\n\n1. The `UserCreationForm` from `contrib.auth.forms` doesn't save data from ManyToMany form fields when used with a custom User model.\n2. The specific cause is identified: unlike its parent class `ModelForm`, the `UserCreationForm.save(commit=True)` method doesn't call `self.save_m2m()`.\n3. There's a reference to a discussion on the Django developers mailing list where this issue has been acknowledged.\n4. The reporter is ready to work on a PR to fix this issue.\n\nThe solution would clearly involve modifying the `save()` method in `UserCreationForm` to ensure it calls `self.save_m2m()` when appropriate, similar to how the parent `ModelForm` class handles this. This is a straightforward bug fix with a well-defined scope and clear expected behavior (ManyToMany fields should be saved when using UserCreationForm).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16343": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue:\n\n1. Clearly identifies a security vulnerability where positional arguments to the Signer constructor can lead to misuse (using email as key instead of salt)\n2. Provides a concrete code example demonstrating the problem\n3. Explains the security implications of the mistake\n4. Proposes a specific solution: modifying the Signer.__init__ signature to force keyword arguments by adding a * parameter\n5. Shows the exact code change required (before and after)\n\nThe task is straightforward: modify the Signer class constructor to require keyword arguments rather than positional arguments to prevent this type of security mistake. This is a common pattern in Python to improve API safety, and the implementation details are fully specified in the issue description.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16379": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The exact error is provided: `FileNotFoundError` when trying to open a cache file\n2. The full stack trace shows where the error occurs\n3. The specific code with the race condition is identified in `has_key()` method of `FileBasedCache`\n4. The cause of the race condition is clearly explained: between checking if a file exists with `os.path.exists()` and opening it with `open()`, the file could be deleted by another thread\n5. The reporter even identifies that `_is_expired()` method deletes expired files, which explains why this race condition can occur in a multi-threaded environment\n\nThe issue is specific, includes the relevant code, explains the problem, and implicitly suggests what needs to be fixed (handling the case where a file might disappear between checking existence and opening it). A developer could immediately understand and address this issue without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16398": {
        "explanation": "The issue description is quite vague and lacks important context. It references a specific commit and a line of code in Django's compiler.py file, suggesting that there's a bug in the `select_related()` method of Django's QuerySet when dealing with multiple filtered relations to a OneToOneField. The description mentions that it \"should use partial, just like the remote_setter\" but doesn't explain:\n\n1. What the actual problem behavior is\n2. What the expected behavior should be\n3. Any steps to reproduce the issue\n4. Why using \"partial\" would fix the issue\n5. What \"remote_setter\" refers to in this context\n\nWithout understanding the specific bug, its impact, or how to reproduce it, it would be difficult for a developer to implement a solution confidently. The links to the code provide some context, but the description itself doesn't clearly articulate what needs to be fixed or why.",
        "label": "VAGUE"
    },
    "django__django-16408": {
        "explanation": "This issue is reasonably well-specified with a clear test case that demonstrates the bug. The issue describes a problem with Django's ORM where using multi-level FilteredRelation with select_related() results in incorrect related objects being set.\n\nThe test case shows:\n1. A specific scenario using PoolStyle.objects with a FilteredRelation on 'pool__tournament__pool'\n2. A select_related on the annotated field 'tournament_pool'\n3. The expected behavior: p[0].pool.tournament should equal p[0].tournament_pool.tournament\n4. The actual behavior: They're different objects, with one being a Tournament and the other incorrectly being a PoolStyle\n\nThe failure trace clearly shows the assertion error and the specific objects that don't match.\n\nWhile the issue doesn't explicitly explain the underlying cause or provide the model definitions, an experienced Django developer would understand that this is likely a bug in how Django's ORM handles the combination of FilteredRelation and select_related across multiple relation levels. The test case provides enough context to reproduce the issue and understand what's going wrong.\n\nThe only missing information is the exact model structure, but this can be inferred from the test case and the Django test suite would have the complete models defined.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16411": {
        "explanation": "This issue is reasonably well-specified. It clearly describes:\n\n1. The feature request: Adding a \"manifest_hash\" property to ManifestFilesMixin that changes whenever the manifest file changes\n2. The purpose: To allow single-page applications to detect asset changes via an HTTP header and trigger full reloads when needed\n3. The component to modify: ManifestFilesMixin (part of ManifestStaticFilesStorage)\n\nThe issue provides enough context for a developer familiar with Django's static file handling to understand what needs to be implemented. The request is for a hash value that represents the entire manifest state, which would change whenever any asset in the manifest changes.\n\nThere are some implementation details that aren't specified:\n- The specific hashing algorithm to use\n- How exactly the hash should be calculated (e.g., hash of the serialized manifest or something else)\n- Whether this should be a property, method, or something else\n- How it integrates with Django's response cycle for the header suggestion\n\nHowever, these are reasonable implementation details that a developer could determine based on Django conventions and best practices. The core requirement is clear enough to begin work on a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16429": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug: `timesince()` raises a TypeError when used with `USE_TZ=True` and a time interval greater than 1 month.\n\n2. It provides the exact error message: \"TypeError: can't subtract offset-naive and offset-aware datetimes\"\n\n3. It includes a test case that reproduces the issue, showing the expected behavior.\n\n4. It identifies the root cause: the pivot datetime instantiated in the timesince.py file doesn't account for the tzinfo of the input datetime.\n\n5. It even suggests a specific fix: adding `0, d.tzinfo` arguments to the datetime.datetime call.\n\n6. The issue references a specific commit (8d67e16493c903adc9d049141028bc0fff43f8c8) and provides a link to the relevant code.\n\nWith this information, a developer could easily reproduce the issue, understand the cause, and implement the suggested fix. The issue is very well-specified and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16454": {
        "explanation": "This issue is well-specified with clear details about the problem and expected behavior:\n\n1. The issue clearly identifies the problem: Django's CommandParser doesn't copy its error formatting arguments to subparsers created via add_subparsers().add_parser().\n\n2. A complete, reproducible example is provided showing the issue with a simple management command that uses subparsers.\n\n3. The current behavior is clearly demonstrated with command output examples showing:\n   - The main parser correctly formats errors\n   - The subparser fails with a stack trace instead of a formatted error\n\n4. The expected behavior is described: subparsers should inherit the error formatting configuration from the main CommandParser.\n\n5. A general solution approach is suggested: ensure the subparser action returned by add_subparsers() copies the relevant arguments to constructed subparsers.\n\nThe issue provides all the necessary context to understand the problem, reproduce it, and implement a solution without requiring additional clarification. A developer could immediately start working on this with the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16485": {
        "explanation": "This issue is well-specified with a clear problem description. The issue reports that the `floatformat()` template filter in Django crashes when given the string '0.00' or Decimal('0.00') with a precision of 0. The error message is clearly stated: \"ValueError: valid range for prec is [1, MAX_PREC]\".\n\nThe issue includes:\n1. Exact code to reproduce the problem\n2. The specific inputs that cause the crash ('0.00' and Decimal('0.00'))\n3. The precision parameter value (0)\n4. The exact error message received\n\nWith this information, a developer can:\n1. Reproduce the issue\n2. Understand what's happening (the filter is failing to handle zero values with the specified precision)\n3. Locate the relevant code in Django's template filters\n4. Fix the issue by properly handling this edge case\n\nThe solution would likely involve modifying the `floatformat` filter to properly handle zero values when precision is set to 0.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16493": {
        "explanation": "This issue is well-specified with clear details about the bug and its cause. The description:\n\n1. Clearly identifies the problem: When a FileField has a callable storage that returns default_storage, it's incorrectly omitted from the deconstructed form.\n\n2. Provides a complete code example demonstrating the issue, including the model definition with the problematic callable storage.\n\n3. Explains the expected behavior: The callable should be included as a reference in the deconstructed form.\n\n4. Describes the actual behavior: makemigrations randomly generates migrations that sometimes include and sometimes omit the storage parameter.\n\n5. Identifies the root cause: The deconstruct method tests if self.storage is not default_storage, but at that point self.storage is already evaluated, causing incorrect behavior when a callable returns default_storage.\n\n6. References a related issue (#31941) that this issue was overlooked in.\n\nWith this information, a developer could understand the problem, reproduce it, and implement a fix without needing additional clarification. The fix would likely involve modifying the deconstruct method to properly handle callables that return default_storage.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16502": {
        "explanation": "This issue is well-specified with clear steps to reproduce, expected vs. actual behavior, and a precise technical problem description. The issue describes that after a specific change (#26052), Django's runserver development server is not properly handling HTTP HEAD requests according to RFC 2616 specifications - it's returning response bodies when it shouldn't. The issue includes:\n\n1. Clear context about the problem (a change in Django 1.10 that removed body stripping)\n2. Detailed steps to reproduce with exact commands\n3. Expected vs. actual output showing the difference\n4. Version information (Python 2.7.13, 3.4.5, Django 1.10)\n5. Confirmation that it doesn't happen in Django 1.9.13\n\nThe solution would need to modify Django's runserver to properly strip response bodies for HEAD requests, since other servers do this but runserver apparently doesn't. This is a straightforward compliance issue with a clear technical specification to follow (RFC 2616, section 4.3).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16511": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solutions. The author wants to extend Django's `update_or_create` method to support different default values for create operations versus update operations.\n\nThe issue includes:\n1. A clear use case with example code showing the desired functionality\n2. A specific model example demonstrating when this would be useful (setting created_by vs modified_by fields)\n3. Two potential implementation approaches:\n   - Adding a new `create_defaults` parameter\n   - Modifying how the existing `defaults` parameter works\n\nThe only open question is which implementation approach to take, but this is a reasonable design decision that can be made during implementation. The requirements themselves are unambiguous.\n\nThe issue has already been discussed on the Django forum with apparent consensus that it's a valuable addition, and potential backward compatibility concerns have been identified and addressed.\n\nA developer could implement this feature with the information provided, making reasonable decisions about the specific implementation approach based on Django's coding standards and the backward compatibility requirements mentioned.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16514": {
        "explanation": "The issue is reasonably well-specified. It describes a clear problem: when multiple AdminSite instances are registered in Django, the LogEntries shown on each site include entries for all models across all admin sites, rather than being filtered to only show entries relevant to models registered on that specific admin site.\n\nThe issue provides:\n1. A clear description of the current behavior (all admin sites show all LogEntry records)\n2. The expected behavior (each admin site should only show LogEntry records for models registered on that specific site)\n3. The rationale (it's confusing for users to see log entries for models they can't access)\n4. A reference to a discussion about this issue\n\nWhat's missing is specific implementation details like:\n- The exact code changes needed\n- How to determine which LogEntries belong to which AdminSite\n- Edge cases to consider\n\nHowever, for someone familiar with Django's admin system, this provides enough information to understand the problem and develop a solution approach. The issue is pointing to a specific component (LogEntries filtering in AdminSite) with a clear goal (filter by registered models).",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16517": {
        "explanation": "This issue is reasonably well-specified. The reporter describes a specific problem with Django's admin documentation system where links to class-based views with mixed-case names result in 404 errors. They provide:\n\n1. A clear example of the problem: A class named `OrderSearch` with a docstring reference `:view:orders.views.Orders` generates a link that becomes lowercase (`orders.views.orders`) causing a 404.\n2. The specific URL that fails: `http://localhost:8000/admin/doc/views/orders.views.orders/`\n3. A pointer to potentially relevant code: `django/contrib/admindocs/views.py` and the `_get_view_func` method.\n\nWhile the reporter doesn't know exactly where the case conversion happens, they've provided enough information for a developer to:\n1. Reproduce the issue\n2. Narrow down the search area in the codebase\n3. Understand what the expected behavior should be (preserving case in view/template names)\n\nThe solution would involve finding where Django is converting the case of view names in the admin docs system and fixing it to preserve the original case. There's enough context to start investigating and implementing a fix without needing additional clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16527": {
        "explanation": "This issue is well-specified. The reporter clearly identifies:\n\n1. The exact file and line number where the change should be made: \"django/contrib/admin/templatetags/admin_modify.py\", line 102\n2. The specific problem: The \"show_save_as_new\" functionality in the admin interface doesn't check for add permission\n3. The proposed solution: Add an additional check for \"has_add_permission\" in the condition\n4. Even provides the exact code change they're suggesting\n\nThe issue makes logical sense - the \"Save as new\" functionality in Django admin essentially creates a new object based on an existing one, so it should require add permission, not just change permission.\n\nA developer could easily locate the code, understand the issue, and implement the suggested fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16560": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly stated: Currently, users can customize the `violation_error_message` of a `ValidationError` raised by a constraint, but not the `code` attribute.\n\n2. The proposed solution is explicit: Add a new parameter to `BaseConstraint` to allow customization of the error code, similar to how `violation_error_message` works.\n\n3. The rationale is provided: Django's documentation recommends providing descriptive error codes when raising `ValidationError`, but constraints don't currently support this customization without subclassing.\n\n4. The context is clear: This relates to Django's validation system, specifically the `BaseConstraint` class and how it raises `ValidationError`.\n\nThe issue provides enough information for a developer to understand what needs to be changed, why it should be changed, and how it should be implemented. The solution would involve modifying the `BaseConstraint` class to accept and use a new parameter for customizing the error code in validation errors.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16569": {
        "explanation": "This issue is very well-specified with clear details:\n\n1. The problem is precisely identified: Formsets' add_fields() method fails when index is None, specifically when can_delete=True and can_delete_extra=False.\n\n2. The exact error is provided: TypeError: '<' not supported between instances of 'NoneType' and 'int'\n\n3. The root cause is identified with a specific line reference (line 493 in django.forms.formsets.py) where index is compared to initial_form_count without checking if index is None.\n\n4. A solution is proposed: add a check for \"index is not None\" before the comparison.\n\n5. A complete, self-contained reproduction script is provided that demonstrates the issue.\n\nThe issue contains all the necessary information to understand, reproduce, and fix the problem. A developer could implement the fix without needing any additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16578": {
        "explanation": "This issue is well-specified with a clear objective. The task is to modify the BaseConstraint class to make its arguments keyword-only, specifically the 'name' and 'violation_error_message' parameters. The issue references a pull request review that suggested this change, and it notes that all BaseConstraint subclasses in Django already use these parameters as keyword-only. This is a straightforward code refactoring task with a clear goal and rationale. A developer would need to:\n\n1. Locate the BaseConstraint class in the Django codebase\n2. Modify its signature to make the specified arguments keyword-only (using Python's * syntax for keyword-only arguments)\n3. Ensure this change doesn't break existing functionality\n\nThe issue provides enough context about what needs to be changed and why, making it well-specified for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16588": {
        "explanation": "This issue is well-specified with clear details about the bug:\n\n1. The problem is clearly identified: the `floatformat()` template filter crashes when used with \"0.0000\" values.\n2. Specific examples are provided showing how to reproduce the error: `floatformat('0.0000', 2)` and `floatformat(Decimal('0.0000'), 2)`.\n3. The exact error message is included: `ValueError: valid range for prec is [1, MAX_PREC]`.\n4. The issue includes the git commit that introduced the bug: 08c5a787262c1ae57f6517d4574b54a5fcaad124.\n5. The submitter has analyzed the code and explained why the error occurs: when using \"0.0000\", the calculation results in a precision of 0, which is not allowed.\n6. The relevant code snippet showing the calculation that leads to the invalid precision is included.\n\nWith this information, a developer can reproduce the issue, understand the root cause, and develop a fix that ensures the precision calculation doesn't result in a value of 0 when handling values like \"0.0000\".",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16595": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific behavior in Django's migration optimizer where multiple consecutive AlterField operations on the same field are not being reduced/optimized.\n\n2. The user provides a concrete example with code showing the current behavior.\n\n3. The issue explains what happens (the optimizer reduces AddField + AlterField operations, but doesn't reduce multiple AlterField operations).\n\n4. The user has identified the specific part of the code that's missing functionality (AlterField.reduce doesn't handle the case where the operation is also an AlterField).\n\n5. The user even proposes a specific solution with code.\n\nThe issue is focused on a specific component (migration optimizer), provides reproducible examples, and clearly identifies the expected behavior versus the actual behavior. A developer could implement a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16599": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information. The problem occurs when trying to access a Django admin page with an extremely large integer ID value (1111111111111111111111) that exceeds SQLite's INTEGER capacity, resulting in an OverflowError. \n\nThe issue includes:\n1. Clear reproduction steps: Accessing a specific URL with a very large integer ID\n2. The exact error message: \"Python int too large to convert to SQLite INTEGER\"\n3. Detailed environment information: Django 1.10.2, Python 2.7.11, SQLite backend\n4. The specific location of the error: django/db/backends/sqlite3/operations.py line 129\n\nThe solution would need to handle large integer IDs gracefully rather than crashing with an OverflowError. This could involve input validation, error handling, or adjusting how Django interfaces with SQLite for large integers. The issue is asking whether this behavior should be accepted or if there should be a maximum length limit implemented.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16603": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly identified: Django's ASGI handler is not properly handling the `http.disconnect` message for requests that include a body.\n\n2. The issue provides the exact location of the problem in the codebase with a link and code snippet showing that `http.disconnect` is only handled within the `read_body` method.\n\n3. The issue explains the expected behavior: handling client disconnections during long-polling requests that have a body.\n\n4. A failing test case is provided that demonstrates the issue, showing exactly how to reproduce the problem.\n\n5. The issue even suggests a potential solution direction by referencing Channel's `await_many_dispatch()` approach.\n\nThe description provides all the necessary context for a developer to understand the problem, reproduce it, and begin working on a solution. The issue is about implementing proper handling of the ASGI `http.disconnect` message throughout the request lifecycle, not just during body reading.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16612": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue identifies a specific bug in Django's AdminSite.catch_all_view() function where query strings are dropped during redirects when APPEND_SLASH=True.\n\n2. It provides a concrete example with:\n   - The provided URL: http://127.0.0.1:8000/admin/auth/foo?id=123\n   - The expected redirect: http://127.0.0.1:8000/admin/auth/foo/?id=123\n   - The actual redirect: http://127.0.0.1:8000/admin/auth/foo/\n\n3. It identifies the root cause of the problem: the redirect in the catch_all_view() function only uses request.path without including the query string.\n\n4. It points to the exact line of code causing the issue with a GitHub link: https://github.com/django/django/blob/c57ff9ba5e251cd4c2761105a6046662c08f951e/django/contrib/admin/sites.py#L456\n\n5. It references a previous issue (#31747) that introduced this functionality.\n\nThe solution would clearly involve modifying the redirect in catch_all_view() to preserve the query string when redirecting. This is a straightforward fix with a clear success criterion: the query string should be preserved in the redirected URL.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16614": {
        "explanation": "This issue is well-specified with a clear problem statement and solution requirement. The issue identifies:\n\n1. The exact location of the code in question (django.forms.fields.URLField.to_python)\n2. The current behavior (URLField assumes \"http://\" as the default scheme)\n3. The desired behavior (URLField should assume \"https://\" as the default scheme)\n4. The rationale (HTTPS is the safer option)\n\nThe issue even includes a reference to the specific lines of code that need to be modified. The change required is straightforward - modify the default scheme from \"http\" to \"https\" in the URLField implementation. There's no ambiguity about what needs to be done, and the solution would be easy to verify by testing that entering \"example.com\" in a URLField returns \"https://example.com\" instead of \"http://example.com\".",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16631": {
        "explanation": "This issue is reasonably well-specified. The user has identified a potential bug in Django where the SECRET_KEY_FALLBACKS setting is not being used for session authentication as documented. They've provided:\n\n1. A clear problem statement: After rotating their secret key and setting the old one in SECRET_KEY_FALLBACKS, all users were logged out, suggesting the fallback isn't working for sessions.\n\n2. A reference to the documentation that appears to be incorrect.\n\n3. Their investigation into the source code, specifically noting that the `salted_hmac` function uses SECRET_KEY by default and that `AbstractBaseUser.get_session_auth_hash` doesn't pass a secret argument to it.\n\nWhat makes this not fully \"well-specified\" is that it doesn't include:\n- The exact Django version they're using\n- Complete code examples showing their configuration\n- The exact expected behavior vs. actual behavior\n\nHowever, there's enough information to investigate the issue by examining how Django's session authentication system interacts with SECRET_KEY_FALLBACKS, and to determine if there's a bug in the implementation or documentation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16642": {
        "explanation": "This issue is well-specified. It clearly identifies a problem with Django's FileResponse class not correctly identifying the MIME type for files with \".br\" (Brotli compression) and \".Z\" (Unix compress) extensions. \n\nThe issue provides specific examples of the problem:\n1. `FileResponse(open('test.html.Z', 'rb'))` incorrectly sets content type as text/html\n2. `FileResponse(open('test.html.br', 'rb'))` incorrectly sets content type as text/html\n\nThe expected behavior is implied: FileResponse should recognize these compression formats and set the appropriate MIME types (likely application/x-compress for .Z and application/x-brotli for .br) rather than using the MIME type of the file before compression (html in this case).\n\nThis is a straightforward bug that can be fixed by updating the MIME type detection logic in the FileResponse class to properly handle these compression formats. The solution would involve modifying how Django's FileResponse determines content types for these specific file extensions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16649": {
        "explanation": "This issue is reasonably specified but lacks some concrete details. The core problem is clear: when using Django's QuerySet methods, specifically `annotate()` and `values()` in combination with `union()`, the positioning of columns added with `annotate()` is not controllable with `values()`, which can cause issues when trying to union querysets.\n\nThe issue description identifies:\n1. The specific Django ORM methods involved (`annotate()`, `values()`, and `union()`)\n2. The nature of the problem (column positioning not being controllable)\n3. The impact (potential disruption of union operations)\n\nWhat's missing:\n1. A concrete code example demonstrating the issue\n2. The specific version of Django where this occurs\n3. The expected behavior versus actual behavior\n4. A clear proposal for what documentation change is needed\n\nHowever, a developer familiar with Django's QuerySet API would likely understand the issue and could investigate how annotations are positioned in the SQL query and how this interacts with union operations. The solution would likely involve either documenting this behavior or modifying how Django handles column ordering in these operations.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16661": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in Django's ModelAdmin.lookup_allowed() method when dealing with foreign keys used as primary keys.\n\n2. A complete failing test case is provided that demonstrates the issue, showing exactly how to reproduce the problem.\n\n3. The submitter has included a hypothesis about the root cause: the admin system is confusing a foreign key used as a primary key with concrete inheritance, causing it to incorrectly transform the lookup path.\n\n4. The issue mentions that this worked in Django 1.8 but fails in the current version, providing version context.\n\n5. The problem is clearly defined: when using a OneToOneField as a primary key in a model relationship chain, the lookup_allowed() method incorrectly raises DisallowedModelAdminLookup for valid lookups that are specified in list_filter.\n\nA developer could take this information and:\n1. Run the provided test to confirm the issue\n2. Examine the lookup_allowed() method implementation\n3. Verify the hypothesis about the confusion between foreign key primary keys and concrete inheritance\n4. Implement a fix that correctly handles this case\n\nThe issue contains all the necessary information to understand, reproduce, and fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16662": {
        "explanation": "This issue is well-specified with clear details about the problem and the expected solution:\n\n1. The problem is clearly identified: Django migration files are being generated with imports sorted by module, regardless of import style (mixing `import module` and `from module import objects`).\n\n2. The issue provides specific examples of the current problematic output and the expected correct output.\n\n3. The issue references the Django coding style guidelines that are being violated, specifically that \"import module\" statements should come before \"from module import objects\" statements.\n\n4. It mentions that the current behavior causes isort validation to fail, providing context for why this is important.\n\n5. The issue includes references to related previous issues (#24155, #25384) for additional context.\n\nA developer would have a clear understanding of what needs to be fixed: modify the migration file generation code to ensure imports follow Django's coding style by grouping \"import module\" statements before \"from module import objects\" statements. The solution would involve changing how imports are sorted in the migration file generation process.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16667": {
        "explanation": "This issue is well-specified with clear reproduction steps and a detailed explanation of the problem. The issue describes:\n\n1. The exact code that causes the problem (SelectDateWidget in a form)\n2. Precise reproduction steps with a URL example\n3. The specific error (OverflowError) and its cause (large integers in date constructor)\n4. The vulnerable code path (SelectDateWidget.value_from_datadict)\n5. Technical explanation of why the error occurs (int conversion without bounds checking)\n\nThe solution would clearly involve adding validation or error handling to prevent the OverflowError when processing user input in the SelectDateWidget. This is a common security pattern - validating user input before processing it to prevent crashes or exploits. The fix would likely involve catching the OverflowError and handling it gracefully, or validating the input values before passing them to datetime.date().",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16686": {
        "explanation": "This issue is well-specified and clear. It describes a specific problem with Django's ORM error messages: when a user tries to access an annotated field with a typo in the field name (e.g., \"annotation_typo\" instead of \"annotation\"), the error message doesn't include the annotated field (\"annotation\") in the list of valid choices.\n\nThe issue provides:\n1. A clear code example showing the problematic scenario\n2. The exact error message that's displayed\n3. The expected behavior (implicitly): the error message should include annotated fields in the list of valid choices\n\nA developer would understand exactly what needs to be fixed: modify the error reporting mechanism to include annotated fields in the list of valid choices when a FieldError is raised due to an invalid field name. This would make debugging easier for users who make typos in annotated field names.\n\nThe solution would involve modifying how Django generates the list of valid field choices in error messages to include annotated fields alongside model fields.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16693": {
        "explanation": "This issue is well-specified with a clear problem statement and solution proposal. The issue describes:\n\n1. The specific problem: When a template context processor returns None (or another non-iterable type) instead of a dictionary, the error message is cryptic and unhelpful.\n\n2. A concrete example of the problematic code and the resulting error message.\n\n3. A comparison to how Django handles a similar situation for views, where it provides a more helpful error message.\n\n4. A specific suggestion for improvement: Add a try/except block around the `updates.update(processor(self.request))` line to catch the TypeError and provide a more descriptive error message.\n\n5. A reference to Python 3.11's \"zero-cost\" exception handling to address potential performance concerns.\n\nThe issue provides all the necessary information to understand the problem, locate the relevant code, and implement a solution. The fix would involve adding error handling in the context processor execution to provide a more user-friendly error message when a context processor returns an incorrect type.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16745": {
        "explanation": "This issue is well-specified and provides clear information about the problem:\n\n1. The issue describes a specific inconsistency between client-side and server-side validation for number inputs with both min and step attributes.\n\n2. It clearly explains the expected behavior: when using `min=1` and `step=2`, the client-side validation allows only positive odd numbers (1, 3, 5, etc.).\n\n3. It identifies the specific problem: Django's `StepValueValidator` always uses 0 as the base for step validation, which means it only allows even numbers (0, 2, 4, etc.) when step_size=2.\n\n4. The conflict is clearly articulated: users cannot submit any value because client-side validation requires odd numbers while server-side validation requires even numbers.\n\n5. The issue points to the specific component causing the problem (`StepValueValidator`) and explains how it should be modified (to take into account the `min_value`).\n\nA developer can understand exactly what needs to be fixed: modify `StepValueValidator` to use the `min_value` as the base for step validation instead of always using 0, which would align with browser behavior.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16746": {
        "explanation": "The issue describes a specific problem with the Paginator class: there is no ability to customize or internationalize error messages raised by the Paginator.validate_number method. \n\nWhile the description is brief, it clearly identifies:\n1. The specific class (Paginator)\n2. The specific method (validate_number)\n3. The specific problem (inability to customize error messages)\n4. The implied solution (adding ability to customize these messages)\n\nThe issue is reasonably well-defined for a developer familiar with the codebase. A successful solution would involve modifying the Paginator class to allow for customization of error messages, likely through some parameter or configuration option.\n\nHowever, the issue lacks some details that would make it more complete:\n- It doesn't specify what error messages are currently being raised\n- It doesn't provide examples of how the customization should work\n- It doesn't mention if there are specific internationalization requirements\n\nDespite these missing details, a developer could make reasonable assumptions about the implementation based on common patterns for message customization and internationalization in the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16750": {
        "explanation": "This issue is well-specified with a clear problem and suggested solution:\n\n1. The problem is clearly identified: Using the Chr function with a Value(77) in an annotation raises a ValueError because it's trying to convert 'M' (the character representation of ASCII 77) to an integer.\n\n2. The error message is provided, showing exactly where the problem occurs in the code.\n\n3. A specific solution is suggested: Set output_field = CharField() in the Chr function to prevent the error.\n\nThe issue provides enough information for a developer to:\n- Reproduce the error (using Author.objects.annotate(first_letter=Chr(Value(77))))\n- Understand the root cause (the function is trying to convert a character to an integer)\n- Implement the suggested fix (modify the Chr function to use CharField as its output_field)\n\nThis is a straightforward bug fix with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16757": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is that Django's admin site currently checks and reports an error (admin.E109) when a ManyToManyField is used in list_display, but it doesn't do the same for reversed foreign keys, which leads to a 500 error when the admin page is accessed.\n\nThe issue provides:\n1. Clear description of the current behavior\n2. Detailed reproduction steps using the Django Tutorial's Question and Choice models\n3. The exact code needed to reproduce the issue: `list_display = [\"question_text\", \"choice_set\"]`\n4. Complete error traceback showing what happens\n5. Expected behavior: \"Ideally, using a reversed foreign key would also result in a system check error instead of a 500 response.\"\n\nThe solution would involve adding a system check in Django's admin validation to detect when a reversed foreign key (like 'choice_set') is used in list_display and raise an appropriate error message during system checks, similar to how it already does for ManyToManyFields. This would prevent the 500 error that currently occurs when accessing the admin page.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16786": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug with FilteredRelation when using Coalesce in its conditions\n2. It provides a concrete code example that demonstrates the problem\n3. It includes a reproduction case with a link to a GitHub repository\n4. It shows the SQL query that's being generated, highlighting the missing JOIN\n5. It even provides a workaround solution that the reporter has verified works\n\nThe problem is clearly defined: when using Coalesce with fields that require JOINs inside a FilteredRelation condition, Django fails to add the necessary JOIN to the SQL query, resulting in an \"Unknown column\" error. The issue appears to be that FilteredRelation resolves its conditions too late in the query compilation process.\n\nA developer would have enough information to investigate and fix this issue without needing additional clarification. The reproduction repository provides a test case that demonstrates both the failing scenario and the working workaround.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16801": {
        "explanation": "This issue is well-specified with clear information about the problem and a potential solution. The description identifies:\n\n1. A specific performance issue: ImageField adds a post_init signal handler that consumes significant time (30%) when initializing models after DB fetches.\n2. The root cause: The signal handler is unnecessary when width_field and height_field are not being used.\n3. A proposed solution: Remove the post_init signal handler when these fields aren't set.\n4. Evidence of improvement: The reporter has already tested this change and observed a 30-40% speedup.\n\nThe issue provides enough context for a developer to locate the relevant code in Django's ImageField implementation, understand the performance impact, and implement a solution that conditionally adds the signal handler only when needed. The success criteria is also clear: maintain the same functionality while improving performance by avoiding unnecessary signal handlers.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16816": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific scenario: When adding a non-existent field \"choice\" to the list_display in QuestionAdmin, Django doesn't raise the expected E108 error during validation but instead crashes at runtime with an AttributeError.\n\n2. The error message and stack trace are provided in detail, showing exactly where and how the error occurs.\n\n3. The issue clearly states the expected behavior: error E108 should catch this case during validation rather than allowing it to fail at runtime.\n\n4. The models involved (Question and Choice) are mentioned, and the specific admin configuration that causes the issue is provided.\n\n5. The reporter mentions that there's a GitHub repository for reproducing the issue, though the specific link isn't included.\n\nThe issue is requesting an enhancement to the E108 error checking to catch cases where a field in list_display refers to a related model that isn't properly configured as a method or property. This is a clear, actionable request with enough context to understand and address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16819": {
        "explanation": "The issue description is quite vague and lacks critical details needed for a meaningful solution. It states that we should \"reduce AddIndex/RemoveIndex operations when optimizing migration operations,\" but provides no context about:\n\n1. What system or codebase these operations are part of\n2. Why these operations need to be reduced (performance issues? maintenance concerns?)\n3. What \"optimizing migration operations\" means in this context\n4. What the current behavior is and what the desired behavior should be\n5. Any specific scenarios or examples where this is problematic\n6. Any metrics or criteria to determine success\n\nWithout this information, a developer would need to make many assumptions about what the issue is actually asking for. There's no clear path to a solution because the problem itself isn't clearly defined. The developer would need to request clarification on what specific aspect of the AddIndex/RemoveIndex operations needs improvement and what the expected outcome should be.",
        "label": "VAGUE"
    },
    "django__django-16824": {
        "explanation": "This issue is well-specified with clear information about the problem and a suggested solution. The issue describes:\n\n1. The exact scenario: Using Django's dbshell management command with a DATABASES configuration that uses a PosixPath object for the NAME parameter\n2. The specific error: A TypeError occurs when exiting the dbshell using Ctrl+D\n3. Complete error traceback showing where the error occurs\n4. The root cause: In the error handling code, it's trying to join command arguments with \" \".join(e.cmd), but e.cmd contains a PosixPath object which can't be joined with strings\n5. A clear solution: Convert each item in e.cmd to a string before joining\n\nThe issue provides all the necessary context, including the configuration that triggers the bug, the exact error message, and a straightforward fix. A developer could easily reproduce this issue and implement the suggested fix without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16830": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is precisely described: when using non-abstract model inheritance with custom primary keys and force_insert=True, the force_insert flag is not passed to parent models during save operations.\n\n2. The issue includes a concrete code example showing the model hierarchy and how to reproduce the problem.\n\n3. The current behavior is clearly documented with the exact SQL queries that are generated.\n\n4. The expected behavior is explained: the force_insert flag should be passed to parent models to prevent the unnecessary UPDATE query.\n\n5. The submitter has already implemented a local fix and confirms it works, suggesting a straightforward solution path.\n\n6. The real-world impact is explained: unnecessary UPDATE queries that can cause deadlocks in MySQL/InnoDB when creating models in parallel.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The only minor question raised is about potential intentionality behind the current behavior, but this wouldn't prevent someone from implementing a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16865": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible steps. The reporter identifies that when using Django's `makemigrations --update` command after previously creating a migration with a custom name (via `--name`), the custom name is discarded and replaced with an auto-generated one. The reporter provides exact command examples and the observed behavior.\n\nThe issue has a clear scope: make the `--update` option respect the previously set custom name rather than replacing it with an auto-generated one. The reporter also mentions some additional observations about the `--update` command that could be considered, but these are clearly marked as additional thoughts and don't obscure the main issue.\n\nA developer would be able to:\n1. Reproduce the issue using the provided steps\n2. Understand what the expected behavior should be (preserve the custom name)\n3. Identify where in the codebase to make changes (the Django migration system's handling of the `--update` option)\n\nThe solution would involve modifying the `--update` functionality to preserve custom migration names rather than replacing them with auto-generated ones.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16873": {
        "explanation": "This issue is well-specified with clear details about the bug and expected behavior. The description:\n\n1. Clearly identifies the problem: When using the `join` filter inside an `autoescape off` block, the joining string is still being escaped even though it shouldn't be.\n\n2. Provides a detailed code example that reproduces the issue.\n\n3. Includes test cases that demonstrate both the current behavior and the expected behavior.\n\n4. Shows the actual test failure with specific output that makes it clear what's happening.\n\n5. Implicitly defines what a successful solution would look like: modifying the `join` filter to respect the `autoescape off` directive for the joining string.\n\nThe issue provides all the necessary context to understand the problem and implement a solution without needing additional clarification. A developer could locate the relevant code in Django's template filters and modify the `join` filter implementation to fix this behavior.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16877": {
        "explanation": "This issue is reasonably well-specified. It clearly describes the need for a new template filter called `escapeseq` that would work similarly to how `safeseq` relates to `safe`, but for escaping content. The issue provides a concrete example of usage: `{{ some_list|escapeseq|join:\",\" }}` where each item in the list would be escaped before joining.\n\nThe context is also provided - this filter would be useful when autoescape is off, and it's a follow-up to issue #34574 with input from the security team.\n\nWhat makes this not fully \"well-specified\" is that it doesn't explicitly detail:\n1. The exact escaping behavior expected (though it's likely HTML escaping based on context)\n2. The implementation details or edge cases to handle\n3. Any specific tests that should be written\n\nHowever, an experienced Django developer would understand what's being requested here, as it follows Django's template filter naming patterns and functionality. The implementation would likely mirror `safeseq` but apply escaping instead of marking as safe.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-16888": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: querying for decimal values larger than max_digits crashes with a decimal.InvalidOperation error in Django 4.2, whereas it previously returned ObjectNotFound.\n\n2. The issue includes a complete test case that reproduces the problem, showing the exact model definition and query that causes the issue.\n\n3. A full stack trace is provided, showing exactly where the error occurs in Django's code.\n\n4. The issue suggests two possible solutions: either accept larger values and return \"not found\" (previous behavior), or make the error more descriptive so it can be caught specifically.\n\n5. The issue even mentions when the bug was introduced: in a change related to \"Refs #33308 -- Improved adapting DecimalField values to decimal.\"\n\nWith this information, a developer could understand the problem, reproduce it, and implement a fix without needing additional clarification. The issue is specific to SQLite database backend and involves how Django handles DecimalField values that exceed the field's max_digits parameter.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16899": {
        "explanation": "This issue is well-specified and clear. It describes a specific problem with error messages in Django's ModelAdmin when using readonly_fields. Currently, when there's an error with readonly_fields, the error message only shows the index of the problematic field (e.g., 'readonly_fields[0]') but doesn't include the actual field name, unlike other similar fields like list_editable which do include the field name in their error messages.\n\nThe issue provides:\n1. A clear description of the current behavior (error messages for readonly_fields don't include field names)\n2. A comparison with the expected behavior (other fields like list_editable do include field names)\n3. A concrete example from the test suite showing the current error message format\n4. A clear goal: unify the error message format to include the field name for readonly_fields\n\nA developer would know exactly what needs to be changed - they need to modify the validation code for readonly_fields to include the field name in the error message, similar to how it's done for other fields. This would involve finding the relevant check function in Django's admin validation code and updating the error message format.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16901": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a bug in Django's SQL generation for XOR operations (^) on databases that don't natively support XOR (like PostgreSQL).\n\n2. It precisely describes the expected behavior: XOR operations with multiple arguments should follow parity logic (true when an odd number of arguments are true).\n\n3. It shows the current incorrect behavior: Django's fallback implementation treats multiple XOR operations as \"exactly one argument is true\" instead of following parity logic.\n\n4. The issue includes specific test cases with actual vs. expected results, showing the pattern of what should happen (1, 0, 1, 0, 1) versus what actually happens (1, 0, 0, 0, 0).\n\n5. It references the PR (#29865) that introduced the bug.\n\nA developer would have all the information needed to understand the problem, reproduce it, and implement a fix by correcting the SQL generation for XOR operations on databases without native XOR support to properly implement parity logic.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16903": {
        "explanation": "This issue is well-specified with a clear description of the problem, its root cause, and a proposed solution:\n\n1. The issue describes a specific scenario: diamond inheritance with models that have primary keys with default values causes a duplicate PK error.\n\n2. The submitter has provided:\n   - A concrete example of the inheritance structure causing the issue\n   - A comparison to the documentation example that works differently\n   - The exact error message (django.db.utils.IntegrityError: UNIQUE constraint failed)\n   - The root cause in the Django codebase (in django/db/models/base.py)\n   - The specific code section causing the issue\n   - A detailed explanation of why the issue occurs\n   - A proposed solution with code changes\n\n3. The issue can be reproduced with the information provided, as the submitter has included test cases (both with UUIDField and AutoField with custom default).\n\n4. The submitter has analyzed the Django source code and identified the specific logic that's causing the problem, making it clear what needs to be fixed.\n\nThe proposed solution involves moving a specific check from _save_table to save_base, which is a concrete and testable change. A developer could implement this solution, test it against the described scenario, and verify if it resolves the issue without introducing regressions.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16910": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies the specific Django functionality that's not working correctly: `QuerySet.only()` with `select_related()` on a reverse OneToOneField relation.\n\n2. It provides complete sample models that demonstrate the relationship structure.\n\n3. It includes the exact code that reproduces the issue.\n\n4. It shows the SQL queries generated by both Django 4.2.1 (problematic) and Django 4.1.9 (correct), highlighting the regression.\n\n5. The expected behavior is clear: when using `only()` with `select_related()`, only the specified fields should be included in the SQL query, but in Django 4.2.1, all fields from the related model are being included.\n\nThe issue provides all the necessary information to understand, reproduce, and potentially fix the problem. A developer could use this information to investigate why Django 4.2.1 is generating different SQL than Django 4.1.9 for the same query and implement a fix to restore the correct behavior.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16920": {
        "explanation": "This issue is well-specified with clear requirements for implementation. The issue describes an accessibility problem with Django's default field rendering where error messages and help text are not properly associated with form fields for screen reader users, violating WCAG accessibility guidelines. \n\nThe description:\n1. Clearly identifies the problem (lack of programmatic association between form fields and their errors/help text)\n2. Explains why it's a problem (violates accessibility standards and creates usability issues for screen reader users)\n3. Proposes a specific solution (using aria-describedby attributes)\n4. Provides concrete examples of both the current implementation and the desired implementation\n5. References established accessibility techniques and standards\n6. Includes information about testing with VoiceOver\n\nThe only minor uncertainty is about the implementation detail regarding whether errorlist should be a ul or div, but this is a small implementation detail that can be resolved during development by testing with screen readers. The core requirements are very clear.\n\nA developer would have all the information needed to implement this accessibility improvement without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16938": {
        "explanation": "This issue is well-specified with a clear description of the problem, complete with code examples, error messages, and a reproduction case. The issue describes a specific bug where serialization of many-to-many relations fails when using a custom manager with select_related. The error occurs because Django's serialization process is incompatible with the select_related optimization in the custom manager.\n\nThe issue includes:\n1. A complete code example showing the model definitions with the custom manager\n2. The exact steps to reproduce the issue\n3. The full error traceback\n4. A mention of a workaround (setting a simple default manager)\n5. The specific Django error: \"Field TestTag.master cannot be both deferred and traversed using select_related at the same time\"\n\nA developer would have all the necessary information to understand the problem, reproduce it, and work on a solution. The issue appears to be related to how Django's serialization interacts with select_related in custom managers, and the solution would likely involve modifying how serialization handles models with custom managers that use select_related.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16950": {
        "explanation": "This issue is well-specified with clear details that would allow a developer to reproduce and fix the problem:\n\n1. The reporter provides specific versions (Django 3.1.3, Python 3.8.5)\n2. Complete model definitions are included showing the UUIDModel base class, Thing model, and SubThing model\n3. The admin configuration is fully provided\n4. The exact steps to reproduce are clearly stated: \"When logging into the admin, if you delete all of the entries for 'subthings', add a name, and save the model, it will work. As soon as you try to add a subthing alongside the main Thing, it fails\"\n5. The error message is linked via dpaste\n6. The reporter identifies the specific issue: \"the value of 'id' in the Thing model is being set to null\" when using inlines with UUIDField that has a default value\n\nA developer could easily set up a test project with these exact models and admin configuration to reproduce the issue. The problem appears to be that when using inlines with a model that has a UUIDField with a default value, the default value is not being applied correctly during form submission. This is a specific, reproducible bug with clear expectations for what correct behavior should be.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-16952": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific crash that occurs when creating objects with nested multi-table inheritance (MTI).\n\n2. It provides exact code examples showing:\n   - The model definitions (adding `ItalianRestaurantManyParents` class with diamond inheritance)\n   - The test case that reproduces the crash (`test_create_diamond_mti_common_parents`)\n\n3. It includes the complete error traceback showing exactly where and how it fails:\n   - The error occurs in `_save_parents` method\n   - The specific error is `AttributeError: 'OneToOneField' object has no attribute 'attname'`\n\n4. The context is clear - this is about a bug in Django's ORM when handling diamond inheritance patterns in multi-table inheritance.\n\nA developer would have all the necessary information to:\n1. Reproduce the issue\n2. Understand the inheritance structure causing the problem\n3. Locate the specific code in Django that's failing\n4. Understand what needs to be fixed (the handling of OneToOneField in the _save_parents method)\n\nThe issue provides a complete test case that demonstrates the bug, making it straightforward to verify when a fix is working correctly.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17029": {
        "explanation": "This issue is well-specified with clear information about the problem and a proposed solution:\n\n1. The problem is clearly identified: `apps.clear_cache()` does not clear the cache for `get_swappable_settings_name` which uses LRU caching.\n\n2. The context is provided: This was discovered while working on django-stubs to reset state between mypy runs.\n\n3. Evidence is provided: Links to the relevant code and discussion are included.\n\n4. A specific solution is proposed: Add `self.get_swappable_settings_name.cache_clear()` to the `clear_cache` method.\n\n5. The issue aligns with the documentation expectation that `clear_cache()` should \"Clear all internal caches, for methods that alter the app registry.\"\n\nThe issue provides enough information for a developer to understand the problem, verify it, and implement the proposed solution. The fix appears straightforward - adding a single line of code to clear an additional cache that's currently being missed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17046": {
        "explanation": "This issue is well-specified with clear reproduction steps and error details. The problem occurs when:\n\n1. A Django admin page is set up with a Post model that has a ManyToMany relationship with User\n2. The PostAdmin has search_fields that include a related field (authors__email)\n3. When searching for posts, then selecting and trying to delete them, a 500 error occurs\n4. The specific error is \"TypeError: Cannot call delete() after .distinct()\"\n\nThe issue includes:\n- Complete model definitions\n- Admin configuration\n- Exact steps to reproduce\n- Full error traceback\n- Information about database (Postgres)\n- Version information (works in Django 3.1.8, broken in 3.2.x)\n- A hypothesis about the cause (related to preserving filters on delete)\n\nThis provides all the necessary information to investigate and fix the issue. A developer can reproduce the problem and has a starting point for where to look in the codebase (the changes between Django 3.1.8 and 3.2.x related to preserving filters on delete).",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17058": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: Add support for the SQL feature \"NULLS [NOT] DISTINCT\" to UniqueConstraint in Django.\n2. The context is well explained: PostgreSQL 15 implements this SQL standard feature, and different databases have different default behaviors for how NULL values are treated in unique constraints.\n3. The proposed solution is specific: Add a `nulls_distinct: Optional[bool]=None` parameter to UniqueConstraint that defaults to the backend's behavior but allows explicit setting when supported.\n4. References are provided: A link to an article on the subject, the SQL standard reference (F292), and a forum thread discussing the issue.\n5. There's even a reference to a specific commit (cff59bedc23fd4d53557f677ddc42402b56963d0) that provides additional context about SQL Server's behavior.\n\nA developer would have enough information to implement this feature by:\n1. Adding the new parameter to UniqueConstraint\n2. Modifying the constraint generation code to include the NULLS DISTINCT/NULLS NOT DISTINCT clause when specified\n3. Ensuring it's only used with database backends that support the feature\n\nThe only minor detail not specified is exactly which backends beyond PostgreSQL 15 support this feature, but that could be determined during implementation.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17084": {
        "explanation": "This issue is well-specified with clear information about the problem. The user has:\n\n1. Identified a specific error: \"psycopg2.errors.GroupingError: aggregate function calls cannot contain window function calls\"\n2. Provided the exact versions of all relevant dependencies (Django 4.2.3, psycopg2 2.9.3, PostgreSQL 13.4)\n3. Included a clear code example showing the problematic query\n4. Indicated when the issue started occurring (after upgrading to Django 4.2)\n\nThe issue describes a regression in Django 4.2 where using an aggregate function (Sum) over a window function annotation no longer works, whereas it presumably worked in previous Django versions. The error message from PostgreSQL is clear about what's happening technically - aggregate functions cannot contain window function calls.\n\nA developer would have enough information to reproduce the issue and investigate what changed in Django 4.2 that might be causing this behavior. The solution would likely involve finding a workaround for this query pattern or determining if this is a bug in Django 4.2's ORM that needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-17087": {
        "explanation": "This issue is well-specified with clear details about the problem and expected solution. The issue describes:\n\n1. The exact code structure causing the problem (a nested class with a classmethod being used as a default value for a field)\n2. The specific error that occurs (wrong class path in the migration)\n3. The expected correct behavior (the migration should reference the nested class correctly)\n\nThe issue provides a complete code example showing the model definition and the resulting migration that contains the error. It clearly identifies that the migration is incorrectly referencing \"appname.models.Capability.default\" when it should be \"appname.models.Profile.Capability.default\".\n\nThis is a specific technical issue with Django's handling of nested class methods when used as default values in model fields. The solution would involve fixing how Django generates the import path for these nested class methods in migrations. A developer would have enough information to reproduce the issue and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-7188": {
        "explanation": "The issue is reasonably well-specified. It clearly states the problem: Django projects should be able to use auth machinery (specifically the authentication middleware and auth context processors) without having to include django.contrib.auth in INSTALLED_APPS. \n\nThe issue includes:\n1. A clear goal - allow auth machinery to work without the auth app being installed\n2. Specific components mentioned - AuthenticationMiddleware and auth context processors\n3. A link to a discussion that likely contains more context\n4. A statement about the version (Django 1.9) where this should be fixed\n5. A mention of adding tests to prevent regression\n\nWhat's missing is:\n1. The exact error or limitation currently encountered when trying to use auth machinery without the app\n2. Specific technical details about what changes are needed\n3. Whether there are any intentional dependencies that should be preserved\n\nHowever, an experienced Django developer would be able to investigate the current implementation, understand the dependencies between the auth app and the middleware/context processors, and make appropriate changes to decouple them where possible. The Google Groups discussion link likely provides additional context that would help clarify the issue.\n\nGiven the information provided, a developer could make a reasonable attempt at solving this issue by examining the auth machinery code and removing unnecessary dependencies on django.contrib.auth being in INSTALLED_APPS.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-7475": {
        "explanation": "This issue is well-specified with a clear request and rationale. The user wants to enhance the `diffsettings` management command to allow comparing the current settings to another specified settings module, rather than only comparing to Django's default settings (which is the current behavior).\n\nThe request includes:\n1. A clear description of the current limitation (diffsettings only compares to Django defaults)\n2. A specific use case (hierarchical settings modules)\n3. A concrete example of the desired command syntax (`./manage.py diffsettings --default=settings.base`)\n\nThe implementation would require modifying the diffsettings command to accept an optional parameter that specifies an alternative settings module to use as the baseline for comparison. This is a reasonable and well-defined enhancement request with a clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-7530": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a bug in Django's makemigrations command where the allow_migrate() method in database routers is being called with incorrect (app_label, model) pairs. Specifically, it's calling allow_migrate() for each app with all models in the project, rather than just with the models that belong to that app. This causes problems for database routers that expect valid app/model combinations, particularly in sharded database setups.\n\nThe issue includes:\n1. A clear description of the problem\n2. A reference to a ticket where it was originally reported\n3. A link to a pull request that presumably addresses the issue\n\nWhat's missing is:\n1. Specific code examples showing the incorrect behavior\n2. Details about the expected behavior\n3. Information about which Django version(s) are affected\n\nHowever, with the information provided, a developer familiar with Django's migration system and database routers would likely understand the problem and be able to investigate the code to find where allow_migrate() is being called incorrectly. The referenced PR would also provide additional context for understanding and solving the issue.\n\nGiven that there's enough information to identify where the problem is occurring and what needs to be fixed, this issue is reasonably specified for someone to attempt a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-8119": {
        "explanation": "The issue is asking for the ability to use expressions in Django ORM's `.filter()` calls, specifically to allow for more complex query operations like applying functions to fields before comparison (e.g., `.filter(F('some_field').lower() == 'anssi')`). \n\nWhile the issue provides a clear example of what the feature should look like, it lacks several important details:\n\n1. It doesn't specify which Django version this should be implemented for\n2. There's no information about how this should interact with existing filter functionality\n3. No details on what types of expressions should be supported\n4. No mention of potential backward compatibility concerns\n5. No technical details about how the implementation should work\n\nHowever, the core concept is clear: extend Django's ORM to allow expressions in filter calls, similar to how Django already supports expressions in other contexts. The example provided gives a good indication of the expected syntax and behavior. A Django developer familiar with the ORM would understand what needs to be implemented.\n\nGiven the context of Django's existing expression support in other areas of the ORM, a developer could make reasonable assumptions about how to implement this feature consistently with the rest of the framework.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-8961": {
        "explanation": "This issue is well-specified with clear problems to solve:\n\n1. The issue identifies a specific problem: when using `python -m django help`, the help text incorrectly shows `__main__.py` instead of `python -m django`.\n\n2. It raises a clear question about whether `manage.py` should be replaced with `./manage.py` in help text on non-Windows platforms.\n\n3. It provides context with examples showing the current behavior across different command invocation methods.\n\n4. It suggests a potential solution for the first problem (replacing `__main__.py` with `python -m django` or using `os.path.basename(sys.executable)`).\n\nThe issue is focused on improving the user experience by making the help text accurately reflect the command that should be used. The developer would need to:\n1. Fix the `__main__.py` reference in help text when using the module approach\n2. Decide on the correct representation for manage.py (with or without ./)\n3. Update the relevant code to implement these changes\n\nThe codebase location isn't explicitly mentioned, but an experienced Django developer would know to look at the command-line utility code in Django's core. The issue provides enough information to understand the problem and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-9296": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: The Paginator class currently doesn't implement `__iter__`, requiring users to use `page_range` to iterate through pages.\n2. The proposed solution: Implement the `__iter__` method on the Paginator class with specific code that yields each page object.\n3. The expected behavior: Allow users to iterate through a Paginator object directly using standard Python iteration patterns.\n\nThe issue includes the exact implementation code for the `__iter__` method, showing how it should yield page objects by iterating through `self.page_range` and calling `self.page(page_num)` for each page number.\n\nThis is a straightforward enhancement request with a clear implementation path. A developer could implement this solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "django__django-9703": {
        "explanation": "The issue is reasonably specified. The user wants to enhance Django's management command system to suggest similar commands when a user mistypes a command name. The description mentions that the user often remembers part of a command but not the full name, and they want a feature that would suggest the correct command in such cases.\n\nThe user also mentions they've attached a screenshot and a patch, which would provide more context, but these attachments aren't visible in the issue description provided to me. However, the core request is clear enough - implement a \"did you mean?\" feature for Django management commands that would suggest similar commands when a user mistypes a command name.\n\nThis is a common feature in command-line tools (like Git's \"did you mean?\" suggestions), so there are established patterns to follow. The implementation would likely involve:\n\n1. When a command is not found, calculate string similarity between the entered command and available commands\n2. If any commands are sufficiently similar, suggest them to the user\n3. Display these suggestions in a helpful format\n\nWhile some details about the exact implementation are missing (like the similarity threshold, formatting of suggestions, etc.), these are reasonable design decisions that can be made during implementation based on common practices.",
        "label": "REASONABLY_SPECIFIED"
    },
    "django__django-9871": {
        "explanation": "The issue is reasonably specified but has some missing details. It clearly describes a problem with the current Django management command help output, where command-specific arguments appear at the end after boilerplate text. The issue proposes reordering the options to show command-specific information first.\n\nWhat's missing:\n1. The actual highlighted yellow text mentioned in the description is not visible in the provided issue.\n2. The proposed output format that \"looks like the following\" is also missing.\n3. No specific code references are provided for where this change should be made.\n\nHowever, despite these missing elements, the core problem and desired solution are clear enough. The issue links to a discussion on django-developers which likely contains additional context. For an experienced Django developer familiar with the management command system, this would be enough information to locate the relevant code and implement a solution that reorders the help output to prioritize command-specific arguments.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-13859": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information. The bug report describes a specific problem: when creating a figure with zero width (using `figsize=(3, 0)`) and attempting to save it as a PNG file, libpng crashes with a \"libpng signaled error\" message. The reporter has provided:\n\n1. A complete, minimal code example that reproduces the issue\n2. The exact error message received\n3. Environment details including OS (Ubuntu), package manager (conda), and versions of relevant libraries (Matplotlib 3.0.2/3.0.3 and libpng 1.6.35/1.6.36)\n\nThe issue is clearly about handling zero-width figures in Matplotlib when saving to PNG format. A developer would understand that the solution likely involves adding validation to prevent zero-width figures from being passed to libpng, or handling this edge case more gracefully. The expected behavior isn't explicitly stated, but it's reasonable to infer that Matplotlib should either prevent creation of zero-width figures or handle them gracefully without crashing.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13908": {
        "explanation": "This issue is well-specified with clear information about what the problem is and how to reproduce it. The user has provided:\n\n1. A complete, runnable code example that demonstrates the bug\n2. Clear screenshots showing both the current (problematic) behavior and the expected behavior\n3. Specific details about what's wrong: minor ticklabels at '00:00' hours (which coincide with major ticks) are missing in the current version but were present in version 3.0.2\n4. Environment information including OS, Python version, and Matplotlib version\n\nThe issue is specifically about a regression in Matplotlib where minor tick labels that coincide with major ticks (specifically the '00:00' hour labels) are no longer displayed. The solution would need to ensure that minor ticklabels are shown even at positions where major ticks exist.\n\nThis is a focused, well-defined bug with clear reproduction steps and expected outcome, making it straightforward to understand what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13913": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual behavior. The bug report describes a specific inconsistency in the `matplotlib.colors.is_color_like` function, which returns `True` for strings containing integers even though these strings cannot be used as valid colors in functions like `plt.scatter`. \n\nThe issue includes:\n1. A complete code snippet that reproduces the bug\n2. The exact error message received\n3. The expected behavior (either `scatter` should accept these values or `is_color_like` should return `False`)\n4. Detailed version information for all relevant components\n5. Information about platform differences (works on Windows with different versions)\n\nThe problem is clearly defined: there's an inconsistency between what `is_color_like` considers a valid color and what the plotting functions actually accept. A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13959": {
        "explanation": "This issue is well-specified with clear examples demonstrating the inconsistent behavior. The problem is that matplotlib's scatter() function handles the shape of parameter 'c' differently than it handles 'x' and 'y'. Specifically:\n\n1. For 'x' and 'y', the function accepts arrays of any shape as long as they have the same size (number of elements), and implicitly flattens them.\n2. For 'c', the function requires that the shape matches either 'x' or 'y' exactly, not just having the same number of elements.\n\nThe issue provides concrete examples showing what works and what doesn't work, along with the error message. The inconsistency is clearly demonstrated:\n- Arrays with different shapes but same size work for x and y\n- For c, only arrays with shapes matching x or y work, even if they have the same number of elements\n- Even a flat array with the correct number of elements fails for c\n\nThis is a well-defined bug with reproducible examples and a clear expectation: the 'c' parameter should be handled consistently with 'x' and 'y', allowing any shape as long as the size matches.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13980": {
        "explanation": "This issue is well-specified with clear information about the problem and expected behavior. The user is reporting that when creating polar plots, matplotlib automatically sets a negative minimum value for the radial scale, which creates a confusing \"hole\" in the center of the plot. The user expects the default behavior to set the minimum radial value to 0, placing the origin at the center of the plot.\n\nThe issue includes:\n1. A complete, reproducible code example\n2. The actual output (including the unexpected y-limits that are negative)\n3. A visual comparison showing the expected vs. unexpected behavior\n4. A clear explanation of why this is problematic for users\n5. Environment details (OS, matplotlib version, backend, Python version)\n\nThe solution would involve modifying matplotlib's default behavior for polar plots to set the minimum radial value to 0 by default, or at least providing a clear way to control this behavior. This is a well-defined problem with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13983": {
        "explanation": "This issue is well-specified with clear details about the bug:\n\n1. The problem is clearly described: removing a shared axis with `remove()` prevents the remaining axes from using unit-provided formatters, while using `fig.delaxes()` works correctly.\n\n2. A complete, minimal reproducible example is provided that demonstrates the issue.\n\n3. The output showing the bug is included as a screenshot.\n\n4. The reporter has done investigation and identified the likely source of the problem in the `_remove_ax` method, specifically explaining how the calls to `set_major_formatter` are causing Matplotlib to incorrectly believe there's a user-provided formatter.\n\n5. The version information is provided (mpl master, ca. 3.0.2).\n\nA developer would have all the necessary information to understand, reproduce, and fix this issue without needing additional clarification. The issue is about fixing the `remove()` method so that it doesn't interfere with the unit-provided formatters on shared axes.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13984": {
        "explanation": "This issue is well-specified with clear information about what's not working and what the expected behavior should be. The description includes:\n\n1. A clear problem statement: tick mark colors cannot be set on Axes3D using the standard tick_params method\n2. A complete code example that reproduces the issue\n3. A screenshot showing the current behavior (tick labels are colored but tick marks are not)\n4. Reference to the fact that this used to work in version 1.3.1\n5. Mention of existing workarounds on StackOverflow\n\nThe issue is asking for the ability to change the color of tick marks in 3D plots using the standard tick_params method, which works for 2D plots but not for 3D plots. The solution would involve fixing this inconsistency in the API.\n\nA developer would have enough information to investigate and implement a fix without needing additional clarification. The expected behavior is clear - tick marks should change color when using tick_params, just like they do in 2D plots.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-13989": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A concise description of the problem: `hist()` function doesn't respect the `range` parameter when `density=True`\n2. A minimal code example that reproduces the issue\n3. The actual output showing bins that don't match the specified range\n4. The expected output (bins should start at 0 and end at 1)\n5. Information that the bug doesn't occur when `density=False`\n6. A bisect to the specific commit that introduced the bug\n7. Complete environment details including OS, Matplotlib version, Python version, and other relevant libraries\n\nThe issue is specific enough that a developer could immediately understand the problem, reproduce it, and begin investigating the code changes in the identified commit that caused the regression. The expected behavior is clearly defined - when specifying `range=(0, 1)` with `density=True`, the histogram bins should respect that range.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-14471": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific regression between Matplotlib v3.0.3 and v3.1.0 where calling `plt.figure()` now destroys existing subclassed FigureCanvasQT objects.\n\n2. The reporter provides exact reproduction steps using the nexpy package, with clear commands to run in the IPython shell.\n\n3. Both the actual outcome (all non-pyplot windows are destroyed) and expected outcome (all windows should coexist) are clearly stated.\n\n4. The reporter has done preliminary debugging and identified that the issue occurs during `FigureCanvasQT.__init__(figure)` but not during `FigureCanvasBase.__init__(figure)`.\n\n5. All relevant version information is provided (Matplotlib 3.1.0, Qt5Agg backend, Python 3.7.2, Mac OS 10.14.5).\n\n6. The reporter has a hypothesis about what might be happening (subclassed canvases not being registered somewhere).\n\nA developer would have enough information to reproduce the issue and investigate the changes between v3.0.3 and v3.1.0 that affect how FigureCanvasQT objects are managed, particularly focusing on the initialization process and how figures are tracked in the pyplot interface.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-14623": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual behavior. The bug report includes:\n\n1. A clear description of the problem: inverting a log axis using its limits no longer works in matplotlib 3.1.0\n2. Complete reproduction code that demonstrates the issue\n3. Screenshots showing the actual outcome for both linear and log scales\n4. A clear statement of the expected outcome: the y-axis should be inverted for both linear and log scales\n5. Version information for the relevant software\n\nThe issue is specific enough that a developer could immediately understand the problem, reproduce it, and begin working on a fix. The expected behavior is clearly defined (the log scale axis should invert when setting limits from max to min, just like the linear scale does). There's no ambiguity about what constitutes a successful solution - the fix would make the log scale axis invert properly when limits are set from max to min.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-17810": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code example, error message, and environment details. The problem is clearly defined: when trying to save both an animation and a figure in the same script, a StopIteration error occurs. The user has provided:\n\n1. A complete, minimal code example that reproduces the issue\n2. The exact error traceback\n3. Information that each part works fine in isolation\n4. Complete environment details (OS, Python version, Matplotlib version and backend)\n\nThe issue appears to be related to how the animation's frame iterator is being consumed when both animation.save() and fig.savefig() are called in the same script. The use of iter(range(10)) as the frames parameter and the StopIteration error suggest that the iterator is being exhausted prematurely.\n\nA developer would have enough information to reproduce the issue and investigate a solution, likely focusing on how to prevent the animation's iterator from being fully consumed or how to reset it between operations.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-18869": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solutions. The author wants to add a more easily comparable version information structure to matplotlib's top level, beyond the existing `__version__` string.\n\nThe problem is clearly articulated: string comparisons of version numbers can be problematic when version numbers reach 3.10 or 4.10 (as \"3.2\" > \"3.10\" in string comparison). The author proposes two specific solutions:\n1. Add a `version_info` tuple (similar to Python's sys.version_info)\n2. Add a `LooseVersion` instance that can properly compare version strings\n\nBoth solutions are common patterns in other packages, making this a standard and well-understood feature request. The implementation would be straightforward - either creating a tuple representation of the version or using the existing `LooseVersion` class from distutils.\n\nThe issue provides enough context for a developer to implement a solution without needing further clarification. The only \"bikeshedding\" mentioned is which of the two well-defined approaches to take, but either would satisfy the requirement.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-19743": {
        "explanation": "The issue is requesting a feature to add constrained_layout support for figure.legend in Matplotlib. While the request is brief, it is clear what functionality is being requested: the ability for the constrained_layout manager to properly handle figure legends.\n\nFor context, constrained_layout is a layout manager in Matplotlib that automatically adjusts subplot positions to make room for labels and titles, but apparently it doesn't currently handle figure legends properly. A figure legend (as opposed to an axes legend) is placed at the figure level and is shared across multiple subplots.\n\nThe request is straightforward in terms of what needs to be implemented, though it doesn't provide details on the specific behavior expected or edge cases to consider. However, an experienced developer familiar with Matplotlib would understand what needs to be done: modify the constrained_layout algorithm to account for figure legends when calculating space requirements and positioning elements.\n\nThe issue could benefit from more details about specific use cases or expected behavior, but the core request is clear enough for a developer to begin implementing a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-19763": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual behavior. The bug report includes:\n\n1. A clear description of the problem: multicursor disappears when not moving the mouse in the nbagg backend, and causes high CPU usage\n2. A complete, minimal code example to reproduce the issue\n3. Visual evidence (GIF) showing the problem\n4. Expected outcome: red line should remain visible and CPU usage should be normal\n5. Detailed environment information including OS, Matplotlib version, backend, Python version, and Jupyter version\n\nThe issue is specific to the nbagg backend (works fine in qt backend) and relates to the MultiCursor widget with useblit=False. The reporter has provided all necessary information to understand and attempt to fix the issue. The problem is reproducible with the given code snippet, and the expected behavior is clearly stated.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20374": {
        "explanation": "This issue is very well-specified with clear reproduction steps, expected outcomes, and a detailed explanation of the problem. The reporter has:\n\n1. Provided a complete minimal reproducible example with all necessary files (conf.py, index.rst, a.rst, b.rst)\n2. Included clear step-by-step instructions to reproduce the issue\n3. Explained the actual outcome with error messages\n4. Described the expected outcome\n5. Provided a technical analysis of why the issue occurs\n6. Suggested a potential solution approach\n\nThe issue relates to Matplotlib's plot_directive and how it handles context variables when files are included via RST include directives. The problem occurs because plot_directive's out_of_date() function doesn't consider a file out of date when an included file changes, causing context variables to be lost.\n\nThe suggested solution is clear: make plot directives with :context: option always be considered out of date when the file is recompiled, regardless of file modification times. This would ensure that context-dependent plots are always regenerated properly.\n\nA developer would have all the information needed to implement a fix based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20470": {
        "explanation": "This issue is well-specified with clear information about what the problem is and what the expected behavior should be. The bug report includes:\n\n1. A clear description of the bug: Text objects accept a `label` parameter but the label is not being added to the legend.\n2. Complete reproducible code that demonstrates the issue.\n3. A screenshot showing the actual outcome (only the line appears in the legend, not the text).\n4. A clear statement of the expected outcome (a legend entry for the text should appear).\n5. The specific Matplotlib version where this occurs (3.3.3).\n\nThe issue is straightforward: when creating a Text object with a label parameter, that label should appear in the legend just like it does for other plot elements like lines. This is a reasonable expectation for consistent behavior across Matplotlib objects. A developer would have enough information to investigate why Text objects aren't being added to the legend despite having a label parameter, and implement a fix to ensure they are properly included.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20488": {
        "explanation": "This issue is reasonably well-specified. It provides:\n\n1. A specific failing test: `test_huge_range_log` in `lib/matplotlib/tests/test_image.py`\n2. The error message and stack trace showing a ValueError with \"Invalid vmin or vmax\"\n3. The context that the reporter cannot reproduce it locally but suspects a NumPy change\n4. A reference to the PR (#18458) where the test was introduced\n\nThe issue provides enough information to understand what's happening:\n- The LogNorm transformation is failing when transforming vmin/vmax values\n- The error occurs in the CI but not locally for the reporter\n- The error is intermittent (\"failing quite a few of the CI runs\")\n\nWhat's missing but can be reasonably inferred:\n- The exact conditions that trigger the failure (but we can examine the test code)\n- The specific NumPy version difference between CI and local environment\n- The exact fix needed (but this would be determined during investigation)\n\nA developer could investigate this by:\n1. Examining the test code in question\n2. Checking the NumPy versions in CI vs local\n3. Understanding why the LogNorm transformation is failing with certain values\n4. Implementing a fix to handle the edge case properly\n\nThis is enough information to start a meaningful investigation and solution attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-20518": {
        "explanation": "This issue is well-specified with clear information about what's not working:\n\n1. The bug is clearly defined: sketch parameters set via `set_sketch_params()` are being ignored by the PGF backend.\n2. A complete, minimal reproduction script is provided.\n3. Visual examples of both the actual (incorrect) and expected (correct) outcomes are included as screenshots.\n4. The Matplotlib version is specified (3.0.2).\n5. The user even mentions they're working on a fix, indicating they understand the problem well.\n\nThe issue provides all necessary information to understand the problem and work on a solution. A developer would need to investigate why the PGF backend isn't properly handling the sketch parameters that are working in other backends, and implement the missing functionality in the PGF backend code.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20584": {
        "explanation": "This issue is well-specified with a clear problem statement, a minimal working example (MWE), and visual evidence of the issue. The user has:\n\n1. Clearly described the problem: When retrieving contour line segments with `get_segments()` and setting them back with `set_segments()`, the lines become coarser/less smooth.\n\n2. Provided a complete, reproducible code example that demonstrates the issue.\n\n3. Included before/after screenshots showing the visual difference.\n\n4. Specified the matplotlib version (3.4.2) where this occurs.\n\nThe issue is focused on a specific API behavior that doesn't match expectations - the user expects that `set_segments(get_segments())` should be a no-op (not change anything), but it's clearly changing the visual appearance of the contour lines.\n\nA developer would have all the information needed to reproduce the issue, investigate why this is happening in the matplotlib codebase, and implement a fix. The solution would likely involve understanding why the segments are being simplified or modified during this get/set operation.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20676": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A complete, minimal code example that reproduces the issue\n2. Clear description of the actual outcome (axes xlimits are expanded to include x=0)\n3. Clear description of the expected outcome (axes xlimits should remain at (10, 20) + margins)\n4. Reference to previous behavior in Matplotlib 3.4\n5. Environment details including Matplotlib version, backend, Python version, and OS\n\nThe issue is specifically about the SpanSelector widget with interactive=True incorrectly forcing axes limits to include 0, when it should maintain the original limits. This is a regression from previous behavior. The problem is clearly defined and a developer would have all the necessary information to investigate and fix the issue.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20679": {
        "explanation": "This issue is well-specified with clear details about what's happening and what should happen instead:\n\n1. The problem is clearly defined: When using a SpanSelector with `interactive=True` (which replaced `span_stays=True`), clicking and immediately releasing no longer triggers the `onselect` callback, whereas it did with the old implementation.\n\n2. A minimal reproduction code is provided that demonstrates the issue.\n\n3. The expected behavior is explicitly stated: The click-release action should report an event where `vmin = vmax = clicked position`.\n\n4. The actual behavior is clearly described: Only click-drag events are reported, but click-release events are not.\n\n5. The environment details are provided, including the Matplotlib version and backend.\n\nThe issue provides all the necessary information to understand the problem and what a successful solution would look like: modifying the SpanSelector implementation to ensure that \"empty\" clicks (where the span has zero width because it's a click-release) still trigger the onselect callback when using `interactive=True`, maintaining backward compatibility with the previous behavior.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20693": {
        "explanation": "This issue is reasonably well-specified. The user is reporting a bug where updating selector colors in MNE (a Python library that uses Matplotlib) now emits a deprecation warning after changes in Matplotlib issue #20113. The user clearly describes:\n\n1. The current code that's causing the warning: `selector.rect.set_color(color)` and `selector.rectprops.update(dict(facecolor=color))`\n2. The fact that the deprecation warning doesn't provide migration guidance\n3. A potential workaround they've tried: `selector.artists[0].set_color(color)`\n4. Their concern that the workaround might not be the proper solution\n\nThe issue links to the relevant Matplotlib issues (#20113 and #20558) that caused this change. The user is asking for the correct way to update their code to avoid the deprecation warning while maintaining the same functionality.\n\nWhile the issue doesn't provide the exact deprecation message text or the full context of the MNE codebase, it provides enough information for someone familiar with Matplotlib's selector objects to understand the problem and suggest a proper solution. The solution would involve identifying the new recommended way to update selector colors in Matplotlib after the changes in issue #20113.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-20805": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a complete, runnable code example that demonstrates the issue\n2. Clearly described the problem: when `ax.spines[\"top\"].set_position((\"axes\", 1.05))` is placed after `tick_params`, the `labelrotation` parameter is ignored\n3. Specified the expected outcome: labels should rotate regardless of where the spine command is placed\n4. Included relevant environment information (Matplotlib version, Python version, OS)\n\nThe issue appears to be a potential bug in how spine positioning interacts with tick label rotation. The user has commented out different placements of the spine command to demonstrate the issue. This is a specific, reproducible problem with a clear definition of what would constitute a successful fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20826": {
        "explanation": "This issue is well-specified with clear details that would allow a developer to address it effectively:\n\n1. The bug is clearly described: calling `ax.clear()` on shared axes causes ticks and tick labels to be shown that should be hidden.\n\n2. A minimal, complete reproduction code is provided that demonstrates the issue.\n\n3. The expected and actual outcomes are clearly documented with screenshots showing the difference between matplotlib 3.4.1 (working correctly) and 3.4.2 (showing the bug).\n\n4. The environment details are comprehensive, including OS, matplotlib version, backend, Python version, and Jupyter version.\n\n5. The issue identifies that this is a regression between specific versions (3.4.1 and 3.4.2).\n\n6. The reporter has done troubleshooting to identify that removing the `ax.clear()` call resolves the issue, pinpointing the problematic function.\n\nA developer would have all the necessary information to reproduce the issue, understand what's wrong, and investigate what changed between the two matplotlib versions to cause this regression.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-20859": {
        "explanation": "This issue is well-specified with clear information about the bug, reproduction steps, and a proposed solution:\n\n1. The problem is clearly stated: Adding a legend to a `SubFigure` doesn't work\n2. A minimal reproducible example is provided that demonstrates the error\n3. The actual error message is included (TypeError: Legend needs either Axes or Figure as parent)\n4. The expected behavior is described (legend should work on subfigures)\n5. The reporter even identified the root cause and suggested a specific code change (changing L437 to check against `FigureBase` instead of just `Figure`)\n6. All relevant version information is provided (Matplotlib 3.4.2, Python 3.8.3, macOS 11.4)\n\nThe issue has a clear scope, the problem is well-defined, and there's even a suggested implementation path. A developer could immediately start working on this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21042": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A clear description of the problem: updating RC params produces deprecation warnings\n2. A minimal code example that reproduces the issue\n3. The exact error messages/warnings being generated\n4. The expected outcome: Matplotlib should not load deprecated rcParams\n5. Complete environment information including OS, Matplotlib version, Python version, and backend\n\nThe issue is specifically about how Matplotlib handles deprecated rcParams when using `mpl.rcParams.update(rc)` with parameters obtained from `mpl.rc_params()`. The user even points to a potential source of the issue in the codebase (the `_validators` dictionary in matplotlib.rcsetup.py).\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification. The solution would likely involve filtering out deprecated parameters when updating rcParams or modifying how rc_params() works to exclude deprecated parameters.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21318": {
        "explanation": "This issue is well-specified with clear information for a meaningful solution attempt:\n\n1. The bug is clearly described: A RuntimeError occurs when using both `sharex=True, sharey=True` and `axis('equal')` together in Matplotlib 2.2.2, while it works in version 2.0.2.\n\n2. Complete reproduction code is provided, which is concise and demonstrates the exact problem.\n\n3. The exact error message and traceback are included, pointing to the specific error: \"adjustable='datalim' is not allowed when both axes are shared.\"\n\n4. The expected outcome is clearly stated: the figure should be drawn without errors, as it does in Matplotlib 2.0.2.\n\n5. Environment details are comprehensive, including OS, Matplotlib version, backend, and Python version.\n\nThe issue appears to be a regression in Matplotlib 2.2.2 that prevents using axis('equal') with shared axes. A developer would have enough information to investigate the root cause in the Matplotlib codebase, specifically looking at how aspect ratio adjustments interact with shared axes in version 2.2.2 compared to 2.0.2.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21481": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A clear description of the problem: subfigures break when using certain gridspec slices with constrained_layout=True\n2. A minimal code example that reproduces the issue\n3. The complete error traceback showing where the problem occurs\n4. The expected outcome (no error, same behavior as with constrained_layout=False)\n5. Environment details (OS, Matplotlib version, backend, Python version, installation method)\n\nThe error appears to be in the constrained layout manager when handling certain gridspec slices for subfigures. The traceback points to a shape mismatch in the _layoutgrid.py file, specifically when broadcasting arrays with shapes (3,) and (2,). This suggests an issue with how the layout grid handles the dimensions of the gridspec slice.\n\nA developer would have enough information to reproduce the issue and understand where to look in the codebase to fix it. The problem is in the constrained layout implementation's handling of gridspec slices for subfigures.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21490": {
        "explanation": "This issue is well-specified with a clear problem description and reproduction steps. The bug is that Line2D doesn't copy its input arrays, which can lead to unexpected behavior when those arrays are modified in-place after creating the Line2D object. The issue includes:\n\n1. A clear explanation of the bug and its impact\n2. Complete code to reproduce the issue\n3. Screenshots showing the actual outcome\n4. A comparison with AxesImage which handles this situation correctly\n5. A clear expected outcome: modifying the input array after creating the Line2D should not affect the drawn line\n\nThe solution would be straightforward: modify Line2D to make a copy of its input arrays rather than using them directly. This would make Line2D's behavior consistent with other Matplotlib objects like AxesImage.\n\nAll necessary context is provided, including the matplotlib version, backend, and Python version. The issue even references a previous PR (#736) where this behavior was discussed.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21542": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: Colorbar uses old-style format strings while normal axes use new-style format strings (introduced in PR #16715)\n2. The location of the code that needs to be changed (with specific GitHub links)\n3. A proposed solution: Remove the if statement in the colorbar code to use the default logic from Axis\n4. Consideration of backward compatibility: Mentions that this change would require a deprecation cycle\n\nThe issue provides enough context for a developer familiar with matplotlib to understand what needs to be done. The only aspects that could use more clarification are:\n- The exact difference between old-style and new-style format strings\n- The specific behavior changes that users would experience\n- Details on how the deprecation cycle should be implemented\n\nHowever, these details could reasonably be determined by examining the referenced code and the previous PR (#16715). A developer with matplotlib knowledge would be able to make a meaningful attempt at solving this issue.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-21550": {
        "explanation": "This issue is well-specified with clear information about the bug:\n\n1. The problem is clearly defined: A NetworkX graph visualization example that works in matplotlib 3.4.3 doesn't display correctly in matplotlib 3.5.0 (development version).\n\n2. Complete reproduction code is provided, showing a Jupyter notebook with interactive NetworkX graph visualization.\n\n3. Clear visual evidence is provided with screenshots showing both the actual (broken) outcome and the expected outcome.\n\n4. Detailed environment information is included:\n   - Operating system (Windows 10)\n   - Matplotlib version (3.5.0.dev2445+gb09aad279b)\n   - Python version (3.9.7)\n   - Jupyter version (3.2.1)\n   - Backend information (module://matplotlib_inline.backend_inline)\n   - Comprehensive list of installed packages\n\nThe issue shows a regression in matplotlib's handling of NetworkX graph visualization between versions 3.4.3 and 3.5.0. The screenshots clearly show that in the newer version, the graph nodes and edges are not being displayed properly (they appear to be missing or not rendering correctly).\n\nA developer would have all the necessary information to reproduce the issue and investigate what changed in matplotlib 3.5.0 that broke compatibility with NetworkX graph visualization.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21559": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A clear description of the problem: `eventplot` cannot handle multiple datetime-based series\n2. Complete reproduction code showing the exact issue\n3. The full error traceback showing where the failure occurs\n4. The expected outcome: \"An eventplot with one time series per 'row' in the y-axis\"\n5. Environment details including OS, Matplotlib version, Python version, and installation method\n\nThe error is clearly identified: when trying to plot multiple series of datetime.date objects using eventplot, a TypeError occurs because the function cannot handle datetime.date objects. The issue references a previous bug (#5248) that was supposedly fixed but is still occurring in the reporter's environment.\n\nThe solution would need to modify the eventplot functionality to properly handle datetime.date objects when multiple series are provided. The error specifically occurs in the collections.py file where it's trying to sort the positions but can't handle datetime.date objects.\n\nWith this information, a developer could locate the relevant code, understand the issue, and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21568": {
        "explanation": "This issue is well-specified with clear information about the bug:\n\n1. The problem is clearly defined: datetime axis spacing is unclear when using usetex=True in matplotlib 3.4 compared to 3.3.\n\n2. Complete reproduction code is provided that demonstrates the issue.\n\n3. Visual evidence is included with screenshots showing the difference between versions 3.3.4 and 3.4.3.\n\n4. The expected outcome is clearly stated: to have the spacing from version 3.3 while maintaining tex formatting.\n\n5. Environment details are provided (Windows, matplotlib 3.4.3, conda installation from conda-forge).\n\nA developer would be able to reproduce this issue and understand what needs to be fixed - restoring the datetime axis spacing behavior from matplotlib 3.3 while maintaining tex support in 3.4. The issue provides all necessary information to begin investigating and implementing a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-21617": {
        "explanation": "This issue is well-specified with clear information for reproducing and solving the problem:\n\n1. Clear bug description: Text (labels and title) is missing when saving figures to EPS format in matplotlib 3.4.3, but works correctly in 3.3.4.\n\n2. Complete reproduction steps:\n   - Exact environment setup instructions using conda\n   - Complete, minimal code example that demonstrates the issue\n   - Clear instructions on how to run the code\n\n3. Clear expected vs. actual outcome:\n   - Screenshots showing the difference between working and non-working versions\n   - Specific description of what's missing (the title text)\n\n4. Comprehensive environment details:\n   - Operating system\n   - Matplotlib version and backend\n   - Python version\n   - Related libraries (cartopy)\n   - Installation method\n\n5. The issue is specific to a particular combination of:\n   - Matplotlib version (3.4.3)\n   - Cartopy usage with polar projection\n   - EPS output format\n   - Specific gridline spacing (30 degrees vs 40 degrees)\n\nThe issue appears to be a regression in matplotlib 3.4.3 that affects text rendering in EPS files when using certain parameters with cartopy. A developer would have all the necessary information to investigate and fix this bug without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22719": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected outcomes, and detailed error information. The reporter has:\n\n1. Provided a minimal code example that reproduces the issue\n2. Included complete error tracebacks\n3. Demonstrated that the suggested solution in the warning message doesn't work\n4. Explained their understanding of the problem (a deprecation warning that appears when passing empty data to an axis with category units)\n5. Specified what they expect to happen instead\n6. Included the Matplotlib version (3.5.1)\n\nThe issue appears to be that when empty data is passed to an axis with category units, a deprecation warning is triggered that suggests using `Axis.convert_units`, but using that method still produces the same error. The reporter has analyzed the problem in depth, noting that the warning seems to be triggered incorrectly for empty data structures.\n\nA developer would have enough information to investigate the issue in the codebase, particularly in the unit conversion system and how it handles empty arrays. The problem is clearly in the interaction between empty data and category unit converters, with specific files and functions identified in the traceback.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22767": {
        "explanation": "This issue is well-specified with clear information about the bug and how to fix it. The bug description identifies the exact line number (1377) in contour.py where the problem exists and provides the specific correction needed: changing \"indices = range(len(self.levels))\" to \"indices = range(len(self.layers))\".\n\nThe issue includes:\n1. A complete code snippet to reproduce the error\n2. The exact error message received (IndexError: list index out of range)\n3. The expected output when the bug is fixed\n4. Environment details (OS, Python version, Matplotlib version)\n\nThe root cause is clearly identified - the function is using self.levels instead of self.layers when setting up the default indices, causing an index out of range error. This is a straightforward bug fix that requires changing a single line of code in the specified file. A developer could implement this fix without needing any additional information.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22835": {
        "explanation": "This issue is well-specified with clear reproduction steps and error messages. The bug occurs when using a BoundaryNorm with imshow and hovering over the image, which crashes with a specific error message because BoundaryNorm is not invertible. The issue description includes:\n\n1. Complete code to reproduce the issue\n2. The exact error message and traceback\n3. The version of Matplotlib where the issue occurs (3.5.0)\n4. The root cause: BoundaryNorm's inverse method raises a ValueError because it's not invertible\n5. Potential solution approaches are even suggested: adding a special check, using try-except, or making BoundaryNorm approximately invertible\n\nA developer would have all the necessary information to understand the problem, locate the relevant code in the codebase, and implement one of the suggested fixes or another appropriate solution to prevent the crash when hovering over an image with BoundaryNorm.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22865": {
        "explanation": "This issue is well-specified with clear reproduction steps and visual examples of both the actual and expected outcomes. The bug is precisely described: when creating a matplotlib colorbar with both `drawedges=True` and `extend='both'`, the black edge lines at the extremities of the colorbar are not being drawn. The reproduction code is complete and can be run to observe the issue. The expected outcome is clearly illustrated with an image showing how the colorbar should appear with edge lines at all boundaries including the extremities. The matplotlib version (3.5.1) is provided, which is important for reproducing and fixing the issue. This is a specific, technical bug with a clear definition of what constitutes a successful fix: ensuring that edge lines appear at the extremities of the colorbar when both `drawedges=True` and `extend='both'` are set.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22871": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected outcome, and actual outcome. The bug is clearly described: when plotting data spanning less than a year and January is not included in the x-axis, the ConciseDateFormatter doesn't display the year anywhere. The user provides:\n\n1. Complete reproducible code example\n2. A screenshot showing the actual outcome\n3. A clear description of the expected outcome (year \"2021\" should show in the offset to the right of the x-axis)\n4. Environment details (OS, Python version, Matplotlib version, backend, etc.)\n\nThe issue is specific enough that a developer could understand the problem, reproduce it, and work on a fix without needing additional information. The user even distinguishes this from a previously fixed bug (#21670), showing they've done their research.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22883": {
        "explanation": "This issue is well-specified. It clearly identifies a specific problem: there are strings in the codebase that appear to be intended as f-strings (for variable interpolation) but are missing the 'f' prefix, which means the variables won't be interpolated as expected. \n\nThe issue provides:\n1. A clear description of the problem\n2. A specific example with a direct link to the code in question (tripcolor.py line 87)\n3. The solution is implicitly clear: add the missing 'f' prefix to strings that need variable interpolation\n\nA developer can easily understand what needs to be fixed and where to start looking. The fix itself is straightforward - identify strings that contain curly braces for variable interpolation but lack the 'f' prefix, and add the prefix where needed. The example link provides a concrete starting point, and the developer can search for similar patterns throughout the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22926": {
        "explanation": "This issue is well-specified with clear information about the bug and how to reproduce it. The user has:\n\n1. Provided a complete, reproducible code example that demonstrates the error\n2. Included the exact error message and stack trace\n3. Identified the specific line in the matplotlib code that's causing the problem (line 915 in widgets.py)\n4. Suggested a solution (commenting out the problematic line)\n5. Explained the expected behavior (range slider with user initial values)\n6. Provided relevant environment information (OS, Python version, matplotlib version)\n\nThe issue is clearly a bug in the RangeSlider widget's set_val method where it's trying to access index 4 of an array that only has 4 elements (indices 0-3). The user has correctly identified that commenting out the problematic line fixes the issue. This is a straightforward bug with a clear solution - either remove the line or modify the code to handle the array bounds correctly.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22929": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected outcomes, and error messages. The bug is precisely described: `ax.bar_label()` fails when there are NaN values in either the bar heights or error bars. The issue provides:\n\n1. Three specific test cases that reproduce the bug\n2. Complete code examples for each case\n3. The exact error message and traceback\n4. The expected behavior for each case\n5. A link to the specific code in the matplotlib repository where the error likely occurs\n6. Environment details (OS, matplotlib version, Python version, etc.)\n\nThe issue also references a similar previous issue (#20058), providing context. The reporter has even identified the approximate location in the code where the problem occurs, which gives a clear starting point for fixing the bug.\n\nA developer would have all the necessary information to understand, reproduce, and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-22945": {
        "explanation": "This issue is well-specified with clear information for debugging and fixing:\n\n1. The exact commit where the regression was introduced is identified (396a010a6147e232d7ab101db9e4b5c6a48d0b36 from PR #22175)\n2. A complete code example for reproduction is provided\n3. The full error traceback is included, showing exactly where the error occurs\n4. The expected outcome is clearly stated (no error)\n5. Environment details are provided (OS, Python version, Matplotlib version, backend)\n\nThe error is clearly related to a regression in animation functionality, specifically when using clip paths with contour collections during animation initialization. The traceback points to a specific issue with `get_tightbbox` and `clip_path.get_extents()` returning None, causing the AttributeError.\n\nA developer would have all the necessary information to:\n1. Reproduce the issue\n2. Examine the changes in the identified commit\n3. Understand the root cause (likely related to how clip paths are handled)\n4. Implement a fix that ensures clip paths have proper extents or handle None values appropriately\n\nThe issue is specific, reproducible, and contains all the necessary context for a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23031": {
        "explanation": "This issue is well-specified with clear technical details about the problem:\n\n1. The issue clearly identifies that matplotlibrc and style files are currently read with locale encoding (since PR #3575).\n2. It demonstrates the problem with a concrete example showing how importing matplotlib fails when using non-standard encodings like utf-32-be.\n3. It explains the historical context (the change was implemented for Py2/Py3 transition, not for supporting non-standard encodings).\n4. It proposes a specific solution: require these files to use utf-8 encoding (or optionally support encoding cookies).\n\nThe issue provides enough technical context, code examples, error messages, and a clear direction for the solution. A developer could implement this change without needing additional clarification. The task involves modifying how matplotlib reads configuration files to enforce utf-8 encoding instead of using locale-dependent encoding, which is a well-defined scope of work.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23057": {
        "explanation": "This issue is very well-specified with all the necessary information to understand and attempt a solution:\n\n1. Clear bug description: Figures fail to redraw in IPython after initial plot when using pyplot interface\n2. Specific version information: Regression between 3.5.1 and 3.5.2\n3. Detailed reproduction steps: Complete instructions to set up environment and reproduce the issue\n4. Expected vs actual behavior: Clearly described difference between what happens and what should happen\n5. Bisected to a specific commit: f937b0ab5ef9d5ffe9f2f58f6391357783cc4afa\n6. Environment details: OS, Python version, backend information, and complete package list\n7. Confirmation that the issue occurs with multiple backends (Qt5 and Tk)\n\nThe issue has been thoroughly investigated by the reporter, including bisecting to the exact commit that introduced the regression. A developer would have all the information needed to understand the problem, reproduce it, and investigate the specific code changes in the identified commit that caused the regression.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23088": {
        "explanation": "This issue is well-specified with clear information about the bug, reproduction steps, and a proposed solution. The user has:\n\n1. Provided a complete code snippet that reproduces the error\n2. Included the full traceback showing the error message\n3. Identified the root cause of the problem (silent KeyError when accessing DataFrame with incorrect column name)\n4. Located the specific code in matplotlib that causes the issue\n5. Suggested a specific fix (adding a warning when the KeyError occurs)\n6. Noted a potential documentation inconsistency\n7. Provided environment details (OS, Python version, matplotlib version and backend)\n\nThe issue is about improving error messages in matplotlib when plotting from a DataFrame with incorrect column names. Currently, the error is misleading because a KeyError is caught silently, and the user receives a confusing formatting error instead. The proposed solution is to add a warning when this specific error occurs.\n\nThis issue contains all the necessary information for a developer to understand the problem, reproduce it, and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23174": {
        "explanation": "This issue is well-specified with all the necessary information to understand and attempt a solution:\n\n1. Clear bug description: The issue is about a crash when adding clabels to contour plots in subfigures.\n2. Complete reproduction code: A minimal, complete example that demonstrates the problem is provided.\n3. Detailed error traceback: The full error message shows exactly where the code fails - in the `get_renderer` function when trying to access a `_cachedRenderer` attribute that doesn't exist on a `SubFigure` object.\n4. Expected outcome: The user clearly states what should happen instead of the crash.\n5. Environment details: All relevant version information is provided (Matplotlib 3.5.2, Python 3.9.13, QtAgg backend, Gentoo Linux).\n\nThe root cause is evident from the traceback: the `get_renderer` function in `matplotlib.tight_layout` is trying to access a `_cachedRenderer` attribute on a `SubFigure` object, but this attribute doesn't exist. This suggests that the implementation of `clabel` doesn't properly handle the case when it's used with subfigures.\n\nA developer would need to modify the relevant code to either:\n1. Add the missing `_cachedRenderer` attribute to the `SubFigure` class, or\n2. Update the `get_renderer` function to handle `SubFigure` objects differently, or\n3. Modify the contour labeling code to use a different approach when working with subfigures.\n\nThe issue is specific, reproducible, and the error is clear, making it well-specified for a solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23188": {
        "explanation": "This issue is well-specified with clear information about the problem and proposed solution:\n\n1. Problem: The current default date limits (2000-01-01 to 2010-01-01) cause issues when users add day tickers without specifying limits, with a reference to a specific issue (#20202).\n\n2. Proposed solution: Change the default date limits to 1970-01-01 to 1970-01-02, which would yield default limits of 0-1 in float conversion.\n\n3. Impact assessment: The change should only affect empty date axes, not axes with actual data. The author has already tested this on the main branch and found that only empty axes tests fail.\n\nThe issue provides enough context to understand what needs to be changed, why it needs to be changed, and what the expected outcome is. The specific dates for both the current implementation and the proposed change are clearly stated. The author has also done preliminary testing to identify potential impacts.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23267": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The issue describes:\n\n1. The current behavior: When creating a colorbar with an auto-created axes, the `location` parameter works and sets both orientation and ticklocation. However, when using a manually created axes, the `location` parameter is not accepted, and users must explicitly set both `orientation` and `ticklocation` separately.\n\n2. A code example that demonstrates the problem clearly, showing both the working case with auto-created axes and the workaround needed for manually created axes.\n\n3. A specific proposed solution: Add a `location` kwarg to the Colorbar constructor that would set both `orientation` and `ticklocation` parameters, making it consistent with the auto-created axes case.\n\n4. An alternative solution: Better document the current workaround.\n\nThe issue provides enough context about the current implementation, the desired behavior, and even suggests a specific implementation approach. A developer could implement this enhancement without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23299": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual behavior. The bug description is precise: calling `matplotlib.get_backend()` removes figures from `Gcf.figs` if the first figure was created in an `rc_context`. The reproduction code is complete and demonstrates the issue clearly, showing that uncommenting certain lines fixes the problem. The error output is provided, showing exactly what's happening - figures are being cleared from the OrderedDict. The expected outcome is clearly stated: figures should not be removed from `Gcf`, and the consequences of this bug are explained (e.g., `plt.close(fig2)` doesn't work). The environment details are comprehensive, including OS, Matplotlib version, backend, and Python version. A developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23314": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A clear description of the problem: `set_visible(False)` not working for 3D projections\n2. Complete reproducible code example\n3. Clear explanation of actual vs. expected outcome\n4. Relevant version information (Matplotlib 3.4.2, Python 3.8.10, Qt5Agg backend)\n\nThe issue is straightforward: when calling `set_visible(False)` on a 3D axis, the axis remains visible when it should be hidden. The expected behavior is clearly stated - the subplot should not be visible when this function is called with False.\n\nA developer would have all the necessary information to reproduce the issue, investigate why the visibility setting isn't working for 3D projections, and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23412": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A clear description of the problem: offset dash linestyle not working in patch objects\n2. Complete reproduction code for both the problematic case and the expected behavior\n3. Screenshots showing the actual vs expected output\n4. Information about testing on different systems and with different patch types\n5. Relevant environment details (OS, Matplotlib version, backend, Python version, installation method)\n\nThe issue is specifically about the offset parameter in dash line styles not having any effect when applied to patch objects (like Rectangle), while it works correctly with Line2D objects. The expected behavior is clearly demonstrated with a comparison example.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed - the offset parameter in dash line styles should work for patch objects the same way it works for Line2D objects.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23476": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual behavior. The bug description is precise: when a figure is unpickled on an M1 Mac, its DPI is doubled each time, eventually leading to an OverflowError. The reproduction code is complete and demonstrates the issue clearly. The expected outcome shows that the DPI should remain constant at 200.0 through multiple pickle/unpickle cycles. The issue includes relevant system information (M1 Mac, Python 3.9.12, Matplotlib 3.5.2, MacOSX backend). The problem is specific to M1 MacBooks according to the additional information. A developer would have all the necessary information to investigate and fix this bug without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23562": {
        "explanation": "This issue is well-specified with a clear problem description and a minimal reproducible example. The user is reporting that calling `get_facecolors()` on a `Poly3DCollection` object (returned by `plot_surface()`) raises an AttributeError because the object has no attribute `_facecolors2d`. The error is consistently reproducible with the provided code snippet, and the traceback clearly shows where the error occurs. The issue has been tested on multiple versions of matplotlib (1.3.1 and 1.4.2), which helps establish that it's not version-specific. The problem is clearly in the implementation of the `get_facecolors()` method in the `Poly3DCollection` class, which is trying to access an attribute that doesn't exist. A developer would be able to locate the issue in the codebase and implement a fix without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23563": {
        "explanation": "This issue is reasonably well-specified with enough information to attempt a solution. The user has provided:\n\n1. A clear bug description: When plotting 3D lines in matplotlib, they encounter an AttributeError: 'Line3D' object has no attribute '_verts3d' after previously encountering a dimensionality error.\n\n2. A code sample for reproduction, though it's not complete since the actual array values are replaced with placeholders ('my int number list').\n\n3. The full error traceback showing where the issue occurs.\n\n4. Environment details including OS, Python version (3.8.12), and matplotlib version (3.5.0).\n\n5. A clear description of the sequence of events that leads to the error: first running with numpy arrays causes a dimension error, then subsequent runs with int values cause the '_verts3d' attribute error that persists until restarting the kernel.\n\nThe issue appears to be related to a state corruption in matplotlib's 3D plotting functionality after encountering an initial error. While the exact array values aren't provided, the general pattern of the bug is clear enough that a developer could investigate by:\n\n1. Creating sample arrays that would trigger the initial dimension error\n2. Examining how this affects the state of the Line3D objects\n3. Looking at the matplotlib source code to understand how the '_verts3d' attribute is managed\n\nThe issue is reproducible and has a clear sequence of steps leading to the error, making it reasonably specified for someone familiar with matplotlib's internals to investigate.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-23742": {
        "explanation": "This issue is reasonably well-specified with enough information to understand the problem and attempt a solution:\n\n1. The issue clearly identifies a bug with toolbar instantiation in a notebook environment.\n2. The error message is specific: \"The 'toolbar' trait of a Canvas instance expected a Toolbar or None, not the NavigationToolbar2WebAgg\"\n3. The issue provides:\n   - A link to CI logs showing the error\n   - A basic code snippet for reproduction\n   - A detailed traceback\n   - Environment information (OS, Python version, Matplotlib version, etc.)\n   - The specific package versions (ipympl 0.9.2, Jupyter 6.4.11, Matplotlib 3.6.0rc1)\n\nThe issue appears to be related to a compatibility problem between MNE-Python's widget abstraction layer and a recent update in dependencies. The error occurs specifically when creating a Canvas instance.\n\nWhile the submitter mentions they'll work on a more minimal working example (MWE), the provided information is sufficient to:\n1. Understand the context of the problem\n2. See where the error is occurring\n3. Begin investigating the compatibility issue between MNE-Python's Canvas implementation and the NavigationToolbar2WebAgg class\n\nA developer familiar with the codebase should be able to trace the issue to the Canvas implementation in the backend and identify why it's rejecting the NavigationToolbar2WebAgg instance when it previously worked.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-23964": {
        "explanation": "This issue is very well-specified with all the necessary information to understand and solve the problem:\n\n1. Clear bug description: TypeError when saving a figure with PostScript backend if text contains an empty line\n2. Complete reproduction code provided\n3. Full error traceback showing exactly where the error occurs\n4. Expected outcome clearly stated\n5. Additional information that helps narrow down the problem:\n   - Only happens with PostScript backend, not with other backends\n   - Works in older versions (<=3.5.3)\n   - Suggests a potential fix (adding `if curr_stream:` check)\n6. Environment details provided (OS, Matplotlib version, Python version)\n\nThe issue even points to the specific line in the code that's causing the problem and suggests a potential fix. With this information, a developer could easily locate the issue in the codebase, understand what's happening, and implement a fix without needing any additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-23987": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A clear description of the problem: UserWarning about incompatible layout engines is shown even when constrained_layout=False\n2. Complete, minimal code to reproduce the issue\n3. The exact warning message that appears\n4. The expected outcome (no warning)\n5. Additional information that helps narrow down the cause (warning disappears when constrained_layout=False is removed)\n6. Environment details (OS/X, Matplotlib 3.6.0, conda installation)\n\nThe issue is straightforward: when using plt.subplots_adjust() with constrained_layout=False explicitly set, a warning is still generated that shouldn't be. This is a regression from previous versions. A developer would have enough information to investigate why the constrained_layout=False parameter isn't properly preventing the warning message when using subplots_adjust.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24013": {
        "explanation": "This issue is well-specified with a clear problem description and a proposed solution. The issue describes a naming conflict in the matplotlib codebase where a function shadows its own module name, specifically with `matplotlib.tri.tripcolor`. The reporter provides:\n\n1. Clear code examples demonstrating the problem\n2. The root cause (importing with `from .tripcolor import *` in `tri/__init__.py`)\n3. The specific challenge this creates (difficulty in creating a bijection between objects and their fully qualified names)\n4. A concrete solution proposal (rename the tripcolor submodule to _tripcolor or something else)\n5. Consideration of backward compatibility implications\n\nThe reporter even acknowledges this might not be a \"bug\" per se but an \"unexpected interaction.\" The issue is specific enough that a developer could understand the problem, evaluate the proposed solution, and implement it without needing additional clarification. The only minor question at the end about whether this is a general concern in matplotlib could be answered by examining the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24026": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user wants to use color cycle references (like 'C0', 'C1', etc.) in the `stackplot` function's `colors` parameter, similar to how they work in other matplotlib functions like `plot()` and `Rectangle()`. The error message clearly shows that stackplot is rejecting these color cycle references with a \"Cannot put cycle reference ('C2') in prop_cycler\" error.\n\nThe issue includes:\n1. A clear description of the use case (keeping colors synchronized across different plot types)\n2. A complete, minimal code example that reproduces the issue\n3. The exact error message and traceback\n4. The expected behavior (stackplot should accept 'CN' color references like other matplotlib functions)\n\nFrom the error message, we can see that the problem occurs because stackplot is trying to set the property cycle with these color references rather than resolving them to actual colors first. This is a clear inconsistency in the API that should be fixed to maintain consistent behavior across matplotlib functions.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24149": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report includes:\n\n1. A clear description of the problem: `ax.bar` raises an exception in matplotlib 3.6.1 when passed all-nan data\n2. A minimal code example to reproduce the issue\n3. The exact error traceback\n4. The expected behavior (based on 3.6.0 version)\n5. Additional debugging information showing that the issue is specifically with NaN x-values\n6. Version information for matplotlib and the operating system\n\nThe reporter has even done some investigation to narrow down the issue, noting that it's specifically related to NaN values in the x-position parameter, not the height parameter. They've also identified a potential connection to a bullet point in the release notes.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed - they need to make `ax.bar([np.nan], [np.nan])` work without raising an exception, as it did in version 3.6.0.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24177": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Clearly identified the bug: when using `histtype='step'` with `density=True` in matplotlib's histogram function, the density axis is not properly scaled.\n\n2. Provided complete reproduction code that demonstrates the issue.\n\n3. Included screenshots showing both the actual (problematic) and expected outcomes.\n\n4. Created a GitHub repository with test data for further investigation.\n\n5. Specified the exact versions of matplotlib (3.6.0), Python (3.10.4), and OS (OS/X).\n\n6. Demonstrated that the issue is specific to the `histtype='step'` parameter, as other histogram types work correctly.\n\n7. Shown that the issue is related to scaling, as changing the `scale` parameter affects the density values, which shouldn't happen when `density=True`.\n\nThe issue is specific, reproducible, and contains all the necessary information for a developer to understand and fix the problem. The expected behavior is clear: the density axis should be properly scaled regardless of the `histtype` parameter used.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24189": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected outcome, and a detailed error message. The bug occurs when using `width_ratios` with nested mosaic layouts in `subplot_mosaic()`. The user provides:\n\n1. A complete code example that reproduces the error\n2. The exact error traceback\n3. A visual representation of the expected outcome\n4. A thoughtful analysis of why the error might be occurring (recursive passing of gridspec_kw)\n5. Suggestions for potential solutions (allow specifying for outer list or provide better error messages)\n6. Complete environment information (OS, Matplotlib version, backend, Python version, installation method)\n\nThe issue is clearly a bug where nested mosaic layouts don't properly handle width_ratios parameters. The expected behavior is either for the width_ratios to apply to the top-level layout or for a more helpful error message to be provided. A developer would have enough information to investigate and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24257": {
        "explanation": "This issue is well-specified in terms of what needs to be done. The request is to document how to distribute style files (style dictionaries) in Python packages for Matplotlib. The issue clearly explains:\n\n1. The context: Style dictionaries can be distributed as Python packages\n2. The benefits: It works across most Matplotlib versions and is relatively easy to implement\n3. The specific deliverables:\n   - Documentation on how to distribute style files as Python packages\n   - Potentially creating a template package (similar to an existing colormap template)\n   - Documenting additional benefits like distributing colormaps with styles\n\nThe issue provides enough background information, including a reference to a previous discussion, and outlines what the documentation should cover. While some implementation details are left to the developer, the requirements are clear enough to create meaningful documentation without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24334": {
        "explanation": "This issue is well-specified with a clear problem and proposed solution. The issue describes that:\n\n1. The `Axis.set_ticks` method (and by extension `Axes.set_xticks`) only validates text-related keyword arguments when the `labels` parameter is provided.\n2. When users incorrectly use parameters like `xticklabels` instead of `labels`, these parameters are silently ignored without any validation or warning.\n3. The proposed solution is to validate that all kwargs are valid Text properties regardless of whether `labels` is provided, and potentially make it an error to provide kwargs when `labels` is not set.\n\nThe issue provides a specific example of the problem (`ax.set_xticks([0, 1], xticklabels=[\"a\", \"b\"])`) and clearly explains the current behavior versus the expected behavior. The documentation reference is included to support the explanation.\n\nA developer would have enough information to implement the proposed solution by:\n1. Modifying the `Axis.set_ticks` method to validate kwargs regardless of whether `labels` is provided\n2. Potentially adding error handling for kwargs when `labels` is not provided\n\nThis is a straightforward enhancement request with a well-defined scope and clear expectations.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24362": {
        "explanation": "This issue is very well-specified with clear reproduction steps, expected behavior, and even a suggested fix. The bug is precisely described: when using integer values 0 or 1 instead of boolean False or True in the `sharex` or `sharey` arguments of `pyplot.subplots()`, an error is raised instead of treating them as boolean values.\n\nThe issue includes:\n1. A clear code snippet to reproduce the bug\n2. The exact error message received\n3. The expected behavior (that 0 and 1 should be treated as boolean values)\n4. A suggested patch with the exact code changes needed\n5. Relevant system information (OS, Matplotlib version, Python version, etc.)\n\nThe suggested fix is also reasonable and straightforward - modifying the type checking in gridspec.py to accept 0 and 1 as boolean equivalents. The issue reporter has even provided a specific patch that can be applied.\n\nThis is a common expectation in Python where 0 and 1 are often treated as boolean equivalents, and the fix would improve API consistency. There's nothing ambiguous about what needs to be done to resolve this issue.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24431": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible example. The user is reporting that the `eventplot` function in matplotlib allows a list of colors to be passed (one for each data series), but does not allow a similar list of alpha values. The error message is provided, showing that the `set_alpha` method only accepts a float or None, not a list. The issue includes the exact code to reproduce the problem, the error traceback, and the versions of matplotlib and Python being used. The desired behavior is implied but clear: the user wants to be able to specify different alpha values for each data series in an eventplot, similar to how different colors can be specified. This is a feature request to extend the existing functionality in a consistent way.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24538": {
        "explanation": "This issue is well-specified. The user is requesting documentation for the `legendHandles` attribute of the Legend class in matplotlib. They clearly identify:\n\n1. The specific attribute that needs documentation (`legendHandles`)\n2. The location of the attribute in the codebase (with a GitHub link)\n3. The problem: the attribute is not documented and there's no corresponding getter method\n4. A use case for why this documentation would be helpful (getting handler positions)\n5. Alternative solutions that would also address their need (adding a pointer to `ax.get_legend_handles()` in the legend documentation)\n\nThe issue provides enough context about what needs to be done (document an existing attribute) and why it's important. A developer could implement a solution without needing additional clarification - either by adding documentation for the `legendHandles` attribute, adding a getter method, or adding cross-references in the documentation to related methods.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24570": {
        "explanation": "This issue is well-specified with clear reproduction code, expected and actual outcomes shown through images, and a concise description of the problem. The bug report states that the `align` parameter in `HPacker` appears to have reversed behavior for the 'top' and 'bottom' options. The reproduction code demonstrates the issue with a simple example creating two drawing areas of different heights and aligning them with `align=\"bottom\"`. The actual outcome shows the rectangles aligned at the top, while the expected outcome shows them aligned at the bottom. The issue includes the Matplotlib version (3.6.2) where this occurs. A developer would have all the necessary information to investigate and fix this bug without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24604": {
        "explanation": "This issue is well-specified and clear about what is being requested. The author is proposing to add a new function `figure.gridspec_mosaic(...)` that would return a dictionary of subplotspecs instead of actual axes, allowing users to create subplots with different projections (polar, 3D, etc.) using the mosaic layout.\n\nThe issue:\n1. Clearly identifies the problem: users want to combine subplot_mosaic with axes of different projections\n2. Explains why adding projections directly to subplot_mosaic would be problematic (API bloat)\n3. Proposes a specific solution with code examples showing how it would work\n4. Even discusses an alternative approach (SubplotSpec.add_subplot()) that was previously rejected\n\nThe implementation details are straightforward - create a new method that works like subplot_mosaic but returns subplotspecs instead of axes. The API design is well thought out and the use case is clear. A developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24619": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected outcome, and even suggested fixes. The bug report includes:\n\n1. A clear description of the error: \"ValueError: RGBA values should be within 0-1 range\" when using integer color arrays with pcolorfast\n2. Complete reproducible code example that demonstrates the issue\n3. The exact error message received\n4. The expected outcome (a plot in bluish color)\n5. Detailed suggestions for fixes, including specific code locations and proposed changes\n6. Complete environment information (OS, Matplotlib version, backend, Python version, etc.)\n\nThe issue is specifically about Matplotlib not properly handling integer/uint8 color arrays in pcolorfast/quadmesh functions, expecting values in 0-1 range when they should accept 0-255 range for integer types. The reporter has even identified the specific code locations in colors.py that need modification.\n\nA developer would have all the necessary information to reproduce the issue and implement a fix based on the suggested changes or develop their own solution to the problem.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24627": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible example. The issue describes that when an artist (like a line) is removed from an axes using `remove()`, its `.axes` attribute is correctly set to None, but when the axes is cleared using `cla()`, the `.axes` attribute of the artist still points to the axes object even though it's no longer part of it. The expected behavior is that `cla()` should also unset the `.axes` attribute of artists that are no longer associated with the axes. The issue includes a clear code example demonstrating the problem, making it easy to understand and verify. The title also mentions that this should apply to both `cla()` and `clf()` methods, and should also unset the `.figure` attribute of deparented artists. This is a specific, technical issue with a clear expected behavior.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24637": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user provides a complete, reproducible code example that demonstrates the issue.\n2. The expected behavior is clearly stated: the `gid` attribute set on an `AnnotationBbox` should be included in the SVG output.\n3. The actual behavior is documented: the `gid` is set correctly (as shown by the print statement) but doesn't appear in the SVG output.\n4. The user provides the output SVG file (as a text attachment) for verification.\n5. The user includes relevant context by referencing a similar issue that was previously fixed (PR #15087).\n6. The user provides their environment details (matplotlib 3.3.4, Python 3.7.7).\n7. The user even provides the test image used in their example.\n\nThe issue is clearly a bug where the `gid` attribute of an `AnnotationBbox` is not being passed to the SVG renderer, despite being correctly set on the object. The solution would involve ensuring that the `gid` attribute is properly passed to the renderer when an `AnnotationBbox` is drawn in an SVG figure, similar to the fix referenced in PR #15087.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24691": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The user wants to be able to set different alpha (transparency) values for the facecolor and edgecolor of patches separately, rather than having a single alpha value apply to the entire patch.\n\nThe issue includes:\n1. A clear description of the use case (solid edges with semi-transparent fill)\n2. A visual example showing the desired effect\n3. The current workaround they're using (manually setting RGBA values)\n4. A specific proposed solution (allowing alpha to take a tuple for face/edge colors)\n5. Reference to similar functionality already implemented elsewhere in the codebase (vectorized alphas in imshow)\n\nThe request is reasonable and has a clear implementation path - extend the alpha parameter to accept a tuple that would apply different transparency values to the face and edge of patches. This is a straightforward enhancement that builds on existing functionality.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24768": {
        "explanation": "This issue is well-specified with clear details for reproduction and debugging:\n\n1. The bug is clearly described: using both `pcolormesh(rasterized=True)` and `set_rasterization_zorder()` together causes a conflict.\n\n2. A complete, minimal reproduction code is provided that demonstrates the issue.\n\n3. The error message/stack trace is included, showing exactly what's failing.\n\n4. The expected outcome is shown with an image.\n\n5. Version information is comprehensive (Matplotlib versions where the bug appears and doesn't appear).\n\n6. The reporter even provides insight into the potential cause, pointing to specific commits and code locations that might be related to the issue.\n\n7. Operating system, Matplotlib version, backend, and Python version information are all provided.\n\nThis issue contains all the necessary information to understand the problem, reproduce it, and begin working on a solution. The reporter has done thorough investigation work that gives clear direction for fixing the bug.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24849": {
        "explanation": "This issue is well-specified with clear information about the bug and how to fix it:\n\n1. The problem is clearly defined: LineCollection doesn't support the 'gapcolor' parameter that works with other line types.\n\n2. A minimal reproduction code is provided that demonstrates the error.\n\n3. The error message is included, showing exactly what happens when trying to use 'gapcolor' with LineCollection.\n\n4. The expected outcome is shown with a visual example (image).\n\n5. The reporter even suggests a specific solution approach: adding `set_color` and `get_color` methods to LineCollection, modeled after existing methods.\n\n6. The exact version of Matplotlib is provided.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The suggested fix direction gives a clear starting point for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24870": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The user wants matplotlib to automatically detect when a boolean array is passed to the `contour()` function and set the default contour level to [0.5] in that case, rather than using the standard 8 levels.\n\nThe issue includes:\n1. A clear description of the current behavior\n2. A code example demonstrating the problem\n3. A visual example showing the difference between the current behavior and desired behavior\n4. A specific proposed solution (auto-detect boolean arrays and use level=[0.5])\n5. Even mentions a similar auto-detection feature in the library (imshow handling of RGBA arrays)\n\nThe implementation would involve modifying the `contour()` function to check if the input array is boolean, and if so, default to a single level at 0.5 unless the user explicitly specifies other levels. This is a straightforward enhancement with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24924": {
        "explanation": "This issue is well-specified with clear information about the bug:\n\n1. The problem is clearly defined: `Figure.set_tight_layout(False)` does not disable the tight layout algorithm as expected.\n\n2. A minimal reproducible example is provided that demonstrates the issue.\n\n3. Both the actual outcome (AssertionError) and expected outcome (clean exit) are clearly stated.\n\n4. The reporter even suggests a specific fix, pointing to the exact code location that likely needs modification (setting `self.set_layout_engine(None)` in the falsey case).\n\n5. The issue references a specific PR (#20426) that introduced the bug.\n\nThe only missing information is about the operating system, backend, Python version, etc., but these are not critical for understanding or reproducing this particular bug since it appears to be a logical issue in the codebase rather than an environment-specific problem. The bug can be reproduced with the provided code snippet regardless of these details.\n\nA developer would have all the necessary information to understand the problem, reproduce it, and implement a fix based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-24970": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The bug is clearly identified: NumPy 1.24 deprecation warnings are appearing in Matplotlib.\n2. A minimal reproducible example is provided that triggers the warnings.\n3. The exact warning messages are included, which point to specific lines in matplotlib/colors.py where out-of-bound Python integers are being converted to uint8 arrays.\n4. The expected outcome is clearly stated: no warnings should appear.\n5. System information is provided (OS, Matplotlib version, Python version, etc.)\n\nThe issue is related to NumPy 1.24 changing how it handles out-of-bound integer conversions, and Matplotlib code needs to be updated to follow the new recommended pattern shown in the warning messages. The solution would involve modifying the identified lines in colors.py to use the suggested approach: `np.array(value).astype(dtype)` instead of direct conversion.\n\nThis is a straightforward compatibility fix with clear guidance from the NumPy deprecation warnings themselves.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25027": {
        "explanation": "This issue is reasonably specified. It clearly identifies the problem: `pcolor` needs to support RGB(A) arrays similar to how `pcolormesh` already does (since PR #24619). The context is well-explained - Cartopy's implementation of `pcolormesh` uses `pcolor` under the hood, so RGB(A) support needs to be added to `pcolor` to enable this functionality in Cartopy.\n\nThe proposed solution provides a starting point by mentioning that PR #25027 is likely needed first before adding RGB(A) support. While the exact implementation details aren't fully specified, this is reasonable since:\n\n1. There's a clear reference implementation (the existing RGB(A) support in `pcolormesh` from PR #24619)\n2. The dependency on another PR (#25027) is identified\n3. The goal is clearly defined: make `pcolor` handle RGB(A) arrays in the same way `pcolormesh` does\n\nA developer familiar with the codebase would have enough information to implement this feature by:\n1. Examining how RGB(A) support was added to `pcolormesh` in PR #24619\n2. Understanding the relationship between PR #25027 and this feature\n3. Implementing similar RGB(A) handling in `pcolor`\n\nThe only missing details are the specific implementation approach, but this is typical for enhancement requests and can be determined during development.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25052": {
        "explanation": "This issue is well-specified with clear information about the problem and a suggested solution. The user has:\n\n1. Identified a specific documentation issue in matplotlib.axes.Axes.table's bbox parameter\n2. Provided the exact documentation link\n3. Clearly described the discrepancy between documentation and actual behavior\n4. Included a complete error traceback showing the problem\n5. Noted the specific matplotlib version (3.6.2)\n6. Explained their empirical finding that bbox=[1, 1, 1, 1] works while using a Bbox instance doesn't\n7. Provided a specific suggestion for improvement (either make the parameter accept a proper Bbox object as documented, or update the documentation to specify it uses from_bounds())\n\nThe issue is focused on a specific API inconsistency between documentation and implementation, with enough context to understand and address the problem. A developer could reproduce this issue and implement a solution based on the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25122": {
        "explanation": "This issue is well-specified with clear information about the bug:\n\n1. The bug is precisely located in `mlab._spectral_helper` with a direct link to the problematic code.\n2. The issue explains exactly what's wrong: the `np.abs` function is incorrectly applied to window values, causing problems with windows that have negative values (like `flattop`).\n3. A reference to the correct implementation in SciPy is provided for comparison.\n4. A minimal reproducible example is included that clearly demonstrates the issue.\n5. The expected outcome (0) and actual outcome (4372.942556173262) are clearly stated, showing the numerical discrepancy.\n\nThe solution path is clear: remove the unnecessary `np.abs` call in the window correction calculation in `mlab._spectral_helper`. This matches the SciPy implementation which is provided as a reference. A developer could implement this fix without needing any additional information.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25126": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected outcomes. The bug description clearly states that when changing the y-scale from 'log' to 'linear', the plot line disappears. The user has provided:\n\n1. Complete, runnable code to reproduce the issue\n2. The actual outcome (PDF files showing the bug)\n3. The expected outcome (PDF files showing correct behavior)\n4. Version information (Matplotlib 3.6.3 shows the bug, 3.5.3 works correctly)\n5. Environment details (MacOSX, Python 3.9.13, pip installation)\n\nThe issue is a regression bug since it works in an older version (3.5.3) but fails in the newer version (3.6.3). The problem is specifically with the `plot()` function when changing scales, as the user notes that other functions like `fill_between` and `hist` work fine.\n\nA developer would have all the necessary information to investigate and fix this bug without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25281": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and a detailed explanation of the bug. The issue describes that:\n\n1. When passing non-string values to the `loc` parameter of the `legend()` function in matplotlib, validation is not performed until `plt.show()` is called, which results in runtime errors.\n\n2. The issue provides a complete code example that reproduces the bug, showing how passing an invalid tuple `(1.1, .5, 1.1, \"abc\")` to `loc` doesn't raise an error immediately but fails later during rendering.\n\n3. The expected behavior is clearly stated: errors should be raised immediately when invalid arguments are passed to `loc`, similar to how invalid string values are handled.\n\n4. The issue even points to the specific location in the code where the bug might be occurring (lack of validation for `_loc_real`).\n\n5. All relevant environment information is provided (OS, matplotlib version, backend, Python version, installation method).\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The solution would likely involve adding validation for non-string `loc` values similar to how string values are validated.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25287": {
        "explanation": "This issue is well-specified with clear information about the bug and how to fix it:\n\n1. The problem is clearly defined: the exponent label (offsetText) is using the tick.color instead of tick.labelcolor when the latter is set.\n\n2. A minimal reproducible example is provided showing how to trigger the bug.\n\n3. Both the actual outcome (with screenshot) and expected outcome (with screenshot) are included, making it very clear what's wrong.\n\n4. The submitter even provided a patch that fixes the issue, showing exactly where in the code the problem is occurring (in the axis.py file).\n\n5. The Matplotlib version (3.6.3) is specified.\n\nThe issue contains all the necessary information to understand the problem and implement a solution. The patch provided shows that the fix is straightforward - when setting the color for the offset text, it should check if the labelcolor is set to something other than 'inherit' and use that instead of the tick color.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25311": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user provides:\n\n1. A complete, minimal code example that reproduces the error\n2. The exact error message received: `TypeError: cannot pickle 'FigureCanvasQTAgg' object`\n3. The expected outcome: successful pickling\n4. Important context: the issue occurs specifically with draggable legends (and annotations)\n5. Environment details: Windows 10, Python 3.10, Matplotlib 3.7.0, pip installation\n\nThe user even identifies the specific line causing the problem: `leg.set_draggable(True)` and notes that pickling works when this line is removed.\n\nThis is a clear bug report with all the necessary information to understand, reproduce, and potentially fix the issue. The problem appears to be that making a legend draggable introduces a non-picklable object into the figure structure.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25332": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible example. The bug report describes that calling `fig.align_labels()` on a matplotlib figure prevents it from being pickled, with a specific error message: \"TypeError: cannot pickle 'weakref.ReferenceType' object\". The user provides a complete code example that reproduces the issue, and clearly states that removing the `fig.align_labels()` line resolves the problem. The expected outcome (successful pickling) is clearly stated. The operating system (Windows) and matplotlib version (3.7.0) are provided, which is sufficient information to reproduce and investigate the issue. This appears to be a genuine bug in matplotlib's implementation of `align_labels()` that introduces unpicklable weak references into the figure object. A developer would have all the necessary information to reproduce the issue and attempt a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25346": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected outcomes. The bug description clearly identifies that constrained layout doesn't allocate enough space for wrapped titles (both figure suptitles and axes titles). The issue includes:\n\n1. A complete, minimal code example that reproduces the problem\n2. A screenshot showing the actual outcome (titles being cut off)\n3. A clear statement of the expected outcome (enough space for the titles)\n4. Additional information noting that explicit newlines work fine, suggesting the issue is specific to automatic wrapping\n5. Complete environment details (OS, Matplotlib version, backend, Python version, installation method)\n\nThe issue is specific enough that a developer could immediately understand the problem, reproduce it, and begin working on a solution. The solution would need to modify how constrained layout calculates space requirements for wrapped titles, ensuring they have enough room to display completely.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25404": {
        "explanation": "The issue is reasonably well-specified. The user has:\n\n1. Clearly identified the bug: An AttributeError occurs when trying to use `set_props` on a `LassoSelector` object\n2. Provided the specific error message: \"AttributeError: 'LassoSelector' object has no attribute '_props'\"\n3. Described their use case: They're trying to update the line color of a LassoSelector used to select a single point in a scatterplot\n4. Even proposed a potential fix: Commenting out the line \"self._props.update(props)\" in the widget.py file\n\nWhat's missing is:\n- The exact version of matplotlib being used\n- A minimal reproducible example showing the code that triggers the error\n- Confirmation that the proposed fix actually resolves the issue without breaking other functionality\n\nHowever, this is enough information to investigate the issue in the matplotlib codebase, specifically looking at the LassoSelector implementation and the set_props method in widget.py. The error suggests that the LassoSelector class doesn't have a _props attribute that the set_props method is trying to access, which is a clear starting point for debugging.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25425": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem with the current implementation of `AxesImage.__str__` and its inconsistency with NumPy's API conventions. The issue proposes two alternative solutions:\n\n1. Change `__str__` to use \"shape\" instead of \"size\" and add `get_shape` as an alias for `get_size`\n2. Make `get_shape` return the full shape including depth information (from `self._A.shape`) and use that in `__str__`\n\nWhile the issue doesn't explicitly state which solution is preferred, it provides enough context about the problem and potential solutions that a developer could make a reasonable implementation decision. The issue references a previous PR (#22485) which provides additional context.\n\nThe implementation would involve modifying the `AxesImage` class to either add a new method or modify existing ones, which is a straightforward task given the information provided. The developer would need to understand the Matplotlib codebase, particularly how `AxesImage` works, but the changes required are clearly outlined.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25430": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected outcomes, and error messages. The bug is clearly defined: when using `savefig` with a JPG file and providing the `metadata` parameter, an unhelpful error message is produced. The issue includes:\n\n1. Complete reproducible code example\n2. The exact error message received\n3. Clear expectations for resolution (either support metadata for JPG, ignore the parameter, or provide a better error message)\n4. Environment details including OS, Python version, and Matplotlib version\n\nThe problem is technically well-defined and the solution path is clear - either implement metadata support for JPG format, gracefully ignore the parameter for formats that don't support it, or improve the error message to be more informative. This is a straightforward bug that can be addressed without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25433": {
        "explanation": "This issue is well-specified with a clear bug description, reproducible code example, expected vs. actual behavior, and relevant environment details. The bug involves a specific interaction problem with matplotlib widgets: when using `pyplot.clf()` and `pyplot.draw()` within a RangeSlider's `on_changed` callback, all widgets become unresponsive to mouse input, while the same operations work fine in a Button's `on_clicked` callback. The issue description includes:\n\n1. A complete, minimal code example that reproduces the issue\n2. Clear description of the actual behavior (widgets become unresponsive)\n3. Clear description of the expected behavior (RangeSlider callback should behave like Button callback)\n4. Environment details (OS, Python version, Matplotlib version and backend)\n5. Confirmation that the issue occurs on multiple systems\n\nA developer would have all the necessary information to investigate and fix this bug without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25442": {
        "explanation": "This issue is well-specified with clear reproduction steps and all necessary information to understand and attempt a solution. The bug description includes:\n\n1. A clear summary of the problem: AttributeError when combining matplotlib 3.7.1 with mplcursor after clicking data points\n2. Complete reproduction code that demonstrates the issue\n3. The exact error traceback\n4. Expected behavior (no terminal output)\n5. Version information that narrows down the problem (works in matplotlib 3.7.0 but not 3.7.1)\n6. Environment details (OS, Python version, installation method)\n7. Backend information (QtAgg)\n\nThe issue appears to be a regression in matplotlib 3.7.1 that affects its interaction with mplcursor. The error occurs in the offsetbox.py file when trying to access the canvas property of a reference artist that has become None. This is specific enough that a developer could investigate the changes between matplotlib 3.7.0 and 3.7.1 that might affect this interaction, particularly focusing on the offsetbox implementation and how reference artists are handled.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25479": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible steps. The user has identified a bug in matplotlib's colormap handling where there's a discrepancy between the internal name of a colormap and the name under which it's registered.\n\nThe issue provides:\n1. A complete code example that reproduces the problem\n2. The exact error message received\n3. A clear explanation of the expected behavior vs. actual behavior\n4. The matplotlib version (1.4.3)\n5. A demonstration that manually looking up the colormap works, pinpointing where the issue occurs\n\nThe problem is that when registering a colormap with `cm.register_cmap(name='my_cmap_name', cmap=my_cmap)`, the user expects to be able to use that colormap with `plt.set_cmap('my_cmap_name')` followed by `plt.imshow()`. However, the error shows that matplotlib is looking for a colormap with the name that was originally given to the colormap object (`some_cmap_name`) rather than the name used when registering it (`my_cmap_name`).\n\nThis is a clear bug in how matplotlib handles colormap names, and a developer would have all the information needed to reproduce and fix the issue.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25498": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible example. The user is trying to update a colorbar after changing the norm of a mappable object in matplotlib. They've provided:\n\n1. A complete code example that demonstrates the issue\n2. The specific error message they're encountering\n3. The two approaches they've tried (`update_normal` and `update_bruteforce`)\n4. The exact behavior observed (one method has no effect, the other throws a ZeroDivisionError)\n\nThe issue is related to matplotlib's colorbar functionality, specifically how to properly update a colorbar when the norm of the associated mappable is changed after the colorbar is created. The error occurs because the LogNorm is applied to the mappable after the colorbar is created, but the colorbar isn't properly updated to reflect this change.\n\nA solution would need to address how to correctly update a colorbar when the norm of its associated mappable changes, either by fixing the existing update methods or providing an alternative approach.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25547": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes a specific inconsistency in matplotlib's behavior when handling NaN values in scatter plots:\n\n1. The issue clearly identifies the problem: When all values in the color array are NaN, a UserWarning is thrown, but when at least one value is not NaN, no warning appears.\n\n2. Complete reproduction code is provided with two test cases that demonstrate the inconsistent behavior.\n\n3. A full traceback is included showing exactly where the warning is coming from.\n\n4. The expected outcome is clearly stated: the reporter expects consistent behavior between the two cases.\n\n5. All relevant version information is provided (Matplotlib 3.3.1, Python 3.8.5, OS, backend).\n\nThe issue also includes a second bug report about `ax.errorbar` raising a `StopIteration` error when `yerr` contains only `NaN` values, which appears to be a related but separate issue. This second report is also well-specified with reproduction code, expected outcome, and version information.\n\nA developer would have all the necessary information to investigate and fix these issues without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25565": {
        "explanation": "This issue is well-specified with all the necessary information to understand and solve the problem:\n\n1. Clear bug description: The legend for Poly3dCollection fails with a specific error.\n2. Complete reproducible code example that demonstrates the issue.\n3. Detailed error traceback showing exactly where the failure occurs.\n4. Expected outcome with a screenshot showing what should happen.\n5. Root cause analysis: The issue is identified in the `first_color` function in `legend_handler.py` which assumes `colors` is a numpy array with a `size` attribute, but with `Poly3dCollection` it's receiving a tuple.\n6. Suggested solution: Convert the input to a numpy array using `np.array(colors)` to ensure it has the `.size` attribute.\n7. Environment details: OS, Matplotlib version, backend, Python version, and installation method.\n\nThe issue provides all the information needed to understand the problem, reproduce it, and implement a fix. The suggested solution is also reasonable and directly addresses the identified issue.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25624": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report states that when the `figure.autolayout` setting is set to `True` in the matplotlib rc parameters, any keyword arguments passed to `fig.tight_layout()` (specifically `w_pad=10` in the example) have no effect. The issue includes:\n\n1. A clear description of the problem\n2. A minimal code example that reproduces the issue\n3. The specific version of Matplotlib where the issue occurs (1.5.3)\n4. The environment information (Ubuntu Linux with conda installation)\n\nThe expected behavior is implied but clear: when passing `w_pad=10` to `tight_layout()`, there should be padding between the subplots regardless of the `figure.autolayout` setting. The actual behavior is that no padding is inserted when `figure.autolayout` is set to `True`.\n\nA developer would be able to reproduce this issue and understand what needs to be fixed without requiring additional information.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25631": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected outcomes. The bug report includes:\n\n1. A clear description of the problem: `bbox_inches=\"tight\"` causes issues with `writer.grab_frame()` resulting in a \"snowflake movie\"\n2. Complete reproducible code example that demonstrates the issue\n3. Visual examples of both the actual (problematic) and expected outcomes via video links\n4. The specific Matplotlib version (3.7.1) where the issue occurs\n\nThe problem is clearly defined: when using `bbox_inches='tight'` with `moviewriter.grab_frame()`, the animation produces an undesired \"snowflake\" effect instead of a smooth sine wave animation. The expected behavior is demonstrated by removing the `bbox_inches='tight'` parameter.\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification. The issue is specific to a particular API usage pattern and the expected behavior is unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25667": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description clearly states that the `bar` and `barh` functions in Matplotlib don't properly check the units of bottom/left parameters to determine if datetime conversion is needed.\n\nThe issue includes:\n1. A clear description of the bug: bar/barh don't trigger datetime units conversion\n2. Complete code examples to reproduce the issue\n3. Screenshots showing the actual outcome\n4. A workaround that demonstrates the expected behavior\n5. Screenshots showing the expected outcome\n\nThe solution would involve modifying the bar/barh functions to properly detect datetime units in the bottom/left parameters and apply the appropriate converter, similar to how other plotting functions in Matplotlib handle datetime data. The issue provides enough context for a developer to understand the problem and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25712": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected outcome, and actual outcome. The bug is precisely described: when using `fill_between` with an axes transform, the axes limits are incorrectly set if the axes coordinates are larger than the data coordinates.\n\nThe issue includes:\n1. Complete reproducible code example\n2. Clear description of the problem\n3. Screenshots showing the actual outcome\n4. Explanation of the expected outcome with reference to documentation\n5. A hypothesis about the cause of the bug\n6. Additional examples showing when it works vs. when it fails\n7. Complete environment information (OS, Matplotlib version, Python version, etc.)\n\nThe solution would need to fix the `fill_between` and `fill_betweenx` functions to properly respect the axes transform without incorrectly expanding the y-axis limits based on the fill coordinates. The expected behavior is clearly defined by reference to the documentation example.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25772": {
        "explanation": "This issue is well-specified with clear reproduction steps and detailed error information. The bug description clearly states the problem: when using VS Code interactive mode, `%matplotlib qt` fails with an error indicating it cannot import Qt bindings, despite PySide6 being installed. \n\nThe issue includes:\n1. A clear error trace showing the exact error message\n2. Simple code to reproduce the issue\n3. Detailed environment information including:\n   - Operating systems (Windows 10 and Windows 11)\n   - Python version (3.10.10)\n   - Matplotlib version (3.7.1)\n   - Backend being used (Qt5Agg)\n   - IPython/Jupyter related package versions\n\nThe error message itself is informative: \"Failed to import any of the following Qt binding modules: PyQt6, PySide6, PyQt5, PySide2\" despite the user stating that PySide6 is installed.\n\nThis provides enough information to investigate why VS Code's interactive mode cannot find the Qt bindings that are installed in the system. A developer would be able to reproduce the issue and investigate potential causes such as environment isolation in VS Code, path issues, or potential conflicts between packages.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25775": {
        "explanation": "This issue is well-specified with clear problem statement and proposed solution. It requests adding `get_antialiased` and `set_antialiased` methods to Text objects in the codebase, similar to what exists for other artist objects. The issue explains:\n\n1. The current behavior: Text objects use global rcParams[\"text.antialiased\"] setting\n2. The desired behavior: Allow per-Text object configuration like other artists\n3. Implementation guidance: Replace rcParams access with GraphicsContext state\n4. Scope: Text objects and possibly Annotations\n\nThe issue provides enough context about the current architecture (mentioning GraphicsContext state and backends) and clearly defines what success looks like - having getter/setter methods for antialiasing on Text objects that work similarly to other artists. The implementation details are reasonably outlined, noting that some backend code understanding is required.\n\nA developer with access to the codebase would be able to implement this feature without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25794": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solutions. The problem is that `plt.scatter` accepts both singular and plural forms of attributes like `linewidth`/`linewidths` and `edgecolor`/`edgecolors`, with the singular forms taking precedence when both are specified. This causes confusion for users and complexity for downstream libraries.\n\nThe proposed solution is clearly articulated with two options:\n1. A small change: Raise an exception when both singular and plural forms are specified\n2. A larger change: Consider deprecating the plural forms if there's no strong reason for their existence\n\nThe issue provides context about similar parameter handling in matplotlib and mentions a specific example of user confusion (with a link to a seaborn issue). The description includes enough technical detail about the current behavior and the desired behavior to implement a solution.\n\nA developer could implement the small change solution without needing additional clarification, as it's clear what needs to be done: modify the `plt.scatter` function to check if both singular and plural forms of these attributes are provided, and if so, raise an exception.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-25859": {
        "explanation": "This issue is reasonably well-specified. The reporter has identified a potential problem with the `Figure.add_axes()` method in Matplotlib, where it appears to accept arbitrary positional parameters (`False, 1` in the example) that may be ignored. \n\nThe issue includes:\n1. A clear example showing the problematic code\n2. A link to the documentation for the method in question\n3. A specific concern about the readability and functionality of the code\n4. A proposed fix: either document what these parameters do, or deprecate accepting extra positional parameters\n\nWhat makes this not \"well-specified\" is that it doesn't include the actual implementation of `add_axes()` or confirm that the parameters are indeed ignored. The reporter is making an assumption (\"I suspect extra positional parameters are ignored\") rather than verifying this behavior.\n\nHowever, this is enough information for a developer to:\n1. Check the implementation of `Figure.add_axes()`\n2. Verify whether extra positional parameters are used or ignored\n3. Either improve documentation or modify the method to reject extra parameters\n\nThe solution path is clear even if some investigation is needed to confirm the exact nature of the problem.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-25960": {
        "explanation": "This issue is well-specified with clear information about what's not working. The bug report includes:\n\n1. A clear description of the problem: `wspace` and `hspace` parameters in `Figure.subfigures` are not having any effect\n2. A complete, minimal code example that reproduces the issue\n3. A clear statement of expected vs. actual behavior\n4. A reference to the relevant code in the Matplotlib codebase that should be handling these parameters\n5. Environment details including OS, Matplotlib version, Python version, and installation method\n\nThe issue is straightforward to understand: when creating subfigures with the `subfigures()` method, the spacing parameters `wspace` (width space between columns) and `hspace` (height space between rows) are being ignored, regardless of what values are passed. The expected behavior is that these parameters should adjust the spacing between subfigures, as indicated by the linked code in the Matplotlib repository.\n\nA developer would have all the necessary information to investigate and fix this bug without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26011": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The bug is clearly described: \"xlim_changed\" callbacks are not being triggered when axis limits change via a shared axis (sharex=).\n\n2. The reporter has identified the specific code in matplotlib/lib/matplotlib/axes/_base.py that causes the issue, showing that set_xlim() calls sibling axes with emit=False, which prevents callbacks from firing.\n\n3. The reporter has even investigated potential solutions, noting that moving the callback call outside the \"if emit:\" statement seems to solve the issue.\n\n4. There's a clear question about whether there's a reason for the current implementation (emit=False for shared axes) and whether it's intentional that callbacks don't fire in this case.\n\nThe issue provides enough context and code details for a developer to understand the problem, reproduce it, and implement a solution. The reporter has even suggested a potential fix that could be evaluated. This is a well-defined bug with a clear path to investigation and resolution.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26020": {
        "explanation": "This issue is well-specified with clear details about the bug, how to reproduce it, and the expected outcome. The description includes:\n\n1. A clear problem statement: Creating `AxesGrid` with cartopy `GeoAxes` as `axis_class` raises a TypeError.\n2. A specific error message: `TypeError: 'method' object is not subscriptable`\n3. A hypothesis about the cause: Different behavior of the `axis` attribute between different axes classes.\n4. A complete, minimal code snippet to reproduce the issue.\n5. The full traceback showing exactly where the error occurs.\n6. Detailed environment information including OS, Python version, and library versions.\n\nThe issue points to a specific line in the code where the error occurs (`ax.axis[\"bottom\"].toggle(ticklabels=bottom_off, label=bottom_off)`), and explains that the problem is likely because the `axis` attribute is a callable method in `GeoAxes` but is expected to be subscriptable in the `_tick_only` function.\n\nA developer would have all the necessary information to diagnose and fix this issue without needing additional clarification. The solution would likely involve modifying the `_tick_only` function to handle the case where `axis` is a method rather than a subscriptable object.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26024": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly stated: certain mathematical operators are missing from the `tex2uni` mapping in `_mathtext_data.py`. The user has provided:\n\n1. A specific list of Unicode hex codes for the missing symbols\n2. A link to view what these symbols look like\n3. A reference to find the corresponding LaTeX names\n4. A specific file location where the changes need to be made (`tex2uni` in `_mathtext_data.py`)\n\nThe solution would involve adding these mathematical symbols to the mapping so they can be used in the software.\n\nThere are some minor ambiguities:\n1. The user suggests being \"discriminate when adding these\" without specifying exact criteria, though they do mention prioritizing standard LaTeX and AMS symbols\n2. The exact LaTeX command names for each hex code aren't provided directly, requiring research using the provided PDF reference\n3. There's no mention of testing requirements or potential impacts on other parts of the system\n\nHowever, these ambiguities don't prevent a meaningful attempt at a solution. A developer could research the LaTeX names for these Unicode symbols, add them to the mapping, and implement appropriate tests. The core task is clear and actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "matplotlib__matplotlib-26078": {
        "explanation": "This issue is well-specified with clear information about the bug, reproduction steps, and expected behavior:\n\n1. The bug is clearly defined: when plotting on a parasite axis using `axes_grid1.parasite_axes`, units are not automatically applied to the parasite axis.\n\n2. A complete, minimal reproduction code is provided that demonstrates the issue.\n\n3. Screenshots showing both the actual (incorrect) and expected (correct) outcomes are included.\n\n4. The reporter has done significant investigation into the root cause of the issue, identifying the specific line of code that's causing the problem (`self._get_lines = self._parent_axes._get_lines` in `ParasiteAxesBase.cla`).\n\n5. The reporter has even proposed a potential solution with code changes that would fix the issue while preserving the original functionality.\n\n6. All relevant environment information is provided (OS, Matplotlib version, backend, Python version).\n\nThis issue contains all the necessary information for a developer to understand, reproduce, and fix the problem without needing additional clarification. The root cause has been identified and a potential solution has been proposed, making it very straightforward to implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26101": {
        "explanation": "This issue is well-specified with clear reproduction steps and a clear description of the problem. The user has provided:\n\n1. A clear bug description: Star markers using mathtext are not center-aligned\n2. A minimal code example that reproduces the issue\n3. A screenshot showing the problem\n4. The expected outcome: center-aligned markers regardless of whether mathtext is used\n5. The Matplotlib version (3.7.1)\n\nThe issue is about the alignment of markers in Matplotlib plots, specifically when using mathtext for markers like \"$\\star$\". The screenshot clearly shows the misalignment compared to regular markers. The reproduction code is simple and complete.\n\nWhile some information like Python version, backend, and OS are missing, these are not critical for understanding or reproducing this particular issue since the problem is specific to Matplotlib's handling of mathtext markers and their alignment. The core problem is well-defined and can be addressed without this additional information.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26113": {
        "explanation": "This issue is very well-specified with clear details about the bug:\n\n1. The issue describes a specific inconsistency in the behavior of the `mincnt` parameter in matplotlib's `hexbin` function, depending on whether the `C` parameter is supplied.\n\n2. The reporter provides a complete, reproducible code example that clearly demonstrates the inconsistency with different parameter combinations.\n\n3. The actual behavior is clearly described: when `C` is not specified, `mincnt=1` shows gridpoints with at least 1 datum; when `C` is specified, `mincnt=1` shows gridpoints with at least 2 data points.\n\n4. The expected behavior is explicitly stated: the reporter expects the same gridpoints to be plotted whether `C` is supplied or not when `mincnt=1`.\n\n5. The reporter has done thorough investigation, providing links to the relevant code in the matplotlib repository that handles the `mincnt` parameter differently in the two cases.\n\n6. A specific solution is proposed: change the behavior when `C` is not None to use `len(vals) >= mincnt` instead of `len(vals) > mincnt`.\n\n7. The environment details are fully provided, including OS, matplotlib version, backend, and Python version.\n\nThis issue contains all the information needed to understand the problem, reproduce it, and implement a solution. The reporter has even identified the specific lines of code that need to be modified.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26122": {
        "explanation": "This issue is well-specified with clear technical details about the problem:\n\n1. The issue clearly identifies the bug: `imshow()` modifies the axes aspect even when `transform` is not set to `ax.transData`.\n\n2. It explains the current behavior: When `aspect=None` (default), it uses `rcParams[\"image.aspect\"]` which defaults to \"equal\", causing square image pixels.\n\n3. It explains why this is problematic: When using a custom transform that's not in data coordinates, modifying the aspect ratio doesn't make sense and causes confusion.\n\n4. It proposes a specific solution: When `transform` is not `ax.transData`, `aspect=None` should mean \"don't modify the aspect\" rather than using the rcParams default.\n\n5. It references related issues (#14057, #14117, #13642) that provide additional context.\n\n6. It acknowledges a potential drawback of the proposed solution: parameter interaction complexity.\n\n7. It includes environment information.\n\nThe issue provides enough context for a developer to understand the problem, reproduce it, and implement a solution that addresses the specific behavior change requested.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26160": {
        "explanation": "This issue is well-specified for implementation. It clearly identifies:\n\n1. The problem: _AxLine class lacks public setters for its key properties (_xy1, _xy2, _slope)\n2. The desired outcome: Add setter methods for these properties to allow modifying them after instantiation\n3. Constraints and considerations:\n   - No need to allow switching between point-point and point-slope representations\n   - The implementation should follow normal Artist design patterns\n   - The change would likely make _AxLine public\n\nThe issue provides enough context about the current limitation and what needs to be implemented. The implementation would involve:\n1. Adding setter methods for the properties\n2. Ensuring proper validation in the setters\n3. Making _AxLine public (removing the underscore prefix)\n\nThis is a straightforward enhancement request with clear requirements that any developer familiar with Matplotlib's codebase could implement without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26184": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected outcomes. The bug report includes:\n\n1. A clear description of the problem: AnnotationBbox causes constrained layout warnings\n2. Complete, runnable code to reproduce the issue\n3. The actual outcome (warning message)\n4. The expected outcome (no warning)\n5. Additional information including a working alternative approach\n6. A workaround that the reporter has found (using set_in_layout(False))\n7. Environment details (Ubuntu 22, Matplotlib 3.6.2, pip installation)\n\nThe issue is specifically about AnnotationBbox not returning the correct window_extent before the first draw, which causes constrained layout to fail. The reporter has provided enough context and code to understand and reproduce the problem. A developer would be able to investigate why AnnotationBbox behaves differently from text objects with respect to layout calculations and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26208": {
        "explanation": "This issue is well-specified with clear details for reproduction and understanding the problem:\n\n1. The bug is clearly described: when using twin x-axis charts, the data limits of the first axis (ax1) get changed to \u00b1inf when plotting a stackplot on the second axis (ax2).\n\n2. A complete, minimal code example is provided that reproduces the issue.\n\n3. The actual outcome is shown with the specific output demonstrating the problem.\n\n4. The expected outcome is clearly stated: the data limits of ax1 should not change when plotting on ax2.\n\n5. Version information is provided: the bug appears in Matplotlib 3.4.0 through 3.7.1.\n\n6. The reporter has done investigation work, noting that the behavior differs when swapping the plot order, and that this appears to be a regression from versions prior to 3.4.0.\n\n7. Operating system, Python version, and installation method are all specified.\n\nThe issue contains all the necessary information to understand, reproduce, and potentially fix the bug. A developer could work on this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26232": {
        "explanation": "This issue is well-specified with clear information about the bug, reproduction steps, and expected outcome:\n\n1. The bug is clearly defined: When using `pcolor` with masked arrays that have read-only masks, it fails with a `ValueError: array is read-only` error.\n\n2. A complete reproduction code snippet is provided that demonstrates the issue.\n\n3. The exact error traceback is included, showing where the problem occurs in the code.\n\n4. The expected outcome is clearly stated: \"No error\" should occur.\n\n5. The submitter even identifies the specific code location in the codebase that needs to be fixed (related to a previous fix for issue #26093) and offers to fix it.\n\n6. All relevant environment information is provided (OS, Matplotlib version, backend, Python version).\n\nThis issue contains all the necessary information to understand the problem and implement a solution without needing additional clarification. The fix would likely involve modifying the code to handle read-only masks properly in the identified location.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26278": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The user is describing an inconsistency in how keyword arguments (kwargs) are handled in matplotlib's ContourSet compared to other plotting functions. Specifically:\n\n1. The problem is clearly defined: When using `contourf()`, the `clip_path` kwarg is ignored, unlike other plotting functions like `pcolor()`.\n2. The current workaround is demonstrated: Users must manually apply `set_clip_path()` to each collection after creating the ContourSet.\n3. The desired solution is articulated: Either add a `set_clip_path` method to ContourSet or modify how kwargs are passed to make it consistent with other plotting functions.\n4. The context and use case are well-explained: Clipping plots inside polygons for map visualization.\n5. The issue includes code examples that demonstrate both the problem and the current workaround.\n\nA developer would have enough information to understand the issue and implement one of the two proposed solutions without needing further clarification. The issue also references a mailing list thread for additional context if needed.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26285": {
        "explanation": "This issue is well-specified with a clear bug description, reproducible code example, and expected outcome. The user reports that `set_yticks(ticks, which=\"minor\")` produces a confusing error message: \"ValueError: labels argument cannot be None when kwargs are passed\". \n\nThe user identifies two specific problems:\n1. The parameter naming inconsistency: `which='minor'` is incorrect for `set_yticks`, and should be `minor=True` instead\n2. The error message is unhelpful and doesn't clearly indicate the actual problem (incorrect parameter name)\n\nThe reproduction code is complete, showing exactly how to trigger the error. The expected outcome is clearly stated - either the function should work with the `which` parameter or provide a more helpful error message that indicates the parameter name issue.\n\nThe issue is reported against the main branch of Matplotlib. While some system information is missing, it's not critical for understanding or reproducing this specific bug, which appears to be a code-level issue in the Matplotlib API rather than a platform-specific problem.\n\nA developer would be able to locate the `set_yticks` method in the codebase, understand the parameter handling issue, and implement a fix to either accept the `which` parameter (for consistency with other API methods) or improve the error message to be more informative.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26291": {
        "explanation": "This issue is well-specified with clear information about the bug. The user has provided:\n\n1. A clear description of the problem: Unable to create inset axes using `mpl_toolkits.axes_grid1.inset_locator.inset_axes`\n2. A minimal reproducible code example that demonstrates the issue\n3. The complete error traceback showing the specific error: `AttributeError: 'NoneType' object has no attribute '_get_renderer'`\n4. The expected outcome: an empty box in the top right of the first subplot\n5. Environment details including:\n   - Operating system (Arch Linux)\n   - Matplotlib version (3.7.2)\n   - Matplotlib backend (module://matplotlib_inline.backend_inline)\n   - Python version (3.8.17)\n   - Jupyter version (3.6.5)\n   - Installation method (conda)\n\nThe issue appears to be a bug in the interaction between inset_axes and the inline backend in Jupyter. The error occurs because the figure object doesn't have a renderer when trying to create the inset axes. This is a specific technical issue with clear reproduction steps and a clear error message, making it well-specified for someone to investigate and fix.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26300": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected vs. actual outcomes, and even a likely cause of the bug. The reporter has:\n\n1. Provided a minimal code example that reproduces the issue\n2. Clearly stated what happens (warning appears) vs. what should happen (no warning)\n3. Identified when the issue started appearing (between 3.7.1 and 3.7.2)\n4. Pointed to the specific commit and PR that likely introduced the issue\n5. Suggested a potential fix approach (\"not warn if we set the place holder due to `fig.tight_layout`\")\n6. Included relevant environment information\n\nThe issue is about `fig.tight_layout()` showing a warning when called multiple times, which started happening in matplotlib 3.7.2. The reporter even identified the likely cause as their own PR (#25624/#25626). This provides enough context for a developer to understand the problem and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26311": {
        "explanation": "This issue is well-specified with clear information about the bug and how to reproduce it:\n\n1. The problem is clearly defined: labels can't be placed at the start of contours, causing an IndexError.\n2. A complete reproduction code is provided that demonstrates the error.\n3. The exact error message and traceback are included.\n4. The expected outcome is shown with a screenshot from a working version (3.7.1).\n5. The submitter even points to a specific commit that fixes the issue.\n6. All relevant environment information is provided (OS, Matplotlib version, backend, Python version).\n\nThe issue includes all the necessary information to understand the problem, reproduce it, and even has a pointer to the solution. The only minor challenge mentioned is creating a test case, but that doesn't affect the clarity of the issue itself. A developer could take this information and immediately start working on a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26342": {
        "explanation": "This issue is well-specified with a clear problem and proposed solution. The issue describes:\n\n1. A specific problem: Cartopy is using a workaround to replace paths in a ContourSet object using direct list assignment (`paths[:] = transformed_paths`), which is considered inelegant.\n\n2. A clear proposed solution: Add a `set_paths()` method to the ContourSet class that would provide a cleaner API for this operation.\n\nThe issue includes code examples showing both the current approach and the desired approach, making it very clear what needs to be implemented. The enhancement request is straightforward - add a method to a class to encapsulate functionality that's currently being done through direct attribute manipulation.\n\nA developer would know exactly what to implement: a new `set_paths()` method for the ContourSet class that takes transformed paths as input and updates the internal paths collection. This is a well-defined task with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26466": {
        "explanation": "This issue is well-specified with a clear description of the bug, reproducible code, and visual evidence of the problem. The issue describes that when an array is passed as the `xy` parameter to an annotation with arrows, modifying the array after calling the function changes the arrow position. This suggests that Matplotlib is storing a reference to the array rather than making a copy.\n\nThe bug report includes:\n1. A clear description of the problem\n2. Complete code to reproduce the issue\n3. A screenshot showing the actual outcome\n4. A clear statement of the expected outcome (both arrows should be horizontal)\n5. Detailed environment information\n\nThe solution would require modifying the annotation functionality to create a copy of the input arrays rather than storing references to them. This is a well-defined task with a clear success criterion: after the fix, modifying arrays after they've been passed to the annotation function should not affect the annotation's appearance.",
        "label": "WELL_SPECIFIED"
    },
    "matplotlib__matplotlib-26469": {
        "explanation": "This issue is well-specified with clear context and a defined problem. The issue explains:\n\n1. The current behavior: PdfPages defaults to keep_empty=True, which allows creating PDF files with zero pages\n2. The problem: PDF files with zero pages are invalid according to the PDF standard, and they don't open in common PDF readers\n3. The proposed solution: Change the default to keep_empty=False and eventually deprecate the parameter entirely\n4. The rationale: Creating invalid PDF files by default is problematic, and the issue cites the PDF standard to support this claim\n5. A rough deprecation strategy is mentioned (warning with suppress_warning kwarg)\n\nThe issue provides enough context about the history of the feature (#2453), the current behavior, the problem with that behavior, and a clear direction for the solution. A developer could implement this change without needing additional clarification, as the technical requirements are straightforward:\n1. Change the default value of keep_empty from True to False\n2. Add deprecation warnings for the parameter\n3. Eventually remove the parameter entirely\n\nThe only minor detail not fully specified is the exact deprecation timeline, but that's typically determined by project policies and can be discussed during review.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-2457": {
        "explanation": "This issue is well-specified with clear information about what's happening and what should happen instead:\n\n1. The issue clearly identifies the problem: `sns.lineplot()` is ignoring the `ci=None` parameter.\n2. It provides a specific code example that reproduces the issue.\n3. It clearly states the expected behavior: the function should warn the user and reformat the arguments to use `errorbar=None` instead.\n\nThe issue appears to be related to parameter handling in the Seaborn lineplot function, where an older parameter (`ci=None`) is being ignored when it should be translated to a newer parameter format (`errorbar=None`). This is likely related to API changes in Seaborn where `ci` parameter might be deprecated in favor of `errorbar`.\n\nA developer would be able to:\n1. Locate the lineplot function in the Seaborn codebase\n2. Identify how parameter handling works for `ci` and `errorbar` parameters\n3. Implement a warning when `ci=None` is used\n4. Add code to reformat the arguments appropriately\n\nThe issue provides all the necessary information to understand the problem and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-2766": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue clearly identifies a specific DeprecationWarning that occurs when using setuptools>=60.0.0 with seaborn.\n2. It provides the exact warning message and a reproducible test case with commands to trigger the issue.\n3. It references the root cause (distutils version classes being deprecated) and points to the relevant setuptools commit.\n4. It mentions that a fix has already been implemented in PR #2466 on the master branch but hasn't been released yet.\n5. The request is clear: consider releasing a new patch version sooner to address this warning, as it can impact CI/CD setups.\n\nThe solution path is straightforward - release a new version that includes the already-implemented fix from PR #2466. There's no ambiguity about what needs to be done or what would constitute a successful resolution.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-2853": {
        "explanation": "This issue is reasonably well-specified. It describes a specific behavior change for the x_estimator error bars in what appears to be a visualization library (likely seaborn or a similar plotting library). The issue states that x_estimator error bars should inherit the alpha (transparency) parameter from scatterplot settings when present, since these error bars replace the scatterplot.\n\nThe issue references a fix for GitHub issue #2538, which suggests this is implementing a requested feature or fixing a bug that was previously documented.\n\nWhat makes this reasonably specified rather than well-specified:\n1. It doesn't explicitly mention which function or class this applies to (though it's likely a plotting function with x_estimator parameter)\n2. It doesn't provide code examples of before/after behavior\n3. It doesn't specify edge cases (what happens if alpha is not specified in scatter_kws?)\n\nHowever, a developer familiar with the codebase should be able to:\n1. Locate the code handling x_estimator error bars\n2. Identify where the alpha parameter needs to be inherited from scatter_kws\n3. Implement the change so that error bars respect the alpha setting\n\nThe issue is clear enough about what needs to be changed and why, making it reasonably actionable.",
        "label": "REASONABLY_SPECIFIED"
    },
    "mwaskom__seaborn-3010": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue states that PolyFit is not robust to missing data (None values), and provides a specific code example that demonstrates the error:\n\n```python\nso.Plot([1, 2, 3, None, 4], [1, 2, 3, 4, 5]).add(so.Line(), so.PolyFit())\n```\n\nThe full traceback is included, showing that the error occurs in the numpy.polyfit function when it encounters None values in the data. The error is a LinAlgError: \"SVD did not converge in Linear Least Squares\".\n\nFrom this information, I can clearly understand:\n1. The specific functionality that's failing (PolyFit)\n2. The exact condition causing the failure (presence of None values in data)\n3. Where in the code the error is occurring (in the polyfit function call in _fit_predict method)\n\nThe solution would need to handle None values in the input data before passing it to numpy.polyfit, likely by filtering them out. This is a common pattern in data visualization libraries to gracefully handle missing data points.\n\nThe issue is specific, reproducible, and the solution approach is clear, making it well-specified for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3069": {
        "explanation": "This issue is reasonably well-specified, describing a feature request to make the `Nominal` scale in seaborn behave like categorical scales with three specific behaviors:\n\n1. The scale should extend +/- 0.5 from first/last ticks\n2. Grid lines should be suppressed\n3. Y-axis should be inverted\n\nThe issue provides good context about what needs to be done and even includes implementation suggestions for each point. The author acknowledges some implementation challenges, particularly for point #1, mentioning potential approaches like using invisible artists with sticky edges.\n\nWhile the issue provides clear direction, there are some ambiguities that would need to be resolved during implementation:\n- For point #2, the author raises questions about deferring to theme settings and how to handle user preferences for grids\n- The exact location for implementing point #3 is uncertain\n- The implementation details for point #1 are described as \"tricky\" with some potential approaches but no definitive solution\n\nDespite these uncertainties, there's enough information to make a meaningful attempt at a solution. A developer familiar with seaborn's codebase would likely be able to implement this feature based on the description provided, possibly with some exploration of the existing code to determine the best implementation approach.",
        "label": "REASONABLY_SPECIFIED"
    },
    "mwaskom__seaborn-3187": {
        "explanation": "This issue is well-specified with clear information about what's wrong and how to reproduce it:\n\n1. The problem is clearly defined: legends for large numeric ranges created with ScalarFormatter don't show the multiplicative offset value.\n\n2. A complete reproducible example is provided with code that demonstrates the issue.\n\n3. The issue includes a screenshot showing the incorrect output.\n\n4. The submitter has done investigation work and points to the specific part of the codebase that likely contains the issue (the ScalarFormatter usage in scales.py).\n\n5. The submitter suggests a potential solution approach: retrieving the offset value from formatters and using it to create proper legend titles/labels.\n\n6. Relevant configuration parameters that might affect the issue are mentioned (rcParams settings).\n\nThe issue provides all the necessary information for a developer to understand the problem, reproduce it, and start working on a solution. The root cause is identified, and a direction for the fix is suggested.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3190": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible error. The issue shows that using boolean data for color mapping in seaborn's Plot function causes a TypeError. The error occurs specifically when trying to perform subtraction on boolean values during scale setup in the plotting process. The traceback provides the exact location of the error in the codebase (in scales.py, line 351), showing that the problem happens when trying to subtract boolean values with the `-` operator, which is not supported for numpy boolean types. The error message even suggests alternatives like using bitwise_xor or logical_xor instead. With this information, a developer would be able to identify the issue and implement a fix that properly handles boolean data in color mappings.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3394": {
        "explanation": "This issue is well-specified with a clear problem statement, expected behavior, and a complete reproducible example. The issue describes that when using `pd.NA` in a dataset that is plotted with seaborn, the axis ordering gets reversed to descending order, whereas this doesn't happen when using `np.nan`. The expected behavior is that NAs should be excluded without affecting the axis order.\n\nThe provided code example is complete and demonstrates the issue clearly. It creates three datasets: one with no missing values, one with `pd.NA`, and one with `np.nan`, then plots them side by side using seaborn's FacetGrid. The resulting image shows that the x-axis ordering is reversed only in the plot with `pd.NA`.\n\nThe issue also includes relevant system information (Python, seaborn, and pandas versions), which helps in reproducing and debugging the problem.\n\nA developer would have all the necessary information to investigate why `pd.NA` causes this behavior and implement a fix that ensures consistent axis ordering regardless of the type of missing value used.",
        "label": "WELL_SPECIFIED"
    },
    "mwaskom__seaborn-3407": {
        "explanation": "This issue is well-specified with a clear problem description, a minimal reproducible example (MRE), the error message, and even a workaround. \n\nThe issue describes that when trying to use Seaborn's `pairplot` function with a Pandas DataFrame that has a MultiIndex (hierarchical indexing for columns), it raises a KeyError. The error occurs because `pairplot` is trying to access individual columns by name, but with MultiIndex columns, the access pattern is different.\n\nThe MRE provides complete code to reproduce the issue, showing:\n1. How to create a sample DataFrame with MultiIndex columns\n2. The exact error that occurs when calling `sns.pairplot(df)`\n3. A detailed traceback showing where the error occurs in Seaborn's code\n4. A workaround by flattening the column names\n\nThis is sufficient information for a developer to understand the problem, reproduce it, and implement a fix that would allow `pairplot` to properly handle DataFrames with MultiIndex columns.\n\nThe solution would likely involve modifying Seaborn's code to properly handle MultiIndex DataFrames when extracting columns for plotting.",
        "label": "WELL_SPECIFIED"
    },
    "pallets__flask-5014": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: Blueprints with empty names cause functionality issues (with a specific reference to issue #4944)\n2. The desired solution: Raise a `ValueError` when a Blueprint is given an empty name\n\nThe issue provides enough context to understand what needs to be fixed and how to fix it. The solution would involve adding validation to the Blueprint class to check if the name is empty and raise a ValueError if it is. The reference to another issue (#4944) provides additional context that could be consulted if needed.\n\nThe implementation would be straightforward - find where Blueprint names are set (likely in the constructor or a setter method) and add validation to ensure the name is not empty before proceeding.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1142": {
        "explanation": "This issue is well-specified enough to understand the problem and attempt a solution. The user clearly states:\n\n1. The specific problem: requests.get is automatically adding a 'Content-Length' header to GET requests\n2. Why this is problematic: Some servers (like amazon.com) return a 503 error when a GET request contains a Content-Length header\n3. Suggested solutions: Either stop adding this header automatically for GET requests or provide an option to disable it\n\nThe HTTP specification doesn't require a Content-Length header for GET requests since they typically don't have a body, so this behavior could be considered unexpected. The issue provides a concrete example that can be verified (amazon.com returning 503 errors) and suggests reasonable approaches to fix it.\n\nA developer could investigate why the Content-Length header is being added to GET requests in the requests library, verify the issue with the provided example, and implement one of the suggested solutions (either removing the automatic addition for GET requests or adding an option to disable it).",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1327": {
        "explanation": "This issue is well-specified with clear details about the problem and potential solutions. The issue describes:\n\n1. A specific bug in the Session.get_adapter() method where adapter selection depends on dictionary key order, making adapter mounting unreliable\n2. The expected behavior according to documentation\n3. A clear example demonstrating the problem\n4. Three potential solutions with their pros and cons:\n   - Sorting keys in get_adapter() (slow but compatible)\n   - Using a custom dictionary type (faster but with maintenance costs)\n   - Using a list of tuples (fastest but with backward compatibility issues)\n5. A specific sorting strategy (longest to shortest, then alphabetically)\n\nThe issue provides enough context about the problem in the requests library's transport adapter functionality and outlines clear options for fixing it. A developer could implement any of the proposed solutions or devise their own based on the detailed problem description. The issue even links to the relevant code and documentation, making it very straightforward to understand what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1339": {
        "explanation": "This issue is very well-specified with clear details about the problem and a proposed solution:\n\n1. The issue is clearly identified: CaseInsensitiveDict's __setitem__ method doesn't properly handle keys with different cases, causing only the first assignment to \"work\" effectively.\n\n2. The reporter provides a complete test case that demonstrates the problem, with both code and output showing the unexpected behavior.\n\n3. The root cause is identified: the __setitem__ method is storing keys with their original case rather than normalizing them.\n\n4. A specific fix is proposed: modify __setitem__ to store keys in lowercase form.\n\n5. The reporter even shows the expected output after applying the fix, demonstrating that it resolves the issue.\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and implement a solution. The proposed fix is straightforward and logical, making this a very well-specified issue that could be addressed with a PR without needing any additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1376": {
        "explanation": "This issue is reasonably well-specified. It provides:\n\n1. A specific failing test case (`test_unicode_multipart_post_fieldnames`)\n2. The exact error message and assertion failure\n3. The test code that's failing\n4. The specific line that's failing (`self.assertFalse(b'name=\"b\\'stuff\\'\"' in prep.body)`)\n\nThe problem is clear: when using a byte string as a field name in a multipart POST request, the field name in the request body sometimes includes the Python representation prefix (`b'stuff'`) rather than just the content (`stuff`). This is causing the test to fail because the assertion expects the field name to be properly encoded without the byte string representation markers.\n\nWhile the issue doesn't explicitly state the expected behavior or provide details about when exactly it fails (\"fails sometimes\"), the test itself makes the expected behavior clear - the field name should be encoded as \"stuff\" not \"b'stuff'\". A developer familiar with the codebase should be able to identify where the multipart encoding happens and fix the issue with the information provided.\n\nThe only missing information is the specific conditions under which the test fails (since it says \"fails sometimes\"), but this can be determined during investigation of the code.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1537": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The user provides a specific error scenario with complete code examples\n2. The error traceback is included showing exactly where the problem occurs\n3. The user mentions that the issue worked in version 1.2.0 but fails in 1.2.3\n4. The root cause is identified: when using both `data` with float values and `files` parameters in a POST request\n5. The user also mentions their original problem involved datetime objects in the data dict\n\nThe issue is a regression bug where the library can no longer handle certain data types (floats, and by extension datetimes) when making multipart/form-data requests with both data and files parameters.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - the encode_multipart_formdata function needs to properly handle float values (and likely other non-string types) when preparing the request body.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1657": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a specific problem: session hooks are being merged with request hooks in a way that causes one to overwrite the other, rather than properly combining them. The issue even includes a link to the specific line of code in the GitHub repository (line 264 in requests/sessions.py) where the problem occurs.\n\nFrom this description, I can understand:\n1. The problem is in the requests library's session hook handling\n2. The specific issue is that when merging hook dictionaries, one overwrites the other\n3. The expected behavior would be to combine the lists rather than overwrite them\n4. The location of the problematic code is identified\n\nThe only minor detail missing is an explicit description of what the correct behavior should be, but it's strongly implied that the hooks should be combined (likely by extending/appending the lists) rather than having one overwrite the other. This is a common pattern in Python when merging collections.\n\nA developer with knowledge of the requests library would be able to understand and address this issue without needing additional clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1713": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue identifies a specific regression in requests-2.0.1 when using MozillaCookieJar.\n2. It provides a complete, minimal reproducible example that demonstrates the error.\n3. It includes the full error traceback showing exactly where the problem occurs.\n4. It points to the specific commit that likely caused the regression (https://github.com/kennethreitz/requests/commit/012f0334ce43fe23044fc58e4246a804db88650d).\n5. The error is clearly explained: MozillaCookieJar is being incorrectly treated as a dict when it should be handled as a CookieJar.\n\nThe root cause is identified in the error trace - in requests/cookies.py, the cookiejar_from_dict function is trying to use dictionary access (__getitem__) on a MozillaCookieJar instance, which doesn't support that operation.\n\nA developer would have all the necessary information to fix this issue by modifying how the cookies parameter is handled when it's a CookieJar subclass like MozillaCookieJar, ensuring it's not processed through cookiejar_from_dict if it's already a CookieJar instance.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1724": {
        "explanation": "This issue is well-specified with clear reproduction steps and error messages. The problem is clearly defined: using a Unicode string for the HTTP method parameter (method=u'POST') causes a UnicodeDecodeError in Python 2.7.2 when using the requests library, while using a regular string (method='POST') works fine.\n\nThe issue includes:\n1. Complete reproduction code for both working and failing cases\n2. Full traceback of the error\n3. The specific version of the requests library being used (1.2.3)\n4. A hypothesis about the cause of the issue (unicode method name \"infecting\" the header)\n5. Reference to the specific line in the code that might be causing the issue (sessions.py:313)\n\nThe error occurs when the Unicode method name propagates through the request chain and eventually causes a conflict when trying to concatenate with binary data. This is a common Python 2.x Unicode handling issue.\n\nA developer would have enough information to diagnose and fix this issue by ensuring proper encoding/decoding of the method name before it reaches the point where it's concatenated with binary data.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1733": {
        "explanation": "This issue is well-specified with a clear problem statement and reproduction steps. The issue is requesting to make the Response class in the requests library compatible with Python's pickle module. The error message clearly indicates the problem: the Response class defines __slots__ but doesn't implement __getstate__, which is required for pickling objects with __slots__. \n\nThe solution would involve implementing the __getstate__ and possibly __setstate__ methods in the Response class to properly handle serialization and deserialization. The error message provides all the necessary information to understand what needs to be fixed, and the reproduction steps are complete and straightforward.\n\nA developer with access to the codebase would be able to:\n1. Locate the Response class in the requests library\n2. Add the required __getstate__ (and possibly __setstate__) methods\n3. Test that the Response objects can now be pickled\n\nThis is a common pattern for making classes with __slots__ pickle-compatible, and the solution approach is standard and well-documented in Python.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1766": {
        "explanation": "This issue is well-specified. It clearly identifies a problem with the Digest Authentication implementation: according to RFC2617, the 'qop-options' directive should be quoted with double quotes, but the current implementation apparently doesn't do this. The issue provides:\n\n1. A reference to the specific RFC (RFC2617) that defines the standard\n2. A direct quote from the RFC showing that qop-options should be a \"quoted string\"\n3. Evidence that other implementations (curl) follow this standard\n4. A practical impact: some server implementations are sensitive to this difference\n\nFrom this description, I can understand that the solution would involve modifying the Digest Authentication code to properly quote the qop options with double quotes as specified in the RFC. This is a clear, specific change with a well-defined expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1776": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The problem: Request cookies are being incorrectly persisted to the session after a fix for another issue (#1630)\n2. The specific location of the problem in the code (with a direct link to the problematic line)\n3. A note that simply removing the problematic code breaks a test case for the previous fix\n\nWhat makes this not fully \"well-specified\" is:\n- It doesn't explicitly state what the correct behavior should be (though it's implied that request cookies should not be persisted to session)\n- It doesn't provide a test case or example demonstrating the issue\n- It doesn't explain why persisting request cookies to the session is problematic\n\nHowever, a developer familiar with the codebase would likely understand the issue and be able to work on a solution that fixes the cookie persistence problem while maintaining compatibility with the test case for #1630. The issue provides enough context to start investigating and implementing a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-1888": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The exact environment is specified (Windows 64-bit with specific package versions)\n2. The problem is clearly described: 301 redirects are broken with the latest pyopenssl/SNI\n3. The error is well-documented with a stack trace showing the TypeError: \"buf must be a byte string\"\n4. The user has identified the root cause: after a redirect, the URL is decoded to a Unicode string, which is then passed to pyopenssl which expects bytes\n5. A minimal test case is provided that reproduces the issue\n6. The specific code path is even referenced with a link to the relevant line in the codebase\n\nA developer would have enough information to understand and fix this issue. The solution would likely involve ensuring that Unicode strings are properly encoded to bytes before being passed to pyopenssl, particularly in the redirect handling code path.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1921": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly describes the expected behavior: setting a session header to None should prevent that header from being sent, as documented.\n\n2. It describes the actual behavior: when setting `session.headers['Accept-Encoding'] = None`, the string \"None\" is sent as the header value instead of removing the header.\n\n3. It provides a code example demonstrating the issue.\n\n4. It references documentation that supports the expected behavior.\n\n5. It provides a workaround that currently works (`del session.headers['Accept-Encoding']`).\n\nThe issue is specific enough that a developer could reproduce the problem, verify the expected behavior from documentation, and implement a fix to ensure that setting a header value to None removes the header rather than sending \"None\" as the value. The solution would likely involve modifying how the Session class handles None values for headers.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1944": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: Requests fails when accessing a URL that returns a 301 redirect with an incorrectly labeled gzipped content.\n\n2. The exact problematic URL is provided: `http://www.whatbird.com/forum/index.php?/gallery/image/291517-foo/`\n\n3. The root cause is identified: When handling redirects, Requests attempts to decode the response body even though it's not necessary for redirects.\n\n4. A detailed error trace is provided showing exactly where the failure occurs.\n\n5. The issue includes comparison with other tools (Chromium, Firefox, curl) that handle this situation correctly.\n\n6. The proposed conceptual question is clear: \"Why decode the response body of a redirect, which won't get returned?\"\n\nA developer could implement a solution based on this information by modifying the redirect handling in Requests to avoid unnecessarily decoding the response body of redirects. The issue provides enough context to understand both the problem and a potential solution approach.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-1962": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a specific problem: the `request.history` attribute can sometimes be a list and sometimes be a tuple, which creates inconsistency in the API. The user suggests that it should always be a list for consistency and \"least surprise\" to users.\n\nThe issue includes a specific link to the relevant code in the repository (line 530 in sessions.py) where the inconsistency occurs. This makes it easy to locate the problematic code.\n\nWhat's missing is:\n1. A complete explanation of all the cases where `request.history` is returned as a tuple vs. a list\n2. Any potential side effects of changing the return type\n3. Test cases that might be affected\n\nHowever, these gaps can be filled by examining the codebase. The core issue is clear: make `request.history` consistently return a list rather than sometimes returning a tuple. This is a reasonable request for API consistency and would be straightforward to implement after reviewing the relevant code.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2148": {
        "explanation": "This issue is well-specified with a clear problem description. The user is reporting that when using the requests library, a raw socket.error exception is being raised instead of being caught and wrapped in a requests-specific exception like ConnectionError. The issue includes a detailed stack trace showing exactly where and how the error occurs, making it clear what needs to be fixed.\n\nThe problem is that when a socket connection is reset by peer (error 104), the raw socket.error bubbles up through the call stack instead of being converted to a requests.exceptions.ConnectionError or similar exception that would be consistent with the requests library's error handling pattern.\n\nThe solution would involve modifying the requests library code to properly catch and wrap socket.error exceptions that occur during HTTP communication. This is a straightforward bug fix with a clear expected outcome: socket errors should be wrapped in appropriate requests exceptions.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2153": {
        "explanation": "This issue is well-specified with clear information about the problem. The user has provided:\n\n1. A complete error stack trace showing exactly where the exception occurs\n2. A clear description of the issue: sometimes getting a raw `socket.timeout` exception instead of the expected `requests.exceptions.Timeout`\n3. Context about the code environment (using the requests library in a SoCo plugin)\n4. The specific line causing the issue: `out = requests.post(url, headers=headers, data=body, timeout=1.0)`\n\nThe problem is clearly defined: the requests library should be catching the underlying `socket.timeout` exception and converting it to a `requests.exceptions.Timeout`, but in some cases it's not doing so. This is likely a bug in the requests library's exception handling.\n\nWhile the user mentions that reproducing the exact scenario requires specific hardware (Sonos speaker) and a subscription service (Wimp), this doesn't prevent understanding or solving the core issue, which is about exception handling in the requests library. A developer familiar with the requests codebase should be able to identify where socket timeouts should be caught and converted to requests exceptions.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2193": {
        "explanation": "The issue is reasonably specified. It describes a regression where a `urllib3.exceptions.ProtocolError` is not being properly wrapped by the requests library in version 2.4.0. The example shows that when trying to connect to a non-existent server (localhost:1), the raw urllib3 exception is being raised instead of being wrapped in a more user-friendly requests exception.\n\nWhile the issue doesn't explicitly state what the expected behavior should be, it's reasonable to infer that in previous versions of requests, this exception would have been wrapped in a requests-specific exception (likely a ConnectionError). The term \"regression\" indicates that this is a change from previous behavior.\n\nTo solve this issue, I would need to:\n1. Verify the behavior in previous versions\n2. Identify where in the code the exception handling for urllib3 exceptions occurs\n3. Fix the code to properly wrap the ProtocolError in an appropriate requests exception\n\nThe issue provides enough information to reproduce the problem and understand what needs to be fixed, even though it doesn't explicitly state the expected behavior.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2317": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly identified: In Python 3.4, when using requests 2.4.3 with python-neutronclient 2.3.9, there's an issue with how binary strings are handled for HTTP methods.\n\n2. The specific code causing the problem is identified: `method = builtin_str(method)` in requests/sessions.py is converting a binary string like `b'GET'` to a literal string `\"b'GET'\"` instead of just `\"GET\"`.\n\n3. The impact is clear: This causes 404 Not Found responses when making requests.\n\n4. The context is well-explained: The issue happens because neutronclient uses `utils.safe_encode_list(args)` which converts values to binary strings.\n\n5. The reporter has done troubleshooting: They've verified the issue exists in Python 3.4 but not in Python 2.6, narrowing down the cause.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - either modify how requests handles binary string methods or adjust how neutronclient encodes arguments.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2617": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information:\n\n1. The problem is clearly stated: prepared requests with binary files fail to send when `unicode_literals` is imported in Python 2, but work fine in Python 3 or in Python 2 without `unicode_literals`.\n\n2. A complete, minimal reproduction script is provided that demonstrates the issue.\n\n3. The exact error message and traceback are included, showing a `UnicodeDecodeError` when trying to send the prepared request.\n\n4. The environment details are clear (Python 2.7, requests 2.7.0).\n\n5. The user has already narrowed down the problem to the combination of prepared requests, binary files, and `unicode_literals` in Python 2.\n\nThe issue is specific to Python 2's handling of unicode and binary data when `unicode_literals` is imported, which affects how the requests library processes binary file data in prepared requests. This is a well-defined compatibility problem that can be investigated and fixed without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2674": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that urllib3 exceptions (specifically DecodeError and TimeoutError) are \"passing through\" the requests API instead of being wrapped by requests' own exception types. \n\nThe issue provides:\n1. Clear examples of the specific exceptions that are leaking through (DecodeError and TimeoutError)\n2. A specific context for at least one of them (TimeoutError occurs when a proxy times out)\n3. An implied expectation that requests should wrap all urllib3 exceptions with its own exception types\n\nWhat's missing:\n1. Code examples demonstrating the issue\n2. Specific versions of the libraries where this occurs\n3. Whether this is a regression or has always been the case\n\nHowever, a developer familiar with the requests codebase should be able to:\n1. Verify if wrapping all urllib3 exceptions is indeed a design goal\n2. Check the exception handling code to see if these specific exceptions are being missed\n3. Implement a fix to properly wrap these exceptions if needed\n\nThe issue is actionable with the information provided, even though additional details would be helpful.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2678": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that urllib3 exceptions (specifically DecodeError and TimeoutError) are \"passing through\" the requests API instead of being wrapped by requests' own exception types. \n\nThe issue provides:\n1. Clear examples of the specific exceptions that are leaking through (DecodeError and TimeoutError)\n2. A specific context where one occurs (TimeoutError when a proxy times out)\n3. The user's expectation that requests should wrap all urllib3 exceptions\n\nWhat's missing is:\n1. Code examples showing how these exceptions are encountered\n2. The specific version of requests being used\n3. Whether this is a regression or has always been the case\n\nHowever, this is still actionable information. A developer could investigate the exception handling in requests to see if these specific urllib3 exceptions are being properly caught and wrapped, and if not, implement the appropriate exception handling. The issue is clear enough to understand what needs to be fixed.\n\nThe solution would involve ensuring that all urllib3 exceptions, particularly the two mentioned, are properly caught and wrapped by requests' own exception types.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-2821": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The user provides a specific error message and traceback showing a TypeError: \"cannot make memory view because object does not have the buffer interface\"\n2. The issue occurs after upgrading from requests 2.7.0 to 2.8.0\n3. The user confirms that downgrading back to 2.7.0 resolves the issue\n4. Complete version information is provided for all relevant packages\n5. A link to the full traceback is included\n\nThe error is clearly related to the requests library upgrade and specifically involves the pyopenssl integration with requests. The traceback shows the error occurs in the urllib3/contrib/pyopenssl.py file when trying to create a memory view of data being sent.\n\nThis is a regression bug that appears to be related to how requests 2.8.0 handles data when using PyOpenSSL. A developer would have enough information to:\n1. Reproduce the issue\n2. Investigate the changes between requests 2.7.0 and 2.8.0\n3. Focus on the specific component (pyopenssl.py) where the error occurs\n4. Develop a fix that ensures compatibility with the buffer interface\n\nThe issue is specific, reproducible, and contains all necessary context for debugging.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2873": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible examples. The issue describes a specific bug where POST requests hang when using a StringIO object as the request body if the StringIO's position is at the end rather than the beginning. The reporter has identified:\n\n1. The exact problematic scenario (using a StringIO that has been written to without seeking back to position 0)\n2. A clear reproduction case with code examples\n3. Two workarounds (calling s.seek(0) or using an empty StringIO)\n4. The expected behavior (getting a 405 error response rather than hanging)\n5. A hypothesis about the root cause (request not fully formed)\n\nA developer would have enough information to reproduce the issue, investigate why StringIO objects at the end position cause requests to hang, and implement a fix that likely involves automatically seeking to the beginning of StringIO objects when they're used as request bodies. The issue is specific, reproducible, and has a clear definition of success.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-2931": {
        "explanation": "This issue is reasonably well-specified. It provides:\n\n1. A clear problem statement: requests with binary payload are failing in version 2.9 but worked in 2.8.1\n2. A specific code example that reproduces the issue\n3. A reference to a related GitHub issue that introduced the problem\n4. The specific versions where it works (2.8.1) and doesn't work (2.9)\n\nThe issue description doesn't explicitly state the error message that occurs, which would have been helpful. However, the title mentions \"calling to_native_string\" which gives a strong hint about where the problem might be occurring. The reference to the GitHub issue #2844 also provides context about related changes.\n\nWith this information, a developer could:\n1. Reproduce the issue using the provided code example\n2. Compare the behavior between versions 2.8.1 and 2.9\n3. Investigate the changes introduced in the referenced GitHub issue\n4. Look for issues in how binary data is handled, particularly around the to_native_string function\n\nWhile some additional details would be helpful (like the exact error message), there's enough information to make a meaningful attempt at solving the problem.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-3362": {
        "explanation": "This issue is reasonably well-specified. The user is describing a specific behavior they're observing with the `requests` library where `iter_content(decode_unicode=True)` is returning bytes instead of unicode strings, while `r.text` returns unicode strings. They expected both to return unicode when `decode_unicode=True`. \n\nThe issue includes:\n1. The specific methods being compared (`iter_content` vs `text`)\n2. The specific parameter being used (`decode_unicode=True`)\n3. The observed behavior (bytes vs unicode)\n4. The expected behavior (both should return unicode)\n5. The version information (Python 3.5.1 and requests 2.10.0)\n\nWhat makes this not fully well-specified is that the user hasn't provided a complete reproducible example with the exact code and response they're working with. However, there's enough information to investigate the issue in the codebase and determine if there's a bug or misunderstanding in how `iter_content` with `decode_unicode=True` works compared to `r.text`. The user also offered to provide an example if needed, showing they're willing to provide more details.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-4106": {
        "explanation": "This issue is well-specified with clear information about what's wrong and how to reproduce it. The issue describes:\n\n1. The specific error: AttributeError: module 'requests.packages' has no attribute 'urllib3'\n2. Expected behavior: requests.packages.urllib3 should be the urllib3 package\n3. Actual behavior: The attribute error occurs\n4. Clear reproduction steps with a simple code snippet\n5. Detailed system information including versions of Python (3.6.1), requests (2.17.1), and urllib3 (1.21.1)\n6. Reference to a specific commit that likely caused the issue\n\nThe issue points to a specific commit that appears to have broken the functionality, providing a clear starting point for investigation. A developer would have enough information to understand the problem, reproduce it, and work on a fix by examining how that commit changed the package structure. The solution would likely involve restoring access to urllib3 through the requests.packages namespace.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-4356": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem occurs when a proxy URL is malformed (missing a slash in the protocol specification, e.g., \"http:/my.proxy.com\" instead of \"http://my.proxy.com\"). Instead of providing a helpful error message that points to the proxy configuration issue, the system throws a confusing exception: \"AttributeError: 'NoneType' object has no attribute 'startswith'\".\n\nThe issue includes:\n1. Clear reproduction steps with exact commands\n2. The exact error traceback\n3. System information with version details\n4. Expected result: \"A better exception\" that identifies the proxy configuration problem\n\nWhile the submitter is unsure where the fix should be implemented (requests, urllib3, or urlparse), this doesn't affect the clarity of the issue itself. The goal is clear: improve error handling to provide a more helpful message when proxy URLs are malformed. A developer with access to the codebase could trace through the error and determine the appropriate place to add validation or better error handling for malformed proxy URLs.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-5087": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Clearly described the problem: accessing `response.content` after an error on the first access returns an empty string instead of raising the same error again\n2. Provided a specific code example demonstrating the issue\n3. Included a test case as a patch file that reproduces the problem\n4. Specified the expected behavior: subsequent access to `response.content` should raise the same exception\n5. Included system information and version details\n\nThe issue is focused on a specific bug in the requests library's handling of response content after an error. The reproduction steps are straightforward and can be tested with the provided code. The expected behavior is clearly defined, making it possible to implement and verify a solution.\n\nA developer would have enough information to:\n1. Understand the problem\n2. Reproduce the issue using the provided test case\n3. Investigate how `response.content` is implemented\n4. Fix the handling of errors so that they are preserved for subsequent accesses\n\nThis is a well-defined bug report with all the necessary information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-5414": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is that when making a request to a URL with an empty label (like \"http://.example.com\"), the code raises a UnicodeError instead of the expected InvalidUrl exception.\n\nThe issue includes:\n1. Clear reproduction steps with a simple code example\n2. The expected result (based on a previous PR)\n3. The actual result with the specific error message\n4. Detailed system information including versions of Python and all relevant packages\n5. A reference to a similar issue (#4168) and a specific line in the codebase that suggests the intended behavior\n\nThe solution would involve catching the UnicodeError that occurs when processing URLs with invalid labels and re-raising it as an InvalidUrl exception, which is consistent with how similar issues are handled in the codebase. The issue even mentions that \"even catching the error just to rethrow as a requests exception would be beneficial,\" indicating a clear path to resolution.\n\nThis is a well-defined bug with a straightforward fix that doesn't require additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-6028": {
        "explanation": "This issue is reasonably well-specified with some key details provided:\n\n1. Clear problem statement: The user is experiencing a 407 Proxy Authentication Required error when using proxies in Python 3.8.12, while other Python versions work fine.\n\n2. Expected behavior: A 200 status code response.\n\n3. Actual behavior: A 407 status code response.\n\n4. Sample code provided: A simple reproducible example using the requests library.\n\n5. System information: Detailed environment information including Python version, OS, and library versions.\n\n6. Potential cause identified: The user references a specific Python 3.8.12 release note that might be related.\n\nHowever, there are some missing details that would make this better specified:\n- The exact proxy configuration (`proxies` variable) is not provided, only mentioned that it's a paid service\n- No specific error message or traceback is included\n- No mention of which other Python versions were tested and worked correctly\n- No information about authentication methods being used with the proxy\n\nDespite these gaps, there's enough information to investigate the issue by:\n1. Reviewing the Python 3.8.12 release notes for proxy-related changes\n2. Testing with a proxy configuration to reproduce the issue\n3. Comparing behavior between Python 3.8.12 and other versions\n4. Examining how proxy authentication is handled in the requests library for this specific Python version\n\nThe issue is specific enough to begin meaningful troubleshooting and solution development.",
        "label": "REASONABLY_SPECIFIED"
    },
    "psf__requests-774": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The description includes:\n\n1. A specific error that occurs in the Requests library (version 0.13.1 on Python 2.6)\n2. The exact line causing the problem: `netloc = netloc.encode('idna').decode('utf-8')`\n3. A complete traceback showing the error path\n4. The specific exception being raised: `UnicodeError: label empty or too long` from the IDNA encoding process\n5. A clear suggestion for the fix: adding a try/except block around the problematic code\n\nThe issue provides enough context to understand the problem and implement a solution. A developer would need to add appropriate exception handling around the IDNA encoding operation to prevent the UnicodeError from propagating and causing the application to crash. The solution would involve catching the specific UnicodeError that occurs when attempting to encode domain names that don't comply with IDNA standards.",
        "label": "WELL_SPECIFIED"
    },
    "psf__requests-863": {
        "explanation": "This issue is well-specified with clear details about the problem and the desired solution:\n\n1. The problem is clearly identified: The Request class's hooks argument in the __init__() method doesn't properly handle lists of hook functions. When a list is passed, it wraps the list in another list, causing failures when hooks are consumed.\n\n2. The current workaround is explained: Users must create a request without sending it, call .register_hook() multiple times, and then call .send().\n\n3. The desired solution is explicitly stated: Modify Request.__init__() to accept lists as values in the hooks dictionary parameter.\n\nThe issue provides enough context to understand the problem, the current behavior, and the expected behavior. A developer familiar with the codebase would be able to locate the Request class, understand how the hooks parameter is currently processed, and implement the necessary changes to allow lists of hook functions.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-2905": {
        "explanation": "This issue is well-specified with a clear problem description, minimal reproducible example, and expected output. The issue describes a specific bug where objects with a `values` property are being incorrectly coerced when assigned to a DataArray using indexing. The user provides:\n\n1. A minimal code example that demonstrates the problem\n2. Clear expected vs. actual output\n3. The specific version where the issue was introduced (v0.10.1)\n4. A reference to the specific pull request and line of code that introduced the bug\n5. A complete environment description with version information\n\nThe issue even points to the exact line of code that likely needs to be modified to fix the problem. The expected behavior is clearly stated - objects with a `values` property should be stored as-is in arrays with dtype=object rather than having their values property extracted.\n\nA developer could take this information and create a targeted fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-2922": {
        "explanation": "The issue requests adding an `average()` function to a data structure (presumably \"ds\" refers to a dataset object in the codebase). While the basic intent is clear - to compute weighted averages - there are several ambiguities:\n\n1. The exact implementation details are not specified - the description mentions \"this would require the axes to be in a predictable order\" but doesn't specify what that order should be.\n\n2. It mentions \"Or to give a weight per dimension...\" as an alternative approach but doesn't specify which approach is preferred or how the weights should be provided.\n\n3. There's no specification of how the function should handle missing data, what the return type should be, or what the exact signature of the function should look like.\n\n4. The specific use case mentioned (\"e.g. for geo data\") suggests there might be domain-specific requirements, but these aren't elaborated.\n\nWhile the core request is understandable (add an averaging function that supports weights), the implementation details are left quite open-ended. A developer would need to make several design decisions to implement this feature, possibly requiring further clarification about the expected behavior.",
        "label": "VAGUE"
    },
    "pydata__xarray-3095": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is clearly identified as a regression in xarray >= 0.12.2 where `copy(deep=True)` and related copy operations are casting unicode indices to object dtype.\n\n2. The issue description provides a complete reproducible example showing:\n   - How to create a dataset with unicode indices\n   - The expected behavior (shown with `ds.copy()`)\n   - The problematic behavior (shown with `ds.copy(deep=True)`)\n   - Multiple scenarios demonstrating the issue (Dataset.copy and DataArray.copy)\n\n3. The version information is specific - the bug appears in xarray >= 0.12.2 but not in 0.12.1 and earlier.\n\n4. The issue is clearly a bug/regression rather than a feature request or enhancement.\n\nThe problem is well-defined: when performing a deep copy operation on a Dataset or DataArray, IndexVariables with unicode dtype ('<U*') are incorrectly cast to object dtype. A developer would have a clear understanding of what needs to be fixed and could verify the solution using the provided example.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3114": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and a detailed explanation of the problem. The user has:\n\n1. Provided a complete, step-by-step reproduction of the issue with code examples\n2. Clearly identified the problem: `expand_dims()` modifies numpy array flags to be read-only, and after manually making it writable again, `.loc` indexing behaves unexpectedly by setting all values in an inner array instead of just one value\n3. Demonstrated the expected behavior with a separate example\n4. Included version information for all relevant libraries\n\nThe issue is specifically about two behaviors:\n- The `expand_dims()` method unexpectedly changing array flags to read-only\n- After manually making the array writable again, `.loc` indexing not working as expected\n\nThe reproduction steps are thorough and allow for a clear understanding of the problem. The issue appears to be related to how xarray handles views vs. copies of arrays when expanding dimensions, and how this affects subsequent indexing operations.\n\nA developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3151": {
        "explanation": "This issue is well-specified with a clear problem description, minimal reproducible example, and expected behavior. The issue describes a bug where `xr.combine_by_coords()` raises a ValueError when combining datasets that have identical but non-monotonic coordinates, despite the documentation stating that \"any coordinate dimensions which do not vary between each dataset\" should be ignored. The error message \"Resulting object does not have monotonic global indexes along dimension y\" is provided, and the example clearly demonstrates the problem by showing that the function works with monotonic coordinates but fails with non-monotonic ones. The issue includes the relevant version information and provides a clear expectation that the function should work without error regardless of the monotonicity of identical coordinates across datasets. A developer would have all the necessary information to reproduce and fix this issue.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3159": {
        "explanation": "This issue is well-specified. The user is requesting a feature enhancement for the xarray library to allow passing a default value (like a scalar) for the `data` argument when creating a DataArray, instead of requiring a numpy.ndarray of the correct size. \n\nThe request is clear:\n1. Currently, xarray requires a numpy.ndarray for the `data` argument when creating a DataArray\n2. The user wants to be able to pass a scalar value instead, which would be used to fill the entire array\n3. The user provides a clear example from pandas showing the desired behavior\n4. The use case is explained - creating \"empty\" DataArrays with known coordinates that will be filled later\n\nThe solution would involve modifying the DataArray constructor to accept scalar values for the `data` argument and automatically expand them to fill an array of the appropriate size based on the provided coordinates. This is a common pattern in other libraries like pandas (as demonstrated in the example), so the expected behavior is well understood.\n\nA developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3302": {
        "explanation": "This issue is well-specified and clear about what functionality is being requested. The user wants to enhance the `interpolate_na()` function by adding a new argument that would allow interpolation only for gaps of NaN values that are smaller than or equal to a specified size, while leaving larger gaps untouched.\n\nThe issue description:\n1. Clearly explains the current behavior: `limit=2` fills the first two NaNs of any NaN block regardless of length\n2. Describes the desired behavior: only interpolate NaN blocks that are smaller than or equal to a certain size\n3. Provides context with real-world use cases and links to similar issues in other libraries (Pandas)\n4. Acknowledges existing workarounds but suggests this is a common enough need to warrant a dedicated solution\n\nA developer would understand exactly what needs to be implemented: an additional parameter to `interpolate_na()` that controls whether interpolation should be applied based on the total size of consecutive NaN blocks, not just limiting how many NaNs to fill from the beginning of each block.\n\nThe solution would involve modifying the existing function to identify the length of consecutive NaN blocks and only apply interpolation to those blocks that are smaller than or equal to the specified threshold.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3305": {
        "explanation": "This issue is well-specified with a clear problem statement, minimal reproducible example, and expected output. The issue describes that the `keep_attrs=True` parameter in the `DataArray.quantile()` method is not working as expected - it should preserve the attributes of the original DataArray in the output, but it's not doing so. \n\nThe example code clearly demonstrates the problem:\n1. A DataArray is created with an attribute 'units':'K'\n2. The quantile method is called with keep_attrs=True\n3. The resulting DataArray has empty attributes instead of preserving the original 'units':'K'\n\nThe expected output is clearly stated: the attributes should be preserved as OrderedDict([('units', 'K')]).\n\nThe issue also includes detailed version information which helps in understanding the environment where the bug occurs.\n\nA developer would be able to understand exactly what needs to be fixed: ensure that the quantile method properly respects the keep_attrs parameter by preserving the original DataArray's attributes in the output when keep_attrs=True is specified.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3520": {
        "explanation": "This issue is well-specified with clear information about what's not working correctly:\n\n1. The issue provides a Minimal, Complete, and Verifiable Example (MCVE) showing the problematic code.\n2. It clearly shows the current output that's being produced.\n3. It provides the expected output that should be produced instead.\n4. The problem is about selection from a MultiIndex not working properly - specifically when trying to select by the 'y' coordinate after creating a MultiIndex with 'x' and 'y'.\n5. The issue includes detailed version information about the environment.\n\nThe problem is clearly defined: when selecting with `data.sel(y='a')` on a DataArray with a MultiIndex, it should return only the elements where y='a' (which would be the first two elements with values [0, 1]), but instead it's returning all elements [0, 1, 2, 3].\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed in the xarray library's MultiIndex selection functionality.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3631": {
        "explanation": "This issue is well-specified with clear technical details about the problem:\n\n1. The issue provides a complete, minimal reproducible example showing the error when using `interp` with long cftime coordinates.\n\n2. It includes the full error traceback, pointing to the specific problem in the code: a TypeError when trying to convert datetime.timedelta objects to float.\n\n3. The issue description explains the root cause: the current implementation relies on pandas to convert datetime.timedelta objects to timedelta64[ns], which fails silently when there's an integer overflow with very long time periods.\n\n4. It provides a clear explanation of why this is a problem: cftime dates are meant to represent long time periods that can't be represented with standard DatetimeIndex, but the current implementation doesn't support this use case.\n\n5. The issue even suggests a solution direction: writing custom logic to convert datetime.timedelta objects to numeric values without relying on pandas/NumPy's nanosecond resolution.\n\n6. It includes version information for all relevant packages.\n\nA developer would have all the necessary information to understand the problem, reproduce it, and implement a solution that allows interp to work with long cftime coordinates by handling the datetime.timedelta conversion differently.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3649": {
        "explanation": "This issue is well-specified with clear details about the problem and expected solution:\n\n1. The issue provides a complete Minimal, Complete, and Verifiable Example (MCVE) with sample code that reproduces the problem.\n2. It clearly shows the expected output format.\n3. It includes the exact error message being encountered.\n4. The issue explains the root cause: `combine_by_coords` requires datasets to form a complete hypercube, but the user wants it to work with incomplete hypercubes by filling missing values.\n5. The user even proposes a specific solution approach: separating the dimension depth check from the complete hypercube requirement.\n6. The expected behavior is clearly defined: replace missing tiles of the hypercube with `fill_value`.\n7. The issue includes detailed version information for debugging.\n\nA developer could implement a solution based on this information without needing additional clarification. The task is to modify `combine_by_coords` to allow for missing panels in the hypercube by filling them with a specified value rather than requiring a complete hypercube.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3677": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is reporting that while the top-level `xr.merge()` function works correctly when merging a Dataset and a DataArray, the equivalent method call `ds.merge(da)` fails with an AttributeError. \n\nThe issue includes:\n1. A complete code example that reproduces the problem\n2. The expected behavior (demonstrated with the working top-level function)\n3. The actual behavior (error message with traceback)\n4. The specific error: AttributeError: 'DataArray' object has no attribute 'items'\n\nFrom the traceback, we can see that the issue is in the merge implementation, where it's trying to call `.items()` on a DataArray object, which doesn't have that method (unlike Dataset objects). This suggests that the Dataset.merge() method isn't properly handling DataArray inputs, while the top-level merge function does.\n\nThis is a clear bug with a specific, reproducible case that would be straightforward to fix by modifying the Dataset.merge() method to properly handle DataArray inputs, similar to how the top-level merge function does it.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3733": {
        "explanation": "This issue is well-specified with clear requirements for implementing a `polyfit` method for xarray's DataArray objects. The description provides:\n\n1. Clear motivation with links to existing community needs and workarounds\n2. A specific API proposal with example code showing the expected behavior\n3. Comparison to equivalent NumPy code to clarify the expected functionality\n4. Suggestions for implementation approach (reshaping data for ND arrays)\n5. Considerations for dask compatibility\n\nThe issue author has provided a complete example that demonstrates the expected input and output behavior, making it clear what a successful implementation would look like. The proposal builds on NumPy's existing polyfit functionality, extending it to work with xarray's dimensional data structures.\n\nThe implementation would need to:\n- Add a `polyfit` method to DataArray\n- Support fitting along a specified dimension\n- Handle N-dimensional data appropriately\n- Return results in a properly structured xarray object\n- Support dask arrays with certain constraints\n\nThis is sufficient information for a developer to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3905": {
        "explanation": "This issue is well-specified with a clear problem and expected solution. The user is requesting that xarray's DataArray representation should truncate its output when displaying large arrays, similar to how it already does for very large arrays (as shown in the second example with the \"...\" notation). \n\nThe issue provides:\n1. A clear example of the current behavior (showing all lines for medium-sized arrays)\n2. A clear example of the desired behavior (truncating with \"...\" for large arrays)\n3. A specific suggestion for implementation (truncate at around 40 lines)\n\nThe problem is well-defined: when displaying medium-sized arrays, xarray shows too many lines, making the output unwieldy. The solution is straightforward: implement line count-based truncation similar to what already exists for larger arrays.\n\nA developer could implement this feature without needing additional clarification. The implementation would involve modifying xarray's display logic to count the number of lines in the array representation and truncate it with \"...\" when it exceeds a certain threshold (suggested to be around 40 lines).",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3976": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code example, expected behavior, and error message. The user is reporting that when they have two DataArrays with the same dimensions but with coordinates in different orders, the regular addition (A + B) works fine, but in-place addition (A += B) fails with a MergeError about conflicting values for the index. The error message is fully provided, and the environment details are included. The expected behavior is clearly stated: the user expects A += B to work the same way as A = A + B. This is a specific bug report with all the necessary information to understand and attempt to fix the issue.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-3993": {
        "explanation": "This issue is well-specified and clear. It identifies a specific inconsistency in the API where `DataArray.integrate` uses a parameter named `dim` while `Dataset.integrate` uses `coord`, despite both classes using `coord` for the `differentiate` method. The issue provides:\n\n1. Clear examples showing the inconsistency\n2. A specific recommendation (change `dim` to `coord` in `DataArray.integrate`)\n3. Rationale for why this change makes sense (consistency and conceptual accuracy)\n4. Reference to previous discussion that supports this change\n\nThe only open question is whether this change requires a deprecation cycle, which is a standard consideration for API changes and doesn't make the issue vague. The implementation path is clear: modify the `DataArray.integrate` method to accept `coord` instead of (or in addition to) `dim`, potentially with a deprecation warning for the old parameter name.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4075": {
        "explanation": "This issue is very well-specified with all the necessary information to understand and solve the problem:\n\n1. It provides a clear, minimal code example that reproduces the bug\n2. It shows the actual output and the expected output\n3. It explains the root cause of the problem - boolean weights are not being properly normalized in weighted mean calculations\n4. It even provides the specific internal implementation detail causing the issue (the `sum_of_weights` calculation using `xr.dot` with boolean arrays)\n5. It suggests a solution approach (converting to int or float)\n6. It includes detailed version information\n\nThe issue is about fixing the weighted mean calculation in xarray when boolean weights are provided. The problem occurs because when calculating the sum of weights using boolean arrays, the result is a boolean `True` instead of the numeric value `2`, causing incorrect normalization. The fix would likely involve ensuring boolean weights are converted to numeric types before calculations.\n\nThis is a clear, focused bug report with all the information needed to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4094": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code example, expected output, and version information. The issue describes a bug in xarray's `to_unstacked_dataset` function when working with single-dimension variables. The provided MCVE (Minimal, Complete, and Verifiable Example) demonstrates the error: when a Dataset with single-dimension variables is stacked using `to_stacked_array` and then unstacked using `to_unstacked_dataset`, it raises a MergeError. The expected behavior is a working roundtrip conversion. The error message is clearly shown, and the version information is comprehensive. A developer would have all the necessary information to reproduce the issue and understand what needs to be fixed in the `to_unstacked_dataset` function to properly handle single-dimension variables.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4098": {
        "explanation": "The issue description \"groupby should work with name=None\" is extremely vague and lacks critical details needed to understand the problem and implement a solution. \n\nThe description doesn't specify:\n1. What software/library/framework this relates to (though it's likely pandas or a similar data analysis library)\n2. What the current behavior is when using groupby with name=None\n3. What the expected behavior should be\n4. Any error messages or unexpected results that occur\n5. Any code examples demonstrating the issue\n6. Context about why this functionality is needed\n\nWithout knowing which codebase this refers to, what the current implementation does, what the desired behavior is, or any reproduction steps, it would be nearly impossible to create a meaningful PR to address this issue. A developer would need to make many assumptions or request significant clarification before being able to work on this.",
        "label": "VAGUE"
    },
    "pydata__xarray-4248": {
        "explanation": "This issue is well-specified. The user is requesting a feature enhancement to display units in the dataset overview representation. The request includes:\n\n1. A clear example of the current output format\n2. A specific example of the desired output format showing how units should be displayed\n3. The context is clear - this is about improving the string representation of xarray Dataset objects\n\nThe implementation would involve modifying the string representation code to include the units information from the dataset's variables and coordinates. The units information is likely already available in the dataset metadata (probably as attributes), and the task is to expose this in the string representation.\n\nThe scope is well-defined and the expected outcome is clearly illustrated, making this a straightforward feature request to implement.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4356": {
        "explanation": "This issue is well-specified with clear problem description, reproduction steps, and a suggested solution:\n\n1. The problem is clearly defined: The `sum` function with `min_count` parameter fails when trying to sum over multiple dimensions.\n2. A minimal reproducible example is provided showing the exact code that produces the error.\n3. The submitter has identified the specific location in the codebase where the issue likely exists.\n4. A specific solution is proposed: replacing `mask.shape[axis]` with `np.take(a.shape, axis).prod()` in the nanops.py file.\n5. The issue links to potentially related issues for additional context.\n\nThe issue provides enough information for a developer to understand the problem, locate the relevant code, and implement a fix based on the suggested approach. The only minor detail missing is the exact error message produced, but the problem is still clear enough to work on.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4419": {
        "explanation": "This issue is well-specified with clear examples, expected outputs, and a detailed problem description. The issue describes a bug where `xr.concat()` changes the dimension order in xarray Datasets when coordinates are present, but maintains the original order when coordinates are not present.\n\nThe issue provides:\n1. Two clear code examples demonstrating the problem (one without coordinates, one with coordinates)\n2. The actual output for both cases\n3. The expected output\n4. A clear statement of what should happen: \"xr.concat should not change the dimension order in any case\"\n5. A second example showing the same issue with a different dataset\n6. Detailed version information\n\nThe problem is reproducible with the provided code samples, and the expected behavior is clearly defined. A developer would have all the necessary information to understand the issue, reproduce it, and implement a fix that ensures `xr.concat()` maintains the original dimension order regardless of whether coordinates are present or not.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4442": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible code example. The user is reporting that when a DataArray has one multiindex dimension (created using stack()) along with other dimensions, the to_series() and to_dataframe() methods fail with a NotImplementedError. The error message is clearly provided: \"isna is not defined for MultiIndex\".\n\nThe user also demonstrates that the methods work when there is only one dimension that is stacked, but fail when there are additional dimensions. They've provided their expected outcome (a series/dataframe with a multiindex with names a,b,c) and the actual error they're receiving.\n\nThe issue includes complete environment information with version details of xarray and its dependencies, which helps in reproducing and diagnosing the problem.\n\nA developer would have all the necessary information to:\n1. Reproduce the issue with the provided code sample\n2. Understand the expected behavior\n3. Identify where the error is occurring (in pandas' handling of MultiIndex)\n4. Implement a fix to properly handle DataArrays with both stacked and unstacked dimensions when converting to pandas objects\n\nThis is a clear bug report with all the necessary details to begin working on a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4510": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user has identified that when using the `rolling()` method on an xarray DataArray followed by an aggregation operation like `mean()`, the attributes and name of the original DataArray are lost, even when specifying `keep_attrs=True`. \n\nThe issue includes:\n1. A complete code example that reproduces the problem\n2. The expected behavior (attributes and name should be preserved)\n3. The actual behavior (attributes and name are dropped)\n4. The user has already tried the obvious solution (using `keep_attrs=True`)\n\nThe solution would need to address why attributes and name are being dropped during rolling operations and implement a fix to preserve them when `keep_attrs=True` is specified. This is a well-defined bug with clear acceptance criteria - the attributes and name should be preserved after rolling operations when requested.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4629": {
        "explanation": "This issue is well-specified with a clear problem description and a minimal reproducible example. The issue describes a bug where using `merge()` with `combine_attrs='override'` creates a reference to the attributes of the first dataset rather than copying them. This means that modifying attributes in the merged dataset also modifies the original dataset's attributes, which is unexpected behavior.\n\nThe issue includes:\n1. A clear description of the problem\n2. The expected behavior\n3. A complete, minimal code example that reproduces the issue\n4. The actual output showing the problem\n5. Even a suggestion for the fix (changing `return variable_attrs[0]` to `return dict(variable_attrs[0])`)\n6. Environment details with version information\n\nThe solution path is clear: modify the implementation of the `merge` function in xarray to ensure that attributes are properly copied rather than referenced when using the 'override' option for `combine_attrs`. The specific line of code that needs to be changed is even identified in the issue description.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4684": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible test case, and expected output. The user has provided:\n\n1. A clear description of the problem: millisecond precision is lost when datetime64 data is written to disk and read back.\n2. A minimal, complete, and verifiable example with code that demonstrates the issue.\n3. Sample data (as a downloadable zip file containing a pickle).\n4. The actual output showing the precision loss (2017-02-22T16:24:10.586000000 becomes 2017-02-22T16:24:10.585999872).\n5. The expected output (maintaining the original precision).\n6. Detailed version information about the environment.\n\nThe issue is specific to xarray's handling of datetime64 data during IO operations, particularly when writing to netCDF format and reading back. The problem is clearly reproducible and the expected behavior is well-defined. A developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4687": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user reports that when using `xr.where()` on a DataArray with attributes, the resulting DataArray loses those attributes. The issue includes:\n\n1. A clear description of the current behavior: attributes are lost when using `xr.where()`\n2. The expected behavior: attributes should be preserved or there should be an option to preserve them\n3. Two minimal code examples that reproduce the issue\n4. Complete environment information with version details\n\nThe user also notes a secondary issue about dtype preservation, but acknowledges this might be a numpy issue rather than xarray.\n\nThe issue is straightforward to understand and reproduce. A developer would be able to implement a solution that either preserves attributes by default or adds an option (like `keep_attrs=True`) to the `xr.where()` function without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4695": {
        "explanation": "This issue is well-specified with a clear problem description and a minimal reproducible example. The user has identified a specific bug where using \"method\" as a dimension name in an xarray DataArray causes an error when using the .loc accessor. The error message suggests that the dimension name \"method\" is being interpreted as a parameter for another function rather than as a dimension name.\n\nThe issue includes:\n1. A complete, copy-pastable code example that demonstrates the problem\n2. The exact error message received\n3. A clear explanation of the expected behavior (dimension names should be irrelevant)\n4. Version information for all relevant packages\n5. Evidence that the issue persists in the latest version they've tested (xarray 0.12.0)\n\nThe problem is clearly a bug in xarray where the dimension name \"method\" is conflicting with a parameter name in some internal function. A developer would have enough information to reproduce the issue and locate the source of the problem in the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4759": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes a bug where coordinate data types in xarray Datasets and DataArrays change from their original types (like '<U1' for strings or '|S1' for byte strings) to 'object' dtype when:\n\n1. Adding a new DataArray to a Dataset (first example)\n2. Concatenating DataArrays using xr.concat (second and third examples)\n\nThe issue includes:\n- Multiple reproducible code examples showing the problem\n- Clear expected behavior (coordinate dtypes should not change)\n- Environment information including xarray version\n- Comparison of input and output dtypes showing exactly what's changing\n- Reference to a potentially related issue\n\nThe problem is consistent across different scenarios and the expected behavior is unambiguous - the coordinate dtypes should remain the same after these operations. A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4767": {
        "explanation": "This issue is well-specified with clear examples demonstrating the inconsistent behavior between Dataset and DataArray when using transpose with Ellipsis (...) and non-existent dimensions. \n\nThe issue shows:\n1. A working example using Dataset.transpose() with a non-existent dimension and Ellipsis\n2. A failing example using DataArray.transpose() with the same parameters\n3. The exact error message and traceback\n\nThe problem is clearly identified: DataArray.transpose() doesn't handle the Ellipsis (...) correctly when non-existent dimensions are included, while Dataset.transpose() does. The error message is also noted to be inaccurate since Ellipsis is included.\n\nThe traceback provides useful information about where the issue occurs in the code, pointing to the infix_dims function in utils.py and how it's called from variable.py and dataarray.py.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - likely ensuring consistent behavior between Dataset and DataArray when handling transpose operations with Ellipsis and non-existent dimensions.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4802": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug: `decode_cf` fails when `scale_factor` is a length-1 list, but works when it's a numpy array.\n2. The reporter provides complete code to reproduce the issue.\n3. The error message and traceback are included, pointing to the exact line causing the problem: `scale_factor.item()` failing because lists don't have an `.item()` method.\n4. The reporter explains when this regression was introduced (after Xarray 0.16.1).\n5. The reporter explains a real-world scenario where this occurs: when opening a netcdf file with the h5netcdf engine and writing to zarr, where numpy arrays get encoded as lists in zarr json metadata.\n6. There's a reference to related issues (#4471 and #4485) that provide context.\n\nThe solution would clearly involve modifying the code in `xarray/coding/variables.py` to handle the case where `scale_factor` or `add_offset` are Python lists rather than numpy arrays, likely by converting lists to numpy arrays before attempting to call `.item()` on them.\n\nThis is a straightforward bug fix with all the necessary information provided to implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4819": {
        "explanation": "This issue is well-specified with a clear problem statement and desired solution. The user wants to drop specific indices from a dimension that doesn't have coordinates in an xarray Dataset. They've provided:\n\n1. A clear code example demonstrating the problem\n2. The error they're encountering (though not the exact error message)\n3. A specific feature request: implementing a `drop_isel` method that would work analogously to `drop_sel` but for positional indices rather than coordinate labels\n4. Alternatives they've already considered and why they're not ideal\n\nThe issue is straightforward to understand - the user wants to drop elements at specific numerical indices from a dimension without having to create coordinates or rebuild the dataset. The solution would involve implementing a new method `drop_isel` that accepts positional indices rather than coordinate labels. This is a reasonable feature request with a clear implementation path that follows xarray's existing API patterns.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4879": {
        "explanation": "This issue is well-specified with a clear description of the problem, expected behavior, and a minimal reproducible example. The user has identified a specific bug in xarray where the `open_dataset` function continues to return cached data from a previous run even after the underlying netCDF file has been deleted and recreated with different content. The issue only occurs in Jupyter notebooks after the repr of an xarray object has been displayed.\n\nThe issue description includes:\n1. A clear explanation of the problem behavior\n2. The expected behavior\n3. A minimal reproducible example with code snippets\n4. A link to a notebook demonstrating the issue\n5. Detailed environment information\n\nThe steps to reproduce are clear:\n1. Create a netCDF file with data of size 4\n2. Open and print it (works correctly)\n3. Create a new netCDF file with data of size 5\n4. Open and print it (works correctly)\n5. But if at any point the repr is used to display the dataset, subsequent reads will show stale data\n\nThis is a well-defined caching bug that can be investigated and fixed based on the information provided. The issue is related to Jupyter's repr caching mechanism interacting with xarray's file handling, and the solution would likely involve ensuring that xarray properly invalidates cached data when files change on disk.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4911": {
        "explanation": "This issue is well-specified with a clear problem description, a minimal complete verifiable example (MCVE), expected vs. actual behavior, and detailed environment information.\n\nThe issue describes a specific bug: when using the `sum` method on a lazy xarray DataArray with the `min_count` parameter set, it forces immediate evaluation of the array instead of preserving lazy evaluation. This behavior is unexpected and not documented.\n\nThe provided code example clearly demonstrates the issue:\n1. It creates a lazy DataArray using `map_blocks`\n2. It shows that `sum` with default parameters preserves laziness\n3. It shows that `sum` with `min_count` parameter triggers evaluation (raising the \"I was evaluated\" exception)\n\nThe expected behavior is explicitly stated: no evaluation should occur until the result is actually used (printed or saved).\n\nThe environment details are comprehensive, including xarray version (0.16.2) and all relevant dependencies.\n\nA developer would have all the information needed to reproduce the issue, understand the expected behavior, and implement a fix (either by modifying the code to support lazy evaluation with min_count or by updating the documentation to clarify this limitation).",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4940": {
        "explanation": "This issue is well-specified with a clear problem description and a complete reproducible example. The issue describes that when calling `Dataset.mean()` on an xarray Dataset, the coordinates are dropped, whereas when calling the same method on a DataArray, the coordinates are preserved. The user provides:\n\n1. A complete code sample that demonstrates the issue\n2. The actual output showing the problem (coordinates are preserved for DataArray.mean() but dropped for Dataset.mean())\n3. Additional context showing that data arrays contained in the dataset also lose their coordinates\n4. Version information for all relevant packages\n\nThe issue is also referencing a similar bug (#1470), which provides additional context. The expected behavior is clearly implied: the Dataset.mean() operation should preserve coordinates like DataArray.mean() does.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed - the Dataset.mean() method needs to be modified to preserve coordinates in the same way that DataArray.mean() does.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4966": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is precisely defined: xarray doesn't handle the `_Unsigned=False` attribute for unsigned bytes from OPeNDAP via pydap, which causes incorrect data representation.\n\n2. The issue provides specific context about the conventions and hacks involved:\n   - netCDF3 convention using `_Unsigned=True` for unsigned bytes\n   - OPeNDAP hack using `_Unsigned=False` for signed bytes\n\n3. The exact location in the code where the fix should be implemented is identified: the same place where the `_Unsigned=True` convention is currently handled (with a direct link to the relevant code).\n\n4. The issue includes a clear demonstration of the problem with reproducible code examples showing the different behavior between engines.\n\n5. The expected solution approach is suggested: handle the case symmetrically with something like `if .kind == \"u\" and unsigned == False`.\n\nThe issue reporter even offers to prepare a PR for the fix, showing they have a good understanding of the problem and potential solution. All the necessary information is provided to understand and address the issue without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-4994": {
        "explanation": "This issue is very well-specified with clear information about what the problem is and what the expected solution should be:\n\n1. The issue clearly identifies that the `date` attribute is missing from the datetime accessor in xarray, while similar attributes like `time` are supported.\n\n2. A complete, minimal, and verifiable example is provided that demonstrates the issue - showing how `time` works and how `date` fails with an AttributeError.\n\n3. The issue includes a specific suggestion for how to fix the problem, including the exact code that needs to be added and where it should be added (in core/accessor_dt.py).\n\n4. The environment details are comprehensive, including the version of xarray and all relevant dependencies.\n\n5. The submitter has even offered to create a PR to implement the fix.\n\nThe issue is straightforward - add a `date` attribute to the DatetimeAccessor class that works similarly to the existing `time` attribute. The solution path is clear and well-defined, making this issue very actionable without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5033": {
        "explanation": "This issue is reasonably well-specified. The user is requesting a feature enhancement to simplify how custom backends can be added to xarray. Currently, custom backends need to be registered as external plugins in setup.py, but the user wants to be able to directly pass a function to the `engine` parameter.\n\nThe issue clearly:\n1. Identifies the current limitation (custom backends must be registered via setup.py)\n2. Proposes a specific solution (allow passing functions directly to the `engine` parameter)\n3. Provides a code example of the desired API\n4. Suggests that implementation might be straightforward (bypassing string checks)\n\nWhat's missing or could be clearer:\n1. The exact interface that the custom engine function should implement\n2. How this would interact with the existing backend registration system\n3. Potential edge cases or compatibility concerns\n\nHowever, these details could reasonably be determined during implementation by examining the existing codebase and backend system. The core request is clear enough that a developer familiar with xarray's codebase could make a meaningful attempt at implementing this feature.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-5126": {
        "explanation": "This issue is reasonably well-specified. The user is requesting an option to collapse the HTML display in notebooks for xarray objects, specifically the data part of the representation. They provide clear context about why this is needed (the HTML representation used to collapse the data part but was reverted in version 0.16.0).\n\nThe suggested solution is also clear - adding an option similar to existing `xarray.set_options` to control whether the data part appears collapsed in the HTML representation.\n\nWhat makes this not fully well-specified:\n1. The exact implementation details are not provided, which is why the user is asking for guidance on which files need to change and potential pitfalls.\n2. There's an additional question about whether this should also apply to text representation, which indicates some uncertainty about the scope.\n\nHowever, these are reasonable questions for someone preparing to implement a feature, and the core request is clear enough that an experienced developer familiar with the codebase could make a meaningful attempt at implementing this feature. The user understands what they want to achieve and has articulated it clearly.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-5131": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific bug: trailing whitespace in the text representation of DatasetGroupBy objects.\n2. The issue provides concrete examples demonstrating the problem, including code snippets and expected output.\n3. The issue explains why this is problematic: it causes conflicts between flake8 linting (which flags trailing whitespace) and doctests (which require exact output matching).\n4. The issue describes attempted workarounds and why they're insufficient.\n5. The issue proposes a clear solution: removing the trailing whitespace from the DatasetGroupBy representation.\n\nThe reporter has provided all necessary context to understand the problem, its impact, and a potential solution. A developer could immediately begin working on this issue without needing additional clarification. The fix would involve modifying how DatasetGroupBy objects are represented as strings to remove the trailing whitespace.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5180": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a minimal complete verifiable example (MCVE) that demonstrates the issue. \n\nThe problem is clearly defined: when opening a NetCDF file with a time coordinate that has the calendar attribute set to \"Gregorian\" (with capital G), xarray uses cftime._cftime.DatetimeGregorian objects instead of datetime64[ns] objects, even though the dates are within the valid range for datetime64.\n\nThe expected behavior is also clear: the user expects xarray to use datetime64[ns] for time coordinates with a \"Gregorian\" calendar when the dates are within the valid range, as documented in the xarray documentation.\n\nThe MCVE demonstrates the issue by:\n1. Creating a dataset with datetime64 times\n2. Setting the calendar attribute to \"Gregorian\"\n3. Saving and loading the dataset, showing that the time type changes\n4. Demonstrating a workaround by adding \"Gregorian\" to the _STANDARD_CALENDARS set\n\nThe user even suggests a specific fix: modify the `_decode_datetime_with_pandas` function to use case-insensitive comparison for calendar names, which aligns with the CF conventions that state applications should not be sensitive to case in these attributes.\n\nThe environment information is also provided, making this a complete bug report that can be addressed without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5187": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code example, and expected output. The user has identified that the `bfill` (backward fill) method in xarray doesn't work correctly with dask arrays when the chunk size is very small (specifically when chunks=1). The issue shows:\n\n1. A complete code example that demonstrates the problem\n2. The actual output showing that with unchunked arrays and medium chunks (10), the NaN value is properly filled, but with small chunks (1), the NaN remains\n3. Clear expected output: \"absence of nans\"\n4. Complete version information for relevant libraries\n\nThe issue is specific to xarray's handling of backward fill operations on dask arrays with small chunk sizes. This is a concrete bug that can be investigated and fixed by examining how the `bfill` operation works across chunk boundaries in dask arrays. The solution would likely involve modifying how the fill operation handles small chunks to ensure values are properly propagated across chunk boundaries.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5233": {
        "explanation": "This issue is well-specified with clear requirements for calendar utilities in xarray. The user has:\n\n1. Clearly identified three specific problems related to calendar handling in xarray\n2. Proposed specific solutions for each problem with function signatures\n3. Provided context about existing implementations in xclim that could be moved to xarray\n4. Included links to the existing implementations and related discussions\n5. Offered detailed explanations of how each function works in xclim\n\nThe issue describes concrete functionality that needs to be implemented, with existing reference implementations that can be adapted. The user has even offered to help move the code. The requirements are technical but specific, and there's enough information to understand what a successful implementation would look like for each of the three requested features.\n\nThe only minor ambiguity might be in the exact implementation details, but since reference implementations exist in xclim, these can be used as a starting point, making this a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5362": {
        "explanation": "This issue is well-specified with a clear problem statement and a complete reproducible example. The user is pointing out an inconsistency in the behavior of weighted operations in xarray:\n\n1. The problem is clearly stated: weighted operations don't raise an error when non-existent dimensions are passed, while the equivalent unweighted operations do.\n\n2. A minimal, complete, verifiable example is provided that demonstrates the issue:\n```python\nimport xarray as xr\nds = xr.tutorial.open_dataset(\"air_temperature\")\nds.weighted(xr.ones_like(ds[\"air\"])).mean(\"dummy\")\n```\n\n3. The expected behavior is stated: the weighted operation should be consistent with the unweighted operation in terms of error handling for non-existent dimensions.\n\n4. The environment details are comprehensive, including the xarray version and all dependencies.\n\nThe issue is asking for a change in behavior to make weighted operations consistent with unweighted operations when handling non-existent dimensions. This is a well-defined task that can be addressed by modifying the error handling in the weighted operations code to check for dimension existence before proceeding.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5365": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The user is requesting a new feature: an `xarray.cross()` function to calculate vector cross products, similar to the existing `xarray.dot()` function.\n2. The user has provided a complete implementation of the function they're proposing.\n3. They've included example usage showing how the function would work.\n4. The user has clearly stated what they want to know: whether this function should be added to the xarray namespace or placed in another package.\n5. They've offered to create a PR if the maintainers want this feature in xarray.\n\nThe implementation details are complete, showing:\n- How to handle the spatial dimension\n- Error checking for dimensions\n- Compatibility with dask for parallelization\n- Output dtype handling\n\nThere's nothing ambiguous about what's being requested, and a developer could implement this feature based on the provided code and explanation. The only \"decision\" needed is whether this belongs in the core xarray package or elsewhere, which is a maintainer decision rather than a specification issue.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5455": {
        "explanation": "This issue is well-specified with clear technical details and a proposed solution. The issue describes:\n\n1. The current problem: Xarray's internal backends don't get registered unless dependencies are installed\n2. A specific goal: Improve error messages to suggest specific backends to install when open_dataset() fails\n3. A detailed technical solution with specific changes to the plugin protocol:\n   - Make `guess_can_open()` work regardless of backend installation\n   - Add `installed()` method to report installation status\n   - Keep `open_dataset()` behavior consistent\n4. A concrete example of the improved error message that would result\n\nThe issue provides links to relevant code and related issues, mentions specific backends (netCDF3, h5netcdf, scipy, netcdf4), and even tags relevant contributors. The proposal is clear enough that an engineer could implement the changes without needing additional clarification. The issue author also asks for feedback on whether the approach is reasonable, but this doesn't impede implementation.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5580": {
        "explanation": "This issue is well-specified and clear about what needs to be done. The user is requesting an increase to the default value of `display_max_rows` in the `xr.set_options()` function. The issue explains:\n\n1. The current default is 12 rows, which the user finds too low\n2. This limit breaks backward compatibility with previous versions\n3. It affects the output format when using `print()` and breaks the user's doctests\n4. The user suggests specific alternative values (100, 1000, or 10000)\n\nThe request is straightforward - increase the default value of a specific configuration parameter. The solution would involve locating where this parameter is defined in the codebase and changing its default value. The issue provides clear context about why this change is needed and suggests specific alternative values, making it actionable without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-5731": {
        "explanation": "This issue is well-specified with clear information about what's happening and what's expected. The user provides:\n\n1. A clear problem statement: Using xarray.corr on two DataArrays results in a NameError about dask not being defined, even though dask is not installed.\n\n2. Expected behavior: The correlation function should work without dask since it should be optional.\n\n3. A complete, minimal reproducible example with the exact code that fails and the full error traceback.\n\n4. Comprehensive environment information showing all relevant package versions, including confirmation that dask is not installed.\n\nThe issue appears to be a bug in xarray where the corr() function is trying to use dask functionality even when dask is not installed. The error occurs in the map_blocks function when it tries to check if the object is a dask collection, but dask is not available.\n\nThis is a well-defined issue that can be addressed by modifying the xarray code to properly handle the case when dask is not installed, likely by adding proper conditional imports or checks before attempting to use dask functionality.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6386": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a minimal reproducible example. The user reports that when using the `resample` and `map` functions on an xarray Dataset where the mapping function returns a DataArray (instead of a Dataset), they get a TypeError about an unexpected keyword argument 'variables' in `_overwrite_indexes()`. \n\nThe issue includes:\n1. A clear description of what happened (TypeError when resampling)\n2. What was expected (it worked before a specific PR #5692)\n3. A minimal complete verifiable example with code that reproduces the issue\n4. The full error traceback\n5. Additional context about the potential cause (different signatures between DataArray._overwrite_indexes and Dataset._overwrite_indexes)\n6. Environment details\n\nThe user even suggests a potential fix direction, noting that either DataArray should be accepted as a return type from the mapping function (which would require fixing the code) or the documentation should be updated to clarify that only Dataset returns are supported.\n\nWith this information, a developer could reproduce the issue, understand the regression, and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6394": {
        "explanation": "This issue is well-specified with a clear problem description, a minimal reproducible example, and detailed error output. The issue describes a bug where `DataArrayGroupBy._combine` method is broken when the mapped function returns a Dataset, which previously worked before PR #5692. The error occurs specifically when using `resample().map()` with a function that returns a Dataset.\n\nThe issue includes:\n1. A clear description of the problem\n2. A minimal reproducible example showing the exact code that fails\n3. Complete error traceback showing the TypeError about `_overwrite_indexes() got an unexpected keyword argument 'coords'`\n4. Environment details including xarray version and dependencies\n5. A hint at a potential solution, referencing issue #6386 which might have a similar fix\n\nThe issue even points to the specific line in the code that's causing the problem and mentions that this is the reverse of another issue (#6379). This provides enough context for a developer to understand the problem and work on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6400": {
        "explanation": "This issue is well-specified with clear information about what's happening:\n\n1. The problem is clearly stated: poor HTML representation performance for large multi-indexed data arrays, with a specific example showing it takes 12 seconds.\n\n2. A complete, minimal reproducible example is provided with exact code that demonstrates the issue:\n   - Uses xarray's tutorial dataset\n   - Shows how to create the problematic data structure\n   - Includes timing information showing the performance problem\n   - Shows the shape of the array (3,869,000 elements)\n\n3. The environment details are comprehensive, including:\n   - Full version information for xarray and dependencies\n   - Python version and OS information\n   - Reference to a previous PR that attempted to fix similar issues\n\nThe issue is focused on a specific performance problem with a clear reproduction path. A developer would have all the necessary information to investigate the issue, reproduce it, and work on a solution to improve the HTML representation performance for large multi-indexed arrays.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6461": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user reports that `xr.where()` fails when using a scalar value as the second argument with `keep_attrs=True` (which is the default). The error occurs because the function tries to access attributes from the second parameter, but when it's a scalar (like the value 1 in the example), there are no attributes to access, causing an IndexError.\n\nThe issue includes:\n1. A clear code example that reproduces the error\n2. The exact error message and traceback\n3. A workaround (using `keep_attrs=False`)\n4. The version of xarray being used (2022.3.0)\n\nFrom the error traceback, we can see that the problem is in the implementation of the `where` function, specifically in how it handles attributes when a scalar is provided as the second argument. The function tries to access `attrs[1]` but when the second argument is a scalar, this index is out of range.\n\nA developer would have enough information to locate the issue in the codebase and implement a fix that properly handles the case when scalars are provided as arguments to `xr.where()` while maintaining attribute preservation functionality.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6548": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a minimal complete verifiable example that demonstrates the issue. \n\nThe problem is that when using `xr.polyval()`, if the first argument (a coordinate DataArray) doesn't have a name attribute that matches its dimension name, it raises a KeyError. The user has provided:\n\n1. A clear code example showing both the failing case and a working case\n2. The exact error message and traceback\n3. The specific line causing the error (`x = get_clean_interp_index(coord, coord.name, strict=False)`)\n4. A reasonable hypothesis about the expected behavior vs. actual behavior\n5. Environment details including xarray version\n\nThe issue is specific and reproducible. The solution would likely involve modifying the `polyval` function to handle coordinate DataArrays without a name attribute, or improving the error message to make the requirement clearer. This is a well-defined bug that can be addressed without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6598": {
        "explanation": "This issue is well-specified with clear information about what's happening and what should happen instead. The reporter has:\n\n1. Clearly described the problem: xarray improperly decodes times from a NetCDF file when the time variable is stored as a uint type.\n\n2. Provided a complete test case:\n   - Attached a CDL file that can be converted to a NetCDF file with instructions on how to do so\n   - Included a minimal code example showing how to reproduce the issue\n   - Demonstrated the expected behavior (good_time) vs. the problematic behavior (time)\n\n3. Included relevant output showing the difference between the correctly decoded time variable (good_time) and the incorrectly decoded one (time)\n\n4. Provided detailed environment information including versions of xarray, Python, and related dependencies\n\nThe issue is specific and focused on a single problem with a clear demonstration of what's wrong and what correct behavior should look like. A developer would have all the necessary information to investigate and fix the issue without needing to ask for clarification. The comparison between the working and non-working variables in the same file makes it particularly clear what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6599": {
        "explanation": "This issue is well-specified with a clear problem statement, expected behavior, and a complete reproducible example. The user has:\n\n1. Clearly identified the problem: `polyval` with timedelta64 coordinates produces different results between xarray version 2022.3.0 and the latest unreleased version.\n\n2. Provided a complete, minimal, verifiable example that demonstrates the issue, including all necessary imports and data creation.\n\n3. Shown the expected output (from version 2022.3.0) and the incorrect output (from the unreleased version).\n\n4. Included detailed environment information.\n\nThe issue appears to be a regression in the `polyval` function when working with timedelta64 coordinates. The example shows that in the older version, the function produces reasonable numerical results, while in the newer version it produces extremely large values that are clearly incorrect.\n\nA developer would have all the information needed to reproduce the issue, investigate the changes between the two versions, and fix the regression. The issue is focused on a specific function with a specific data type, making it well-scoped and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6601": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user has:\n\n1. Provided a complete, reproducible code example\n2. Shown the different outputs between xarray v2022.3.0 and a development version\n3. Clearly identified the specific issue: the dimension order in the output of `polyval` has changed between versions\n4. Included version information for both the working and problematic versions\n5. Asked specific questions about whether this is expected behavior and if it should be documented\n\nThe issue is focused on a specific function (`polyval`), demonstrates the exact problem (dimension order change from `('azimuth_time', 'axis')` to `('axis', 'azimuth_time')`), and provides all the necessary context to understand and investigate the problem. A developer would have enough information to reproduce the issue, check if this was an intentional change, and determine whether it should be documented as a breaking change.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6721": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user provides:\n\n1. A specific problem: accessing the `chunks` attribute on a zarr-backed xarray dataset loads the entire array into memory instead of just inspecting metadata\n2. A minimal code example to reproduce the issue\n3. Detailed error logs showing the call stack that leads to the problem\n4. Expected behavior (according to @rabernat): accessing chunks should only inspect the encoding attribute\n5. Environment details including versions of relevant packages\n\nThe issue is clearly a bug in xarray's handling of zarr-backed datasets. When accessing the `chunks` attribute, it's triggering a full data load instead of just accessing metadata. The stack trace shows exactly where this is happening - when checking for `hasattr(v.data, \"chunks\")`, it's triggering a full data load through a chain of method calls.\n\nA developer would have enough information to investigate and fix this issue by modifying how xarray accesses chunk information for zarr-backed arrays, likely by checking the encoding attribute directly rather than accessing the data property which triggers loading.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6744": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is experiencing a discrepancy between using `rolling().mean()` with `center=True` and manually iterating over a `DataArrayRolling` object with the same parameters. The expected behavior is that both approaches should produce the same center-justified windows, but they don't.\n\nThe issue includes:\n1. A clear description of the problem\n2. Complete, minimal code examples showing both approaches\n3. The actual outputs from both approaches\n4. The expected behavior (that both approaches should yield the same result)\n\nThe bug is clearly identifiable: when manually iterating over a DataArrayRolling object with `center=True`, the center parameter appears to be ignored, resulting in different output compared to using the built-in aggregation method.\n\nA developer would have all the information needed to investigate and fix this issue without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6804": {
        "explanation": "This issue is proposing a new feature to use PyTorch as a backend for xarray, but it's not structured as a specific implementation request with clear requirements. Instead, it's framed as an exploratory discussion about interest in the feature and seeking guidance on implementation.\n\nThe issue does provide some rationale for why this would be useful (GPU acceleration, deep learning integration), but lacks:\n1. Specific technical requirements or scope\n2. Clear definition of what \"using PyTorch as a backend\" means in practice\n3. Expected behavior or compatibility requirements\n4. Any discussion of potential challenges or limitations\n\nThe issue is essentially asking for thoughts on the feature and suggestions on where to start, which indicates the person is at the ideation stage rather than having a concrete implementation plan. Without further specification about the expected behavior, integration points, and technical approach, it would be difficult to create a meaningful PR that addresses this issue.",
        "label": "VAGUE"
    },
    "pydata__xarray-6823": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a complete reproducible example. The user reports that after upgrading from xarray 2022.3.0 to 2022.6.0, f-string formatting of sparse-backed DataArray raises a RuntimeError. The issue includes:\n\n1. A minimal, complete, and verifiable example that demonstrates the problem\n2. The exact error traceback\n3. Comparison of behavior between the old and new versions\n4. Complete environment details including all relevant package versions\n\nThe error is clearly shown: when using f-string formatting with a sparse-backed DataArray, a RuntimeError is raised with the message \"Cannot convert a sparse array to dense automatically. To manually densify, use the todense method.\"\n\nThe expected behavior is also clearly stated: either the code should not error (as it worked in the previous version), or this breaking change should have been documented in the \"Breaking changes\" section of the documentation.\n\nA developer would have all the information needed to reproduce and fix this issue without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6882": {
        "explanation": "This issue is well-specified with a clear problem description and a complete minimal working example (MWE) that reproduces the error. The issue occurs when trying to open a NetCDF file that contains a zero-sized coordinate with a time index. The error trace is fully provided, showing that the problem happens during the CF datetime decoding process, specifically when trying to perform a minimum operation on a zero-size array.\n\nThe issue description includes:\n1. A clear statement of the problem\n2. A complete reproducible code example\n3. The full error traceback\n4. The output of `ncdump` showing the structure of the problematic NetCDF file\n\nFrom this information, a developer can understand exactly what's happening: when xarray tries to decode a time coordinate with zero elements, it attempts to find the minimum value to calculate a timedelta, but this fails because numpy's min operation has no identity element for empty arrays.\n\nThe solution would need to handle the special case of empty time coordinates during the CF decoding process, likely by adding a check for zero-sized arrays before attempting reduction operations like minimum.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6889": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a minimal complete verifiable example (MCVE) that reproduces the issue. \n\nThe problem is clearly defined: after applying `xr.concat` to a dataset with a MultiIndex, alignment operations like `reindex_like` fail, even when trying to align the dataset with itself. The error occurs in the alignment module, specifically in the `find_matching_indexes` function.\n\nThe MCVE is complete, concise, and can be directly copied and run to reproduce the issue. It includes:\n1. Code to create a dataset with a MultiIndex\n2. The operation that causes the problem (`xr.concat`)\n3. The operations that fail after concatenation\n4. The full error traceback\n\nThe environment details are comprehensive, including the xarray version (2022.6.0) and all relevant dependencies.\n\nThe issue appears to be a bug in xarray's handling of MultiIndex after concatenation, and the solution would involve fixing how the alignment module handles MultiIndex objects in this specific scenario. A developer would have enough information to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6938": {
        "explanation": "This issue is well-specified with a clear problem description and a complete, verifiable example that demonstrates the issue. The user has:\n\n1. Clearly stated the problem: `.swap_dims()` can modify the original object in certain cases, specifically changing the `.dims` of a data variable that was swapped into being a dimension coordinate variable.\n\n2. Provided expectations: They expected `.swap_dims()` not to modify the original object.\n\n3. Included a complete, minimal code example that reproduces the issue, with clear comments explaining what's happening at each step.\n\n4. Confirmed the example is minimal, complete, and verifiable.\n\n5. Provided detailed environment information including versions of xarray and related dependencies.\n\nThe issue is focused on a specific behavior of the `.swap_dims()` method that appears to be modifying objects in place when it shouldn't. The code example clearly demonstrates this behavior by showing how `ds2` is modified after calling `ds2.swap_dims()`. This is sufficient information for a developer to understand the problem, reproduce it, and work on a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6971": {
        "explanation": "This issue is reasonably well-specified. It proposes adding a new public API method `.set_xindex()` to DataArray and Dataset classes in Xarray, similar to the existing `.set_index()` method. The issue provides clear context:\n\n1. It references related PR and issue numbers\n2. It explains that the method already exists in a development branch (`scipy22`)\n3. It provides rationale for why this method should be added to the public API:\n   - Consistency with existing `.xindexes` vs `.indexes` properties\n   - The API design is already defined (setting one index at a time with build options)\n   - It avoids complicating the existing `.set_index()` method\n\nThe issue also mentions a potential long-term strategy of eventually replacing `.indexes` and `.set_index()` with their \"x\" counterparts.\n\nWhat's missing is:\n- The exact signature/parameters of the proposed `.set_xindex()` method\n- Specific implementation details\n- Examples of usage\n\nHowever, these details could be reasonably inferred by examining the existing implementation in the `scipy22` branch that's referenced. The core request - adding a specific method to the public API - is clear enough that a developer could make a meaningful attempt at implementing it.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-6992": {
        "explanation": "This issue is well-specified with clear information about what's happening:\n\n1. The problem is clearly identified: After the index refactor, there can be more entries in `_coord_names` than in `_variables`, which breaks the assumption in `xr.core.dataset.DataVariables` that everything in `_variables` but not in `_coord_names` is a data variable.\n\n2. A minimal, complete, verifiable example is provided that reproduces the issue:\n```python\nds = xr.Dataset(coords={\"a\": (\"x\", [1, 2, 3]), \"b\": (\"x\", ['a', 'b', 'c'])})\nds.set_index(z=['a', 'b']).reset_index(\"z\", drop=True)\n```\n\n3. The error message is provided: `ValueError: __len__() should return >= 0`\n\n4. The specific location in the code where the error occurs is identified:\nhttps://github.com/pydata/xarray/blob/63ba862d03c8d0cd8b44d2071bc360e9fed4519d/xarray/core/dataset.py#L368\n\n5. The issue is linked to a specific PR (#5692) that likely introduced the problem.\n\nWith this information, a developer would be able to:\n1. Reproduce the issue using the provided example\n2. Understand the root cause (more `_coord_names` than `_variables`)\n3. Locate the problematic code in the codebase\n4. Develop a fix that addresses the incorrect assumption in the `DataVariables` class\n\nThe issue provides all necessary context for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-6999": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a minimal complete verifiable example that demonstrates the issue.\n\nThe problem is clearly defined: When using `rename` or `rename_vars` to make a coordinate into a dimension coordinate, the coordinate is displayed correctly as a dimension coordinate (with an asterisk), but it doesn't create an index, causing subsequent operations like `sel` to fail with a KeyError.\n\nThe expected behavior is also clearly stated with two possible solutions:\n1. Either `rename`/`rename_vars` should not allow setting dimension coordinates and should raise an error directing users to use `set_index` instead\n2. Or it should detect this case and set the index correctly\n\nThe provided code example demonstrates the issue step by step, showing:\n- The initial data structure\n- The problem when using `rename`\n- The expected behavior when using `set_index`\n- The difference in the `.indexes` attribute between the two approaches\n\nThe environment details are comprehensive, including the xarray version (0.20.2) and all relevant dependencies.\n\nThis issue contains all the necessary information for a developer to understand the problem, reproduce it, and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7003": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has identified a specific bug where `ds.indexes.get_unique()` raises a TypeError with MultiIndex objects, while `ds.xindexes.get_unique()` works correctly. The error message \"TypeError: unhashable type: 'MultiIndex'\" is provided, which gives a clear indication of the problem. The issue is related to handling pandas MultiIndex objects in the `get_unique()` method of the indexes interface. The contrast between the behavior of `indexes` and `xindexes` provides a good reference point for what the correct behavior should be. A developer would be able to reproduce this issue and understand what needs to be fixed without requiring additional information.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7089": {
        "explanation": "This issue is reasonably well-specified. The reporter is stating that when using the `Dataset.copy(deep=True)` method, the `.attrs` attribute is not being deep copied, which they expected it would be. \n\nThe issue clearly identifies:\n1. The specific method that has the problem (`Dataset.copy(deep=True)`)\n2. The specific attribute that is not being deep copied (`.attrs`)\n3. The expected behavior (that `.attrs` should be deep copied when `deep=True`)\n\nWhat's missing is:\n1. A code example demonstrating the issue\n2. The current behavior (what happens to `.attrs` currently)\n3. The version of the software where this occurs\n\nHowever, for an experienced developer familiar with the codebase, this should be enough information to:\n1. Verify if `.attrs` is indeed not being deep copied\n2. Determine if this is intended behavior or a bug\n3. Implement a fix to ensure `.attrs` is deep copied when `deep=True` is specified\n\nThe issue is straightforward and the expected behavior is clear, making it reasonably specified for a solution attempt.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-7101": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a minimal complete verifiable example (MCVE) that reproduces the issue. \n\nThe user has:\n1. Clearly described the problem: When using `assign_coords` on a dataset that was created using `stack`, the dataset enters a broken state where its length is calculated as less than zero, causing errors.\n\n2. Provided a hypothesis about the root cause: The issue might be with updating `_coord_names` in the coordinates.py file (with a specific line reference).\n\n3. Included a complete, minimal code example that reproduces the issue, with the exact error message.\n\n4. Added a test case that demonstrates the problem and explains the underlying issue in detail - specifically that `_coord_names` doesn't seem to update correctly after assigning coordinates.\n\n5. Provided comprehensive environment information.\n\nThe issue is specific enough that a developer could understand the problem, reproduce it, and work on a fix without needing additional information. The user has even identified a potential location in the code where the bug might be occurring, which provides a good starting point for investigation.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7105": {
        "explanation": "This issue is well-specified with clear information about what's happening and what's expected. The user provides:\n\n1. A clear problem statement: groupby on a multi-index level is not working as expected in xarray 2022.6.0\n2. A minimal, complete, verifiable example that reproduces the issue\n3. The exact output they're getting with version 2022.6.0\n4. The expected output they were getting with version 2022.3.0\n5. Environment details including all relevant package versions\n\nThe issue appears to be a regression in how xarray handles groupby operations on multi-indexed DataArrays. In the newer version, the groupby operation is returning the full multi-index tuples as keys instead of just the level values specified in the groupby. This is clearly a bug since the expected behavior would be to group by just the \"one\" level of the multi-index.\n\nA developer would have all the information needed to reproduce the issue, identify the regression, and implement a fix to restore the previous behavior.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7147": {
        "explanation": "This issue is well-specified with a clear problem statement and a reproducible example. The issue describes that when a dataset contains variables with time values that cannot be represented as `cftime.datetime` objects, the error message does not indicate which variable is causing the problem. The user wants the error message to include the name of the problematic variable to make debugging easier.\n\nThe example provided demonstrates the issue clearly:\n1. It creates two DataArray objects - one with valid times and one with invalid times (values too large)\n2. It merges them into a single dataset\n3. When attempting to decode the CF conventions with `xr.decode_cf(ds)`, an error is raised\n4. The error message does not indicate that \"invalid_times\" is the problematic variable\n\nThe solution would require modifying the error handling in xarray's time decoding functionality to include the variable name in the error message. The code paths involved are clearly visible in the traceback, particularly in the `xarray/coding/times.py` file.\n\nThe issue has a specific, well-defined scope with a clear path to resolution. It includes all the necessary context for a developer to understand and fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7150": {
        "explanation": "This issue is well-specified with a clear problem description, expected behavior, and a complete reproducible example. The user is trying to implement a custom NetCDF4 backend that can handle multiindices when loading a NetCDF dataset using `xr.open_dataset`. \n\nThe issue is clearly defined:\n1. The user has provided complete code examples showing how they create a dataset with a multiindex\n2. They show how they save it to NetCDF and load it manually\n3. They've implemented a custom backend to handle multiindices automatically\n4. They've shown the exact error that occurs when using their custom backend\n5. They've demonstrated a workaround that does work\n6. They've clearly stated what they expected to happen\n\nThe error is specific and reproducible: when calling `decode_compress_to_multiindex` inside the backend's `open_dataset` method, it fails with a ValueError about not being able to assign to the `.data` attribute of a dimension coordinate. However, calling the same function after loading the dataset works fine.\n\nThe environment details are comprehensive, and the user has confirmed this is a minimal, complete, and verifiable example. The issue appears to be that xarray is performing some operation on the dataset returned by the backend that conflicts with multiindices.\n\nA developer would have enough information to investigate why the behavior differs between applying the transformation inside the backend versus after loading, and could work on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7203": {
        "explanation": "This issue is well-specified with clear information about what's happening and what should happen instead:\n\n1. The problem is clearly identified: When displaying representations of datasets with many \"small\" variables, xarray loads them into memory, causing significant performance issues.\n\n2. The specific code causing the issue is pinpointed with a GitHub link to the relevant lines in formatting.py.\n\n3. A complete, minimal example is provided that demonstrates the issue, including:\n   - Code to reproduce the problem\n   - The specific dataset to use (from a public S3 bucket)\n   - Timing measurements showing the performance difference (36.4s vs 6\u03bcs)\n\n4. The expected behavior is clear: \"Fast reprs!\" - specifically, avoiding loading data for representations.\n\n5. The user even tested a potential solution (commenting out the array.size condition) and reported the performance improvement.\n\n6. Detailed environment information is provided.\n\nThis issue contains all the necessary information to understand the problem, reproduce it, and implement a solution. The fix would likely involve modifying the repr generation code to avoid loading data unnecessarily for small variables.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7229": {
        "explanation": "This issue is well-specified with clear information about what's happening and what should happen instead:\n\n1. The problem is clearly identified: `xr.where(..., keep_attrs=True)` is overwriting coordinate attributes with variable attributes since version 2022.06.0.\n\n2. A minimal, complete, verifiable example is provided that demonstrates the issue:\n```Python\nimport xarray as xr\nds = xr.tutorial.load_dataset(\"air_temperature\")\nxr.where(True, ds.air, ds.air, keep_attrs=True).time.attrs\n```\n\n3. The expected output vs. actual output is clearly shown:\n   - Current output: Time coordinate attributes are being overwritten with air variable attributes\n   - Expected output: Time coordinate should keep its original attributes (`{'standard_name': 'time', 'long_name': 'Time'}`)\n\n4. The issue references a specific PR (#6461) that likely introduced the bug.\n\n5. The environment details are comprehensive, showing the exact versions of xarray and related dependencies.\n\nThis issue provides all the necessary information to understand, reproduce, and fix the problem. A developer can easily verify the issue and understand what needs to be fixed - the `xr.where()` function needs to be modified to properly preserve coordinate attributes when `keep_attrs=True` is specified.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7233": {
        "explanation": "This issue is well-specified with a clear problem statement and a complete, verifiable example. The user has:\n\n1. Clearly identified the problem: `ds.Coarsen.construct` is demoting non-dimensional coordinates to variables\n2. Provided the expected behavior: coordinates should remain coordinates after coarsening\n3. Included a minimal, complete, verifiable example that demonstrates the issue\n4. The example shows exactly what happens (coordinates like 'day' become data variables after coarsening)\n5. The code can be easily reproduced in an IPython environment\n\nThe issue provides all the necessary information to understand the problem and attempt a solution. A developer would need to modify the `construct` method in the Coarsen class to ensure that non-dimensional coordinates maintain their coordinate status after the operation is performed.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7347": {
        "explanation": "This issue is reasonably well-specified with enough information to understand the problem and attempt a solution:\n\n1. The issue clearly states the problem: `assign_coords()` is resetting all dimension coordinates to default (pandas) index instead of preserving the index of coordinates that are not being updated.\n\n2. The expected behavior is clearly defined: `assign_coords()` should preserve the index of coordinates that are not updated or not part of a dropped multi-coordinate index.\n\n3. The issue links to a specific GitHub issue (https://github.com/martinfleis/xvec/issues/13) that contains a minimal, complete, verifiable example demonstrating the problem.\n\n4. The user has confirmed that the example is minimal, complete, and verifiable.\n\n5. The Xarray version (2022.11.0) is provided.\n\nWhile the actual code example is not directly included in this description but rather linked to an external issue, the description of the problem is clear enough that a developer familiar with Xarray would understand what needs to be fixed. The linked issue likely contains the necessary code example to reproduce and fix the issue.\n\nThe only minor limitation is that we need to follow the link to see the actual code example, but the issue description itself provides enough context about what's happening and what should happen instead.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pydata__xarray-7391": {
        "explanation": "This issue is well-specified with a clear problem statement and a minimal reproducible example. The user has identified that:\n\n1. When performing arithmetic operations between two Dataset objects, the `keep_attrs=True` option is ignored, resulting in attributes not being preserved.\n2. This behavior is inconsistent with DataArrays/Variables, which do respect the `keep_attrs` option.\n3. A code example is provided that demonstrates the issue.\n4. The expected behavior is clearly stated: Datasets should behave like DataArrays/Variables and preserve attributes during binary operations when `keep_attrs=True` is set.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and implement a solution. The user even mentions that they are preparing a PR to address this issue, indicating they have a clear understanding of what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7393": {
        "explanation": "This issue is well-specified with a clear problem statement and a minimal, complete, verifiable example that demonstrates the issue. The user has:\n\n1. Clearly described the problem: When using the `stack()` method on an xarray Dataset, the data type of coordinates is being changed from int32 ('i4') to int64 ('i8').\n\n2. Provided a specific expected behavior: The data type should be preserved when stacking.\n\n3. Included a minimal code example that reproduces the issue, which can be easily run and verified.\n\n4. Confirmed that the example is minimal, complete, and verifiable.\n\n5. Provided detailed environment information including versions of xarray, numpy, and other relevant packages.\n\nThe issue is focused on a specific behavior in xarray's stack method that changes data types unexpectedly. This is a well-defined bug that can be investigated and fixed without needing additional clarification from the user. The solution would involve modifying the stack method to preserve the original data type of coordinates.",
        "label": "WELL_SPECIFIED"
    },
    "pydata__xarray-7400": {
        "explanation": "The issue description provides a clear task: modify the concat operation for datasets to handle disjoint variables by adding missing variables with np.nan values instead of raising an exception. This is a specific feature enhancement that addresses issue #508.\n\nThe description indicates:\n1. The exact behavior change needed: stop raising exceptions when concatenating datasets with disjoint variables\n2. The specific solution approach: add missing variables with np.nan values\n3. That tests have been added\n4. That code quality checks have been passed\n5. That documentation has been updated\n\nWhile the issue description is brief, it provides enough information for an experienced developer to understand what needs to be implemented. The reference to issue #508 suggests there may be additional context available in that issue, but the core requirement is clear enough to proceed with implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-4330": {
        "explanation": "This issue is well-specified with a clear problem and proposed solution. The user wants the spellchecker to ignore text that is enclosed in double backticks (`` ``) in docstrings and comments, as these typically indicate code references. \n\nThe problem is clearly described with a specific example: when writing comments like \"The ``.qsize()`` method...\", the spellchecker flags \"qsize\" as a spelling error, even though it's intentionally referring to a code method.\n\nThe solution is also clearly specified: implement a filter that ignores text within backticks, similar to existing filters for Sphinx directives. The user even suggests an implementation approach by referencing existing functionality.\n\nWith this information, a developer could implement a solution without needing additional clarification. The scope is well-defined, the use case is clear, and there's even a hint at how to approach the implementation.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4339": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a problem: Pylint doesn't support outputting results to a single file, which is needed when running from within tox on CI servers (since tox doesn't support shell redirection). The issue provides context by mentioning:\n\n1. The previous `--file-output=y` option that was removed in Pylint 2.0\n2. A reference to a similar feature added to flake8 for the same reason\n3. Links to relevant resources (StackOverflow question, tox issue, changelog)\n\nWhat makes it not fully \"well-specified\" is that it doesn't explicitly state the desired solution format. While it mentions HTML as an example output format, it's not clear if:\n- HTML is the only format needed or if other formats should be supported\n- What the exact command-line interface should look like\n- If there are any specific requirements for the file output structure\n\nHowever, there's enough information to make a reasonable implementation that would allow Pylint to output to a single file when run from tox, which appears to be the core requirement. A developer could implement a solution based on the context provided and the referenced flake8 implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-4398": {
        "explanation": "This issue is well-specified with clear context and proposed solutions. The user explains:\n\n1. The current problem: When using `--fail-under` with pylint in CI, error-category issues don't cause a non-zero exit code if the overall score is above the threshold.\n\n2. Their goals: They want to both enforce a minimum score AND ensure no error-category issues exist.\n\n3. Two specific solution approaches:\n   - Modify `--fail-under` to not override other exit statuses\n   - Add a new `--fail-on` option to fail on specific issue types\n\nThe request is technically feasible within pylint's architecture, as it already has exit code handling and issue categorization. The proposed solutions are concrete and reasonable. A developer could implement either approach with the information provided, though they would need to make some implementation decisions about the exact behavior and syntax.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4421": {
        "explanation": "This issue is well-specified with clear steps to reproduce, expected behavior, actual behavior, and even the probable cause of the issue. The reporter has:\n\n1. Provided detailed reproduction steps including commands to run\n2. Included a sample file that triggers the issue\n3. Shared the exact error traceback\n4. Specified the expected behavior\n5. Included version information for all relevant packages\n6. Even identified the likely root cause - a change in Pylint's versioning scheme that's causing a type error in the pylint-quotes plugin\n\nThe issue is that Pylint 2.8.2 breaks compatibility with the pylint-quotes plugin due to a versioning scheme change. The error occurs because the plugin is trying to compare version components as `(2, 2, 2) < pylint_version` but the version is now returned as a string, causing a TypeError.\n\nThis is a clear bug with a well-defined scope that could be fixed either in Pylint or in the pylint-quotes plugin. A developer would have all the necessary information to implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4516": {
        "explanation": "This issue is well-specified with clear steps to reproduce, expected behavior, and current behavior. The issue describes two related problems:\n\n1. The `--ignore` flag doesn't properly ignore subdirectories when using glob patterns\n2. The `--ignore-patterns` flag only works for top-level directories but not for subdirectories\n\nThe issue provides:\n- Concrete reproduction steps with file structures and commands to run\n- Clear examples of the current incorrect output\n- Clear description of the expected behavior\n- Version information for pylint, astroid, and Python\n\nThe problem is technical and specific: when using either the `--ignore` or `--ignore-patterns` flags, pylint is not properly ignoring files in subdirectories that should be excluded based on the patterns. This is a well-defined bug with a clear expected outcome - pylint should respect the ignore patterns for files in subdirectories, not just top-level files.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed in the pylint codebase regarding how it processes ignore patterns for nested directory structures.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4551": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: pyreverse (a tool in pylint) does not read Python type hints when generating UML diagrams.\n\n2. A specific code example is provided showing the issue.\n\n3. The current behavior is shown with a screenshot of the UML output.\n\n4. The expected behavior is explicitly stated: the user wants to see the type hint information (e.g., \"a : String\") in the UML diagram.\n\n5. Version information is provided for pylint, astroid, and Python.\n\nThe issue is asking for pyreverse to be enhanced to recognize and incorporate Python type hints (as defined in PEP 484) in its UML diagram generation. This would involve modifying the code that parses Python files and extracts class information to also extract and display type hint information.\n\nA developer would have a clear understanding of what needs to be implemented and could verify the solution by checking if the type hints appear in the generated UML diagrams.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4604": {
        "explanation": "This issue is well-specified with clear steps to reproduce, current behavior, and expected behavior. The problem is that pylint's \"unused-import\" warning is being incorrectly triggered for the `abc` module when it is actually being used in type annotations/comments. \n\nThe issue provides:\n1. A complete code example that demonstrates the problem\n2. The exact error message being generated\n3. The expected behavior (that the warning should not be emitted)\n4. The version information for pylint, astroid, and Python\n5. A reference to a related issue (#3112)\n\nFrom this information, it's clear that the task would be to modify pylint's unused import detection to recognize when an import is being used in type annotations/comments like `# type: abc.ABC`. This is a specific, well-defined bug with a clear success criterion: the warning should not be emitted for the given example code.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4661": {
        "explanation": "This issue is well-specified with a clear problem and solution path:\n\n1. Problem: The `.pylint.d` directory is being created directly in the user's home folder, which doesn't comply with the XDG Base Directory Specification.\n\n2. Solution requirement: Move the storage location to follow the XDG Base Directory Specification, specifically to use `$HOME/.local/share/pylint` for data files.\n\n3. Implementation guidance: The issue suggests changing the variables/constants that determine where these files are stored to point to the appropriate XDG-compliant directory.\n\n4. Reference: The issue includes a link to the XDG Base Directory Specification for further details.\n\nThe issue provides enough context about what needs to be changed and why. A developer familiar with pylint's codebase should be able to locate where the storage path is defined and modify it to use the XDG Base Directory Specification. The solution would involve checking for XDG environment variables (like XDG_DATA_HOME) and falling back to the default paths when those aren't set.\n\nThis is a common pattern in Linux/Unix software development and the specification linked provides all the necessary details for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-4970": {
        "explanation": "This issue is well-specified with clear information about both the current behavior and the desired solution:\n\n1. Current problem: Setting `min-similarity-lines` to `0` causes pylint to flag every line as duplicate code instead of disabling the check.\n\n2. Desired solution: Setting `min-similarity-lines` to `0` should disable the duplicate code check entirely.\n\n3. Context: The issue references similar behavior in other linters like flake8, where setting numerical thresholds to `0` disables the corresponding check. It also references a related open issue (#214).\n\nThe issue provides enough information to understand what needs to be fixed, where the problem is occurring, and what the expected behavior should be. A developer would be able to locate the code that handles the `min-similarity-lines` parameter and modify it to disable the duplicate code check when the value is set to 0.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-5136": {
        "explanation": "This issue is well-specified with clear context and a proposed solution. The problem is clearly articulated: `MessagesHandlerMixIn` and `PyLinter` have circular type dependencies causing mypy errors. The issue provides:\n\n1. Specific code references with links to the relevant files\n2. A concrete example of the problem (the `add_message` method)\n3. A branch that demonstrates the typing issues\n4. A clear proposed solution (merge `MessagesHandlerMixIn` into `PyLinter`)\n5. An alternative solution that was considered but deemed less desirable\n\nThe issue provides enough context for a developer to understand the problem, verify it, and implement the suggested solution. The task is straightforward: move the functionality from `MessagesHandlerMixIn` into `PyLinter` to resolve typing issues. There are no ambiguities about what needs to be done or what success looks like.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-5446": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that the duplicate-code checker (R0801) in Pylint cannot be disabled for specific portions of a file using the standard #pylint:disable/enable comment syntax that works for other checkers.\n\nThe issue clearly identifies:\n1. The specific problem: inability to disable the duplicate-code (R0801) checker for portions of a file\n2. The user has looked at the source code and acknowledges it's not trivial to implement\n3. The feature request is to allow selective disabling of duplicate code detection within files\n\nWhat makes this not fully \"well-specified\" is the lack of:\n- Specific examples of the code where this would be useful\n- The exact syntax they expect to work\n- Any proposed implementation approach\n\nHowever, a developer familiar with Pylint would understand what's being requested since the standard #pylint:disable/enable pattern exists for other checkers, and the request is to extend this functionality to the duplicate code checker. The implementation challenge is acknowledged in the issue description.\n\nA successful solution would allow users to use comments like \"#pylint:disable=R0801\" to disable duplicate code detection for specific blocks of code within a file.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-5595": {
        "explanation": "The issue is reasonably well-specified in terms of the problem: the negative score in pylint is demotivating for beginners. The user clearly identifies that when pylint returns a negative score like \"-24/10.0\", it makes the tool seem overly critical and unwelcoming.\n\nHowever, the solution part is somewhat vague. The user suggests changing from a \"0-10\" score system but doesn't specify exactly what alternative they prefer. They suggest \"checking if other tools are doing something different\" which leaves the actual implementation details open-ended.\n\nDespite this, there is enough information to understand the core problem and begin working on a solution. A developer could:\n1. Research how other linting tools present scores\n2. Develop a more beginner-friendly scoring system that avoids negative numbers\n3. Implement changes to the scoring algorithm while maintaining the same underlying assessment\n\nThe issue is specific enough to identify the component that needs changing (the scoring system) and the goal (make it more beginner-friendly by avoiding negative scores), even if the exact implementation is left to the developer.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-5613": {
        "explanation": "This issue is well-specified. The user clearly describes:\n\n1. The current problem: They need to use pyreverse (a tool within pylint) but the current approach requires manipulating sys.argv and handling SystemExit exceptions.\n\n2. The desired solution: They want a cleaner API that allows direct function calls with named parameters instead of command-line argument manipulation.\n\nThe issue provides concrete code examples for both the current workaround and the desired interface. The requested functionality is straightforward - create a proper Python API for the pyreverse functionality that's currently only accessible through command-line style invocation.\n\nThe solution would involve creating a wrapper function in pylint that provides the cleaner interface while internally handling the sys.argv manipulation and SystemExit exception. The parameters needed (output format, project name, and files) are clearly specified in the example.\n\nThis is a common pattern in Python libraries - providing both command-line and programmatic interfaces - so the request follows established practices.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-5839": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. Problem: Pylint currently allows reusing message IDs (msgid) and symbols from previously deleted checks, which causes confusion and inconsistencies.\n2. Examples are provided: \"using-f-string-in-unsupported-version\" vs \"apply-builtin\", and others like \"mixed-indentation\", \"bad-whitespace\", \"bad-continuation\".\n3. The issue notes that while there's a mechanism for renamed messages (old_name), there's nothing for completely removed messages.\n4. Expected behavior is clearly stated: It should be impossible to use an old message ID or symbol.\n5. Related issues are referenced for additional context (#5723 and #5607).\n\nThe solution would involve implementing a mechanism to track deleted message IDs and symbols and prevent their reuse in new checks. This is a straightforward feature request with a clear goal and sufficient context to understand what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6059": {
        "explanation": "This issue is well-specified. It clearly identifies a specific method (`BaseChecker.__gt__`) that is not covered by tests and presents a straightforward question: is this method required or can it be removed? The issue includes a direct link to the code in question and references a discussion where this was originally noted. The task is clear - either add a unit test for this method if it's needed, or remove the method if it's not needed. This requires investigating the purpose and usage of the `__gt__` method in the `BaseChecker` class to determine if it's necessary for the functionality of the codebase. The issue doesn't require additional configuration details or command examples since it's a straightforward code maintenance question about test coverage and code necessity.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6196": {
        "explanation": "This issue is well-specified and provides clear context about the problem with the `@check_messages` decorator in Pylint. The author:\n\n1. Clearly explains what the decorator does and how it works\n2. Outlines the current problems with its usage\n3. Identifies specific issues in the codebase\n4. Asks specific questions that need consensus before proceeding with a solution\n\nThe issue is asking for:\n- A consensus on whether to keep or rename the decorator\n- Best practices for using the decorator\n- Whether to expand the decorator's functionality beyond top-level callbacks\n\nThe author has already done significant research and has a good understanding of the problem. They're seeking agreement on the direction before implementing changes to documentation and possibly code. This is a well-defined issue with clear questions that need answers before proceeding with implementation.\n\nThe solution would involve documenting the decorator properly and potentially making code changes based on the consensus reached on the questions posed.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6386": {
        "explanation": "This issue is well-specified with clear details about the bug:\n\n1. The problem is clearly stated: The short option `-v` for the `verbose` option incorrectly expects an argument, while the long option `--verbose` works correctly without requiring an argument.\n\n2. The issue provides specific reproduction steps with the exact command that fails: `pylint mytest.py -v`\n\n3. The actual error output is included: `pylint: error: argument --verbose/-v: expected one argument`\n\n4. The expected behavior is clearly defined: The short option `-v` should behave like the long option `--verbose` and not require an argument.\n\n5. Version information is provided: Pylint 2.14.0-dev0, astroid 2.11.2, Python 3.10.0b2\n\nThis is a straightforward bug with a clear discrepancy between the behavior of the short and long forms of the same option. The fix would likely involve modifying the argument parser configuration to ensure the short option `-v` doesn't require an argument, matching the behavior of `--verbose`. A developer could implement a solution without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6412": {
        "explanation": "This issue is well-specified for implementation. It clearly identifies the current problem (PyLinter categorizes checkers through a non-standard `__implements__` class attribute) and proposes a specific solution with two concrete steps:\n\n1. Change interfaces to use `ABCMeta` metaclass\n2. Modify PyLinter to use isinstance checks for categorization while maintaining backward compatibility with the existing `__implements__` approach\n\nThe issue provides enough context about the current implementation and a clear path forward. An experienced developer familiar with Python's ABCMeta and metaclasses would understand what needs to be done. The goal is also clear: make it easier for others to write plugins by using Python's standard type-checking mechanisms rather than a custom approach.\n\nThe only minor detail missing is which specific interfaces need to be changed, but this would be easily determined by examining the codebase and identifying the relevant checker/reporter interfaces that currently use the `__implements__` pattern.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6506": {
        "explanation": "This issue is well-specified with clear information about the bug and expected behavior:\n\n1. The problem is clearly defined: When an unrecognized option is passed to pylint (e.g., `-Q`), a full traceback is printed instead of a user-friendly error message.\n\n2. The exact command that reproduces the issue is provided: `pylint -Q`\n\n3. The actual output (with the traceback) is shown in detail.\n\n4. The expected behavior is clearly described: The user wants a more user-friendly error message without the traceback, similar to how mypy handles unrecognized options.\n\n5. Version information is provided: pylint 2.14.0-dev0, astroid 2.11.3, Python 3.10.0b2\n\nThe solution would involve modifying how pylint handles unrecognized command-line options to catch the `_UnrecognizedOptionError` exception and display a user-friendly message instead of allowing the traceback to be shown. This is a straightforward enhancement to error handling that improves user experience.\n\nA developer would have all the necessary information to locate the issue in the code (the traceback points to the relevant files) and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6517": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description clearly states that when a pylintrc file contains an empty `confidence=` setting, pylint 2.14 unexpectedly passes the linting check instead of reporting the expected errors. \n\nThe issue includes:\n1. A clear code sample that should trigger linting errors\n2. The exact configuration file content that causes the issue\n3. The exact command used to reproduce the problem\n4. The actual output received\n5. The expected output with specific error messages\n6. The exact version information for pylint, astroid, and Python\n\nThe problem is clearly defined: when the `confidence=` line is present but empty in the pylintrc file, pylint is not reporting any errors even when they exist. This is enough information for a developer to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6526": {
        "explanation": "This issue is reasonably well-specified. The user from the VS Code Python extension team is asking about how pylint decides where to place its stat files. They've provided:\n\n1. Clear context: They're building a VS Code extension using pylint\n2. The specific problem: pylint is creating stat files in the project directory instead of expected locations like %USERPROFILE%/AppData/Local/pylint\n3. Their current implementation details: \n   - They use runpy.run_module to run pylint\n   - They pass source code via stdin\n   - They only use two arguments: --reports=n and --output-format=json\n4. Links to relevant code in their repository\n5. Links to the issue in their GitHub repository\n\nWhat makes this not fully \"well-specified\" is that they're asking for debugging help rather than requesting a specific feature or fix. They want to understand the behavior of pylint regarding stat file placement, which requires investigation into pylint's internals. The solution would involve explaining pylint's behavior and possibly suggesting configuration changes.\n\nHowever, there's enough information for someone familiar with pylint to investigate the issue and provide meaningful guidance on how pylint determines stat file locations and what might be causing the unexpected behavior in their specific implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pylint-dev__pylint-6528": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a clear bug description: Pylint doesn't respect ignore settings in recursive mode\n2. Included specific commands used to reproduce the issue\n3. Shown the actual output received\n4. Explained the expected behavior with reference to documentation\n5. Included version information for both pylint and Python\n\nThe issue demonstrates that when running pylint with `--recursive=y`, files in directories that should be ignored (via `--ignore`, `--ignore-paths`, or `--ignore-patterns`) are still being linted. This contradicts the documented behavior of these flags.\n\nThe reproduction is straightforward - the user shows that a file in a `.a` directory is being linted despite various attempts to ignore it. They've also referenced the documentation that explains how these ignore flags should work.\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6556": {
        "explanation": "This issue is well-specified with clear steps to reproduce, current behavior, and expected behavior. The problem is that the user wants to disable the \"bad-option-value\" warning (E0012) that occurs when using pylint disable directives for warnings that don't exist in older pylint versions. Specifically:\n\n1. The user has code with a comment to disable \"useless-object-inheritance\" warning for Python 2/3 compatibility\n2. When this code is run with older pylint versions (e.g., 2.3.1) that don't have this warning, it generates a \"bad-option-value\" error\n3. The user tries to disable this meta-warning with `# pylint: disable=bad-option-value` or `# pylint: disable=E0012`\n4. These attempts to disable the warning are ignored\n\nThe expected behavior is clear: the user wants a way to write code that works across different pylint versions without having to standardize pylint versions across their entire organization.\n\nThis is a specific, technical issue with a clear problem statement and a reasonable expectation for a solution. A developer would be able to investigate why the bad-option-value warning cannot be disabled and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6820": {
        "explanation": "This issue is well-specified with clear steps to reproduce the problem. The issue describes:\n\n1. The exact bug: Using `--load-plugin` (singular) instead of `--load-plugins` (plural) in the CLI silently fails to load the plugin without any warning.\n\n2. Clear reproduction steps:\n   - The command that works: `pylint b.py --load-plugins=pylint.extensions.redefined_loop_name`\n   - The command that fails: `pylint b.py --load-plugin=pylint.extensions.redefined_loop_name`\n\n3. The expected behavior: A warning should be shown that `--load-plugin` is not the correct argument.\n\n4. The actual behavior: No warnings are shown, and the plugin is not loaded.\n\n5. The version of Pylint being used: 2.14.0\n\nThe issue is specific, testable, and has a clear definition of what a successful fix would look like: when a user mistakenly uses `--load-plugin` instead of `--load-plugins`, Pylint should provide a warning about the incorrect argument rather than silently failing. This would require modifying the CLI argument handling to detect this common mistake and provide appropriate feedback.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-6903": {
        "explanation": "This issue is well-specified with clear details about the bug, its cause, and a potential solution:\n\n1. The problem is clearly identified: When running pylint with `--jobs=0` in a Kubernetes environment, it crashes because the CPU detection logic returns 0, which is invalid for multiprocessing.\n\n2. The exact error message and stack trace are provided, showing the ValueError: \"Number of processes must be at least 1\".\n\n3. The user has investigated the root cause, identifying the specific function (`pylint.run._query_cpu()`) and the calculation that leads to 0 (`2/1024` cast to an int).\n\n4. The environment details are comprehensive (Ubuntu 20.04, Kubernetes v1.18.6, Python 3.9.12, pylint>2.14.0).\n\n5. A reasonable solution is suggested: ensure the calculated CPU count is never 0 by adding an \"or 1\" fallback.\n\nThe issue provides all the necessary information to understand, reproduce, and fix the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7080": {
        "explanation": "This issue is well-specified with clear details about the problem. The user reports that when running pylint with the `--recursive=y` option, the `ignore-paths` configuration in pyproject.toml is being ignored. The issue includes:\n\n1. Clear bug description stating that `ignore-paths` is ignored when running recursively\n2. The exact configuration in pyproject.toml showing the ignore pattern for \"src/gen/*\" files\n3. The exact command used: `pylint --recursive=y src/`\n4. Comprehensive output showing that files in src/gen/ are being linted despite being in the ignore-paths\n5. Clear expected behavior: \"src\\gen\\* should not be checked\"\n6. Version information for pylint, astroid, and Python\n7. OS information\n\nThe issue provides all necessary information to reproduce the problem and understand what's going wrong. The expected behavior is clearly stated - files matching the ignore-paths pattern should be excluded from linting when using the recursive option. The output clearly shows that files in the src/gen directory are being linted despite being in the ignore-paths configuration.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7114": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and current behavior. The problem is clearly defined: when a directory and a module within it share the same name (e.g., directory 'a' contains file 'a.py'), pylint fails with an error about missing '__init__.py'. The issue provides:\n\n1. Clear reproduction steps with directory structure examples\n2. Exact error messages\n3. Two workarounds (renaming the file or adding an __init__.py)\n4. Version information for pylint, astroid, and Python\n5. Additional information about side effects on module resolution\n\nThe expected behavior is clearly stated: pylint should work correctly even when a directory contains a module with the same name, without requiring an __init__.py file. This is a specific bug with a clear path to verification and testing. A developer would have all the information needed to reproduce the issue, understand the problem, and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7228": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The bug is clearly described: Using the Unicode property pattern `\\p{Han}` in a regex pattern in the pylintrc configuration file causes an error.\n\n2. The exact configuration is provided: The specific regex pattern being used in the function-rgx setting.\n\n3. The exact error message is provided: A detailed traceback showing that the error is \"bad escape \\p at position 1\" from the re module.\n\n4. The expected behavior is stated: \"not throw error\" - the user expects the regex with `\\p{Han}` to work properly.\n\n5. Version information is provided: Pylint 2.14.4, Python 3.9.13, and macOS 11.6.7.\n\nThe issue is that Python's standard re module doesn't support Unicode property escapes like `\\p{Han}` (which would match Han/Chinese characters) in regular expressions by default. This is a known limitation in older Python versions. The solution would likely involve either:\n\n1. Using a different regex pattern that doesn't rely on `\\p{Han}`\n2. Using the newer regex module that supports Unicode properties\n3. Implementing a workaround in Pylint to handle these patterns\n\nThe problem is clear and reproducible, making it well-specified for a solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7277": {
        "explanation": "This issue is well-specified with clear information about the bug and expected solution:\n\n1. The problem is clearly identified: pylint is removing the first item from sys.path when running from runpy, which can cause issues when that path is intentionally added by the user.\n\n2. The exact location of the problematic code is provided with a GitHub link: https://github.com/PyCQA/pylint/blob/ce7cccf96454fb6e286e4a8f38919733a0f28f44/pylint/__init__.py#L99\n\n3. A reproducible test case is provided with specific code that demonstrates the issue.\n\n4. The expected behavior is clearly stated: check if the first item in sys.path is \"\", \".\" or os.getcwd() before removing it.\n\n5. The pylint version affected (2.14.5) is specified.\n\nThe issue provides all the necessary information to understand the problem and implement a solution. The fix would involve modifying the code at the specified location to add a conditional check before removing the first item from sys.path.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-7993": {
        "explanation": "This issue is well-specified with clear information about the bug:\n\n1. The problem is clearly defined: Using double braces in message templates (like `{{ \"Category\": \"{category}\" }}`) works in Pylint 1.7.4 with Python 3.6 but fails in Pylint 2.12.2 with Python 3.9.\n\n2. The issue provides:\n   - Exact command used (`pylint test.py --msg-template='{{ \"Category\": \"{category}\" }}'`)\n   - Current output (showing the error and incorrect formatting)\n   - Expected output (dictionary with \"Category\" as the key)\n   - Precise version information for both working and non-working environments\n\n3. The error message is included, showing that Pylint doesn't recognize the argument '{ \"Category\"' in the template.\n\n4. The issue demonstrates a regression between versions, which helps narrow down when the problem was introduced.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - likely an issue with how double braces are handled in the message template parser in newer Pylint versions.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8124": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The bug is clearly described: pylint raises \"useless-import-alias\" errors when using explicit import aliases for re-exports (e.g., `from ._submodule1 import Api1 as Api1`), even though these aliases are intentionally used to make re-exports explicit for mypy compatibility.\n\n2. The issue provides a concrete example of the package structure and code that triggers the problem.\n\n3. The issue includes the exact pylint error messages being generated.\n\n4. The expected behavior is clearly stated: pylint should not flag these explicit re-exports as \"useless-import-alias\" errors.\n\n5. The issue includes version information for pylint, astroid, and Python.\n\n6. There's even a link to a real-world example in the bidict library where this issue occurs.\n\nThe solution would involve modifying pylint's checker for \"useless-import-alias\" to recognize and handle the case where aliases are used for explicit re-exports, particularly when the imported name and the alias are identical. This is a specific, well-defined task with clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8281": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The current problem: Users need to manually list all source roots for multi-package projects, which is inconvenient.\n2. The desired solution: Support globbing patterns (specifically mentioning `src/*` as an example) for the `--source-roots` parameter.\n3. A specific recommendation: Use globbing patterns rather than regex patterns, with a rationale (better support for path-specific matching).\n\nThe issue provides enough context to understand what needs to be implemented - adding support for glob pattern expansion in the `--source-roots` parameter. The example given (`src/*`) makes it clear what kind of functionality is expected. A developer would need to:\n1. Identify where the `--source-roots` parameter is processed\n2. Add glob pattern support to expand patterns like `src/*` into the list of matching directories\n3. Ensure the expanded list is used in the same way as manually specified directories\n\nThis is a common feature in command-line tools, and the implementation approach is straightforward based on the description provided.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8312": {
        "explanation": "This issue is well-specified with clear context and a specific solution path:\n\n1. The problem is clearly defined: Support globbing patterns (like `packages/*/src`) for input specification in pylint.\n\n2. The issue references specific discussions in previous PRs and issues that provide additional context.\n\n3. The desired solution is very specific, pointing to the exact line in the codebase where the change should be made: https://github.com/PyCQA/pylint/blob/dca394035268a234b29d0c103a4fcc201c84061f/pylint/config/config_initialization.py#L123\n\n4. The example use case is provided: `pylint --recursive=y packages/*/src`\n\nA developer can understand what needs to be implemented (globbing support for input paths), where to implement it (specific file and line), and has a clear example of the expected functionality. The solution would involve modifying the input path handling to support glob patterns, likely using Python's built-in `glob` module.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8799": {
        "explanation": "This issue is well-specified with clear details about the problem. The user reports that pylint is taking an unacceptably long time to run (3+ seconds) even on empty files, and 8+ seconds on small projects. They've provided:\n\n1. Clear reproduction steps with exact commands\n2. Timing measurements showing the problem\n3. Expected behavior (faster execution)\n4. Pylint version information (2.17.4)\n5. OS information (Windows 11)\n6. Example of the issue on both an empty file and a small project\n\nThe issue is clearly about performance optimization - specifically, the need to short-circuit pylint's execution when all checks are disabled with `--disable=all`. The title suggests a solution approach (short-circuiting), and the description provides enough context to understand the problem's impact.\n\nA developer could work on this issue without needing additional clarification, as the performance problem is well-documented and the expected outcome (significantly faster execution) is clear.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8819": {
        "explanation": "This issue is well-specified and clearly describes a problem with the order-dependency of enable/disable options in pylint's rcfile configuration. The user explains:\n\n1. The specific problem: In the command line, order matters for --enable and --disable flags, which is understandable. However, in the rcfile, the sections are fixed with 'disable' before 'enable', making it impossible to achieve the same effect as `pylint --enable=all --disable=fixme` without rearranging the config file structure.\n\n2. The expected behavior: The user expects that in an rcfile with `disable=fixme` followed by `enable=all`, the intent should be clear - enable all checks except for the specifically disabled ones, regardless of the order.\n\n3. The current version affected: pylint 2.5.3 / astroid 2.4.2\n\n4. A clear example demonstrating the issue with both command line and rcfile configurations.\n\nThe request is straightforward - make the enable/disable options in the rcfile not depend on the order in which they are specified, so that users can express their intent clearly without having to rearrange the configuration file structure.\n\nThis issue provides enough information for a developer to understand the problem, reproduce it, and implement a solution that would make the rcfile processing more intuitive by handling enable/disable options in a way that matches user expectations.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8898": {
        "explanation": "This issue is well-specified with clear details about the bug, how to reproduce it, and the expected behavior:\n\n1. The problem is clearly defined: The `bad-name-rgxs` option in pylint splits on commas, which breaks regular expressions that contain commas (like quantifiers with ranges `{1,3}`).\n\n2. A minimal reproducible example is provided with:\n   - Configuration that triggers the bug\n   - Command used to run pylint\n   - Complete error traceback showing exactly where the failure occurs\n\n3. The expected behavior is clearly stated: valid regular expressions should be expressible in this option, possibly with a way to escape commas.\n\n4. Environment details are provided (pylint version, Python version, OS).\n\nThe issue points to a specific implementation problem: pylint is using comma as a delimiter for a list of regular expressions, but doesn't provide a way to escape commas that are part of the regex syntax. The error occurs because after splitting on commas, the regex becomes invalid.\n\nA developer would have enough information to locate the problem in the code (likely in the `_regexp_csv_transfomer` function mentioned in the traceback) and implement a solution that either properly handles comma-containing regexes or provides an escaping mechanism.",
        "label": "WELL_SPECIFIED"
    },
    "pylint-dev__pylint-8929": {
        "explanation": "This issue is well-specified with clear steps to reproduce, expected behavior, and current behavior. The problem is clearly defined: when using the JSON output format (`-f json`), the score information (code rating) is not included in the output, even when explicitly requested with the `--score=y` option.\n\nThe issue provides:\n1. Clear reproduction steps with commands and outputs\n2. A comparison between normal output (which shows scores) and JSON output (which doesn't)\n3. Evidence that even with `--score=y` option, the JSON output still doesn't include scores\n4. The expected behavior (score should be added to JSON output)\n5. Version information for pylint, astroid, and Python\n\nA developer would have all the necessary information to understand the problem and implement a solution: modify the JSON formatter to include score information when requested. The solution would likely involve updating the JSON output format to include the score data that's already available in the text output.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10051": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is precisely described: `caplog.get_records()` becomes decoupled from `caplog.records` after calling `caplog.clear()`.\n\n2. The root cause is identified: `caplog.clear()` replaces the records list rather than clearing it, which breaks the reference between the two objects.\n\n3. A specific code location is referenced with links to the relevant parts of the pytest codebase.\n\n4. A complete, minimal reproduction example is provided that clearly demonstrates the issue.\n\n5. The expected behavior is implied: `caplog.get_records()` should remain in sync with `caplog.records` even after calling `caplog.clear()`.\n\n6. Environment details are provided, including Python version and package versions.\n\nA developer could understand the issue, reproduce it, and implement a fix based on this information. The fix would likely involve modifying the `clear()` method to maintain the reference between the two objects, either by clearing the existing list instead of replacing it, or by updating the reference returned by `get_records()`.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10081": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a detailed description of the bug: When using `pytest --pdb`, the `tearDown()` method of `unittest.TestCase` classes is being executed even when the class is decorated with `unittest.skip`.\n\n2. Included a minimal reproducible example with the exact code that demonstrates the issue.\n\n3. Shown the expected behavior (when running without `--pdb`, the test is properly skipped).\n\n4. Shown the problematic behavior (when running with `--pdb`, the tearDown method is executed and causes an error).\n\n5. Provided all relevant environment information (Python version, pytest version, OS version, and full pip list).\n\n6. Referenced a similar issue (#7215) that deals with the same problem but at the function level rather than class level.\n\nThe issue is clearly a bug where skipped test classes should not have their tearDown methods executed when using the `--pdb` flag, just as they don't when running without that flag. The solution would involve modifying pytest's handling of skipped unittest classes when the `--pdb` flag is active to ensure consistency with normal execution.\n\nThis is a well-defined issue with clear reproduction steps and expected behavior, making it straightforward to understand what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10115": {
        "explanation": "This issue is well-specified with clear context and a proposed solution. The problem is that pytest depends on the `atomicwrites` package, which has been declared unmaintained by its developer after they attempted to delete it from PyPI due to disagreement with the 2FA requirement. The issue explains:\n\n1. The exact problem: A dependency (atomicwrites) is now unmaintained and its developer has shown concerning behavior\n2. Why it matters: This creates a risk for pytest as a popular package\n3. Relevant context: Links to GitHub issues and commits documenting the situation\n4. Current usage: The package is only used on Windows\n5. License information: It's MIT-licensed, which allows for code reuse\n6. A proposed solution: Copy the needed parts into pytest directly\n\nThe issue provides enough information for a developer to understand what needs to be done (remove the dependency on atomicwrites by incorporating the necessary code directly into pytest) and why it's necessary. The MIT license makes this legally possible. The issue even links to the source code that would need to be examined.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10343": {
        "explanation": "This issue is well-specified with clear requirements for a solution. The problem is clearly described: when a pytest deprecation warning appears for old-style hook implementations, it doesn't include location information (file path or plugin name) to help identify the source of the problem.\n\nThe issue provides:\n1. The exact warning message that appears\n2. A clear explanation of what's missing (location information)\n3. A specific request for what should be added to the warning (either the plugin name or the Python file path)\n4. Context about why this is important (the user has 12 plugins and can't easily identify which one is causing the issue)\n5. Reference to a previous issue (#9118) that introduced the deprecation warning\n\nA developer would need to:\n1. Find where the deprecation warning is generated in the pytest codebase\n2. Modify the warning to include location information (either plugin name or file path)\n3. Ensure the warning remains informative while adding this context\n\nThis is a straightforward enhancement to an existing feature with clear acceptance criteria - the warning should include location information to help identify the source of the deprecated hook implementation.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10356": {
        "explanation": "This issue is well-specified with a clear problem statement and reproduction case. The issue describes a specific behavior in pytest where markers from multiple base classes are not properly inherited when a class inherits from multiple base classes with markers. The current behavior follows Python's MRO (Method Resolution Order), causing only one marker to be retained (the one from the first base class in the MRO).\n\nThe issue includes:\n1. A clear description of the problem\n2. A complete code example that reproduces the issue\n3. A proposed solution approach (using a metaclass to collect markers from all base classes)\n4. The expected behavior (both markers should be applied to the test)\n5. The affected versions of pytest (3-6)\n6. A note about Python 2/3 compatibility\n\nThe submitter even provides a working solution via the metaclass example, showing exactly how the markers could be collected from the MRO chain. This gives a clear direction for implementing a fix in pytest itself.\n\nA developer would have all the information needed to understand the issue, reproduce it, and implement a solution that merges markers from all base classes rather than having them override each other.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10442": {
        "explanation": "This issue is reasonably well-specified. The core request is clear: modify pytest's behavior to only keep temporary directories for failing tests rather than for all tests, while maintaining the current limit of keeping the last 3 root temporary directories.\n\nThe issue provides:\n1. The current behavior (keeping last 3 root temporary directories for all tests)\n2. The desired behavior (keeping directories only for failed tests)\n3. The rationale (saving disk space for test suites with large data)\n\nThere is also a reference to a more detailed specification in a linked issue (#8141), which suggests there are additional details available for implementation.\n\nHowever, there are some implementation details that would need to be determined:\n- How to identify and track which directories belong to failed tests\n- Whether to keep directories for tests that error or are skipped (not just failures)\n- How to handle the cleanup process for the non-failed test directories\n- Whether any configuration options should be added to control this behavior\n\nSince the core request is clear and there's a reference to a more detailed specification, a developer could make a reasonable attempt at implementing this feature, though they would likely need to consult the linked issue for complete details.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-10552": {
        "explanation": "This issue is well-specified with clear examples demonstrating inconsistent behavior in pytest when handling staticmethod and classmethod decorators. The issue provides:\n\n1. A complete code example showing different combinations of staticmethod/classmethod with pytest.mark.parametrize\n2. Clear descriptions of which cases pass, which fail with warnings, and which silently fail\n3. Full pytest output showing the exact behavior\n4. Environment details (Python version, pytest version, OS)\n5. A clear statement of what the reporter considers problematic: classmethod tests being silently ignored without warnings\n\nThe issue identifies two specific problematic cases:\n- `test_classmethod` which is silently ignored\n- `test_classmethod_outer` which is also silently ignored\n\nThe reporter suggests a reasonable expectation: there should at least be a warning or error to alert users when their test code cannot be run.\n\nThis issue provides all the necessary information to reproduce the problem and understand what a successful solution would entail: ensuring consistent behavior between staticmethod and classmethod decorators in pytest, or at minimum providing appropriate warnings when tests using these decorators won't be executed.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10758": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with the walrus operator (:=) behaving differently in PyTest versus the normal Python terminal. The user has provided a complete test function that demonstrates the issue, which includes:\n\n1. The exact code that's failing\n2. The expected behavior (passes in Python terminal)\n3. The problematic behavior (fails in PyTest)\n\nThe issue is related to the walrus operator's behavior in different execution contexts, which is a concrete technical problem.\n\nHowever, there are some details that would be helpful but are missing:\n- The specific Python version being used\n- The exact error message or failure mode in PyTest\n- The PyTest version\n- Whether this happens with all PyTest configurations or specific ones\n\nDespite these missing details, there's enough information to investigate the issue. The test function is complete and the problem is reproducible. A developer could run this test in both environments to observe the difference and analyze why the walrus operator might behave differently in PyTest's execution environment compared to the standard Python interpreter.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-10893": {
        "explanation": "This issue is well-specified with clear technical details:\n\n1. The problem is clearly identified: Python 3.12 alpha has deprecated the `onerror` parameter in `rmtree` and replaced it with an `onexc` parameter, which causes deprecation warnings in pytest's temp path fixtures.\n\n2. The exact location of the problematic code is provided: https://github.com/pytest-dev/pytest/blob/6dcd652d4a55bacda01a15017e155caa816e15a5/src/_pytest/pathlib.py#L147\n\n3. The impact is clearly stated: When warnings are treated as errors, this makes pytest fail any test using temp paths.\n\n4. There's a second related issue about adding `addDuration` to `TestCaseFunction` to support a new API in Python 3.12 alpha.\n\n5. References to relevant external issues are provided (python/cpython#12271, python/cpython#103309).\n\nThe solution path is straightforward: update the `rm_rf` function to use the new `onexc` parameter instead of the deprecated `onerror` parameter, and add the `addDuration` method to the `TestCaseFunction` class. The issue provides all the necessary context to implement these changes.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-10988": {
        "explanation": "This issue is well-specified with clear reproduction steps and a detailed explanation of the problem. The user has:\n\n1. Provided a clear description of the bug: pytest crashes when a custom flag value is too long due to an uncaught OSError when checking if the argument is a file.\n\n2. Included complete reproduction steps with two examples (one that fails with the error and one that works with a shorter flag value).\n\n3. Identified the specific code in pytest that's causing the issue (the `anchor.exists()` call in `_set_initial_conftests`).\n\n4. Included relevant environment information (OS, pytest version, Python version, and pip list).\n\n5. Suggested a potential solution (adding a try/except block around the `exists()` call).\n\nThe issue is specific, reproducible, and includes all the necessary information to understand and address the problem. The user has even done the investigative work to pinpoint the exact location in the pytest codebase where the error occurs. A developer could immediately start working on a fix based on this information.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11041": {
        "explanation": "This issue is well-specified with clear reproduction steps and context:\n\n1. The problem is clearly stated: a regression in pytest 7.3.x when using the walrus operator in an assert line\n2. A minimal code example is provided that demonstrates the issue\n3. The exact error message is included: \"UnboundLocalError: cannot access local variable 'object' where it is not associated with a value\"\n4. Version information is provided: the issue occurs in pytest 7.3.x but not in 7.2.x, and Python version is 3.11\n5. A potential cause is even suggested: PR #10758\n\nWith this information, a developer could:\n1. Reproduce the issue by running the provided test with different pytest versions\n2. Investigate the changes in PR #10758 to understand what might have caused the regression\n3. Develop a fix that allows the walrus operator to work properly in assert statements again\n\nThe issue provides all necessary details to understand, reproduce, and potentially fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11044": {
        "explanation": "This issue is well-specified with a clear problem and proposed solution:\n\n1. The problem is clearly identified: When `testpaths` configuration doesn't match any files or folders, pytest silently ignores this and proceeds with default behavior (searching recursively from the current directory). This is confusing to users.\n\n2. The solution is explicitly stated: Issue a warning or error when `testpaths` doesn't match any files/folders.\n\n3. The issue includes specific technical details:\n   - The location of the relevant code in the codebase is provided\n   - The current behavior is explained in detail\n   - The documentation status is mentioned\n   - A clear recommendation is made (emit a warning at minimum)\n\n4. The scope is well-defined: This is a focused change to improve user experience by providing feedback when a configuration option doesn't work as expected.\n\nA developer could implement this solution without needing additional clarification. They would need to modify the code around the referenced location to check if the testpaths glob matches anything, and if not, emit a warning or error message.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11047": {
        "explanation": "This issue is very well-specified with clear details about:\n\n1. The problem: pytest doesn't support sub-second precision in log timestamps via the `%f` format specifier in `--log-date-format` options\n2. A concrete example demonstrating the issue with actual code and output\n3. The expected behavior: support for the `%f` specifier in all date format options\n4. The current workaround and why it's not ideal\n5. The rationale for why this feature is needed (aligning pytest logs with other system logs)\n\nThe issue provides all the necessary context for a developer to understand what needs to be fixed. The solution would involve modifying how pytest processes the date format strings to properly handle the `%f` specifier for microseconds. The issue even includes a working example of the current behavior and the desired outcome.\n\nA developer could implement this solution without needing additional clarification. The scope is well-defined and the technical requirements are clear.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11125": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific change in behavior between Pytest versions, with the error occurring in version 7.3.2.\n2. The repository structure is clearly explained - two test directories (`/unit_test` and `/test`), both listed in `testpaths` in pyproject.toml.\n3. The exact error message is provided, showing a conflict with the `--run-podman` option being defined in multiple conftest.py files.\n4. The issue includes the relevant configuration file content (pyproject.toml).\n5. The complete error traceback is provided, showing exactly where the error occurs.\n6. The pip list output is included, showing all installed packages and versions.\n\nThe problem is clearly defined: before Pytest 7.3.2, having the same command-line option defined in multiple conftest.py files worked when those directories were listed in testpaths, but now it causes a conflict error. The user is asking whether this is a bug in Pytest or if their configuration approach needs to change.\n\nA developer could reproduce this issue and investigate a solution without needing additional information. The solution would likely involve either modifying how the conftest.py files are structured or changing how the testpaths are configured in the project.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11143": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue title clearly states the problem: \"Rewrite fails when first expression of file is a number and mistaken as docstring\"\n\n2. The error message is provided in detail, showing a TypeError: \"argument of type 'int' is not iterable\" occurring in pytest's assertion rewrite mechanism.\n\n3. The traceback shows exactly where the error occurs - in the `is_rewrite_disabled` method of the `AssertionRewriter` class, specifically at the line checking if \"PYTEST_DONT_REWRITE\" is in the docstring.\n\n4. The error suggests that when a Python file starts with a number (integer) as its first expression, pytest's rewrite mechanism is mistakenly treating that number as a docstring and then trying to check if a string is contained within an integer (which causes the TypeError since integers are not iterable).\n\n5. The environment details are provided (pip packages list).\n\nThe issue is clear and reproducible - when pytest tries to collect test files that have a number as the first expression, it fails with a specific error. A developer would be able to understand the problem, locate the relevant code in pytest's assertion rewrite mechanism, and fix the issue by ensuring the code properly checks if the first expression is actually a docstring before trying to iterate through it.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-11160": {
        "explanation": "This issue is very well-specified with all the necessary information to understand and solve the problem:\n\n1. Clear problem statement: WarningsRecorder.pop() is returning the wrong warning when trying to pop a specific warning type.\n\n2. Root cause identification: The issue points to the specific line in the code that's causing the problem (using issubclass for comparison instead of exact class matching).\n\n3. Proposed solution: Replace the issubclass check with an exact class comparison using `is`.\n\n4. Complete minimal reproducible example: The issue includes a full test case that demonstrates the bug.\n\n5. Error output: The actual test failure is included, showing exactly how the current behavior differs from expected.\n\n6. Environment details: The pytest version and OS are specified.\n\nThe issue provides everything needed to understand the problem, verify it, and implement a fix. The proposed solution is also reasonable and likely correct - changing from an inheritance-based check (issubclass) to an identity check (is) would make the pop() method match exact warning types rather than also matching subclasses.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5205": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is clearly defined: pytest's XML output with `record-property` places `<properties>` tags inside `<testcase>` elements, which violates the JUnit XML schema (where they should be inside `<testsuite>`).\n\n2. The problem is demonstrated with a complete reproduction case:\n   - Specific package versions are provided\n   - A complete test file is included\n   - The command to run is provided\n   - The problematic XML output is shown\n\n3. The issue includes references to related discussions and the JUnit schema specification.\n\n4. The reporter has already investigated the codebase and identified the relevant file (junitxml.py) that would need modification.\n\n5. The reporter notes potential implications of the fix, showing they've thought through the problem.\n\nThe solution would involve modifying the XML generation in pytest to ensure properties are placed in the correct location according to the JUnit schema. This is a clear, technical task with a well-defined success criterion: JUnit should be able to parse the XML files generated by pytest without errors.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5262": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is clearly defined: `_pytest.capture.EncodedFile` incorrectly advertises its mode as `rb+` (binary mode) when its `write()` method actually requires string input, not bytes.\n\n2. A complete error trace is provided showing exactly where and how the error occurs.\n\n3. The issue includes a minimal reproducible example with:\n   - Clear steps to reproduce: install pytest and youtube-dl, then run a specific test\n   - The exact test code needed to reproduce the issue\n   - The exact error message received\n\n4. Environment details are provided:\n   - Python version (3.7.3)\n   - pytest version (4.5.0)\n   - OS version (macOS 10.14.4)\n   - Complete pip package list\n\n5. The root cause is identified: youtube-dl checks for 'b' in the output stream's mode to determine whether to write bytes or strings, but pytest's EncodedFile incorrectly includes 'b' in its mode while not actually accepting bytes.\n\nA developer would have all the necessary information to understand the issue, locate the problematic code in pytest, and implement a fix by modifying how EncodedFile reports its mode.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5281": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The environment is clearly defined: Jython 2.7.1 and pytest 4.5.0\n2. A complete error traceback is provided showing the exact error: UnicodeDecodeError in terminal.py\n3. The specific problematic code is identified: `u\"\\uD83D\"` in the `_get_line_with_reprcrash_message` function\n4. The issue reporter has analyzed the problem and identified that there's a mix of unicode and bytes handling\n5. A potential solution is even suggested: removing the 'u' prefix from the unicode literal\n\nThe issue provides enough context to understand the problem and attempt a solution. The fix would likely involve modifying how unicode characters are handled in the terminal.py file to ensure compatibility with Jython. The reporter's suggestion to remove the 'u' prefix could be evaluated as part of the solution, or another approach to handle unicode consistently might be needed.\n\nA developer could proceed with investigating this issue by:\n1. Examining the _get_line_with_reprcrash_message function in terminal.py\n2. Understanding how the unicode character is being used\n3. Testing the suggested fix or developing an alternative solution\n4. Ensuring compatibility with both CPython and Jython",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5356": {
        "explanation": "This issue is reasonably well-specified with enough information to attempt a solution. The user reports that pytest version 4.6.0 is skipping tests without apparent reason, which wasn't happening in previous versions. They've provided:\n\n1. A link to a Travis CI build showing the issue\n2. The output of `pip list` showing the environment dependencies\n3. The specific pytest version (4.6.0) and operating system (Ubuntu Xenial on Travis CI)\n4. A link to a minimal example (the Travis CI build)\n\nThe issue is clear: tests that should be running are being skipped after upgrading to pytest 4.6.0. The user had to roll back to 4.5.0 as a workaround.\n\nWhile the description doesn't explain exactly which tests are being skipped or why they believe they should be running, the Travis CI link should provide this information. A developer could examine the CI logs, compare the behavior between pytest 4.5.0 and 4.6.0, and look for changes in pytest 4.6.0 that might explain the unexpected skipping behavior.\n\nThe only thing that would make this more complete would be specific test names or patterns of tests being skipped, but this information can likely be derived from the provided CI logs.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5404": {
        "explanation": "This issue is well-specified with clear information to attempt a solution:\n\n1. The user has provided a detailed error message showing a KeyError: '__wrapped__' that occurs when importing from the 'sty' package\n2. They've created a minimal working example (MWE) in a GitHub repository that reproduces the issue\n3. They've specified their Python versions (3.6.5 & 3.7.1), PyTest version (4.4.0), and OS (MacOS Mojave 10.14.4)\n4. They've identified the specific line causing the issue (from sty import fg)\n5. They've shown the error occurs with both tox and pytest commands\n6. The full traceback is provided showing the error originates in the doctest module\n\nWith this information, a developer could:\n1. Clone the provided repository\n2. Reproduce the error\n3. Investigate the interaction between pytest's doctest functionality and the sty package\n4. Determine if this is a bug in pytest, sty, or an incompatibility between them\n5. Develop a fix or workaround\n\nThe issue is specific enough that a solution could be attempted without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5413": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user is reporting that when using `pytest.raises` context manager to catch exceptions, the string representation of the context variable behaves differently than when catching exceptions with a regular try/except block.\n\nThe issue includes:\n1. A clear example of the expected behavior (using try/except)\n2. A clear example of the current behavior (using pytest.raises)\n3. The specific version of pytest (4.6.2) and OS (macOS 10.14.5)\n4. A complete package list for context\n5. A clear description of the workaround currently needed (`str(e.value)`)\n6. A specific question about whether this behavior could be changed\n\nThe issue is asking if the behavior of `str()` on the pytest.raises context variable could be modified to match the behavior of `str()` on a regular exception object, eliminating the need to use `e.value` to access the full error message.\n\nThis is a well-defined feature request with all the necessary information to understand the problem and consider a solution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5479": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible test case. The issue shows that when using `pytest.raises` with a Unicode match pattern in Python 2, a UnicodeEncodeError occurs. The error message is specific: 'ascii' codec can't encode character u'\\u2603' (snowman character) in position 0.\n\nThe test case demonstrates exactly what fails:\n1. A test function that uses `pytest.raises` with a Unicode match pattern\n2. The assertion that should trigger the expected exception\n3. The exact error that occurs instead of the expected behavior\n\nFrom this information, I can understand that:\n- The issue is specific to Python 2\n- It involves Unicode handling in pytest's exception matching functionality\n- The expected behavior would be for pytest to properly match Unicode strings in the exception message\n\nThis is sufficient information to investigate and fix the issue in the pytest codebase, likely by ensuring proper Unicode handling in the exception matching mechanism for Python 2 compatibility.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5495": {
        "explanation": "The issue is reasonably well-specified. It describes a problem with pytest's assertion rewriting when comparing byte strings. The confusion arises from the error message that shows the ASCII ordinal value (52 for \"4\", 49 for \"1\") when comparing an empty byte string with a non-empty one, rather than showing the actual byte character.\n\nThe issue provides clear examples demonstrating the problem, showing how the current error message can be confusing to users. The reporter indicates that the ASCII ordinal representation isn't immediately intuitive and suggests that something should be done to improve this, though they don't specify exactly what solution they'd prefer.\n\nWhile the issue doesn't explicitly state the desired outcome, it's reasonable to infer that a more user-friendly representation of byte string differences is needed - one that doesn't rely on showing ASCII ordinal values without context. A developer could implement a solution that makes the byte string comparison more intuitive in the assertion error messages.\n\nThe scope is clear (assertion rewriting for byte strings) and the problem is well-demonstrated with examples.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5550": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that when using the `junit_family=xunit2` option with pytest, the XML output format is not as expected. Specifically:\n\n1. The issue clearly states the problem: the XML output is missing a root `<testsuites>` tag and is using a legacy format instead of the expected xunit2 format.\n\n2. The user provides:\n   - Their environment details (Python package versions, OS version)\n   - The exact command they're running (`pytest -o junit_family=xunit2 --junitxml=test_sum.xml backend/tests/`)\n   - A sample of the actual output they're getting\n\nWhat makes this reasonably specified rather than well-specified is that:\n1. The user doesn't explicitly show what they expect the output to look like (though they do mention the missing root `<testsuites>` tag)\n2. There's no reference to specific pytest documentation that would confirm the expected behavior\n3. No information about whether this worked in previous versions (regression) or if it's a new feature that's not working\n\nHowever, a developer familiar with pytest's JUnit XML formats would likely understand what the issue is and be able to investigate why the xunit2 format isn't being properly applied despite the configuration option being set.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5555": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible test case. The user is reporting that pytest's stepwise mode (`--sw`) doesn't work correctly with tests that are marked as `xfail` when `xfail_strict=true` is set.\n\nThe issue demonstrates:\n1. A complete test file with two tests marked as `xfail`\n2. A pytest.ini configuration that sets `xfail_strict=true`\n3. The exact command used to run the tests\n4. The output showing that both tests are failing with `[XPASS(strict)]` status\n\nThe problem is that when tests are marked as `xfail` but actually pass (becoming XPASS), and `xfail_strict=true` is set (which makes XPASS count as failures), the stepwise mode doesn't seem to recognize these as failures for its next run.\n\nThis is a specific, reproducible issue with a clear expectation: stepwise mode should recognize strict XPASS failures and continue from the first failing test in subsequent runs, but it's not doing so.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5559": {
        "explanation": "This issue description actually contains two separate issues:\n\n1. The first issue is about pytest stepwise not working with xfail strict failures. This part is well-specified with a clear reproduction case showing:\n   - The test file content\n   - The pytest configuration\n   - The command being run\n   - The actual output showing the failure\n\n2. The second issue is about the recommendation to add pytest-runner to setup_requires causing installation problems. This part is also reasonably well-specified with:\n   - A clear statement of the problem (unnecessary package installation that bypasses security features)\n   - Links to documentation and related issues\n   - Examples of other projects experiencing the same issue\n\nHowever, these appear to be two completely separate issues that should be addressed independently. The first issue is about a feature not working as expected, while the second is about a documentation recommendation causing problems.\n\nSince the title only mentions the first issue (about stepwise and xfail), I'll focus on that one for my assessment. For that specific issue, the description provides a complete reproduction case with code, configuration, and expected behavior, making it well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5631": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific error that occurs when using pytest 3.6.0 with a test that patches an array using unittest.mock.patch\n2. It provides the exact error message and traceback\n3. It identifies when the issue started occurring (after upgrading from pytest 3.1.3 to 3.6.0)\n4. It points to the specific commit that likely introduced the bug\n5. It explains the technical cause: when p.new is a numpy array, the check \"p.new in sentinels\" returns an array of booleans instead of a single boolean, causing a ValueError\n\nThe issue provides a clear reproduction case with the decorator pattern that causes the problem: `@patch(target='XXXXXX', new=np.array([-5.5, 3.0]))`.\n\nThe solution would need to modify how pytest handles mock patches with numpy arrays in the _pytest/compat.py file, specifically around line 94 where it's checking if \"p.new in sentinels\". A developer would have enough information to locate the problem and implement a fix that properly handles numpy arrays in this context.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5692": {
        "explanation": "This issue is well-specified. The user is asking if there's an option in pytest to include the `hostname` and `timestamp` properties in the JUnit XML reports that pytest generates. \n\nThe issue clearly:\n1. Identifies the specific problem: pytest's JUnit XML reports are missing the `hostname` and `timestamp` attributes in the `testsuite` element\n2. Provides concrete examples of both the current pytest output and the desired JUnit output format\n3. Has a clear question: \"Is there an option to include them?\"\n\nThe solution would involve either:\n1. Finding an existing configuration option in pytest to include these properties\n2. If no such option exists, implementing a feature to add these properties to the XML report\n\nThe examples provided give clear guidance on what the expected output should look like, making it straightforward to verify if a solution works correctly.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5787": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user is reporting that when using pytest with xdist (parallel test execution), chained exceptions are not fully displayed in the test output - only the final exception is shown, whereas without xdist the full exception chain is properly displayed.\n\nThe issue includes:\n1. Clear test code examples showing two different ways of chaining exceptions (with and without the 'from' keyword)\n2. The expected output (when running without xdist)\n3. The problematic output (when running with xdist)\n4. The specific versions of pytest (4.0.2) and pytest-xdist (1.25.0) being used\n\nThe problem is clearly defined: pytest-xdist is not properly serializing and displaying the full chain of exceptions, which makes debugging more difficult. A successful solution would ensure that the full exception chain is displayed when tests are run with xdist, matching the behavior seen when running without xdist.\n\nThis is a concrete, reproducible issue with clear examples and expected behavior, making it well-specified for a developer to work on.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5809": {
        "explanation": "This issue is well-specified with clear information about the problem and a suggested solution:\n\n1. The problem is clearly identified: The `--pastebin` feature in pytest is causing HTTP 400 errors when submitting content to bpaste.net using `lexer=python3`.\n\n2. The exact location of the problematic code is provided with a GitHub link to the specific lines: https://github.com/pytest-dev/pytest/blob/d47b9d04d4cf824150caef46c9c888779c1b3f58/src/_pytest/pastebin.py#L68-L73\n\n3. A reproducible example is provided showing how to trigger the error, along with an attached data file.\n\n4. The issue references a related issue (#5764) that this problem is causing.\n\n5. A clear solution is suggested: change the lexer from `python3` to `text`, with a reasonable explanation that pytest output is not Python code but arbitrary text.\n\nWith this information, a developer could easily locate the code, understand the problem, verify it with the provided example, and implement the suggested fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-5840": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly described: after upgrading from pytest 5.1.1 to 5.1.2, there's an ImportError when loading conftest.py due to a path casing issue on Windows. The error message indicates that pytest is trying to import a module named 'python', which doesn't exist.\n\nThe issue provides:\n1. The exact versions involved (5.1.1 works, 5.1.2 fails)\n2. The error message showing the ImportError and the specific path\n3. The command being run (pytest --collect-only .\\PIsys -m smoke)\n4. The context that this is happening on Windows, where path casing can be an issue\n\nWhat's missing is:\n1. The content of the conftest.py file, which would help understand why it's trying to import 'python'\n2. The project structure to better understand the import paths\n3. Specific details about how imports are being handled in the project\n\nHowever, there's enough information to make a reasonable attempt at solving the issue. The core problem appears to be that pytest 5.1.2 is handling paths differently than 5.1.1, converting them to lowercase on Windows, which is affecting the import resolution. This is a common issue with Python imports on case-insensitive filesystems like Windows.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-5980": {
        "explanation": "This issue is reasonably specified as it provides clear context and goals, but leaves some implementation details open for discussion.\n\nThe issue is about creating a replacement for the `--result-log` feature in pytest before removing it. It provides:\n\n1. Clear context: References to previous discussions and related issues\n2. A starting point: The existing `resultlog.py` plugin\n3. A proposed format: Line-based log file with JSON objects\n4. Some expected functionality: Writing test reports during `pytest_runtest_logreport`\n5. A question about additional functionality: Replaying tests from a log file\n\nHowever, there are some aspects that need further clarification:\n- The exact format and content of the JSON objects in the log file is not fully specified\n- The complete set of requirements for the replacement isn't definitively stated\n- The issue explicitly asks for discussion about how the log file should look\n- There's an unanswered question about why `pytest-tap` isn't an appropriate replacement\n\nDespite these open questions, there's enough information to start working on a solution based on the existing plugin and the proposed JSON line-based format. The developer can make reasonable assumptions about the implementation details while following the general direction provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-6116": {
        "explanation": "The issue is reasonably specified. It clearly states the problem: the `--collect-only` flag is too long to type frequently, and a one-character shortcut is desired. The core functionality request is unambiguous - create a short alias for the existing `--collect-only` command.\n\nWhile the specific character to use for the shortcut is not specified (the issue asks for community input on this), this doesn't prevent implementation. A developer could:\n1. Review existing single-character shortcuts in pytest to avoid conflicts\n2. Choose a logical character (like 'c' for collect)\n3. Implement the shortcut in the command-line parsing logic\n\nThe implementation path is clear - add a new short option that maps to the existing functionality. This is a common pattern in CLI tools and wouldn't require complex architectural changes.\n\nThe issue creator acknowledges it's a simple change but wants community validation first, which is reasonable for a public API addition.\n\nA developer could proceed with a solution while proposing a specific character in their PR, which could then be discussed during review.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-6186": {
        "explanation": "This issue is reasonably specified but lacks some details. The core requirement is clear: show a deprecation warning when the `junit_family` configuration option is not set. The reference to issue #6178 suggests there's more context available in that issue.\n\nWhat's missing:\n1. The specific context of where this warning should appear (though it's likely in test output)\n2. The exact wording of the deprecation warning\n3. The reason for deprecation (though this might be explained in issue #6178)\n4. The recommended alternative setting that users should use instead\n\nHowever, an experienced developer familiar with the codebase could likely implement this feature with the given information by:\n1. Finding where the `junit_family` configuration is processed\n2. Adding logic to check if it's not set\n3. Adding code to display a standard deprecation warning\n\nThe reference to issue #6178 provides a way to get more context if needed. This is enough information to make a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-6197": {
        "explanation": "This issue is well-specified with a clear description of the problem and a minimal reproducible example. The issue describes a regression in pytest 5.2.3 where it tries to collect and import any `__init__.py` file under the current directory, which didn't happen in version 5.2.2. The user provides:\n\n1. A clear description of the problem: pytest 5.2.3 tries to import any `__init__.py` file under the current directory\n2. A complete reproducible example with exact steps to recreate the issue\n3. The expected behavior (demonstrated by pytest 5.2.2)\n4. The actual behavior (demonstrated by pytest 5.2.3)\n5. The environment details (Debian 10 with Python 3.7.3)\n\nThe issue is specific enough that a developer could understand the problem, reproduce it, and work on a fix without needing additional information. The regression is clearly demonstrated by showing that pytest 5.2.2 works correctly while 5.2.3 fails by trying to collect the `foobar/__init__.py` file that contains an intentional failing assertion.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-6202": {
        "explanation": "This issue is well-specified with clear details about the problem and its cause. The reporter has:\n\n1. Provided a clear example of the bug with a reproducible test case\n2. Identified the exact behavior (`..[` being replaced with `[` in test report headlines)\n3. Traced the source code to pinpoint the exact line causing the issue (`return s.replace(\".[\", \"[\")` in the Python module)\n4. Suggested a fix (replacing that line with `return s`)\n5. Verified the fix works by running tests\n6. Noted that this issue also affects VSCode Python test discovery\n\nThe only minor uncertainty is understanding the original purpose of the code that's causing the problem, but this doesn't prevent implementing a solution. The issue provides all necessary information to create a PR that would fix the problem by modifying the identified line of code.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-6323": {
        "explanation": "The issue is reasonably specified, though it could benefit from more details. Here's what I understand:\n\n1. The issue is about restricting the paths that can be used with the `mktemp` function to only normalized relative paths.\n2. It's a follow-up to issue #4202, which likely provides additional context.\n3. The problem being addressed is a security concern: allowing absolute paths or non-normalized paths in `mktemp` is described as a potential attack vector.\n4. The solution direction is clear: modify the code to only accept normalized relative paths for `mktemp`.\n\nWhat's missing:\n- Specific examples of problematic paths and expected behavior\n- Details about the current implementation and how it handles paths\n- Specific test cases that would verify the fix\n- Information about which codebase/language this is for (though this could be inferred from the repository context)\n\nDespite these gaps, there's enough information to understand the security concern and the required solution approach. A developer familiar with the codebase should be able to implement a solution that validates paths passed to `mktemp` to ensure they are both relative and normalized.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-6680": {
        "explanation": "This issue is well-specified with clear requirements for improving the deprecation documentation for Node.from_parent. The issue lists specific tasks that need to be completed:\n\n1. Add an example of the warning that users will see (to help with searchability)\n2. Ensure the warning `NODE_USE_FROM_PARENT` points to the deprecation docs\n3. Show a \"before -> after\" example to illustrate the change\n4. Ensure from_parent will not support config/session\n\nEach task is clearly marked with checkboxes, and they all relate to improving documentation for a specific deprecation. The context is clear - there's a section in the deprecation docs about \"Node Construction changed to Node.from_parent\" that needs to be enhanced with these specific improvements.\n\nThe issue provides enough detail for a developer to understand what needs to be done without requiring additional clarification. The tasks are concrete and actionable.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-6926": {
        "explanation": "The issue description is extremely vague and lacks critical information needed to understand the problem and implement a solution. It only mentions \"SysCaptureBinary: decode in writeorg\" and references a GitHub issue (#6871) without explaining:\n\n1. What SysCaptureBinary is\n2. What the \"decode in writeorg\" means\n3. What the actual problem is that needs fixing\n4. What the expected behavior should be\n5. Any context about the code that needs to be modified\n\nWithout access to the referenced GitHub issue (#6871), it's impossible to understand what needs to be fixed. The description provides no actionable information about what changes are required, where they should be made, or how to verify a successful implementation. A developer would need to look up the referenced issue to get any meaningful information about what needs to be done.",
        "label": "IMPOSSIBLE_TO_SOLVE"
    },
    "pytest-dev__pytest-7046": {
        "explanation": "This issue is well-specified and clearly describes a problem with pytest's `-k` flag behavior. The user has provided:\n\n1. A clear description of the file structure and test code\n2. Specific commands that were run\n3. The actual output received\n4. The expected behavior (only matching test_asdf when using -k asdf)\n5. The observed unexpected behavior (both tests are collected because \"Asdf\" in the parent path is also matching)\n\nThe issue is that pytest's keyword matching with the `-k` flag is matching against the full path including parent directories, not just the test names. This is causing unexpected test collection when a keyword appears in both a test name and a parent directory.\n\nThe problem is clearly reproducible with the information provided, and the expected behavior is well understood. A solution would need to address how pytest's keyword matching works with directory paths.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7122": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a specific pytest version (5.4.1)\n2. Included a minimal test case with three simple test functions\n3. Demonstrated the issue with exact command-line examples and outputs\n4. Shown that `-k 2` works correctly (selects only test_2)\n5. Shown that `-k \"1 or 2\"` doesn't work as expected (selects all tests instead of just test_1 and test_2)\n6. Shown that `-k \"_1 or _2\"` works correctly (selects test_1 and test_2)\n7. Included a reference to relevant documentation\n\nThe issue is clearly about the `-k` option in pytest not handling numeric expressions correctly in the \"or\" syntax. The expected behavior would be for `-k \"1 or 2\"` to select only test_1 and test_2, similar to how `-k \"_1 or _2\"` works. This is a well-defined bug with a clear path to reproduction and verification of a fix.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7151": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is precisely defined: unittest.TestCase cleanup functions registered with `addCleanup` are not being invoked when a test fails in pytest 5.4.0+.\n\n2. Complete reproduction steps are provided:\n   - A minimal test case that demonstrates the issue\n   - The exact environment details (OS, Python version, pytest version)\n   - Command-line output showing the failure\n   - Comparative evidence showing it works correctly in an earlier version (5.3.5)\n   - Full dependency list\n\n3. The expected behavior is clear: The cleanup functions should be invoked even when a test fails, as they were in pytest 5.3.5.\n\n4. The issue appears to be a regression introduced in pytest 5.4.0, which provides a clear starting point for investigation.\n\nA developer would have all the necessary information to reproduce the issue, identify the cause by examining changes between pytest 5.3.5 and 5.4.0, and implement a fix to restore the expected behavior of unittest.TestCase.addCleanup.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7158": {
        "explanation": "This issue is well-specified with a clear problem description and expected behavior. The issue describes:\n\n1. The specific problem: When a test file is located in a directory above the pytest invocation directory, the skip report shows an incorrect relative path.\n\n2. A concrete example with:\n   - The exact file path structure\n   - The command used\n   - The current output\n   - The expected output\n\n3. The issue clearly shows the difference between what is currently happening (`SKIPPED [1] my_tests/test1.py:3`) and what should happen (`SKIPPED [1] ../my_tests/test1.py:3`).\n\nThe problem is that pytest is incorrectly reporting the path relative to the rootdir rather than relative to the invocation directory when displaying skipped tests. This is specific enough that a developer could locate the relevant code in pytest that handles skip reporting and fix the path handling logic.\n\nThe issue provides all necessary information to reproduce and verify a fix, making it well-specified for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7186": {
        "explanation": "This issue is well-specified with clear reproduction steps and detailed error information. The problem is clearly defined: when running pytest on a repository where setup.py contains UTF-8 characters, but the locale is set to C (LC_ALL=C), pytest raises a UnicodeDecodeError.\n\nThe issue provides:\n1. A minimal reproducible example with the exact content of setup.py that causes the problem\n2. Precise steps to reproduce the issue (setting LC_ALL=C and running pytest)\n3. The complete error traceback showing where the error occurs\n4. Environment details including Python version (3.6.8), pytest version (5.4.1), OS (CentOS 7.7.1908), and a complete list of installed packages\n\nFrom the traceback, we can see the error occurs in _pytest/doctest.py when trying to read the setup.py file. The root cause appears to be that when the locale is set to C, Python tries to decode the file using ASCII encoding instead of UTF-8, which fails when encountering non-ASCII characters.\n\nA developer would have enough information to locate the issue in the pytest codebase and implement a fix that ensures proper encoding is used when reading setup.py files, regardless of the system locale setting.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7205": {
        "explanation": "This issue is well-specified with clear information to understand and solve the problem:\n\n1. The issue describes a specific bug: When using pytest with `--setup-show` flag and bytes parameters in a parametrize decorator, a BytesWarning is raised when Python is run with the `-bb` flag.\n\n2. The reporter provides:\n   - A complete minimal reproducible example\n   - The exact Python and pytest versions\n   - The exact command to reproduce the issue\n   - A complete error traceback showing where the problem occurs\n\n3. The root cause is clearly visible in the traceback: In `_pytest/setuponly.py` line 69, there's an implicit string conversion of a bytes object via `tw.write(\"[{}]\".format(fixturedef.cached_param))` which triggers the BytesWarning.\n\n4. The reporter even suggests a potential solution direction: using `saferepr` instead of implicit string conversion.\n\nWith this information, a developer could easily locate the problematic code, understand the issue, and implement a fix by modifying how bytes objects are formatted in the output. The solution would likely involve using `saferepr` or another method to properly handle bytes objects without triggering the BytesWarning.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7231": {
        "explanation": "The issue is reasonably well-specified. It describes a problem where incorrect logging calls (with mismatched format specifiers and arguments) don't cause tests to fail. The example shows a function `func()` that calls `logging.error()` with one format specifier (`%s`) but two arguments (`\"a\"` and `\"b\"`), which would cause an exception. However, this exception doesn't cause the test to fail.\n\nThe implied request is to implement a warning or error mechanism that would cause tests to fail when logging calls are incorrect. While the issue doesn't explicitly state the desired solution approach or which codebase this relates to (presumably a Python testing framework), the problem itself is clear enough that a developer familiar with the codebase could implement a solution that catches these logging errors and converts them to test failures.\n\nA solution would likely involve adding a mechanism to intercept logging errors and either:\n1. Convert them to warnings that testing frameworks can detect\n2. Raise exceptions that would cause tests to fail\n3. Add a logging handler that specifically detects these issues during tests\n\nThe core problem and expected behavior are clear enough to work on a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7236": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior:\n\n1. The issue describes a specific bug: tearDown() is being executed on skipped tests when running pytest with the --pdb flag\n2. A minimal test case is provided that demonstrates the issue\n3. The environment details are clearly specified (Python 3.6.10, pytest 5.4.2)\n4. The exact commands to reproduce the issue are provided\n5. The expected behavior is stated: skipped tests should not execute tearDown(), even with --pdb\n6. The issue includes evidence that this worked in a previous version (pytest 5.4.1)\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and work on a fix. The bug appears to be a regression in pytest 5.4.2 where the tearDown method is incorrectly executed for skipped tests when the --pdb flag is used. A developer would have a clear path to investigate the changes between pytest 5.4.1 and 5.4.2 that might have caused this behavior change.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7283": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a minimal test case that demonstrates the problem\n2. Included their Python version and package versions\n3. Shown the command-line output both with and without the `--pdb` flag\n4. Identified the specific behavior change (tearDown being executed on skipped tests when using `--pdb`)\n5. Narrowed down the version where the behavior changed (between pytest 5.4.1 and 5.4.2)\n\nThe issue is that when running pytest with the `--pdb` flag, the tearDown method is being executed for skipped tests, which shouldn't happen. The reporter expects skipped tests to be fully skipped (including tearDown) even when the `--pdb` flag is used, which was the behavior in pytest 5.4.1.\n\nA developer would have all the necessary information to investigate and fix this regression without needing additional clarification. The reproduction steps are clear and complete, making this a well-specified issue.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7314": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed:\n\n1. The problem is clearly identified: an `UnboundLocalError` occurs in pytest 4.6.x because the variable 'letter' is referenced before assignment in the `pytest_report_teststatus` function.\n\n2. The exact location of the bug is specified: in `_pytest/terminal.py`, line 190, in the `pytest_report_teststatus` function.\n\n3. The solution is explicitly described: \"add a fallback variable value before if-blocks\" - essentially initializing the 'letter' variable before it's potentially used.\n\n4. There's confirmation that the fix has already been implemented in the master branch (via PR #6337), but needs to be applied to the 4.6.x branch.\n\n5. The issue even includes a reference to a PR (#7311) that addresses this exact problem, providing further context.\n\nThe issue provides all the necessary information to understand the problem and implement a solution without needing additional clarification. A developer could easily locate the affected code, understand the issue, and implement the suggested fix based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7324": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible test case. The issue describes a specific bug where using `Expression.compile(\"False\")` causes the Python interpreter to crash with an assertion error in debug builds for Python 3.8+. \n\nThe issue includes:\n1. A precise reproduction case with the exact code that triggers the crash\n2. The exact error message and stack trace showing where the assertion failure occurs\n3. A reference to a related Python bug report (bpo-40870) for additional context\n\nFrom this information, a developer would be able to:\n- Reproduce the issue using the provided code snippet\n- Understand that the problem is related to handling literal constants like \"False\" in the Expression.compile method\n- Locate the relevant code in the Python/compile.c file around line 3559\n- Understand that the assertion is failing when trying to compile the string \"False\"\n- See that this only happens in debug builds of Python 3.8+\n\nThe solution would involve fixing how the Expression.compile method handles string literals that match Python keywords like \"False\", \"True\", and \"None\" to prevent the assertion failure.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7352": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue provides a specific error: `FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pytest-of-jenkins/pytest-1681/.lock'`\n\n2. It references a previous related issue (#4181) indicating this is a known problem with some edge cases still not handled correctly.\n\n3. It includes a detailed stack trace showing exactly where and how the error occurs in the pytest code.\n\n4. It provides the context in which the error occurs: when running concurrent pytest processes.\n\n5. It even suggests a potential solution: generating numbered directories with random suffixes instead of sequential ones, and points to the specific code location in pytest that might need modification (https://github.com/pytest-dev/pytest/blob/4.6.3/src/_pytest/pathlib.py#L123).\n\nThe issue is clearly about a race condition when multiple pytest processes try to clean up temporary directories simultaneously, causing one process to attempt to delete a lock file that another process has already removed. The solution would involve modifying how pytest handles these temporary directories and lock files in concurrent environments.\n\nA developer would have enough information to reproduce the issue (by running concurrent pytest processes) and implement a fix based on the suggested approach or by adding proper error handling for the case when the lock file is already gone.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7432": {
        "explanation": "This issue is well-specified with clear details about the bug:\n\n1. The problem is clearly described: when using `--runxfail` flag with pytest, the location reporting for skipped tests (using `@pytest.mark.skip`) is incorrect.\n\n2. The expected behavior is explicitly stated: the skip location should point to the test item itself.\n\n3. The actual behavior is shown with examples: the location points to the pytest internal code instead of the test file.\n\n4. The issue includes a minimal reproducible example with code snippets and command outputs.\n\n5. The issue even provides a hint about where the bug is located: in `src/_pytest/skipping.py` in the `pytest_runtest_makereport` hook.\n\nWith this information, a developer could easily reproduce the issue and has a clear direction for where to look for the fix. The success criteria is also clear: when using `--runxfail`, the skip location should be reported correctly as pointing to the test item itself.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7466": {
        "explanation": "This issue is well-specified with clear requirements for implementation. The request is to support the `NO_COLOR` environment variable in accordance with a community standard (no-color.org). The expected behavior is clearly outlined:\n\n1. By default, the application should emit colored output when attached to a TTY, but not when piped elsewhere\n2. When `NO_COLOR` environment variable is set, the application should not emit colored output even when attached to a TTY\n3. Additionally, a `FORCE_COLOR` environment variable should be implemented to force colored output even when not attached to a TTY\n\nThe issue provides a link to the community standard for reference and clearly explains the expected behavior for each scenario. There's no ambiguity about what needs to be implemented or how it should behave. A developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7468": {
        "explanation": "This issue is well-specified with a clear problem description and a suggested solution. The issue describes that when using the `--log-file` option in pytest with a path that includes non-existent subdirectories, pytest crashes with a `FileNotFoundError` instead of creating the necessary subdirectories. \n\nThe issue includes:\n1. A specific command that reproduces the problem: `pytest --log-file=subtest/test.log`\n2. The exact error message that occurs: `INTERNALERROR> FileNotFoundError: [Errno 2] No such file or directory: '/tmp/test/subtest/test.log'`\n3. A suggested code solution to fix the issue by creating the directory structure if it doesn't exist\n4. An acknowledgment of a potential alternative approach (deciding that users shouldn't be allowed to pass directory paths)\n\nA developer would have enough information to:\n- Reproduce the issue\n- Understand the expected behavior (creating subdirectories automatically)\n- Implement a solution based on the suggested approach or decide on an alternative policy\n\nThe issue is straightforward and the solution approach is clear, making it well-specified for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7481": {
        "explanation": "This issue is reasonably well-specified. It describes a UnicodeEncodeError occurring in pytest's terminalwriter on Windows when trying to display emoji characters. The error happens specifically when pytest is trying to print a test failure that involves emoji characters.\n\nThe issue includes:\n1. A detailed error traceback showing the UnicodeEncodeError\n2. The specific test case that triggers the problem (a pytest-bdd scenario involving emoji)\n3. The environment (Windows on GitHub Actions with pytest 6 rc)\n4. Additional context that the reporter has pytest-instafail installed\n\nWhat makes this reasonably specified rather than well-specified:\n- It's not 100% clear if this is a pytest 6 regression or if it just happened to be noticed with pytest 6\n- The reporter mentions they can't reproduce it consistently, suggesting it might be flaky\n- There's no specific mention of what the expected behavior should be (though it's implied that pytest should handle emoji characters without crashing)\n\nHowever, there's enough information to understand the problem: pytest's terminalwriter is failing to handle emoji characters on Windows due to encoding issues. A solution would likely involve ensuring the terminalwriter properly handles Unicode characters on Windows systems, particularly when displaying test failures that contain emoji.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7490": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Clearly described the problem: dynamically adding an xfail marker to a test using `request.node.add_marker(mark)` worked in pytest 5.x but fails in pytest 6.0.0rc0\n2. Provided a minimal reproducible example with the exact code that demonstrates the issue\n3. Included the exact output from both pytest 5.4.3 (where it works as expected) and pytest 6.0.0rc0 (where it fails)\n4. Specified the versions of pytest being used (5.4.3 vs 6.0.0rc0)\n5. Included environment information (Python version, OS, etc.)\n\nThe issue is clearly a regression in pytest 6.0.0rc0 where dynamically added xfail markers are no longer being respected. The expected behavior is that the test should be marked as xfailed rather than failed when using the dynamic marker approach. This is specific enough that a developer could investigate the changes between pytest 5.x and 6.0 that might have caused this regression.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7499": {
        "explanation": "This issue is reasonably well-specified. The user is suggesting an improvement to pytest's `raises` functionality when using the `match` parameter with strings containing parentheses.\n\nThe core problem is clear: When using `pytest.raises` with the `match` parameter and a string containing parentheses, pytest treats the string as a regex pattern. Since parentheses have special meaning in regex (for capturing groups), this causes confusing error messages where two identical-looking strings are reported as not matching.\n\nThe user suggests that pytest should check if strings have parentheses and provide a more helpful warning or error message to guide users toward properly escaping the parentheses or using the raw string prefix (`r\"...\"`) for regex patterns.\n\nThe example provided shows the issue, though it could be more complete by showing both the failing code and the fixed version. However, the error message shown clearly illustrates the problem.\n\nWhat's missing:\n1. A complete minimal reproducible example showing both the failing and working code\n2. The exact implementation details of how pytest should detect and handle this situation\n\nDespite these gaps, there's enough information to understand the problem and develop a reasonable solution - likely adding logic to detect unescaped parentheses in match strings and providing a more helpful warning message that specifically mentions escaping parentheses or using raw strings.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7500": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: a PermissionError occurring on Windows when pytest cleans up temporary directories.\n2. The exact error message and stack trace are provided.\n3. The conditions that trigger the issue are clearly stated: Windows OS, having at least three temporary directories, and the computer being under load.\n4. The user has provided a hypothesis about the cause: the OS continues deleting files after the first cleanup attempt, causing issues during the second cleanup attempt.\n5. A potential solution is proposed: reversing the order of the two for loops in the `cleanup_numbered_dir` function.\n6. The environment is well-documented with OS version, file system details, and package versions.\n7. The issue has been reproduced in multiple pytest versions (5.4.3 and 6.0.0rc1).\n\nThe issue provides enough context for a developer to understand the problem, reproduce it, and evaluate the proposed solution. The suggested fix (swapping the order of the for loops) is specific and testable. A developer could implement this change and verify if it resolves the issue without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7521": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific regression in pytest 6.0.0rc1 where `capfd.readouterr()` is converting carriage returns (`\\r`) to newlines (`\\n`).\n\n2. The reporter provides:\n   - A clear error message from a real-world application (borgbackup)\n   - A minimal reproducible test case that demonstrates the issue\n   - Comparison of behavior between pytest 5.4.3 (working) and pytest 6.0.0rc1 (failing)\n   - Environment details (Fedora 32/33, Python 3.8/3.9)\n\n3. The expected behavior is clearly defined: `capfd.readouterr()` should preserve carriage returns as they were in the original output.\n\n4. The reporter notes they couldn't find any documentation about this being an intentional change, suggesting it's a regression.\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification. The reproducible test case makes it particularly straightforward to verify both the bug and any potential fix.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7571": {
        "explanation": "This issue is well-specified with clear information to understand and reproduce the problem:\n\n1. The issue clearly states what's broken: the caplog fixture is not restoring log levels after a test completes, which contradicts the documentation.\n\n2. It provides a minimal reproducible example with two test functions that demonstrate the issue.\n\n3. It specifies the expected behavior (printing \"0\" for pytest<6) and the current incorrect behavior (printing \"42\" in pytest 6.0).\n\n4. It references the relevant documentation that states the expected behavior.\n\nThe solution would need to fix the caplog fixture to properly restore log levels between tests as it did in previous versions. This is a regression bug with a clear path to verification - ensuring that setting a log level in one test doesn't affect the log level in subsequent tests.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7637": {
        "explanation": "The issue is reasonably specified. It clearly states that certain warnings were suppressed in pytest 6.0 (with a specific PR reference #7362) and that these warnings need to be reintroduced in version 6.1. \n\nThe issue provides context about why the warnings were suppressed (they couldn't comply with the \"2 versions minimum with warnings\" policy) and links to the specific PR where the suppression occurred, which would allow a developer to identify exactly which warnings need to be reintroduced.\n\nWhile the issue doesn't explicitly list which specific warnings need to be reintroduced, the referenced PR (https://github.com/pytest-dev/pytest/pull/7362) would contain this information, making it possible to determine what needs to be done. A developer would need to review that PR to identify the specific warnings that were suppressed and then reintroduce them.\n\nThe task is straightforward: find the warnings that were suppressed in PR #7362 and reintroduce them in version 6.1. The implementation details are clear enough for someone familiar with the codebase to proceed.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-7648": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific problem: pylint is complaining about unimplemented abstract methods in a custom pytest collector after upgrading to pytest 6.0.0.\n\n2. It provides the exact error messages from pylint, showing which abstract methods are not being overridden.\n\n3. It includes the relevant code snippet showing the custom collector class definition.\n\n4. It provides a link to the full source code in the GitHub repository.\n\n5. It specifies the exact versions of all relevant software components (Python, pytest, pylint, astroid, platform).\n\n6. It clarifies that the code still works functionally despite the pylint warnings.\n\n7. The issue has a clear question at the end: whether a collector has to implement these methods.\n\nA developer would have all the necessary information to investigate why these abstract methods are now being flagged by pylint after the pytest 6.0.0 update, and to determine whether these methods need to be implemented in the custom collector class. The solution would likely involve either implementing the missing abstract methods or finding a way to suppress the pylint warnings if they're not actually needed.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7939": {
        "explanation": "This issue is well-specified with a clear request and implementation details. The user wants to add a shorthand CLI argument `--sw-skip` as an alternative to the existing `--stepwise-skip` option, similar to how `--sw` is already a shorthand for `--stepwise`. \n\nThe issue includes:\n1. The current code showing how the CLI arguments are currently implemented\n2. The specific problem: inconsistency between having a shorthand for `--stepwise` but not for `--stepwise-skip`\n3. The expected solution: add `--sw-skip` as a shorthand for `--stepwise-skip`\n\nThe implementation would be straightforward - modify the `addoption` call for the `--stepwise-skip` option to include the shorthand version, similar to how it's done for the `--stepwise` option. The code snippet provided gives enough context to understand where and how to make the change.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7982": {
        "explanation": "This issue is well-specified with clear information about what the problem is and how to fix it:\n\n1. The problem is clearly identified: symlinked directories in test directories are being skipped rather than followed and collected.\n\n2. The regression point is precisely identified: commit b473e515bc57ff1133fe650f1e7e6d7e22e5d841, which was included in pytest 6.1.0.\n\n3. The cause is explicitly stated: a `follow_symlinks=False` parameter was added.\n\n4. The solution is clearly specified: remove the `follow_symlinks=False` parameter to restore the previous behavior.\n\nThe issue provides all the necessary information to understand the problem, locate the code that needs to be changed, and implement the fix. The developer even mentions they're already working on a PR for this issue, indicating they have a clear understanding of what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-7985": {
        "explanation": "This issue is not well-specified enough for a meaningful attempt at a solution. The description only mentions deprecating the `--strict` flag in pytest, but lacks critical details:\n\n1. It doesn't explain what `--strict` does or its current functionality\n2. There's no context about why it was being considered for removal\n3. No information about what \"reintroducing it again\" means - was there a plan to remove and add back similar functionality?\n4. No clear requirements for what \"deprecation\" means in this context (warning message, timeline, replacement functionality)\n5. No rationale for why deprecation is preferred over removal\n6. No mention of related code or components that would need modification\n\nThe comment appears to be responding to some previous discussion (as indicated by it being \"originally posted\" in another issue), but that context is missing. Without understanding the background discussion, the current state of the codebase, and the specific requirements for deprecation, it would be difficult to implement a solution.",
        "label": "VAGUE"
    },
    "pytest-dev__pytest-8022": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue is clearly stated: when running `pytest --doctest-modules __init__.py`, only a single doctest is collected from `__init__.py` files, even when there are multiple doctests present.\n\n2. The exact location of the problematic code is provided with a GitHub link to the specific lines in the codebase (main.py lines 768-781).\n\n3. The commit that introduced the issue is identified, along with the author (@kchmck).\n\n4. A link to failing tests in PR #8015 is provided as evidence of the issue.\n\nThe issue provides enough context for a developer to understand what's happening, where the problem is located in the code, and what needs to be fixed. The problem is specific to doctest collection in `__init__.py` files, and the solution would involve modifying the code to properly collect all doctests from these files, not just a single one.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8055": {
        "explanation": "The issue is reasonably specified. It clearly states that:\n\n1. Python 3.8 introduced a new feature called `sys.unraisablehook`\n2. Pytest should set this hook to associate unraisable exceptions with tests\n\nThe issue provides a reference to the CPython PR that introduced this feature, which helps understand what `sys.unraisablehook` is.\n\nWhat makes this reasonably specified rather than well-specified:\n- It doesn't provide specific details on how exactly pytest should implement this\n- It doesn't explain what \"associate unraisable exceptions with tests\" means in practice\n- It doesn't provide examples of the expected behavior\n\nHowever, an experienced developer familiar with pytest could reasonably:\n1. Research `sys.unraisablehook` to understand its purpose (handling otherwise uncatchable exceptions)\n2. Determine how pytest should use this hook to track unraisable exceptions that occur during test execution\n3. Implement a solution that associates these exceptions with the specific test that was running when they occurred\n\nThe implementation would likely involve setting a custom hook function that captures unraisable exceptions and associates them with the currently running test, similar to how pytest handles other exceptions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-8250": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug when using pytest with Twisted Logger and the `--capture=no` option.\n2. It provides a detailed explanation of the root cause: Twisted Logger intercepts STDERR with a file-like wrapper that returns `-1` for `fileno()`, which causes pytest's faulthandler to fail during teardown.\n3. The environment is clearly specified (macOS 10.13.6, pytest 6.2.1, pytest-twisted 1.13.2, Twisted 20.3.0).\n4. A complete traceback is provided showing the exact error.\n5. Links to the relevant code in both pytest and Twisted are included.\n6. The issue mentions that a patch (#8250) already exists that mitigates this problem.\n\nA developer would have all the necessary information to understand the issue, reproduce it, and implement a solution. The solution would need to handle the case where `sys.stderr.fileno()` returns an invalid file descriptor when Twisted Logger has wrapped the stderr stream.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8365": {
        "explanation": "This issue is well-specified with clear reproduction steps and error messages. The problem is clearly defined: when a username contains characters that are illegal for directory names (like backslashes in \"contoso\\john_doe\"), pytest's temporary directory creation fails. \n\nThe issue provides:\n1. A detailed explanation of the problem\n2. The specific error message encountered\n3. Two different ways to reproduce the issue (one with SSH into Windows, another with a simple command-line reproduction)\n4. The specific pytest components affected (tmpdir, tmpdir_factory, tmp_path_factory)\n5. The root cause (getpass.getuser() returning a username with illegal characters)\n\nA developer would have enough information to understand the problem, reproduce it, and implement a solution that sanitizes usernames before using them in directory paths. The solution would likely involve modifying how pytest handles the username returned by getpass.getuser() to ensure it's valid for directory creation.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8399": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific change in behavior starting from pytest v6.2.0: unittest setUpClass fixtures are no longer \"private\" (no longer prefixed with underscore).\n\n2. A minimal reproducible example is provided showing the class with setUpClass method.\n\n3. The actual output is shown (fixture name \"unittest_setUpClass_fixture_Tests\" appears in --fixtures output).\n\n4. The expected behavior is clearly stated: these fixtures should have names starting with an underscore so they only appear with the -v flag.\n\n5. The impact is explained: it breaks a CI script that checks for undocumented fixtures, as these auto-generated unittest fixtures don't have docstrings.\n\nThe issue provides all necessary information to understand the problem, reproduce it, and work toward a solution. A developer would need to modify how pytest handles unittest setUpClass fixtures to restore the previous behavior of making them \"private\" (prefixed with underscore).",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8428": {
        "explanation": "This issue is well-specified for the following reasons:\n\n1. The problem is clearly stated: Generate an error when a pytest mark is applied to a fixture.\n2. The context is provided by referencing a previous issue (#1014).\n3. The specific action required is clear: Implement error generation when `@pytest.mark` is applied to a fixture.\n4. There's a specific documentation update mentioned that needs to be addressed once the implementation is complete.\n5. For someone familiar with pytest, this would be a straightforward task as it involves:\n   - Detecting when a mark is applied to a fixture\n   - Generating an appropriate error message\n   - Updating the warning in the documentation to reflect the new error behavior\n\nThe issue provides enough information for an experienced developer to implement a solution without needing further clarification. The scope is well-defined and the expected outcome (error generation) is clear.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8447": {
        "explanation": "This issue is well-specified with clear error messages and reproduction steps. The problem is clearly identified: plugins like pytest-black and pytest-checkdocs are failing with a TypeError about an unexpected keyword argument 'path'. The error occurs in the from_parent method chain, specifically when super().from_parent is called with both 'fspath' and 'path' parameters.\n\nThe issue provides:\n1. Exact error messages with full tracebacks\n2. Clear reproduction steps (creating empty test files and running specific commands)\n3. The exact version of pytest being used (from the git repository at commit 32ad70d)\n4. Multiple examples showing the same issue with different plugins\n\nFrom the traceback, we can see that in _pytest/nodes.py:578, there's a call to super().from_parent that's passing both fspath and path parameters, which is causing the error. This is likely a backward compatibility issue where plugins are using an older API pattern that's no longer supported in the current pytest version.\n\nA developer would have enough information to locate the problematic code in the pytest codebase and understand what needs to be fixed to maintain compatibility with these plugins.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8463": {
        "explanation": "This issue appears to be a combination of two unrelated issues that were accidentally merged together. The first part discusses parameter regressions related to `startpath`/`fspath` hooks, while the second part (which seems to be the actual issue) is about color formatting in test output when a test fails.\n\nLooking at the second part, which appears to be the main issue:\n1. The problem is clearly described: when a test fails in the middle of a test sequence, the color formatting for subsequent passing tests incorrectly remains red instead of turning green.\n2. Steps to reproduce are provided: create a set of tests with a failing test in the middle.\n3. Expected behavior is stated: failing tests should be red, passing tests should be green.\n4. Screenshots are provided showing the issue.\n5. Detailed environment information is included (OS, Python version, pytest version, Flask version, VS Code version).\n\nThe issue is well-documented with all the necessary information to understand and reproduce the problem. The reporter has provided clear examples, screenshots, and environment details. The only minor confusion is the inclusion of an unrelated issue at the beginning, but the actual problem being reported is well-specified.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8641": {
        "explanation": "This issue is well-specified with clear information about what's happening and why. The user reports that pytest fails to output colorful logs when using a specific log format string in pytest.ini that includes precision formatting for the levelname (%-5.5s). They've identified the root cause: the ColoredLevelFormatter.LEVELNAME_FMT_REGEX in pytest's logging.py file doesn't properly match format strings that include precision formatting.\n\nThe issue includes:\n1. The exact configuration that reproduces the problem (the log_cli_format string)\n2. A link to the specific code in pytest that's causing the issue\n3. A clear explanation of why it's happening (the regex doesn't handle precision formatting)\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided format string\n2. Locate the problematic code in the ColoredLevelFormatter class\n3. Modify the regex pattern to properly handle precision formatting in levelname format specifiers\n\nThis is a well-defined bug with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8906": {
        "explanation": "This issue is well-specified with clear examples and a detailed explanation of the problem. The user is describing an issue with pytest's module-level skipping functionality when dealing with Python version-specific syntax. They've provided:\n\n1. A concrete example showing the problem (skipping tests for Python 3.8+ syntax in older Python versions)\n2. The current solution that works (`skip(..., allow_module_level=True)`)\n3. The naive approach that doesn't work (simple `skip()` without the parameter)\n4. The error message that appears\n5. Why the suggested solution in the error message doesn't solve their problem\n6. Two specific proposals for improvement:\n   - Update the error message to mention `allow_module_level`\n   - Create a new, more intuitive API like `skip_module()`\n\nThe issue provides enough context about the problem, why it matters, and potential solutions. A developer could implement either of the suggested improvements without needing additional clarification. The issue is focused on improving user experience for a specific pytest functionality.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-8952": {
        "explanation": "This issue is reasonably well-specified. The author is requesting an enhancement to the `RunResult` class to add capabilities for asserting warnings, similar to how `assert_outcomes()` works for test outcomes. \n\nThe issue clearly:\n1. Identifies the problem: Currently there's no easy way to assert warnings in `RunResult`\n2. Provides context: The author encountered this need while working on other code\n3. Suggests a specific solution: Adding a new method `assert_warnings(count=1)` to the `RunResult` class\n4. Provides a code example of the proposed API\n\nWhat makes it not fully well-specified:\n1. It doesn't specify the full API design (only shows a count parameter example)\n2. It doesn't explain how warnings should be filtered or matched beyond simple counting\n3. It doesn't provide the implementation details or how it should interact with the existing code\n4. It mentions \"I suspect there is a good reason why we don't have `assert_outcomes(warnings=...)`\" which suggests there might be architectural considerations not fully explored\n\nHowever, these gaps are reasonable for an enhancement request. A developer familiar with the codebase would have enough information to implement a solution by:\n1. Adding the new `assert_warnings()` method to `RunResult`\n2. Implementing the count parameter functionality\n3. Potentially adding more sophisticated filtering options similar to other assertion methods\n\nThe issue provides enough context and a clear example of what the author is looking for, making it actionable for implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9064": {
        "explanation": "This issue is very well-specified with clear details about the problem and expected behavior:\n\n1. The issue clearly identifies the bug: `--stepwise-skip` has no effect unless used with `--stepwise`.\n2. The expected behavior is explicitly stated: `--stepwise-skip` should act the same as `--stepwise --stepwise-skip`.\n3. Complete environment information is provided (OS, Python version, pytest version).\n4. A minimal reproducible example is included with the test file and command outputs showing:\n   - Normal test execution\n   - `--stepwise-skip` alone (not working)\n   - `--stepwise` alone (working as expected)\n   - `--stepwise --stepwise-skip` together (working as expected)\n\nThe issue author has demonstrated the current behavior and clearly articulated the desired behavior. The solution would involve modifying the pytest code to make `--stepwise-skip` implicitly enable `--stepwise` functionality when used alone. This is a straightforward enhancement that can be implemented without any additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9066": {
        "explanation": "This issue is well-specified with clear information about the problem and a proposed solution:\n\n1. The problem is clearly identified: When running pytest with many plugins, a deprecation warning appears about direct construction of SpecModule, but it doesn't indicate which plugin is causing the issue.\n\n2. The current behavior is documented with an example error message.\n\n3. The proposed solution is specific: Show the fully qualified name of the offending class in the deprecation message (e.g., \"pytest_relaxed.plugin.SpecModule\" instead of just \"SpecModule\").\n\n4. The rationale for the change is provided: It would help users identify which plugin is causing the deprecation warning without having to look at source code.\n\nThe issue provides enough context for a developer to understand what needs to be changed in the pytest codebase - specifically, the code that generates the deprecation warning for direct Node construction needs to be modified to include the fully qualified name of the class. This is a straightforward enhancement to an existing error message that would improve the developer experience.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9249": {
        "explanation": "This issue is well-specified with clear examples and a proposed solution. The problem is that pytest's auto-generated test IDs can contain characters like slashes (\"/\") that cause errors when using the `-k` option to select tests. The issue provides:\n\n1. A specific version (pytest 6.2.2)\n2. A clear example of code that produces the problem\n3. A clear description of the error (\"unexpected character '/'\")\n4. A proposed solution (sanitize auto-generated IDs)\n5. A related suggestion for improvement (using shell-safe format like colons instead of square brackets)\n\nThe issue describes a concrete problem with a reproducible example and suggests a reasonable approach to fix it. A developer could implement a solution based on this information by modifying how pytest generates test IDs to sanitize characters that cause problems with the `-k` option. The scope is well-defined and the expected outcome (being able to use auto-generated IDs with the `-k` option) is clear.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9279": {
        "explanation": "This issue is well-specified with clear error messages and reproduction steps. The problem is clearly identified: plugins like pytest-black and pytest-checkdocs are failing with a TypeError about an unexpected keyword argument 'path'. The error occurs in the from_parent method chain, specifically when these plugins call super().from_parent() and pass parameters that are no longer accepted.\n\nThe issue provides:\n1. Exact error messages with full tracebacks\n2. Clear reproduction steps with minimal test cases\n3. The specific pytest version where the issue occurs (pytest-6.3.0.dev252+g32ad70dea)\n4. Multiple examples showing the same pattern with different plugins\n\nFrom the traceback, we can see that the issue is related to changes in the pytest API, specifically in how nodes are created and how the from_parent method handles parameters. The plugins are passing a 'path' parameter that's no longer accepted in the current pytest version.\n\nA developer would have enough information to:\n1. Understand the API change that's causing the issue\n2. Identify where in the code the incompatibility exists\n3. Implement a fix that maintains backward compatibility or updates the API usage",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9475": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a backwards compatibility problem with the `deselected` parameter added to `assert_outcomes()` in PR #9133.\n\n2. It provides a specific example of how the behavior changed: `result.assert_outcomes(passed=2)` used to work but now fails because the function now includes `'deselected': ...` in the comparison.\n\n3. It links to concrete examples of affected projects (pytest-bdd) with specific issue references.\n\n4. It includes additional examples of potentially affected codebases with links to the specific code locations.\n\n5. The issue poses clear questions about what should be done for the upcoming 7.0 release and how to prevent similar issues in the future.\n\nThe problem is technical but straightforward - the addition of a new parameter to a function has changed its behavior in a way that breaks existing code. The solution would involve either reverting the change, modifying how the function works to maintain backward compatibility, or documenting it as a breaking change.\n\nA developer could implement a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9624": {
        "explanation": "This issue is reasonably well-specified with a clear error message and context. The error is a Python TypeError: `__init__() missing 1 required positional argument: 'test_method'` occurring during a test run in the skupper-router project.\n\nThe issue provides:\n1. A detailed stack trace showing the error occurs during test teardown in pytest\n2. The specific test that failed: StreamingMessageTest::test_11_streaming_closest_parallel\n3. The environment details (Python packages installed, pytest version 7.0.0, CentOS Stream 8)\n4. A link to the GitHub Actions run where the error occurred\n\nWhat makes this reasonably specified rather than well-specified:\n1. The submitter mentions they can't reproduce it consistently - it happened only once\n2. There's no specific code snippet showing the test class implementation that's causing the error\n3. We don't have the actual implementation of StreamingMessageTest to see what might be wrong with its constructor\n\nHowever, the error is clear enough to understand the problem: there's an issue with the test class constructor missing a required argument during test teardown. This is likely related to how the test class is instantiated by pytest. The error suggests the test class constructor requires a 'test_method' parameter that isn't being provided.\n\nA developer familiar with the codebase should be able to locate the StreamingMessageTest class, examine its constructor, and determine why the 'test_method' parameter is missing during teardown. The solution would likely involve modifying the test class constructor to properly handle this case.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9646": {
        "explanation": "This issue is well-specified with clear information about the problem, reproduction steps, and expected behavior:\n\n1. The problem is clearly stated: Pytest 7 is not ignoring warnings as instructed in the pytest.ini file, specifically for certain PytestWarning messages related to plugin compatibility.\n\n2. A complete reproduction script is provided that allows anyone to clone a sample project and reproduce the issue.\n\n3. The expected behavior is explicitly stated: Pytest should not print the specific warnings that were configured to be ignored in pytest.ini.\n\n4. Comprehensive environment information is provided including:\n   - Python version (3.8.10)\n   - Complete pip list showing all package versions\n   - Operating system details (Ubuntu 20.04.3 LTS)\n   - Tox version (3.24.4)\n\n5. The issue includes the exact warning filter patterns that were attempted in pytest.ini, which is crucial for understanding why the filtering isn't working.\n\nThe issue provides all the necessary information for a developer to understand the problem, reproduce it, and investigate why the warning filters in pytest.ini aren't working as expected. The solution would likely involve understanding why the specific warning pattern in pytest.ini isn't matching the actual warnings being generated.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9681": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a complete minimal reproducible example with all necessary files and their contents\n2. Clearly described the error message received\n3. Identified the specific version where the regression occurs (pytest 7.0.0)\n4. Demonstrated a workaround (renaming conftest.py)\n5. Explained the expected behavior (imports from tests should work with importlib mode despite conftest.py existence)\n6. Noted that this worked in previous versions of pytest\n\nThe issue describes a regression in pytest 7.0.0 where the presence of a conftest.py file interferes with package discovery when using importlib import mode with pythonpath configuration. The problem is clearly demonstrated: when conftest.py exists, imports like \"import tests.subpath.helper\" fail with \"ModuleNotFoundError\", but when the file is renamed, the imports work correctly.\n\nA developer would have all the information needed to investigate and fix this regression without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9709": {
        "explanation": "This issue is well-specified with clear details about the problem and its cause. The user has:\n\n1. Clearly identified the issue: `pytest.approx()` doesn't work correctly with sets\n2. Provided a reproducible code example demonstrating the problem\n3. Explained why the issue occurs by examining the implementation code\n4. Identified the root cause: sets are unordered collections, but the comparison implementation assumes ordered sequences\n5. Suggested potential solutions:\n   - Remove mentions of sets from code and documentation to clarify limitations\n   - Implement a better solution for arbitrary container comparisons\n\nThe issue includes all necessary context (pytest version 7.0.1) and provides a clear path to understanding and resolving the problem. A developer could immediately begin working on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9780": {
        "explanation": "The issue describes a problem where pytest version 7.1.0 is not picking up conftest.py files in the SciPy project, which was working in version 7.0.1. The issue provides:\n\n1. A clear description of the problem: conftest.py files are not being picked up after upgrading to pytest 7.1.0\n2. A specific environment where the issue occurs: Azure Pipelines with SciPy\n3. A hypothesis about the potential cause: unregistered 'slow' mark in pytest.ini\n4. Links to:\n   - A complete example with installation steps (Azure Pipelines build logs)\n   - The project's pytest.ini file\n\nThe issue provides enough context to understand the problem and investigate the root cause. A developer could:\n1. Compare the behavior between pytest 7.0.1 and 7.1.0\n2. Check the pytest 7.1.0 release notes for relevant changes\n3. Examine the provided pytest.ini file and conftest.py files\n4. Reproduce the issue using the provided example\n\nWhile the exact root cause isn't identified, there's sufficient information to begin troubleshooting and developing a solution. The hypothesis about mark registration provides a good starting point for investigation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "pytest-dev__pytest-9798": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The bug is clearly described: When `pytest.approx` fails on list comparisons, it raises a `ModuleNotFoundError` for NumPy, even though NumPy is not required for `pytest.approx` to function.\n\n2. Complete environment information is provided:\n   - Python version (3.9.0)\n   - pytest version (7.0.1)\n   - OS version (macOS Big Sur 11.6.2)\n   - Complete pip list showing installed packages\n\n3. A minimal reproducible example is provided with:\n   - The exact test code that triggers the issue\n   - The actual output showing the error\n   - The expected behavior clearly stated\n\n4. The issue is specific and focused: The error message should not reference NumPy when it fails, as it makes the error message confusing and misleading.\n\nThe solution would need to modify how pytest handles the error representation when `pytest.approx` fails on list comparisons, ensuring it doesn't try to use NumPy for formatting the error message when NumPy isn't available. This is a clear, actionable task with well-defined success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "pytest-dev__pytest-9911": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: passing an empty tuple `()` to `pytest.raises()` or `pytest.warns()` always results in an error because no exception can be an instance of an empty tuple.\n\n2. The solution is explicitly described: implement a check for empty tuples and provide specific error messages (which are even provided as examples in the description).\n\n3. The scope is well-defined: \n   - Check for empty tuples in `pytest.raises()`\n   - Check for empty tuples in `pytest.warns()`\n   - Apply the same logic to the `raises=` argument in `pytest.mark.xfail()`\n   - Provide helpful error messages in each case\n\n4. The issue even suggests bonus functionality (handling `pytest.raises(None)` similarly)\n\nThe only minor ambiguity is the reference to issue #9002 for the warning case, but this doesn't prevent implementing the core functionality requested. A developer could look up that issue for context when implementing the warning-specific message.\n\nOverall, this issue provides enough detail for a developer to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10198": {
        "explanation": "This issue is reasonably well-specified. It clearly states the requirement to add a `get_feature_names` method to the CategoricalEncoder class, which is consistent with similar functionality in the PolynomialFeature class. The issue references previous discussions and related issues that provide context.\n\nFrom the description, I understand:\n1. The task is to implement a `get_feature_names` method for CategoricalEncoder\n2. It should be similar to the implementation in PolynomialFeature\n3. The method should allow passing original feature names to map them to new feature names\n\nWhile the exact implementation details aren't specified, this is typical for feature requests where the developer is expected to follow established patterns in the codebase. The references to previous discussions and similar implementations provide enough guidance for a developer familiar with scikit-learn to implement this feature. The only minor ambiguity is about the exact signature and behavior of the method, but this can be inferred from the existing implementations mentioned in the issue.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10297": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: RidgeClassifierCV doesn't accept the 'store_cv_values' parameter despite the documentation claiming it should.\n\n2. The user provides complete, reproducible code that demonstrates the error.\n\n3. The error message is clearly stated: \"TypeError: __init__() got an unexpected keyword argument 'store_cv_values'\"\n\n4. The expected behavior is clearly described: the model should accept this parameter and store cross-validation predictions as an attribute.\n\n5. The user provides all relevant version information (OS, Python, NumPy, SciPy, Scikit-Learn).\n\n6. The issue points out a specific inconsistency between documentation and implementation.\n\nThe solution path is clear: implement the missing 'store_cv_values' parameter in the RidgeClassifierCV class to match the documentation or update the documentation if this parameter shouldn't be supported. This would involve modifying the class's __init__ method to accept and process this parameter.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10377": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The bug is precisely described: when using label indicator inputs with the `f1_score` function, there's inconsistent behavior when the `labels` parameter includes column index 0. The issue provides:\n\n1. A complete code example that reproduces the problem\n2. The exact expected output values\n3. The actual output values showing the discrepancy\n4. The specific conditions under which the bug occurs (when column index 0 is included in labels)\n5. Environment details including OS, Python version, and library versions\n\nThe problem is clearly defined: the function should only average over the label-specific scores indicated by the labels parameter, but it's calculating the macro average over all label-specific f-scores when column index 0 is included. This is specific enough to identify the issue in the codebase and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10382": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The main problem is clearly defined: the TfidfVectorizer ignores the dtype argument and always outputs np.float64 instead of the specified dtype (np.float32 in the example). The reproduction code is complete and straightforward, showing exactly how to trigger the issue. The expected and actual results are clearly stated, making it easy to verify when a fix is working correctly. The environment details (OS, Python version, library versions) are also provided, which helps in reproducing the issue.\n\nThere is a second issue mentioned about the FitFailedWarning message, but it's separate from the main issue and appears to be an additional observation rather than part of the primary bug report. This secondary issue is less detailed but still reasonably clear in what it's asking for - improving error messages in cross validation.\n\nA developer would be able to understand the primary issue (TfidfVectorizer dtype being ignored) without any additional clarification and could implement a solution by examining how the dtype parameter is handled in the TfidfVectorizer implementation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10397": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is reporting that when using RidgeCV with integer alphas, they encounter a \"ValueError: Integers to negative integer powers are not allowed.\" They've also identified a workaround (making one of the alphas a float) and are suggesting that this conversion should be handled internally by the library.\n\nThe issue includes:\n1. A complete code example that reproduces the error\n2. The exact error message\n3. A working solution (converting integers to floats)\n4. The Python version (3.6)\n5. A clear request: the conversion from integers to floats should be handled internally by the RidgeCV implementation\n\nA developer would be able to:\n1. Reproduce the issue using the provided code\n2. Understand that the problem occurs when all alphas are integers\n3. Implement a fix that automatically converts integer alphas to floats within the RidgeCV implementation\n\nThis is a straightforward bug fix with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10427": {
        "explanation": "This issue is well-specified. It clearly identifies a problem: the `load_sample_images` function is using the deprecated `imread` function from SciPy, which according to the warning will be removed in version 1.2.0. The warning message is explicit about the deprecation timeline.\n\nThe solution would involve updating the `load_sample_images` function to use the recommended alternative to `imread`. While the issue doesn't explicitly mention what the replacement function is, this information can be easily found in SciPy's documentation or by examining the deprecation warning in more detail. Typically, deprecated functions include information about their replacements (likely `imageio.imread` or another SciPy-recommended alternative).\n\nA developer with access to the codebase would be able to:\n1. Locate the `load_sample_images` function\n2. Identify the deprecated `imread` usage\n3. Replace it with the recommended alternative\n4. Test that the function still works correctly\n\nThis is a straightforward maintenance task with a clear objective.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10428": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: ensuring that estimator predictions/transformations are invariant whether performed in batch or on subsets of a dataset.\n\n2. The mathematical requirement is explicitly stated: for any estimator `est`, data `X`, and boolean mask `mask`, the condition `all(est.method(X[mask]) == est.method(X)[mask])` must hold.\n\n3. The specific methods to test are enumerated: `predict`, `predict_proba`, `decision_function`, `score_samples`, and `transform`.\n\n4. The implementation location is specified: the test should be added to common tests in `sklearn/utils/estimator_checks.py`.\n\n5. There's even a reference to a related issue (#9174) that demonstrates a real case where this property is broken.\n\nThe issue provides all the necessary information for a developer to implement the required test without needing additional clarification. The task is to create a common test function that verifies this invariance property across all relevant scikit-learn estimators.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10443": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is clearly defined: when using TfidfVectorizer with a specified dtype of np.float32, the output is still np.float64 instead of respecting the provided dtype parameter. The reproduction code is complete and minimal, showing exactly how to trigger the issue. The expected and actual results are explicitly stated, making it clear what the correct behavior should be. The environment details (OS, Python version, and relevant library versions) are also provided. A developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10452": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a limitation in the `PolynomialFeatures` class - that it doesn't support sparse data. The request is to add sparse data support to this class.\n\nFrom the description, I can understand:\n1. The problem: `PolynomialFeatures` doesn't work with sparse matrices\n2. The desired outcome: Make `PolynomialFeatures` compatible with sparse data\n\nWhile the description is brief and informal (\"Should be easy but I haven't checked ;)\"), it's clear what functionality needs to be implemented. A developer familiar with scikit-learn would understand that this involves modifying the `PolynomialFeatures` transformer to handle scipy sparse matrices in addition to dense numpy arrays.\n\nThe issue doesn't provide implementation details or specific requirements for the sparse implementation, but this is reasonable as those technical decisions would typically be made during the implementation phase. A developer with knowledge of scikit-learn's codebase and sparse matrix operations should be able to implement this feature without additional clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10483": {
        "explanation": "This issue is reasonably well-specified. The author is proposing to move imputation functionality out of the preprocessing module into its own dedicated module (sklearn.impute), similar to how clustering and decomposition algorithms are organized. \n\nThe rationale is clear:\n1. Imputation is a learning task of its own that often uses supervised/unsupervised learning\n2. The preprocessing module is getting \"cramped\"\n3. It would be more intuitive for users to import from a dedicated impute module\n\nThe proposal includes specific examples of where certain imputers might belong (KNNImputer in neighbors) and acknowledges uncertainty about others (MICE).\n\nWhile the issue doesn't provide implementation details or a specific list of all imputers to be moved, the overall architectural change is clear enough that an experienced developer familiar with scikit-learn could implement this reorganization. The developer would need to:\n1. Create a new sklearn.impute module\n2. Move existing imputation classes there\n3. Update imports and documentation\n4. Ensure backward compatibility\n\nThe only ambiguity is whether some imputers should be moved to algorithm-specific modules (like neighbors) or all to the new impute module, but this could be resolved during implementation with reasonable judgment.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10495": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: `check_array(['a', 'b', 'c'], dtype='numeric')` accepts string arrays when it should reject them\n2. The current behavior: dtype='numeric' preserves the dtype unless it's an object\n3. The expected behavior: dtype='numeric' should raise an error or attempt to coerce non-numeric data\n4. The specific function affected: `check_array()` with the `dtype='numeric'` parameter\n5. A reference to a related issue (#9342) for additional context\n\nThe issue provides a concrete example that reproduces the problem and explains why the current behavior is problematic. It also suggests potential solutions (raising an error or coercing the data) and acknowledges that a deprecation cycle might be needed.\n\nA developer could implement a solution by modifying the `check_array()` function to validate that arrays with dtype='numeric' actually contain numeric data, and either raise an appropriate error or attempt to coerce the data to a numeric type.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10508": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The bug report identifies:\n\n1. The exact versions of Python (3.6.3) and scikit-learn (0.19.1) where the issue occurs\n2. A clear description of the inconsistent behavior: LabelEncoder.transform() works with empty lists when fitted on numeric data but fails when fitted on string data\n3. A complete code example that demonstrates the issue, including the exact error message\n4. The expected behavior: \"empty arrays are returned in both cases\"\n\nThe error occurs specifically when trying to transform an empty list after fitting the LabelEncoder on string data, with a clear TypeError about casting between data types. This provides enough information to diagnose the issue in the LabelEncoder implementation and develop a fix that handles empty arrays consistently regardless of the data type used during fitting.\n\nA developer would be able to reproduce this issue, understand the root cause by examining the LabelEncoder code, and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10558": {
        "explanation": "This issue is well-specified and clear about what needs to be done. The request is to deprecate the `axis` parameter from the `Imputer` class, specifically targeting the `axis=1` functionality. The issue provides:\n\n1. Clear rationale for the deprecation with multiple supporting points:\n   - The feature is unusual in a machine learning context\n   - It's not stateful and can be implemented with FunctionTransformer\n   - It complicates the implementation\n   - It's difficult to extend to other features\n\n2. A specific implementation goal: deprecate the `axis` argument on the `Imputer` class\n\n3. Reference to a related issue (#9463)\n\n4. A suggested alternative implementation approach (using FunctionTransformer)\n\nThe issue has already been approved for merging (indicated by [MRG+1]), which further suggests it has been reviewed and deemed appropriate. A developer would have enough information to implement this change without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10581": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible code example. The issue states that when using ElasticNet with `copy_X=True` and `check_input=False`, the input matrix X is being modified despite the `copy_X` parameter being set to True, which should prevent modification of the input data. The code example demonstrates the issue by creating a copy of X before fitting and then checking if X remains unchanged after fitting. The expected behavior is that X should remain unchanged when `copy_X=True`, but the issue shows it's being modified. The title of the issue clearly states the problem, and there's even a reference to a pull request that attempts to fix this issue (#10540). The issue provides all the necessary information to understand the bug and attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10777": {
        "explanation": "This issue is reasonably well-specified. The reporter is suggesting that when using CountVectorizer with an ngram_range parameter where the first value (min_n) is greater than the second value (max_n), the code should throw an error. Currently, they're not sure what happens in this case, but they believe it should be an error condition.\n\nThe issue provides:\n1. A clear example of the problematic input: CountVectorizer(ngram_range=(2, 1))\n2. A clear suggestion for the expected behavior: throw an error when ngram_range[0] > ngram_range[1]\n\nWhat's missing is:\n1. The current behavior (though the title suggests no error is thrown)\n2. Any specific error message that should be displayed\n3. The rationale for why this should be an error\n\nHowever, these missing elements don't prevent understanding what needs to be fixed. A developer would need to:\n1. Verify the current behavior when ngram_range[0] > ngram_range[1]\n2. Add validation logic to check if ngram_range[0] > ngram_range[1] and throw an appropriate error if true\n\nThis is a straightforward validation check that makes logical sense (since an n-gram range from 2 to 1 is logically invalid), and the implementation would be relatively simple once the codebase is examined.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10803": {
        "explanation": "This issue is requesting the addition of weighted kernel density estimation (KDE) functionality to a library (presumably a data science or statistical library). The request is clear in what it wants: the ability to pass weights for each sample in KDE calculations.\n\nThe issue provides helpful context by:\n1. Clearly stating the desired feature (weighted KDE)\n2. Providing a reference to an existing implementation in scipy.stats.gaussian_kde\n3. Including a link to a Stack Overflow post that demonstrates how this functionality could be implemented\n\nWhat makes this reasonably specified rather than well-specified:\n- It doesn't specify which exact library or module this feature should be added to (though context suggests it's likely for a statistical/data visualization library)\n- It doesn't provide details on the expected API (parameter names, behavior with edge cases, etc.)\n- It doesn't specify test cases or expected outputs\n\nHowever, a developer familiar with the codebase could reasonably implement this feature based on the provided information and reference implementation. The Stack Overflow link provides a working example that could be adapted to fit the library's API style.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10844": {
        "explanation": "This issue is well-specified with clear information about the problem and a proposed solution:\n\n1. The problem is clearly identified: The `fowlkes_mallows_score` function in scikit-learn produces a RuntimeWarning when the variables `pk` and `qk` get too large (exceeding the int32 limit).\n\n2. The specific line causing the issue is identified: `return tk / np.sqrt(pk * qk) if tk != 0. else 0.` in the file `sklearn\\metrics\\cluster\\supervised.py` at line 859.\n\n3. The issue describes the actual behavior: When the product `pk * qk` is too large, it causes an overflow and returns 'nan' instead of a proper float value.\n\n4. A specific solution is proposed: Use `np.sqrt(tk / pk) * np.sqrt(tk / qk)` instead, which is mathematically equivalent but avoids the overflow issue.\n\n5. The version of scikit-learn being used is provided (0.18.1).\n\nThe issue provides all the necessary information to understand the problem and implement a fix. The mathematical equivalence of the proposed solution can be verified, and the fix appears straightforward to implement. While a specific code example to reproduce the issue would have been helpful, the problem is described clearly enough that a developer familiar with the codebase could identify and fix it without additional information.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10870": {
        "explanation": "This issue is very well-specified with clear details about the problem and solution:\n\n1. The problem is precisely defined: In Gaussian mixtures, when n_init > 1, the lower_bound_ attribute is not storing the maximum lower bound across initializations, but only the value from the last initialization.\n\n2. The exact location of the bug is identified: BaseMixture.fit() method.\n\n3. A specific fix is proposed: Adding `self.lower_bound_ = max_lower_bound` just before returning self.\n\n4. Complete reproduction code is provided that demonstrates the issue.\n\n5. Expected and actual results are clearly stated.\n\n6. Version information is comprehensive.\n\n7. There's even a suggestion for improving the test to prevent this issue in the future.\n\nThe issue provides all the necessary information to understand, reproduce, and fix the problem without requiring any additional clarification. A developer could immediately implement the fix based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10881": {
        "explanation": "This issue is well-specified with clear information about the problem and what a solution would entail:\n\n1. The problem is clearly stated: LogisticRegression doesn't provide a warning when it fails to converge (hitting max_iter limit).\n\n2. A complete, reproducible code example is provided that demonstrates the issue.\n\n3. The expected behavior is explicitly stated: the user wants a warning to be shown when convergence isn't reached.\n\n4. All relevant version information is included.\n\n5. The solution would be straightforward: add a warning message when the algorithm reaches max_iter without converging.\n\nThe issue provides all the necessary context for a developer to understand the problem, reproduce it, and implement a solution without needing additional clarification. The expected solution (adding a warning) is also reasonable and aligned with good software design practices.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10899": {
        "explanation": "This issue is well-specified with clear details about the problem. The user is trying to persist a TfidfTransformer by saving its parameters in a MongoDB database and then rebuilding it, similar to how they can do with CountVectorizer. The issue is that they cannot set the `idf_` attribute of TfidfTransformer after initialization, which prevents them from recreating a previously fitted transformer.\n\nThe issue includes:\n1. Clear explanation of the problem (inability to set idf_ attribute)\n2. Working code example for CountVectorizer (the approach that works)\n3. Non-working code example for TfidfTransformer (the approach that fails)\n4. Clear use case and motivation (avoiding recomputation for a RestAPI)\n5. Complete version information\n\nThe user is asking if there's an architectural reason for this limitation and if there's a workaround. This is a well-defined feature request/enhancement that could be addressed by adding a way to set the `idf_` attribute after initialization or by providing an alternative method to recreate a fitted TfidfTransformer from saved parameters.\n\nA solution would involve modifying the TfidfTransformer class to allow setting the `idf_` attribute or providing an alternative method to achieve the same result.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10908": {
        "explanation": "This issue is well-specified with clear examples demonstrating the inconsistent behavior. The problem is that when a CountVectorizer is initialized with a vocabulary parameter, the get_feature_names() method still raises a NotFittedError before transform() is called, even though transform() works without fitting. The issue provides:\n\n1. A clear reproduction case with code examples\n2. The expected behavior: get_feature_names() should not raise NotFittedError when vocabulary is provided\n3. The current behavior: get_feature_names() raises NotFittedError while transform() works fine\n4. A technical explanation of why this happens: transform() calls _validate_vocabulary which sets vocabulary_, but get_feature_names() doesn't\n\nThe solution would involve modifying get_feature_names() to validate the vocabulary similar to how transform() does it, ensuring consistent behavior when a vocabulary is provided at initialization. This is a well-defined bug with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10913": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The user identifies that the current implementation of MultiLabelBinarizer raises a KeyError when encountering unseen labels during transform, and proposes adding an \"ignore_unseen\" parameter that would allow the model to handle unseen labels by setting them to 0 in the output array.\n\nThe issue includes:\n1. A clear description of the current behavior (raising KeyError for unseen labels)\n2. A specific use case where this is problematic (when not all classes are known at training time)\n3. A concrete proposed modification with example code showing the desired behavior\n4. Expected output that clearly demonstrates how the new parameter would work\n5. Evidence that others face the same issue (StackOverflow link)\n\nThe user even offers to create a PR for this feature. The proposal is straightforward to implement and would be a useful addition to the MultiLabelBinarizer class. The functionality is well-defined and the expected behavior is unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10982": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: RandomizedSearchCV currently throws an exception when n_iter is greater than the size of the search space.\n2. The specific line of code causing the issue is linked.\n3. The user has provided a clear use case explaining why this is problematic.\n4. A specific solution is proposed: adding an option parameter (allow_smaller_grid=True) to the constructor that would default to False.\n5. The expected behavior is clearly defined: when this option is enabled, n_iter should be treated as an upper bound rather than throwing an exception.\n\nThe issue provides enough context and a specific implementation suggestion that would make it straightforward for a developer to implement a solution. The change is localized to the RandomizedSearchCV class and involves adding a parameter and modifying the validation logic.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-10986": {
        "explanation": "This issue is well-specified with clear details about the bug, how to reproduce it, and even a suggested fix. The reporter has:\n\n1. Provided a complete, minimal code example that reproduces the issue\n2. Clearly explained the expected vs. actual results\n3. Identified the root cause of the bug in the codebase (with a specific line reference)\n4. Proposed a specific solution with code\n5. Included version information for all relevant dependencies\n\nThe bug occurs when using LogisticRegression with binary outcomes, multi_class='multinomial', and warm_start=True. The issue is that when warm starting, the coefficient matrix shape is incorrect, causing broadcasting issues that degrade model performance with each additional fit.\n\nThe reporter has even traced the issue to a specific line in the codebase and provided a clear fix. This is an extremely well-documented bug report that would allow a developer to understand, reproduce, and fix the issue without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11040": {
        "explanation": "This issue is well-specified with a clear problem description and expected solution. The issue identifies that:\n\n1. The `NearestNeighbors` estimator in scikit-learn doesn't properly validate the `n_neighbors` parameter when a float value is provided\n2. This leads to a cryptic error message: `TypeError: 'float' object cannot be interpreted as an integer`\n3. The issue occurs both when initializing the estimator with a float value and when calling the `kneighbors` method with a float value\n\nThe expected solution is clearly stated:\n1. Catch this error earlier in the process\n2. Provide a more helpful error message\n3. Alternatively, consider casting the float to an integer (though the reporter prefers a better error message)\n4. Ensure the fix works for both the constructor and the `kneighbors` method\n\nThe issue includes a complete code example that reproduces the problem, making it easy to understand and test any solution. The error message is also provided, which helps in identifying where in the code the problem occurs.\n\nThis is a straightforward parameter validation issue with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11042": {
        "explanation": "This issue is well-specified with clear steps to reproduce, expected results, and actual results. The problem is clearly defined: when using OneHotEncoder with mixed input data (both categorical and real data types), the encoder ignores the specified dtype parameter (np.float32) and outputs a scipy sparse matrix with dtype np.float64 instead.\n\nThe issue includes:\n1. Complete reproducible code example\n2. The expected output (sparse matrix of type float32)\n3. The actual output (sparse matrix of type float64)\n4. Relevant version information for the platform, Python, NumPy, SciPy, and Scikit-Learn\n\nA developer would have all the necessary information to understand the issue, reproduce it, and implement a fix to ensure OneHotEncoder respects the specified dtype parameter when constructing sparse matrices with mixed input data types.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11043": {
        "explanation": "This issue is well-specified and provides clear information about what needs to be changed:\n\n1. The problem is clearly identified: FunctionTransformer currently converts DataFrames to arrays by default, which is not intuitive for users who want to apply functions to DataFrames using pandas methods.\n\n2. The proposed solution is specific: Change the default behavior of FunctionTransformer to use `validate='array-or-frame'` instead of the current default, which would allow DataFrames to pass through without conversion.\n\n3. Implementation guidance is provided: The change should go through a deprecation cycle, with warnings when the default validation converts a DataFrame to an array.\n\n4. The rationale is explained: This would be more user-friendly and prevent users from missing the need to set `validate=False`.\n\n5. There's a reference to a related issue (#10648) for additional context.\n\nThe issue provides enough information for a developer to understand what needs to be changed, why it should be changed, and how to implement the change with proper deprecation warnings. The only minor ambiguity is whether there's consensus on this change, as the issue ends with \"Do others agree?\" - but this is more about seeking approval rather than lacking specification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11160": {
        "explanation": "This issue is well-specified. The user is requesting an enhancement to the `classification_report` function in scikit-learn to add output options beyond the current string format. Specifically:\n\n1. The problem is clearly stated: currently the function only outputs a string that must be printed, making it difficult to use the results programmatically.\n2. The desired solution is explicit: add options to return the results as a pandas DataFrame, xarray DataArray, or (as implemented in the PR) a Dictionary of Dictionaries.\n3. The PR references a specific issue (#7845) and proposes a concrete implementation for one of the requested output formats.\n4. The context is clear - this is about enhancing an existing function with additional return type options without changing its core functionality.\n\nThe implementation details are straightforward - add a parameter to control the output format and modify the function to return the appropriate data structure based on that parameter. This is a common pattern in scikit-learn and other data science libraries.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11206": {
        "explanation": "This issue description is vague and lacks critical details. The title suggests that `increment_mean_and_var` can now handle NaN values, implying a fix or enhancement has been made, but there's no actual description of:\n\n1. What the problem was with NaN values in the function\n2. How the implementation was changed to fix it\n3. What tests were added to verify the fix\n4. What the expected behavior is now\n\nThe issue references another issue (#10457) but only mentions \"check if incremental_mean_and_var gives a green tick without failing in numerical_stability\" without explaining what this means or what the actual problem was.\n\nThe template sections for explaining changes and additional comments are completely empty, providing no information about what was actually implemented or fixed. Without these details, it would be extremely difficult for a developer to understand what changes were made and why, or to review the PR effectively.\n\nThis appears to be a pull request description rather than an issue description, and it's missing all the substantive information that would be needed to understand the changes.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-11264": {
        "explanation": "This issue is well-specified with clear problem description, reproducible code, expected results, and actual results. The issue describes that `randomized_svd` in scikit-learn performs significantly slower for certain sparse matrix types (specifically dok_matrix and lil_matrix) compared to others, with timing benchmarks showing up to 50x performance difference. The user provides:\n\n1. Complete code to reproduce the issue\n2. Timing benchmarks for different sparse matrix types showing the performance discrepancy\n3. Clear expected behavior (either consistent performance or a warning)\n4. Environment details including OS, Python, NumPy, SciPy, and scikit-learn versions\n5. Confirmation that the issue exists on multiple platforms\n\nThe solution would involve either optimizing the `randomized_svd` function to handle all sparse matrix types efficiently or adding a warning when inefficient matrix types are used. This is a well-defined performance optimization task with clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11281": {
        "explanation": "The issue is asking whether Mixture Models (MMs) should be made more compatible with the clusterer interface in the library. The description clearly outlines the current differences between MMs and clusterers:\n\n1. MMs use `n_components` parameter instead of `n_clusters`\n2. MMs don't store `labels_` of training data\n3. MMs don't have a `fit_predict` method\n4. MMs are documented separately\n\nThe issue is reasonably clear about what changes might be needed (aligning parameter names, adding `labels_` attribute, adding `fit_predict` method, and potentially consolidating documentation). \n\nHowever, there are some details that aren't fully specified:\n- It doesn't specify which specific Mixture Model classes need to be modified\n- It doesn't clarify if this is just an API change or if there are underlying algorithmic changes needed\n- It doesn't specify if backward compatibility needs to be maintained\n- It doesn't provide guidance on implementation details or potential edge cases\n\nDespite these gaps, an experienced developer familiar with the codebase could make reasonable assumptions about the scope and implementation approach. The core request to make MMs more compatible with the clusterer interface is clear enough to start working on a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11310": {
        "explanation": "This issue is well-specified with a clear request and rationale. The user wants to add a new attribute `refit_time_` to the BaseSearchCV class in scikit-learn that would track the time it takes to refit the best model on the full dataset after grid/random search completes.\n\nThe issue includes:\n1. A clear problem statement - wanting to measure refit time separately from search time\n2. A code example demonstrating the current functionality\n3. An explanation of why the current workaround (timing the whole procedure and subtracting) doesn't work with parallel processing\n4. A specific solution proposal (adding a `refit_time_` attribute)\n5. A concrete use case (uploading results to OpenML.org)\n\nThe implementation would likely involve adding timing code around the refit step in BaseSearchCV and storing the result in a new attribute. This is a straightforward enhancement that doesn't require additional clarification to implement successfully.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11315": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue is that the `_BaseCompostion._set_params` method raises an error when a composition (specifically a ColumnTransformer in the example) has no estimators.\n\nThe error occurs in the `_set_params` method when it tries to unpack values from an empty list of estimators using `zip(*getattr(self, attr))`. Since there are no estimators, this results in a ValueError: \"not enough values to unpack (expected 2, got 0)\".\n\nThe issue description provides:\n1. The specific method that's broken (`_BaseCompostion._set_params`)\n2. The exact condition that triggers the error (when there are no estimators)\n3. A complete, reproducible code example showing the error\n4. The full error traceback\n5. A reference to a related issue (#11315)\n\nA developer would have all the information needed to locate the problem in the codebase and implement a fix that handles the case when there are no estimators in the composition.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11333": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue is that the `_BaseCompostion._set_params` method raises an error when a composition (specifically a ColumnTransformer in the example) has no estimators. The error occurs in the line `names, _ = zip(*getattr(self, attr))` when trying to unpack values from an empty list.\n\nThe issue provides:\n1. A clear description of the problem\n2. A specific error message\n3. A reproducible code example showing how to trigger the error\n4. The exact line of code causing the issue\n5. Context that this is related to another issue (#11315)\n\nThe solution would involve modifying the `_set_params` method in `sklearn/utils/metaestimators.py` to handle the case where there are no estimators, likely by adding a check for empty collections before attempting to unpack values.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11346": {
        "explanation": "This issue is well-specified with clear reproduction steps and detailed error information. The user has:\n\n1. Provided a minimal reproducible example showing the code that causes the error\n2. Identified the specific conditions under which the error occurs (when `data_dims` > 2000 and using `n_jobs` > 1)\n3. Included the full error traceback showing the `ValueError: assignment destination is read-only` error\n4. Specified their environment details (OS, Python version, NumPy version, scikit-learn version)\n5. Demonstrated that they've done investigation by identifying the threshold at which the error starts occurring\n\nThe issue appears to be related to parallelization in scikit-learn's `SparseCoder` when using `n_jobs > 1` with larger datasets. The error occurs in the `_gram_omp` function when trying to swap elements in a read-only array. This is likely a bug in scikit-learn's implementation when handling shared memory in parallel processing.\n\nA developer would have all the necessary information to reproduce and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11391": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue provides a specific error case with a complete code example that reproduces the error\n2. It shows the exact error traceback pointing to the problem\n3. It identifies the root cause: in the `_get_mask` function, there's missing type checking between `X` and `missing_values` when comparing a numeric array with a string value (\"NaN\")\n4. It explains that `np.equal` raises `NotImplemented` when comparing numeric and string mixed dtypes\n5. It suggests a specific solution: add a check in the `_get_mask` function which is shared across imputer classes\n\nThe issue provides all the necessary context to understand and fix the problem. The developer would need to modify the `_get_mask` function to properly handle type mismatches between the array and the missing value parameter, particularly when comparing numeric arrays with string missing values.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11496": {
        "explanation": "This issue is well-specified with a clear description of the bug. The problem is that SimpleImputer doesn't handle explicit zeros in sparse matrices correctly. The issue includes:\n\n1. A clear description of the expected behavior: zeros in sparse matrices should be treated the same whether they are implicit or explicit\n2. A complete code example that reproduces the issue\n3. The actual output that is currently produced\n4. The expected output that should be produced\n\nThe issue is specifically about the SimpleImputer class in scikit-learn's impute module, and how it handles sparse matrices with explicit zeros when those zeros are defined as missing values. The bug is that explicit zeros are not being properly identified as missing values that need to be imputed.\n\nWith this information, a developer could understand the problem, reproduce it, and work on a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11574": {
        "explanation": "The issue description is quite brief and lacks specific details about what exactly needs to be done. However, it does provide some key information:\n\n1. It's about the IsolationForest algorithm and specifically about a \"contamination\" parameter\n2. There's a deprecation warning that's currently in the `__init__` method but should be moved to the `fit` method\n3. Tests need to be fixed as a result of this change\n\nFor someone familiar with the codebase and the IsolationForest implementation, this would likely be enough information to understand what needs to be done. The task involves moving deprecation warning code from one method to another and updating tests accordingly.\n\nHowever, the description lacks important details such as:\n- Why the deprecation warning needs to be moved\n- Which specific tests need to be fixed\n- What the current behavior is versus the expected behavior\n- Any specific requirements for the implementation\n\nWithout these details, a developer would need to investigate the codebase to fully understand the issue before implementing a solution. While the core task is clear (move deprecation warning from __init__ to fit), the rationale and full scope of changes required are not well explained.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11578": {
        "explanation": "This issue is well-specified with a clear problem description, a detailed explanation of the cause, and a suggested solution. The user has:\n\n1. Identified a specific bug: When using LogisticRegressionCV with multi_class='multinomial', the scoring function uses OvR (one-vs-rest) approach instead of the multinomial approach for probability calculations.\n\n2. Pinpointed the exact location of the bug in the code (line 922 and 955 of logistic.py) with GitHub links.\n\n3. Provided a detailed explanation of why this is happening: The LogisticRegression instance created in _log_reg_scoring_path() doesn't inherit the multi_class parameter.\n\n4. Suggested a specific fix: Modify line 922 to pass the multi_class parameter.\n\n5. Included a comprehensive minimal example that demonstrates the issue, with clear output showing the discrepancy.\n\n6. Provided version information for reproducibility.\n\nThe PR description confirms this understanding and implements the suggested fix. This issue contains all the necessary information to understand the problem and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11585": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The issue describes a bug in the `SparsePCA` class where the `transform()` method incorrectly scales results based on the number of rows in the data matrix, causing inconsistent scaling between training and test data.\n\nThe issue includes:\n1. A precise description of the problem\n2. The exact location of the problematic code (line 179 in sparse_pca.py)\n3. Two specific proposed solutions: either (a) learn the scale factor from training data or (b) use mean instead of sum to remove the dependency on the number of examples\n4. A complete, reproducible code example that demonstrates the issue\n5. Clear expected vs. actual results showing the scaling discrepancy\n6. An explanation of why this is problematic (test results are on a different scale than training results)\n\nThe issue provides all the necessary information to understand the problem and implement a fix. The proposed solutions are reasonable and address the core issue. A developer could implement either of the suggested approaches without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11596": {
        "explanation": "The issue is reasonably well-specified. It requests adding a `show_versions()` function to scikit-learn (sklearn) similar to the existing `pandas.show_versions()` function, with a specific focus on including numpy BLAS binding information. The rationale is provided - numeric issues can be related to the specific BLAS implementation that numpy is using, and having this information would make it easier for users to report issues.\n\nThe issue provides:\n1. A clear request (add sklearn.show_versions())\n2. A reference implementation (pandas.show_versions)\n3. A specific requirement (include numpy BLAS binding info)\n4. A rationale for why this is useful\n\nWhat's not fully specified:\n1. The exact format or all the information that should be included beyond the BLAS binding info\n2. Where exactly in the codebase this function should be placed\n3. Whether there are any other specific system information that should be included\n\nHowever, these details can be reasonably inferred by examining the pandas implementation and understanding sklearn's codebase structure. The core requirement is clear enough to implement a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-11635": {
        "explanation": "This issue is well-specified. The user is asking about a specific behavior in the `SelectFromModel.transform` method, where it uses `check_array` with `force_all_finite=True` (the default), which raises a ValueError if any value is infinite or NaN. \n\nThe issue clearly identifies:\n1. The specific method (`SelectFromModel.transform`)\n2. The specific parameter in question (`force_all_finite` in `check_array`)\n3. The current behavior (raising ValueError on infinite/NaN values)\n4. The rationale for changing it (the values aren't used in computation, and some models can handle infinite values)\n5. A potential extension of the solution to other feature selection methods\n\nA developer could easily locate the code in question, understand the current implementation, and evaluate whether the check is necessary or could be relaxed. The issue provides enough context to understand the problem and consider a solution without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12421": {
        "explanation": "This issue is well-specified with clear evidence of a discrepancy between scikit-learn's OPTICS implementation and the R implementation regarding core_distances_. \n\nThe issue points out that:\n1. The scikit-learn documentation states that \"Points which will never be core have a distance of inf.\"\n2. However, when running the same dataset through both scikit-learn and R implementations, the R version correctly assigns \"Inf\" to non-core points, while scikit-learn assigns a numerical value (220.04544985).\n\nThe issue provides complete, reproducible code examples for both implementations, along with their outputs, making it clear what the problem is. The expected behavior is that scikit-learn should assign \"inf\" to non-core points as stated in the documentation and as implemented in R.\n\nThe solution would involve modifying the scikit-learn OPTICS implementation to correctly assign infinity values to non-core points, making it consistent with both the documentation and the R implementation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12462": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is clearly defined: SkLearn's `.score()` method and other metrics functions are failing when used with Dask DataFrames after updating to newer versions.\n\n2. A Minimal Working Example (MWE) is provided that reproduces the error.\n\n3. The full error traceback is included, showing exactly where and how the code fails.\n\n4. Version information is provided for both before and after the bug appeared.\n\n5. A workaround is demonstrated, showing that converting Dask arrays to NumPy arrays resolves the issue but isn't ideal for large datasets.\n\n6. There's even context from a related GitHub issue where Dask developers indicated it's an issue with SkLearn's shape check.\n\n7. A potential solution approach is suggested: adding a try-except block to handle cases where shape doesn't return a tuple.\n\nThis provides all the necessary information to understand the problem, reproduce it, and attempt a solution. The issue is in sklearn's validation code that doesn't properly handle Dask objects when checking for consistent lengths. The solution would likely involve modifying sklearn's validation functions to better handle Dask arrays/dataframes.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12486": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The issue description includes:\n\n1. A specific bug report with a clear error message: \"IndexError: index 1 is out of bounds for axis 1 with size 1\" when using GridSearchCV with scoring='roc_auc' and GMM as classifier.\n\n2. Complete code to reproduce the issue, showing exactly how to trigger the bug.\n\n3. The error traceback pointing to the specific line in the code that's causing the problem (in scorer.py, line 175).\n\n4. A reference to the GitHub issue being fixed (#7598).\n\n5. A clear explanation of the proposed solution: adding a test in scorer.py to raise a ValueError if either a non-classifier model is used for a classification problem or a dataset with only one class is used.\n\nThe issue is about handling edge cases in the scoring functionality, specifically when using ROC AUC scoring with models that don't properly implement the classifier interface or with datasets that don't have multiple classes. The solution approach is clearly stated - add validation to prevent this error by checking if the estimator is a classifier and if there are at least 2 classes.\n\nA developer would have all the necessary information to implement a fix for this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12583": {
        "explanation": "This issue is well-specified. It clearly describes:\n\n1. The feature to be added: an `add_indicator` parameter to imputers (especially SimpleImputer)\n2. The functionality of this parameter: when enabled, it should stack a MissingIndicator transform onto the output of the imputer's transform method\n3. The context: both SimpleImputer and MissingIndicator are referenced with links to their documentation\n\nThe implementation details are straightforward - add a boolean parameter to imputers that, when set to True, would combine the imputed values with indicator values showing which values were originally missing. This is a common pattern in data preprocessing where knowing which values were imputed can be valuable information for downstream models.\n\nThe issue doesn't specify default values or exact implementation details, but these can be reasonably inferred from standard practices and the existing codebase. The core requirement is clear enough for an experienced developer to implement without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12585": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, actual results, and even a suggested fix. The user is reporting that the `clone` function in scikit-learn fails when an estimator parameter is a class type rather than an instance. They provide:\n\n1. A clear code example that reproduces the issue\n2. The exact error message received\n3. The expected behavior (no error)\n4. A specific suggestion for fixing the issue (modifying a specific line in base.py)\n5. The version of scikit-learn they're using (0.20.0)\n\nThe user also explains their use case - they're working on a wrapper library for scikit-learn estimators and want to store estimator classes as parameters. The issue is that the current implementation of `clone()` doesn't handle class types properly, only instances.\n\nThe suggested fix is to modify the condition in base.py to also check if the estimator is a type (class) before attempting to call get_params() on it. This is a reasonable and specific suggestion that addresses the root cause of the error.\n\nA developer could take this information and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12625": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific error: TypeError: \"iteration over a 0-d array\" when using preprocessing.scale on a pandas.Series in scikit-learn 0.20.0.\n\n2. It provides complete, minimal reproducible code that demonstrates the issue:\n```python\nimport pandas as pd\nfrom sklearn import preprocessing\ns = pd.Series([1.0, 2.0, 3.0])\npreprocessing.scale(s)\n```\n\n3. It clearly states the expected behavior (with example output) and notes that it worked in the previous version (0.19.1).\n\n4. It includes the full error traceback showing exactly where the error occurs.\n\n5. It provides version information for all relevant packages (scikit-learn, pandas, numpy, etc.).\n\n6. It even identifies a workaround that resolves the issue.\n\nThe issue is describing a regression in scikit-learn 0.20.0 where pandas.Series objects are no longer handled correctly by preprocessing.scale. This is a clear bug that can be investigated and fixed with the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12656": {
        "explanation": "This issue is reasonably well-specified. The description clearly states the problem and the solution:\n\n1. The issue is about removing the `labels` parameter from the `hamming_loss` function in scikit-learn.\n2. The PR explains that since `hamming_loss` is only used with multilabel input, they've replaced `len(labels)` with `y_true.shape[1]`.\n3. The PR references issue #10580 which it aims to fix.\n\nWhat makes this not fully \"well-specified\" is that:\n1. The original issue #10580 is not directly quoted, so we don't have the complete context.\n2. There's no explanation of why the `labels` parameter needs to be removed (though we can infer it's likely deprecated or unnecessary).\n3. There's no code snippet showing the exact changes made.\n\nHowever, for an experienced developer familiar with scikit-learn, there's enough information to understand what needs to be done: remove the `labels` parameter from the `hamming_loss` function and update the implementation to use `y_true.shape[1]` instead of `len(labels)`. This is a focused, specific change with a clear goal.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12682": {
        "explanation": "This issue is well-specified and clear. It identifies a specific problem: the `SparseCoder` class doesn't expose the `max_iter` parameter for the `Lasso` algorithm when using `lasso_cd`, which results in convergence warnings. The issue also provides a concrete example where this problem occurs (`examples/decomposition/plot_sparse_coding.py`).\n\nThe issue suggests a solution direction: allowing users to specify additional parameters for the underlying estimator (in this case, `Lasso`) beyond what's currently exposed in `SparseCoder.__init__`. This would enable users to control parameters like `max_iter` to avoid convergence warnings.\n\nA developer would be able to:\n1. Locate the `SparseCoder` class in the codebase\n2. Understand how it currently initializes and uses the `Lasso` estimator\n3. Implement a mechanism to pass additional parameters to the underlying estimator\n4. Test the solution using the example script mentioned in the issue\n\nThe problem is specific, the context is clear, and the expected solution approach is reasonably implied.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12704": {
        "explanation": "This issue is well-specified with a clear bug report. The user provides:\n\n1. A specific error scenario with complete code to reproduce the issue\n2. The full error traceback showing exactly where the problem occurs\n3. A clear explanation of the problem: when using pandas Index objects as column specifiers in ColumnTransformer, it causes a ValueError\n4. The context: regression in version 0.20.1 that wasn't present before\n5. The root cause is identifiable from the traceback: in _is_deprecated_tuple_order function, there's an attempt to evaluate a pandas Index with a boolean check, causing the \"truth value of an array\" error\n\nThe solution path is clear: modify the _is_deprecated_tuple_order or _validate_transformers functions to properly handle pandas Index objects when checking if a transformer is 'drop' or 'passthrough'. The fix would need to ensure that pandas Index objects are properly handled in these equality checks.\n\nThe user even suggests this should be fixed in version 0.20.2, indicating it's a regression bug that needs immediate attention. With this information, a developer could locate the issue in the code and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12827": {
        "explanation": "The issue is reasonably specified as it clearly identifies a documentation improvement needed for the `QuantileTransformer` class in scikit-learn. The request has two main components:\n\n1. Add implementation details to the documentation of `QuantileTransformer`\n2. Clarify how the mapping to uniform distribution works (as the requester had a misconception)\n\nThe issue provides helpful context by linking to a Stack Exchange question/answer that could serve as a reference for the documentation improvement. This gives a good starting point for understanding what kind of information should be added.\n\nWhile the issue doesn't specify exactly what text should be added or where in the documentation it should go, this is a reasonable documentation task where an experienced contributor could:\n1. Review the current implementation of `QuantileTransformer`\n2. Examine the linked Stack Exchange answer\n3. Add appropriate technical details to the class documentation\n\nThe task has a clear goal and sufficient context to proceed with a solution without requiring further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12860": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a gap in the scikit-learn library: there is no direct way to perform unpenalized logistic regression. The user has to resort to workarounds like setting a very large regularization parameter C in LogisticRegression or use a different library (statsmodels).\n\nThe request is to add support for unpenalized logistic regression, similar to how LinearRegression provides unpenalized OLS and SGDClassifier supports penalty=\"none\". The solution would likely involve adding a penalty=\"none\" option to LogisticRegression or creating a new class for unpenalized logistic regression.\n\nWhile the issue doesn't provide implementation details or specific API suggestions, the request is clear enough that an experienced developer familiar with scikit-learn could implement a solution. The existing pattern in the library (how LinearRegression and SGDClassifier handle unpenalized models) provides guidance for how this feature should be implemented.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12908": {
        "explanation": "This issue is well-specified with clear requirements for adding a new feature to the OneHotEncoder class in scikit-learn. The request is to add an option for \"1 of k-1\" encoding (also referred to as \"drop_first\") which would encode categorical variables with one fewer column than the number of categories.\n\nThe issue description includes:\n1. Clear explanation of the feature request (adding an option to drop one category in one-hot encoding)\n2. Rationale for the feature (avoiding multicollinearity in regression models)\n3. Example code showing the expected behavior\n4. References to related issues and PRs (#6053, #9073, #6488)\n5. Implementation details about how the feature should work\n\nThe issue even includes information about a potential implementation constraint (incompatibility with handle_missing='ignore'). \n\nThe description contains multiple related issues/PRs that have been merged together in this ticket, but they all describe essentially the same feature request - adding the ability to drop one category when one-hot encoding to avoid multicollinearity. This is a standard practice in statistical modeling, so the request is well-grounded in established methodology.\n\nA developer would have enough information to implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12961": {
        "explanation": "This issue is reasonably well-specified with enough information to understand the problem and attempt a solution. The user provides:\n\n1. Clear error message with traceback: \"ValueError: not enough values to unpack (expected 4, got 0)\"\n2. Environment details: lightgbm 2.2.2, sklearn 0.20.1, MacOS Mojave\n3. Specific code location in scikit-learn where the error occurs\n4. Links to the relevant code in the scikit-learn repository\n5. Information about attempted workarounds (setting return_train_score=False)\n6. A hypothesis about what might be causing the issue (lightgbm not returning valid results)\n\nThe issue appears to be that in sklearn's RandomizedSearchCV, the _format_results function is trying to unpack values from an empty list, causing the ValueError. The user has identified the specific lines in the code where this is happening.\n\nWhile the user doesn't provide their complete code, there's enough information to understand the issue and investigate a fix. The solution would likely involve adding better error handling in the _format_results function to gracefully handle cases where no values are returned.\n\nThe only missing piece is the exact code the user is running that triggers this error, but the traceback and description provide sufficient context to work with.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12973": {
        "explanation": "This issue is well-specified with a clear problem description and proposed solution. The reporter has:\n\n1. Identified a specific bug in the `LassoLarsIC` class where the `copy_X` parameter appears both as a class initialization parameter and as a parameter in the `fit` method.\n2. Explained why this is problematic: when a user sets `copy_X=False` during initialization but then calls `fit(X, y)` without explicitly specifying `copy_X`, the default value of `True` in the `fit` method silently overrides the user's choice.\n3. Verified this is inconsistent with other estimators in the linear models module.\n4. Proposed a specific solution: modify the `fit` method to have `copy_X=None` as default and only override the initialization value if explicitly passed.\n\nThe issue includes links to the relevant code and explains the technical details clearly. A developer could implement the solution without needing additional information. The proposed fix is also backward compatible, addressing potential concerns about breaking existing code.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12983": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies a bug where Gradient Boosting fails when initialized with a scikit-learn estimator. The problem is explained: the predict output from the initial estimator doesn't have the expected shape. The issue also mentions that there was no test for this functionality.\n\nThe edit adds another requirement: to accept initial estimators that don't support sample weights, as long as the gradient boosting itself isn't fitted with sample weights.\n\nWhile the issue doesn't provide detailed technical implementation details or code examples of the failure, it gives enough information for a developer familiar with scikit-learn's gradient boosting implementation to:\n1. Understand the bug (shape mismatch in prediction output)\n2. Know what needs to be fixed (handling sklearn estimators as init parameter)\n3. Know what to test (init parameter with sklearn estimators)\n4. Handle the sample weights edge case\n\nA developer would need to examine the codebase to understand the specific shape requirements and how to properly handle the initialization, but the issue provides sufficient direction to begin implementing a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-12989": {
        "explanation": "This issue is well-specified and clear. It identifies a specific inconsistency between two related components in scikit-learn:\n\n1. The `NMF` class has a default initialization parameter of `init=None`\n2. The `non_negative_factorization` function has a default initialization parameter of `init='random'`\n\nThe issue also references a previous issue (#11667) for context and proposes a specific solution: changing the default in `non_negative_factorization` with a deprecation process to make it consistent with `NMF`.\n\nTo solve this issue, a developer would need to:\n1. Verify the inconsistency between the two components\n2. Implement a deprecation warning when using the current default in `non_negative_factorization`\n3. Plan to change the default to match `NMF` in a future release\n4. Update documentation accordingly\n\nThe issue provides all the necessary information to understand the problem and implement a solution without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13010": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is clearly defined: when using StandardScaler on a large float16 numpy array, the mean and standard deviation calculations overflow, resulting in NaN values instead of the expected normalized array.\n\nThe user has provided:\n1. A complete code snippet that reproduces the issue\n2. The exact error messages received\n3. The expected behavior (normalized array)\n4. The actual results (array of NaN values)\n5. Detailed version information for the relevant libraries\n6. A suggestion for a potential fix (adding dtype to the mean/std calculation)\n\nThe issue is related to numeric precision handling in StandardScaler when working with float16 data. The user even suggests a potential solution path (specifying dtype in the mean/std calculations), which indicates they understand the problem well.\n\nWith this information, a developer could reproduce the issue, understand the cause (overflow in numeric calculations with float16), and implement a solution that handles float16 arrays properly in StandardScaler without requiring users to manually convert to higher precision types.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13013": {
        "explanation": "This issue is well-specified with clear objectives:\n\n1. The problem is clearly identified: manual checks for fitted estimators are being used instead of the standard `check_is_fitted` method from `utils.validation`.\n\n2. Specific examples are provided with direct links to the code that needs to be changed.\n\n3. The expected result is explicitly stated: replace manual checks with the `check_is_fitted` method.\n\n4. The motivation is clear: to ensure consistency in error messages across the codebase.\n\nThe task involves:\n1. Identifying places where manual checks are implemented instead of using `check_is_fitted`\n2. Replacing these manual checks with calls to the standard method\n\nThis is a straightforward refactoring task with clear acceptance criteria. The submitter even suggests a methodology (using GitHub search functionality) to find the instances that need to be fixed. There's no ambiguity about what constitutes a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13017": {
        "explanation": "This issue is well-specified with clear steps to reproduce, expected results, and actual results. The problem is clearly defined: when using negative column indices in ColumnTransformer, the behavior is inconsistent with positive indices. The issue demonstrates this with a minimal code example showing that:\n\n1. When using a negative index (-1), the transformed result includes the original column (the last column appears twice - once transformed and once in the original data)\n2. When using a positive index (2), the transformed result correctly replaces the original column\n\nThe expected behavior is clearly stated: either raise an error for negative indices or produce the same result as with positive indices. The actual behavior shows the inconsistency with concrete output.\n\nThe issue provides all necessary information to understand and address the problem, including the specific component (ColumnTransformer), the exact behavior in question (handling of negative indices), and a reproducible test case with clear outputs showing the discrepancy.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13087": {
        "explanation": "This issue is well-specified with clear requirements and context:\n\n1. The user wants to add support for arbitrary bin spacing in the `calibration.calibration_curve` function, similar to how numpy's histogram function allows custom bin edges.\n\n2. The problem is clearly explained: when using calibration curves with uncalibrated models that produce skewed probability distributions (mostly near 0), evenly-spaced bins lead to poor results with some bins having many points and others having few or none.\n\n3. The user has provided a working code example of their workaround solution that demonstrates the desired functionality.\n\n4. They've included a visual comparison showing the difference between the current implementation and their proposed approach, which clearly illustrates the problem.\n\n5. The user has specified their environment details.\n\nThe request is specific, the motivation is clear, and the user has even provided a working implementation that could serve as a starting point for the PR. A developer would have enough information to implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13124": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The user has identified a potential bug or documentation issue in scikit-learn's StratifiedKFold implementation. \n\nThe issue is that:\n1. The documentation states that the `shuffle` parameter should \"shuffle each stratification of the data before splitting into batches\"\n2. However, the user demonstrates with code that instead of shuffling samples within each stratum, only the order of batches is shuffled\n3. The user shows that regardless of the `shuffle` parameter value, the same pairs of indices are always grouped together (e.g., 1 with 11, 2 with 12, etc.)\n4. The user provides complete code to reproduce the issue, along with the actual output showing the problem\n5. The user clearly explains why this behavior is problematic for cross-validation\n\nThe issue includes all necessary information:\n- Detailed description of the problem\n- Complete reproduction code\n- Expected vs. actual results\n- Version information for the relevant software\n\nA developer could take this information and immediately begin investigating the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13135": {
        "explanation": "This issue is well-specified with clear information about the problem and how to reproduce it:\n\n1. The problem is clearly identified: KBinsDiscretizer with strategy='kmeans' fails because the bin_edges are not sorted, which causes np.digitize to throw an error.\n\n2. A minimal reproducible example is provided with specific code that demonstrates the issue.\n\n3. The error message is included, showing exactly where the failure occurs (ValueError: bins must be monotonically increasing or decreasing).\n\n4. The expected behavior is stated: no error should be thrown.\n\n5. The issue provides context about when this might occur in real-world scenarios, not just in the test case.\n\n6. System and dependency versions are provided.\n\nThe root cause is also identified: the kmeans strategy can produce unsorted centers/bin_edges, which conflicts with np.digitize's requirement for monotonically increasing or decreasing bins.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Understand the root cause\n3. Implement a fix (likely by sorting the bin_edges before passing them to np.digitize)\n\nNo additional information is needed to understand and fix this issue.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13142": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is clearly defined: when using GaussianMixture with n_init>1, the fit_predict(X) and predict(X) methods produce different results, which should not happen. The issue includes:\n\n1. A complete code snippet that reproduces the issue\n2. The expected behavior (no exceptions)\n3. The actual behavior (assertion error with detailed output showing the mismatch)\n4. Detailed version information for the environment\n\nThe issue is specific to the GaussianMixture class in scikit-learn when the n_init parameter is set to a value greater than 1. The problem is that fit_predict(X) and predict(X) should return the same cluster assignments for the same data, but they don't when n_init>1.\n\nA developer would have all the information needed to investigate and fix this issue without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13143": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is that the `precision_score` function in scikit-learn is returning an unexpected value for the first element of the array when comparing identical arrays.\n\nThe issue shows:\n1. Clear code to reproduce: Using `precision_score(A,B, average=None)` where A and B are identical arrays\n2. Expected output: `array([ 1., 1., 1.])` (since identical arrays should have perfect precision)\n3. Actual output: `array([ 0., 1., 1.])` (first element is 0 instead of 1)\n4. Complete version information for all relevant libraries\n\nThe issue is specific to the precision_score function in scikit-learn 0.18.1, and the unexpected behavior is clearly demonstrated. A developer would have all the information needed to investigate why the precision calculation for the first class is returning 0 instead of 1 when comparing identical arrays.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13157": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a discrepancy between two parts of the scikit-learn codebase:\n\n1. The `r2_score` function has been updated to use \"uniform_average\" as the default value for the `multioutput` parameter in version 0.19.\n2. However, in `base.RegressorMixin`, the default is still set to \"variance_weighted\".\n\nThe issue also references a specific GitHub issue (#5143) for context and points to a specific code location in the multioutput.py file that contains related \"strange things.\"\n\nWhile the issue doesn't explicitly state what the solution should be (whether to update RegressorMixin to match r2_score or vice versa), it's reasonable to infer that the goal is to make the defaults consistent across the codebase. A developer with access to the full codebase and the referenced issue #5143 would likely have enough information to implement a solution that aligns these defaults.\n\nThe only minor ambiguity is what the \"strange things\" are in the referenced code, but the link to the specific code lines provides enough context for investigation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13165": {
        "explanation": "This issue is well-specified with clear context and a solution approach. The issue references a specific bug (#13194) related to ensuring monotonic bin edges for KBinsDiscretizer when using the \"quantile\" strategy. The description explains that there's a problem with numeric instabilities in the percentiles returned from np.percentile, and proposes a specific solution: enforcing monotonicity by applying a maximum operation on subsequent values. The submitter even acknowledges that this fix is a no-op in most cases but argues it's worth implementing for robustness. With this information, a developer would understand exactly what needs to be fixed, why it needs fixing, and how to implement the solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13174": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies the problem: AdaBoost ensemble methods currently require X to be an array or sparse matrix of numerics, but since AdaBoost only passes this data to its base estimator without directly processing it, this constraint is unnecessarily restrictive.\n\nThe goal is to minimize validation of X in AdaBoost and potentially other ensemble methods to allow for more flexible input types (like lists of text blobs) that the base estimator can handle.\n\nWhat makes this reasonably specified rather than well-specified:\n1. It doesn't provide specific implementation details or code examples showing the current validation constraints\n2. It mentions \"other ensemble methods\" might be affected but doesn't specify which ones\n3. It doesn't clarify exactly what validation should remain versus what should be removed\n4. It doesn't specify test cases or examples of the desired behavior\n\nHowever, an experienced developer familiar with scikit-learn's codebase could reasonably:\n1. Locate the AdaBoost implementation\n2. Identify the input validation code that's restricting X to numeric arrays/matrices\n3. Modify the validation to be less restrictive while ensuring the base estimator receives compatible input\n4. Apply similar changes to other ensemble methods that have the same issue\n\nThe reference to issue #7767 also provides additional context that could help guide the implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13253": {
        "explanation": "This issue is reasonably well-specified. It describes:\n\n1. The goal: Support DataFrames in OneHot/OrdinalEncoder without converting to array\n2. The motivation: Avoid converting mixed-dtype DataFrames to full object arrays\n3. The approach: Process the input column by column to preserve datatypes\n4. Potential impact: Slight change in behavior regarding the dtype of categories_\n5. Implementation guidance: Initially, still use check_array on each column\n\nThe issue provides enough context and direction for a developer to understand what needs to be done. It references previous discussions and related issues, which would provide additional context.\n\nThere are some details that might need to be filled in during implementation:\n- Exactly how to handle the column-by-column processing\n- How to handle the edge cases with mixed dtypes\n- Testing strategy for the new functionality\n\nHowever, these are implementation details that a developer could reasonably work out based on the existing codebase and the guidance provided. The core requirement is clear: modify OneHot/OrdinalEncoder to work with DataFrames without converting them to arrays first.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13283": {
        "explanation": "This issue is well-specified with clear details about the problem and a potential solution:\n\n1. The problem is clearly identified: The Isolation Forest implementation consumes excessive memory during anomaly score calculation, particularly when using a large number of estimators.\n\n2. The specific code causing the issue is identified (lines 267-281 in iforest.py), showing the for loop that calculates anomaly scores for all trees together.\n\n3. The issue includes specific steps to reproduce the problem: Run Isolation Forest with different numbers of estimators and observe memory consumption.\n\n4. The expected results section proposes a specific solution: Move the anomaly score calculation to the base estimator class so it's done for each tree individually rather than all at once.\n\n5. The actual results section clearly describes the observed behavior: Memory consumption increases significantly with more estimators, with a particular spike during the final call to `average_path_length()`.\n\n6. A document with memory consumption details is attached for reference.\n\nThe issue provides enough information for a developer to understand the problem, reproduce it, and implement the suggested solution or an alternative approach to reduce memory consumption during anomaly score calculation in Isolation Forest.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13302": {
        "explanation": "This issue description is not well-specified enough for a meaningful attempt at a solution. The description is marked as \"WIP\" (Work in Progress) and contains a TODO list rather than a clear explanation of the problem. \n\nThe title suggests there's an issue with Ridge regression not casting to float64 when using SAG/SAGA solvers, but there's no explanation of:\n1. What the actual problem behavior is\n2. What the expected behavior should be\n3. Steps to reproduce the issue\n4. Any error messages or code examples\n\nThe description mentions building upon another PR (#11155) and suggests there might be casting rule issues between classes, but doesn't provide any specific details about what needs to be fixed. The TODO items are vague and don't provide actionable information for someone trying to solve the issue.\n\nWithout more specific information about the actual problem, expected behavior, and implementation details, it would be difficult to create a meaningful solution.",
        "label": "VAGUE"
    },
    "scikit-learn__scikit-learn-13313": {
        "explanation": "The issue description points out that two specific test functions in the scikit-learn codebase (`check_class_weight_balanced_classifiers` and `check_class_weight_balanced_linear_classifier`) appear to be defined but never called or executed. The reporter provides evidence by showing the result of a git grep command that finds the function definition but presumably no calls to these functions.\n\nThis is a reasonably clear issue that can be investigated and resolved by:\n1. Confirming whether these test functions are indeed never called\n2. If they aren't called, either:\n   - Adding appropriate calls to these test functions in the test suite\n   - Removing them if they're obsolete\n   - Or documenting why they exist but aren't called\n\nThe issue is specific enough to identify the exact functions in question and the problem (they're not being executed). A developer with access to the codebase should be able to investigate and fix this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13328": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The problem is clearly defined: HuberRegressor.fit() fails with a TypeError when given boolean input arrays, while other regression models like LinearRegression handle boolean arrays by converting them to float. The issue includes:\n\n1. Complete code to reproduce the issue\n2. Clear error message and traceback\n3. Expected behavior (automatic conversion of boolean arrays to float)\n4. Demonstration that the workaround (manually converting to float) works\n5. Version information\n\nThe issue is specific and actionable - the solution would involve modifying HuberRegressor to properly handle boolean input arrays by converting them to float, similar to how other regression models in scikit-learn handle this case. The error occurs in the _huber_loss_and_gradient function when it tries to use the negative operator on a boolean array, which is not supported in NumPy.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13333": {
        "explanation": "This issue is well-specified with clear objectives:\n\n1. The problem is clearly identified: The documentation for `n_quantiles` parameter in `QuantileTransformer` needs improvement because:\n   - It doesn't explain that setting `n_quantiles > n_samples` doesn't improve the transformation\n   - It doesn't clarify when to use `n_quantiles < n_samples`\n   - It doesn't explain the relationship between `n_quantiles` and the actual transformation behavior\n\n2. The issue provides concrete examples demonstrating the behavior:\n   - Shows that using `n_quantiles=n_samples` and `n_quantiles=10000` (much larger) produces identical results\n   - Shows that using `n_quantiles=200` with `n_samples=100` produces different results than using `n_quantiles=n_samples`\n\n3. The issue suggests what information should be added to the documentation\n\nA developer can understand exactly what needs to be done: update the documentation of the `n_quantiles` parameter in `QuantileTransformer` to better explain its behavior, particularly regarding the relationship with `n_samples` and when different values are appropriate. The examples provided give clear guidance on what information should be included.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13363": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The bug report includes:\n\n1. A clear description of the problem: `ridge_regression` with `return_intercept=True` raises an `UnboundLocalError`.\n2. A minimal code example that reproduces the issue.\n3. The expected result: no error should be raised, and a tuple with the coefficient array and intercept should be returned.\n4. The actual result: a detailed error traceback showing the `UnboundLocalError` for the `intercept` variable.\n5. Version information for the relevant software.\n\nThe issue is clearly a bug in the implementation of the `ridge_regression` function where the `intercept` variable is being referenced before it's assigned a value when `return_intercept=True` is specified. The solution would involve ensuring that the `intercept` variable is properly initialized or assigned before being returned in this code path.\n\nThis is a straightforward bug fix that can be implemented by examining the code in the `ridge.py` file, specifically around line 452 where the error occurs.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13368": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is clearly defined: when using `cross_val_predict` with a dataset that has very few samples in one class (in this case, just one sample for class 2), the function returns incorrect predictions (all zeros) instead of proper probability estimates.\n\nThe issue includes:\n1. A complete code example that reproduces the problem\n2. The expected output (probability estimates for each class)\n3. The actual output (all zeros)\n4. Information about the version where the issue occurs\n\nThe root cause is also hinted at in the description - when a class has very few samples, it might be completely missing from some cross-validation splits, causing problems in the prediction process.\n\nA developer would have enough information to investigate why `cross_val_predict` fails when a class is missing from some CV splits and implement a fix to handle this edge case properly. The solution would likely involve ensuring that `cross_val_predict` can handle situations where certain classes are absent in some folds.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13392": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The task is to add common tests for multi-output support in classifiers and regressors in scikit-learn.\n2. The issue references a previous discussion (#11458) for context.\n3. It explicitly mentions removing redundant individual tests introduced by a specific commit (95993a4b2b7d067d8d7fff91ccb2463dbd427e7c).\n4. It provides example code showing how to test multi-output functionality for both classifiers and regressors.\n5. It specifies the expected results: no errors should be thrown for these checks, and some estimators should be excluded from testing.\n\nThe issue provides sufficient information for a developer to:\n1. Understand what needs to be implemented (common tests for multi-output)\n2. Know which existing code to remove (redundant individual tests)\n3. Have a reference implementation to base the common tests on\n4. Understand the expected behavior of the tests\n\nThe only minor detail that could be clarified is which specific estimators should be excluded from testing, but this information could likely be found in the referenced discussion or by examining the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13436": {
        "explanation": "This issue is well-specified and clear. The user is reporting a problem with the pretty print representation of nested Pipeline objects in scikit-learn. They provide:\n\n1. A complete code example that reproduces the issue\n2. The actual output they're seeing\n3. A clear explanation of what's confusing about the current representation (the outer pipeline structure is not fully visible due to truncation)\n4. A suggestion for improvement (trim representation step by step to preserve the outer pipeline structure)\n\nThe issue is about improving the readability of the string representation of complex nested Pipeline objects, specifically ensuring that the structure of the outer pipeline remains visible even when the full representation is too long. The problem is clearly demonstrated with the example where the 'classifier' step disappears in the truncated output.\n\nThis is a well-defined UI/UX improvement request with a clear problem statement and a reasonable suggestion for how to address it. A developer could implement a solution based on this description without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13439": {
        "explanation": "This issue is well-specified with a clear problem statement and reproduction steps. The issue describes that the Pipeline class in scikit-learn should implement the `__len__` method to support operations like `pipe[:len(pipe)]` which currently raises an error.\n\nThe description includes:\n1. A clear problem statement: Pipeline should implement `__len__`\n2. A specific context: With the new indexing support, `pipe[:len(pipe)]` raises an error\n3. Complete reproduction code that demonstrates the issue\n4. Environment details including Python and library versions\n\nFrom this information, I can understand that:\n- The Pipeline class already supports indexing (mentioned as \"new indexing support\")\n- However, it doesn't implement the `__len__` method, which is needed for operations like `len(pipe)`\n- The solution would involve adding a `__len__` method to the Pipeline class that returns the number of steps in the pipeline\n\nThis is a straightforward feature request with clear expectations for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13447": {
        "explanation": "This issue is well-specified with clear details about the bug in the `label_ranking_average_precision_score` function. The description identifies the exact problem: when samples have zero true labels (all false), the sample weighting is not properly applied because the special case handling bypasses the sample weight application.\n\nThe issue includes:\n1. Precise description of the bug with links to the relevant code lines\n2. A complete, minimal code example that reproduces the issue\n3. Clear expected vs. actual results with numerical values\n4. Detailed version information\n\nThe solution path is clear: modify the function to ensure sample weights are properly applied even in the special case where a sample has all false labels. The specific lines of code that need to be fixed are identified (line 740), and the expected behavior is well-defined. A developer could implement and test a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13454": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue identifies two specific problems:\n\n1. The error message in OrdinalEncoder is using outdated terminology (`n_values` instead of `categories`), making it confusing for users.\n2. There might be a way to detect and handle this specific case (passing a single list instead of a list of lists for categories).\n\nThe example code clearly demonstrates the issue, showing exactly how to reproduce the error. The error message is included, and the issue explains why it's confusing (using old terminology). The user has also provided detailed version information for their system, Python, and relevant libraries.\n\nA developer would be able to:\n1. Locate the error message in the OrdinalEncoder implementation\n2. Update the error message to use the current terminology (`categories` instead of `n_values`)\n3. Potentially implement detection for the case where a user passes a single list instead of a list of lists\n\nThis is a straightforward bug fix with clear acceptance criteria - updating the error message and possibly adding validation logic.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13472": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: using a Pipeline as the initial estimator in GradientBoostingRegressor fails due to incompatible function signatures.\n\n2. A complete, reproducible code example is provided that demonstrates the error.\n\n3. The error message is included, showing exactly where the problem occurs.\n\n4. The user has analyzed the root cause: the Pipeline.fit() method expects sample_weight as a keyword argument, but GradientBoostingRegressor is passing it as a positional argument.\n\n5. The user even proposes a specific solution: changing line 1421 in gradient_boosting.py to use sample_weight as a keyword argument instead of a positional argument.\n\n6. Version information is provided, making it clear which version of scikit-learn has this issue.\n\nThis issue contains all the necessary information to understand the problem, reproduce it, and implement a fix. The proposed solution is likely correct and could be verified by examining the code and testing the fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13485": {
        "explanation": "This issue is well-specified. The user clearly identifies:\n\n1. The specific problem: `CalibratedClassifierCV` uses `check_array` with default parameters that doesn't allow 3D inputs, which prevents them from using it with their package's 3D data.\n\n2. The context: They're working with the metric-learn package and need to use `CalibratedClassifierCV` for Weakly Supervised Learners to get probability estimates.\n\n3. A concrete example of their input data format: `pairs=[[[2.3, 5.4], [4.4, 5.6]], [[7.5, 1.2], [4.4, 5.6]]]`\n\n4. A comparison point: Other meta-estimators like `GridSearchCV` don't call `check_array` and work fine with their use case.\n\n5. A proposed solution: Remove the `check_array` call if it's not essential, and they're willing to provide a PR.\n\nThe issue provides enough information to understand the problem, reproduce it, and evaluate potential solutions. A developer could investigate whether the `check_array` call in `CalibratedClassifierCV` is necessary or if it could be modified/removed to support 3D arrays without breaking existing functionality.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13496": {
        "explanation": "This issue is well-specified with clear requirements for implementation. The user is requesting to expose the `warm_start` parameter in the `IsolationForest` class, which is already available in its parent class `BaseBagging` but not explicitly exposed in the `IsolationForest.__init__()` method.\n\nThe issue provides:\n1. Clear context about the current state (the parameter exists but is not exposed)\n2. Specific implementation requirements:\n   - Expose `warm_start` in `IsolationForest.__init__()` with default value `False`\n   - Add proper documentation for the parameter\n   - Add tests to ensure functionality\n   - Update example documentation\n\nThe user has even provided the exact documentation text to use, making the requirements very explicit. They've also verified that the functionality already works when manually setting the attribute after initialization, so this is primarily about improving the API and documentation rather than implementing new functionality.\n\nA developer could implement this change without needing additional clarification, as all the necessary details are provided.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13497": {
        "explanation": "The issue is reasonably well-specified. It identifies a specific problem in the `_estimate_mi` function where there's a comparison between a string ('auto') and potentially an array (discrete_features), which will cause errors in future versions of NumPy. The issue also points out a potential testing gap where the function might not be tested with discrete_features values other than 'auto'.\n\nFrom this description, I can understand:\n1. The specific function affected (`_estimate_mi`)\n2. The exact line of code causing the issue (`discrete_features == 'auto'`)\n3. The nature of the problem (comparing string to array)\n4. The potential consequences (errors in future NumPy versions)\n\nWhat's missing is:\n1. The exact file location of this function\n2. The expected correct behavior or suggested fix\n3. The specific NumPy warning/error that's being triggered\n\nHowever, these gaps can be filled by examining the codebase to locate the function, understanding its purpose, and implementing a fix that properly handles the comparison between discrete_features and 'auto' in a way that's compatible with future NumPy versions. The fix would likely involve checking the type of discrete_features before comparing it to 'auto'.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13536": {
        "explanation": "This issue is well-specified with a clear problem and expected solution. The issue describes:\n\n1. The current behavior: When users try to pass `sample_weight` directly to a Pipeline (instead of prefixing it with the step name), they get a cryptic error message: \"ValueError: not enough values to unpack (expected 2, got 1)\".\n\n2. The expected behavior: The error message should be more user-friendly and explicitly explain the correct format for passing `sample_weight` to a step in a Pipeline (i.e., using the `stepname__sample_weight` format).\n\n3. A reproducible example that demonstrates the issue, including the exact error message.\n\nThe solution would involve modifying the error handling in the Pipeline class to catch this specific case and provide a more informative error message when users try to pass `sample_weight` without the proper prefix. This is a straightforward enhancement to improve user experience with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13549": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The user wants to add stratified subsampling functionality to scikit-learn, specifically by adding a `stratify` option to the existing `utils.resample` function.\n\nThe issue clearly explains:\n1. The current problem: Using `train_test_split` for subsampling has limitations when requesting all samples or nearly all samples\n2. The specific error cases encountered\n3. The proposed solution: Add stratification capability to `utils.resample`\n\nThe user initially considered other options (adding a new `subsample()` utility or adding a `bypass_checks` parameter to `train_test_split`) but ultimately settled on enhancing `utils.resample` as the most appropriate solution.\n\nA developer would have a clear understanding of what needs to be implemented: modify the `utils.resample` function to accept and handle a `stratify` parameter similar to how `train_test_split` handles stratification, but without the limitations mentioned in the issue. The existing stratification code in scikit-learn could likely be leveraged for this implementation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13554": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is precisely defined: the `sklearn.metrics.pairwise.pairwise_distances` function produces different results from `np.linalg.norm` when using `np.float32` arrays, while they agree when using `np.float64` arrays.\n\nThe issue includes:\n1. A complete code snippet that reproduces the problem\n2. The exact expected output\n3. The exact actual output showing the discrepancy\n4. Version information for all relevant libraries\n5. A clear description of what's happening and what should happen instead\n\nThe issue also includes information about a work-in-progress solution that implements block-wise casting to float64 to compute the euclidean distance matrix with better precision while using a fixed amount of additional memory.\n\nA developer would have all the necessary information to understand the problem, reproduce it, and implement a solution that addresses the numerical precision issues when working with float32 arrays.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13584": {
        "explanation": "This issue is well-specified with a clear bug report. The description includes:\n\n1. A complete, minimal reproducible example with the exact code that triggers the bug\n2. The specific error message that occurs when running the code\n3. The context that this is related to the `print_changed_only` setting in sklearn's representation system\n4. The specific component affected (LogisticRegressionCV with numpy array for Cs parameter)\n\nThe issue is clearly about how sklearn's representation system handles numpy array values when `print_changed_only=True` is set. The error occurs because the code is trying to evaluate a numpy array in a boolean context, which raises the specific ValueError shown.\n\nA developer can reproduce this issue easily with the provided code and would understand that the fix needs to address how vector/array values are handled in the representation system when comparing default vs. provided parameters. The solution would likely involve modifying how sklearn checks if a numpy array parameter differs from its default value.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13620": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Clearly identified the bug: feature importances in Gradient Boosting models don't sum to 1 after a certain number of estimators\n2. Provided complete code to reproduce the issue\n3. Included the exact output showing the problem\n4. Provided additional code to investigate the issue at a deeper level\n5. Shared a hypothesis about the cause (floating point calculation error)\n6. Included version information for all relevant software\n\nThe issue demonstrates that after 711 estimators, the feature importance sum drops below 1 and eventually individual trees show 0 for feature importance. This is unexpected behavior since feature importances should sum to 1.\n\nThe expected solution would be to fix the calculation of feature importances in the GradientBoostingRegressor class to ensure they always sum to 1, or to document this behavior if it's an expected limitation. The issue provides enough context for a developer to investigate the code responsible for calculating feature importances and identify where the problem occurs.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13628": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The reporter has identified a potential bug in the `brier_score_loss` function where:\n\n1. When target=1 and prediction=1 (perfect prediction), the score is 1 (worst) instead of the expected 0 (best)\n2. When target=1 and prediction=0 (worst prediction), the score is 0 (best) instead of the expected 1 (worst)\n\nThe reporter provides specific test cases with inputs and outputs that demonstrate the issue. They also suggest the root cause: the `_check_binary_probabilistic_predictions` function is converting the target incorrectly when there's only one class present.\n\nThis is a clear bug report with:\n- Specific function identified (`brier_score_loss`)\n- Expected vs. actual behavior clearly stated\n- Reproducible examples with exact inputs and outputs\n- A hypothesis about the root cause\n\nA developer could investigate this issue without needing additional clarification, by examining the implementation of `brier_score_loss` and `_check_binary_probabilistic_predictions` to verify the behavior and fix the issue if confirmed.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13641": {
        "explanation": "The issue is well-specified and provides a clear example of the problem. The issue describes that when using CountVectorizer with both a custom analyzer function and the input parameter set to 'filename' or 'file', the input parameter is being ignored. The example code demonstrates this by showing that the vectorizer is treating the input as content rather than as a filename.\n\nThe issue includes:\n1. A clear description of the problem\n2. A reproducible code example\n3. The expected behavior (implicitly that the input parameter should be respected)\n4. The current behavior (the input parameter is ignored)\n\nThe reporter even suggests possible solutions (either fix the behavior or document it). This gives a clear understanding of what needs to be investigated and potentially fixed in the CountVectorizer implementation to ensure that custom analyzers properly respect the input parameter.\n\nA developer could take this information and:\n1. Verify the issue by running the example\n2. Investigate the CountVectorizer code to see why custom analyzers ignore the input parameter\n3. Implement a fix or update documentation as appropriate\n\nThe only minor ambiguity is whether to fix the behavior or just document it, but this is a decision that can be made after investigating the issue, and doesn't prevent understanding what the problem is.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13704": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is clearly defined: VarianceThreshold() is not removing a column with zero variance in a specific case (the \"broken\" array) while it works correctly in another case (the \"works_correctly\" array).\n\nThe issue includes:\n1. Complete code to reproduce the problem\n2. The expected output\n3. The actual output\n4. Evidence that the first column has only one unique value (zero variance) via `print(set(broken[:, 0]))` showing `{-0.13725701}`\n5. System information and library versions\n\nThe only difference between the working and broken examples appears to be that the broken array has one additional row, suggesting a potential edge case in the VarianceThreshold implementation. This is a well-defined bug that can be investigated and fixed without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13779": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue describes that the VotingClassifier fails when:\n1. A sample_weight parameter is passed to fit()\n2. One of the estimators is set to None\n\nThe error occurs because the code doesn't check if an estimator is None before trying to call its fit method when sample_weight is provided. The issue includes a complete code example that reproduces the error and shows the exact error message. The expected behavior is implied - the VotingClassifier should handle None estimators gracefully even when sample_weight is provided.\n\nThe solution would involve adding a check for None estimators in the sample_weight handling code path of the VotingClassifier's fit method. This is a specific, focused bug with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13780": {
        "explanation": "The issue is reasonably well-specified. It clearly states that the `VotingClassifier` and `VotingRegressor` classes should be modified to accept a string value 'drop' to drop an estimator from the ensemble, in the same way that `None` currently works for this purpose. \n\nThe issue includes a reference to a specific GitHub PR discussion that provides context. While the issue doesn't explicitly detail the implementation approach or provide code examples, the functionality request is clear: make 'drop' an alternative to None for dropping estimators.\n\nA developer familiar with scikit-learn's codebase, particularly the VotingClassifier and VotingRegressor implementations, would understand what needs to be changed. The implementation would likely involve modifying the code that handles None values to also handle the string 'drop' in the same way.\n\nThe only minor ambiguity is whether 'drop' should completely replace None as the mechanism for dropping estimators, or if both options should be supported. However, the most reasonable interpretation based on the description is that both options should be supported, with 'drop' being an additional way to achieve the same functionality.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13828": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The user is reporting that the AffinityPropagation clustering algorithm in scikit-learn doesn't support sparse affinity matrices when using the 'precomputed' affinity parameter.\n\nThe issue includes:\n1. A clear problem statement: AffinityPropagation doesn't support sparse affinity matrices\n2. A minimal code example that reproduces the issue\n3. The expected behavior: no error should be raised since it works for dense matrices\n4. The actual error message and traceback\n5. Detailed version information about the environment\n\nThe error occurs in the affinity_propagation function when it tries to compute the median of the sparse matrix, which fails because sparse matrices don't support certain operations that dense matrices do.\n\nThis is a clear feature request/bug report that could be addressed by modifying the AffinityPropagation implementation to handle sparse matrices properly when affinity='precomputed'. The solution would likely involve converting the sparse matrix to a dense format at the appropriate point or implementing the algorithm to work directly with sparse matrices.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13910": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug in the `euclidean_distances` function in scikit-learn 0.21.1 that wasn't present in version 0.20.3.\n\n2. The problem is clearly defined: when using float32 data, the function produces incorrect results (many 0 entries and some unreasonably high values) compared to scipy's `cdist` function.\n\n3. Complete reproduction steps are provided with a code snippet that demonstrates the issue.\n\n4. The test data is provided as an attachment (wont.zip).\n\n5. Expected results are clearly described and shown in an attached PDF (the behavior in sklearn 0.20.3 which matches scipy's cdist).\n\n6. Actual results are clearly described and shown in an attached PDF (the incorrect behavior in sklearn 0.21.1).\n\n7. Detailed version information is provided for both environments.\n\nThe issue title suggests it might be related to float32 overflow handling, which gives a starting point for investigation. A developer would have all the necessary information to reproduce the issue, identify the root cause in the code changes between 0.20.3 and 0.21.1, and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13915": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, actual results, and version information. The problem is clearly defined: when using `min_max_axis` on a CSC matrix with `indptr.dtype` as int64 on 32-bit Windows, a TypeError occurs because it cannot cast from int64 to int32. The error message is provided in full, showing exactly where the problem occurs in the code. The reproduction code is simple and complete, demonstrating that the function works with CSR matrices but fails with CSC matrices when the indptr is int64. The environment details are comprehensive, including Python version, OS, and relevant dependencies. A developer would have all the information needed to diagnose and fix this issue.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13933": {
        "explanation": "This issue is reasonably well-specified. It describes a potential data leakage problem in the Gradient Boosted Decision Trees (GBDTs) implementation where validation data is being used in the binning process before train-test split occurs. \n\nThe issue clearly identifies:\n1. The current behavior: binning happens before train_test_split()\n2. The potential problem: validation data influences bin thresholds, creating a minor data leak\n3. The proposed solution: bin X_train and X_val separately\n\nWhat makes this reasonably (rather than perfectly) specified:\n- It doesn't provide code examples of the current implementation\n- It doesn't specify exactly how the binning should be implemented for train and validation separately\n- It doesn't clarify if this is a bug or a feature request\n- It asks for another developer's opinion (@ogrisel), suggesting there might be design considerations to discuss\n\nHowever, an experienced developer familiar with the codebase could understand the issue and implement a solution that bins training and validation data separately to prevent the data leakage, even with these minor gaps in specification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13960": {
        "explanation": "This issue is well-specified with clear problem description, reproduction steps, expected results, actual results, and even a suggested fix.\n\nThe issue describes that IncrementalPCA should accept sparse input matrices but currently doesn't. The user provides:\n\n1. A clear code example that reproduces the issue\n2. The exact error message received\n3. The expected behavior (no error should be thrown)\n4. A detailed suggested implementation that would fix the issue\n5. Version information for relevant libraries\n\nThe suggested fix shows how to modify the IncrementalPCA class to accept sparse matrices by:\n- Modifying the check_array call in fit() to accept sparse formats\n- Converting sparse matrices to dense arrays in partial_fit() and transform()\n- Handling batches appropriately\n\nThis is a well-defined feature request with a clear implementation path. A developer could implement this solution or a variation of it without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-13983": {
        "explanation": "The issue description \"Add matplotlib to show_versions()\" is quite brief, but it is reasonably clear what needs to be done. The request is to add matplotlib version information to an existing `show_versions()` function. \n\nThis function likely displays version information for various dependencies or components of a software package, and the request is to extend it to include matplotlib version information as well.\n\nWhile the description is minimal, for an experienced developer with access to the codebase, this should be straightforward to implement. The developer would need to:\n1. Find the existing `show_versions()` function in the codebase\n2. Understand how it currently displays version information for other dependencies\n3. Add similar code to display the matplotlib version\n\nThe only potential ambiguity is whether there are specific formatting requirements or if matplotlib should be displayed in a particular position in the version list, but these details could be inferred from the existing implementation pattern.\n\nGiven that the task is simple and focused, I believe this issue is reasonably specified for someone with access to the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14053": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible steps. The user reports that the `export_text` function in scikit-learn raises an `IndexError: list index out of range` when trying to export a decision tree that was trained on a single feature.\n\nThe issue includes:\n1. A clear description of the problem\n2. Complete, minimal code to reproduce the issue\n3. The exact error message received\n4. Version information for the relevant software\n\nThe code example is straightforward - it loads the Iris dataset, selects only the first feature, trains a decision tree classifier, and then attempts to export the tree as text using the `export_text` function. This fails with an index error.\n\nThe bug appears to be in the `export_text` function which doesn't properly handle the case when a tree is trained on only one feature. This is a specific edge case that can be investigated and fixed by examining the implementation of the `export_text` function.\n\nA successful solution would modify the `export_text` function to correctly handle trees with a single feature without raising an IndexError.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14067": {
        "explanation": "This issue is well-specified with clear evidence of a regression in the ARD Regressor's accuracy when upgrading from SciPy 1.2.1 to 1.3.0. The reporter has provided:\n\n1. A clear description of the problem: ARD Regressor accuracy degrades significantly (from error ~1E-5 to ~1E-2) when upgrading SciPy versions\n2. Complete reproducible code examples:\n   - A single-seed example showing the degradation\n   - Code to generate 1000 test datasets and evaluate performance\n   - Code to visualize the results\n3. Detailed output showing the difference in behavior between versions\n4. Statistical analysis of the results across 1000 datasets\n5. Visual evidence (histogram) showing the distribution of errors\n6. Environment details including exact versions of relevant packages\n\nThe issue contains everything needed to investigate and fix the problem:\n- The exact versions where the behavior changed (SciPy 1.2.1 \u2192 1.3.0)\n- A minimal reproducible example that clearly demonstrates the issue\n- Quantitative measurements of the performance degradation\n- Evidence that this is a systematic issue, not just a single case\n\nA developer would have all the information needed to reproduce the issue, investigate the cause in the ARD Regressor implementation, and determine if the problem is in scikit-learn or in SciPy's underlying numerical routines.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14087": {
        "explanation": "This issue is well-specified with all the necessary information to understand and reproduce the problem:\n\n1. Clear description of the issue: IndexError thrown when using LogisticRegressionCV with refit=False\n2. Complete reproducible code example with imports, data generation, and the exact function call that causes the error\n3. Expected results: No error should be thrown\n4. Actual results: Full error traceback showing the exact line where the error occurs\n5. Detailed version information for the system, Python, and all relevant dependencies\n\nThe error occurs in the LogisticRegressionCV implementation when refit=False is specified. The traceback points to a specific line in the sklearn code where an IndexError is thrown due to \"too many indices for array\". This suggests there's a bug in how the code handles the case when refit=False.\n\nWith this information, a developer could:\n1. Reproduce the issue exactly as described\n2. Trace through the code execution to understand why the IndexError occurs\n3. Develop a fix that properly handles the refit=False case\n\nThe issue is very specific and contains all the information needed to diagnose and fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14114": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible error. The user is reporting that AdaBoost's \"SAMME\" algorithm has inconsistent behavior: it uses the `predict` method during fitting but requires `predict_proba` during prediction of probabilities. This inconsistency causes errors when using base estimators like SVC that don't implement `predict_proba` by default (without setting `probability=True`).\n\nThe error trace clearly shows the problem: when using AdaBoostClassifier with the SAMME algorithm and an SVC base estimator, calling `predict_proba()` fails with a NotImplementedError because the SVC instance doesn't have probability estimation enabled.\n\nThe solution would involve modifying the AdaBoost implementation to either:\n1. Use the same method (`predict` or `predict_proba`) consistently\n2. Check if the base estimator supports `predict_proba` before using it\n3. Handle base estimators that don't support probability estimation in a more graceful way\n\nThis is a concrete bug with a clear reproduction case and a reasonable path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14141": {
        "explanation": "The issue is reasonably specified. It clearly states that joblib should be added to the dependencies listed in `show_versions` or to the issue template when scikit-learn version is greater than 0.20. \n\nFrom this description, I understand:\n1. The task involves adding joblib to a list of dependencies in a function called `show_versions` or in an issue template\n2. This should only apply when scikit-learn version is > 0.20\n3. Joblib is likely a dependency of scikit-learn that's currently missing from the version reporting\n\nWhat's not explicitly specified but can be reasonably inferred:\n- The `show_versions` function is likely a utility that displays version information for dependencies\n- The issue template probably refers to GitHub issue templates that include version information\n- The implementation would involve modifying the code that handles dependency reporting\n\nA developer familiar with the scikit-learn codebase would be able to locate the `show_versions` function and the issue template, and add joblib to the appropriate places with the version condition. The task is straightforward and the requirements are clear enough to implement a solution without needing further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14237": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, actual behavior, and a detailed explanation of the problem. The user has:\n\n1. Provided a complete, minimal code example that reproduces the issue\n2. Clearly explained the expected behavior (remainder should reference columns by name, not position)\n3. Described the actual behavior (remainder remembers columns by numeric index)\n4. Identified the specific location in the codebase where the problem occurs\n5. Provided a workaround they're currently using\n6. Included detailed version information\n\nThe issue is about a bug in scikit-learn's ColumnTransformer when using named column indexing with the remainder parameter. When the column ordering in a DataFrame changes between fit and transform operations, the remainder incorrectly processes the wrong columns.\n\nThe solution would need to modify how ColumnTransformer handles remainder columns with DataFrames to ensure it tracks columns by name rather than position. This is a well-defined problem with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14450": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible code example. The issue describes that PLS (Partial Least Squares) regression in scikit-learn raises an error message \"array must not contain nan\" when a feature is constant (in this case, a column containing only zeros). The error message is misleading since the actual problem is not related to NaN values but to constant features.\n\nThe issue includes:\n1. A complete code snippet that reproduces the error\n2. A clear explanation of what causes the error (a column with only zeros)\n3. Even a hint at the solution (uncommenting a specific line solves the issue)\n\nThe problem is well-defined: either the error message should be more descriptive about the actual issue (constant features), or the implementation should handle constant features gracefully. A developer can understand what needs to be fixed without requiring additional information.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14464": {
        "explanation": "This issue is well-specified with a clear problem description and a complete reproducible example. The user has identified a bug in scikit-learn's clone functionality where attributes that are dictionaries with values set directly from constructor parameters are not properly preserved during cloning.\n\nThe issue includes:\n1. A clear description of the problem: when cloning a custom transformer, dictionary values set in __init__ are replaced with None\n2. Two complete code examples demonstrating the issue (MyTransformA with the bug and MyTransformB with a workaround)\n3. Expected vs actual results clearly showing the difference in behavior\n4. Complete version information for the environment\n\nThe issue demonstrates that when a parameter is directly assigned to a dictionary value in __init__ (as in MyTransformA), the clone operation fails to preserve that value. However, when the parameter is first assigned to a class attribute and then that attribute is used in the dictionary (as in MyTransformB), cloning works correctly.\n\nThis is a well-defined bug with a clear reproduction case that could be fixed by modifying scikit-learn's clone functionality to properly handle this case. The solution would likely involve ensuring that parameter values are properly preserved when they're used directly in dictionaries during initialization.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14496": {
        "explanation": "This issue is well-specified with clear information about the bug and a proposed solution:\n\n1. The problem is clearly identified: When using OPTICS clustering with a float value for min_samples, the code doesn't convert the calculated min_samples to an integer before passing it to NearestNeighbors, causing a TypeError.\n\n2. The exact location of the bug is specified: In the optics_.py file, lines 439-440 where min_samples is calculated but not converted to an integer.\n\n3. The error message is provided in full, showing the exact TypeError that occurs.\n\n4. A specific fix is proposed: Convert the calculated min_samples to an integer using int(round(...)) with a clear explanation of why this approach is chosen (for Python 2/3 compatibility).\n\n5. The issue includes a reproducible code example showing how to trigger the bug.\n\nThe information provided is sufficient for a developer to understand the issue, locate the problematic code, and implement the suggested fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14544": {
        "explanation": "This issue is discussing the input validation and requirements for the ColumnTransformer class in scikit-learn. The issue raises several specific questions that need to be addressed:\n\n1. Whether ColumnTransformer should have the same input validation requirements as other estimators\n2. Whether to check for column reordering (since ColumnTransformer works with named columns)\n3. Whether to allow additional columns in transform() when remainder is not used\n4. How to define n_features_in_ for ColumnTransformer\n\nThe issue provides context by referencing specific GitHub issues (#14237, #13603, #7242) and explains the current behavior. It clearly outlines the design decisions that need to be made.\n\nWhile the issue doesn't provide a specific implementation suggestion, it clearly articulates the problems and questions that need to be addressed. A developer familiar with scikit-learn's ColumnTransformer would have enough information to investigate the referenced issues and propose a solution that addresses the questions raised.\n\nThe issue is reasonably well-specified, though it would benefit from having a clearer statement of the desired outcome or behavior. However, it provides enough context and specific questions for a meaningful attempt at a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14591": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user has identified a specific issue in the LassoCV implementation where `precompute=False` is set before the final model fitting, regardless of user input.\n2. The exact location of the problem is provided (coordinate_descent.py's LinearModelCV.fit(), line 1223 in Python2.7/sklearn0.19.0).\n3. The user has described the performance impact (cross-validation is fast, but final fitting is extremely slow).\n4. They've verified that commenting out the line fixes the performance issue.\n5. The user is asking for the rationale behind this implementation choice.\n\nThe issue provides enough context to understand the problem, locate the code in question, and investigate why this design decision was made. A developer could reasonably attempt to fix this by:\n1. Examining the code to verify the issue\n2. Determining if there's a valid reason for forcing precompute=False\n3. Implementing a fix that respects the user's precompute parameter value\n4. Testing the solution with large datasets to verify performance improvement\n\nThe issue is specific, reproducible, and includes all necessary information to address it.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14629": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual error messages. The user has:\n\n1. Provided a complete, minimal code example that reproduces the error\n2. Clearly identified the specific error message: `AttributeError: 'MultiOutputClassifier' object has no attribute 'classes_'`\n3. Pointed to the specific location in the codebase where they believe the bug exists (with a GitHub link to the relevant code)\n4. Explained their understanding of the problem - that `MultiOutputClassifier` doesn't have a `classes_` attribute directly, but instead has it in `estimators_[i].classes_`\n5. Provided their environment details including Python and library versions\n6. Indicated what the expected behavior should be (returning prediction probabilities)\n\nThe issue is a clear bug report with a specific technical problem that can be addressed by modifying how `cross_val_predict` accesses the classes when working with a `MultiOutputClassifier`. The user has even offered to submit a patch if guidance is provided.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14710": {
        "explanation": "This issue is well-specified with clear information about the problem and a potential solution:\n\n1. The problem is clearly defined: HistGradientBoostingClassifier fails when using string target values with early stopping enabled.\n\n2. The issue includes:\n   - A complete, minimal code example that reproduces the error\n   - The exact error traceback showing the TypeError\n   - A clear explanation of the root cause: the scorer is provided with integer y_true while y_pred contains original string classes\n\n3. The submitter even provides a potential solution with a specific code diff that shows exactly where and how to fix the issue by converting the encoded integer targets back to their original string classes when computing scores.\n\nThe issue contains all the necessary information for a developer to understand the problem, reproduce it, and implement a fix. The proposed solution is reasonable and addresses the core issue, though a developer might want to verify it or consider alternative approaches.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14732": {
        "explanation": "The issue is reasonably well-specified. It points out that the `median_absolute_error` function does not currently support multioutput, and the user is asking whether this is an intentional design choice or if it's simply not been implemented yet. The user is offering to submit a PR to add this functionality if it's the latter case.\n\nWhile the issue doesn't provide extensive technical details about how multioutput should be implemented in `median_absolute_error`, it's clear what the user is asking about. The context suggests this is related to a machine learning library (likely scikit-learn based on the function name), and multioutput support is a common feature in many metrics.\n\nA developer familiar with the codebase would understand:\n1. What function needs to be modified (`median_absolute_error`)\n2. What capability needs to be added (multioutput support)\n3. That they need to determine if this omission was intentional or not\n\nThe only missing information is the specific implementation details of how multioutput should be handled, but this could be inferred from how other metric functions in the library handle multioutput.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14764": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The problem is clearly identified: The `make_classification()` function in sklearn's datasets module only accepts lists for the `weights` parameter, but not numpy arrays, which is inconsistent with other parameters.\n\n2. The issue includes complete test code that reproduces the problem, showing that using a list works but a numpy array fails.\n\n3. The error message is provided, showing the specific ValueError that occurs when using numpy arrays.\n\n4. The exact lines of code that need to be modified (lines 165 and 171) are identified with specific corrections.\n\n5. The expected behavior is clearly stated: numpy arrays should be accepted just like lists.\n\n6. The issue includes version information for relevant dependencies.\n\nThe proposed solution is straightforward - modify the code to check for the presence of weights using `all(weights)` instead of just `weights`, which would work for both lists and numpy arrays. This is a common pattern in Python to handle both types of sequences.\n\nA developer could implement this fix without needing any additional information or clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14806": {
        "explanation": "This issue is well-specified with clear examples demonstrating the current behavior and the desired behavior. The user is reporting that the IterativeImputer in scikit-learn has a specific behavior when handling missing values during transform that weren't missing during fit - it uses the initial imputation method (mean by default) rather than the iterative estimator.\n\nThe issue includes:\n1. Clear description of the current behavior with reference to documentation\n2. Two code examples showing the different behaviors (when a feature has missing values in fit vs. when it doesn't)\n3. The output of both examples to demonstrate the difference\n4. A specific suggestion for improvement (making this behavior optional with a parameter)\n5. Even points to the specific lines in the codebase that would need modification\n\nA developer could understand exactly what the issue is, reproduce it with the provided examples, and implement a solution based on the suggestion to add a parameter that controls this behavior. The issue provides all necessary context to understand the problem and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14869": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue shows that when using HistGradientBoostingClassifier with 'categorical_crossentropy' loss on a binary classification problem, the model fails silently (produces incorrect predictions) rather than raising an error or handling the binary case correctly. The expected behavior is explicitly stated: either the 'categorical_crossentropy' loss should work properly for binary classification or it should raise an error. The code example is complete, shows the problematic behavior, and mentions that 'binary_crossentropy' works correctly for comparison. This provides all the necessary information to understand and address the issue.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14878": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The bug occurs when SimpleImputer tries to validate a pandas DataFrame containing string data. The error shows that SimpleImputer's _validate_input method attempts to access a 'dtype' attribute on the DataFrame object, which doesn't exist (DataFrames have dtypes for individual columns, not a single dtype). This causes an AttributeError.\n\nThe issue provides:\n1. A complete code snippet that reproduces the problem\n2. The full error traceback showing exactly where the failure occurs\n3. Clear context about what's happening - SimpleImputer is not properly handling DataFrame validation when string data is present\n\nFrom this information, I can understand that the fix would involve modifying the _validate_input method in SimpleImputer to properly handle pandas DataFrames, likely by checking if the input is a DataFrame and handling the dtype validation differently than for NumPy arrays. The solution would need to ensure that appropriate error messages are raised when incompatible data types are provided, rather than failing with an AttributeError.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14890": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The user provides:\n\n1. A complete code example that demonstrates the problem\n2. The exact error message received\n3. The expected behavior (fitting with sample_weight)\n4. Environment details including versions of relevant libraries\n\nThe issue is about the inability to use sample_weight when fitting a TransformedTargetRegressor within a Pipeline. The error message shows that the sample_weight parameter is not being properly passed through the pipeline to the underlying regressor.\n\nThe error occurs specifically at the line where the pipeline tries to split parameter names with \"__\" delimiter, which suggests that the sample_weight parameter is not being properly formatted for the pipeline's parameter passing mechanism.\n\nThis is a concrete bug with clear reproduction steps that a developer could investigate and fix without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14894": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual results. The problem is clearly defined: when using SVM with sparse data where support_vectors_ is empty, a ZeroDivisionError occurs in the _sparse_fit method. The issue provides:\n\n1. A complete code example that reproduces the error\n2. The exact error message and traceback\n3. The expected behavior (no error and empty sparse matrix)\n4. Version information for all relevant libraries\n\nThe error occurs specifically in the _sparse_fit method when dividing by n_class when dual_coef_indices.size is non-zero but n_class is zero. This is a clear bug in the SVM implementation when handling edge cases with sparse matrices. A developer would have all the necessary information to locate the issue in the codebase and implement a fix by adding a check for division by zero in the relevant code path.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14908": {
        "explanation": "This issue is well-specified with clear information about what needs to be improved. The issue describes:\n\n1. The exact problem: The error message for DummyClassifier with constant strategy could be more helpful when the constant value is not in the training data\n2. A reproducible code example showing the issue\n3. The current error message\n4. A specific suggestion for improvement (including what additional information should be in the error message)\n5. A real-world context where this improvement would be helpful\n\nThe solution requirements are clear: modify the error message in the DummyClassifier to include:\n- The constant value that was provided\n- The list of possible values from the training data\n\nThis is a straightforward enhancement to improve user experience with clear acceptance criteria. A developer can implement this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-14983": {
        "explanation": "This issue is very well-specified with clear reproduction steps, expected results, and actual results. The problem is clearly defined: the `RepeatedKFold` and `RepeatedStratifiedKFold` classes in scikit-learn's model_selection module are not displaying the proper string representation when the `repr()` function is called on them.\n\nThe issue includes:\n1. Exact code to reproduce the issue\n2. The expected output (showing parameter values in a readable format)\n3. The actual output (showing only the object type and memory address)\n4. Complete version information for the environment\n\nThe solution would require implementing or fixing the `__repr__` method for these two classes to display their parameters properly, similar to how other scikit-learn estimators display their parameters. This is a straightforward task with a clear definition of success: when the code in \"Expected Results\" produces the output shown there.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15028": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: `DecisionTreeRegressor` has a `classes_` attribute that returns None and is not documented, which is inconsistent and confusing.\n\n2. The solution approach is already outlined:\n   - Change `classes_` to `_classes` in the parent class\n   - Add a test to verify the change\n\n3. Context is provided:\n   - Reference to related issues (#14766 and #14312)\n   - Mention of previous attempts (using a decorator on a property)\n   - Indication that the contributor has already consulted the deprecation guidelines\n\n4. The PR has already been started with a specific implementation approach that was developed in consultation with a team member (@thomasjpfan).\n\nA developer could implement this solution without needing additional clarification. The task involves:\n1. Identifying where `classes_` is defined in the parent class\n2. Changing it to `_classes` (private attribute)\n3. Adding appropriate tests\n4. Following the project's deprecation policy\n\nThis is a straightforward refactoring task with a clear goal and implementation path.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15084": {
        "explanation": "This issue is well-specified with a clear error message and complete reproducible code. The problem is that the user is trying to use a VotingClassifier with a mix of regression models (LinearRegression, Ridge) and classification models (LogisticRegression), which is causing a TypeError when trying to cast float predictions from regression models to integer class labels. The error occurs specifically in the VotingClassifier's predict method when using 'hard' voting, which requires discrete class labels. The error message \"Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'\" clearly indicates the type conversion issue. The user has provided the exact versions (scikit-learn 0.21.2 with Anaconda) and a complete traceback showing where the error occurs. A solution would involve either using only classifier models with VotingClassifier or converting the regression models' outputs to class labels before using them in the ensemble.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15086": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a clear description of the problem: MultiTaskLassoCV with fit_intercept=True returns the same MSE for all alphas when using binary features, leading to incorrect regularization.\n\n2. Included complete code examples that reproduce the issue, showing:\n   - The problematic case (binary features with fit_intercept=True)\n   - A working case with normal features (showing expected behavior)\n   - A working case with binary features but fit_intercept=False\n\n3. Specified their environment details (Python 3.7.1, scikit-learn v0.21.3, numpy v1.16.2, Windows, Anaconda)\n\n4. Demonstrated the issue is specific to MultiTaskLassoCV (and MultiTaskElasticNet) but not LassoCV\n\n5. Provided the exact outputs for each scenario, making it clear what's wrong\n\nThe issue appears to be a bug in the implementation of MultiTaskLassoCV when handling binary features with intercept fitting. The expected behavior is clear from the working examples, and the problem is consistently reproducible with the provided code. A developer could investigate the implementation differences between the working and non-working scenarios to identify and fix the issue.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15094": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user has identified a specific issue: MaxAbsScaler is upcasting pandas DataFrames from float32 to float64 during transformation.\n\n2. The issue includes complete, reproducible code that clearly demonstrates the problem.\n\n3. The expected and actual results are clearly stated, showing the discrepancy between the input dtype (float32) and the output dtype (float64).\n\n4. The user has done investigation and identified that the issue likely occurs in the `check_array` function.\n\n5. The user has a clear question: how to specify the output dtype for MaxAbsScaler, similar to how OneHotEncoder has a dtype parameter.\n\n6. All relevant version information is provided (OS, Python, NumPy, SciPy, Scikit-Learn, Pandas).\n\nThe issue is asking for either:\n1. A way to specify the output dtype for MaxAbsScaler\n2. A common practice to maintain dtype when using ColumnTransformer\n\nThis is a well-defined feature request/bug report that could be addressed by adding a dtype parameter to MaxAbsScaler or by documenting a workaround for preserving dtypes when using transformers.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15096": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user clearly identifies that GridSearchCV is saving fitted estimators in cv_results_['params'] when parameters include estimators, causing excessive memory usage when dumping results.\n\n2. The specific conditions are provided: this happens when n_jobs=1 and when using estimators as parameters in a pipeline.\n\n3. The user provides context by referencing the Pipeline documentation about replacing estimators.\n\n4. The user has a reasonable hypothesis about the cause: estimators aren't being cloned before use in the grid search.\n\n5. The version of scikit-learn (0.19.0) is specified.\n\nThe issue describes a specific bug with observable behavior (high memory usage), a reproducible scenario (using estimators as parameters in GridSearchCV with n_jobs=1), and even suggests a potential cause (lack of cloning). This provides enough information for a developer to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15100": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The problem is precisely defined: the `strip_accents_unicode` function in scikit-learn fails to strip accents from strings that are already in NFKD (Normalization Form Compatibility Decomposition) form.\n\nThe issue includes:\n1. A clear description of the problem\n2. A minimal, complete code example that reproduces the issue\n3. The expected results (both strings should be normalized to \"n\")\n4. The actual results (only s1 is normalized, s2 remains unchanged)\n5. Detailed version information for the relevant software\n\nThe root cause is also identified: the function doesn't handle strings that are already in NFKD form. This gives a clear direction for the fix - the function needs to be modified to properly handle strings regardless of their normalization form.\n\nA developer would have all the information needed to understand the issue, reproduce it, and implement a solution without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15138": {
        "explanation": "This issue is well-specified. It clearly describes a feature request to add an option to the stacking implementation that would allow using the original dataset when training the final estimator (meta-classifier). The issue:\n\n1. Clearly states what functionality is needed: an option to use original features alongside meta-features\n2. Provides a reference implementation from another library (mlxtend) showing how this feature works\n3. Includes the parameter name and description from the reference implementation\n4. Explains the rationale for the feature (useful in Kaggle competitions)\n\nA developer would understand exactly what needs to be implemented: adding a boolean parameter to control whether the final estimator in a stacking model should be trained only on the predictions from base estimators or on both the predictions and the original features. The reference implementation provides a clear template to follow.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15393": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies:\n\n1. The specific file and line number where the problem exists (sklearn/impute/_iterative.py, line 420)\n2. The nature of the problem: the \"ascending\" and \"descending\" options for imputation_order are inverted in the IterativeImputer class\n3. A more specific explanation: the _ImputerTriplets in the fitted imputation_sequence_ are being appended in the wrong order relative to what the imputation_order parameter specifies\n\nThe issue provides enough context for a developer familiar with the codebase to locate the problem and understand what needs to be fixed. The solution would involve correcting the logic that determines the order of imputation based on the imputation_order parameter.\n\nWhile the issue doesn't provide extensive details about the expected behavior, it's clear enough that someone with knowledge of the IterativeImputer class would be able to identify and fix the inversion problem. The reference to a specific line number makes it particularly easy to locate the problematic code.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15495": {
        "explanation": "This issue is well-specified with a clear objective: replace custom sample_weight validation code with the newly introduced `utils.validation._check_sample_weight` function across multiple estimators in the codebase.\n\nThe issue provides:\n1. A clear goal - use the new `_check_sample_weight` function consistently\n2. A specific list of estimators that need to be updated\n3. Context about what the function does (validates sample_weight arrays)\n4. Information about where the changes should be made (mostly in `fit` or `partial_fit` methods)\n5. A helpful tip for locating the classes in the codebase\n\nThe task is straightforward: find the sample_weight validation code in each listed estimator and replace it with calls to the new utility function. The checkboxes in the list suggest this is a well-defined task with clear completion criteria.\n\nThe only minor ambiguity is that the issue mentions \"linear_model module\" is left out because it's \"more involved,\" but this doesn't affect the clarity of what needs to be done for the listed estimators.\n\nThis is a typical refactoring task to improve code consistency, and someone familiar with the codebase would have all the information needed to implement the solution without further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15512": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The documentation states that when affinity propagation clustering doesn't converge, it should return an empty array as cluster_center_indices and -1 as labels.\n\n2. The user provides a complete, reproducible code example showing the discrepancy between expected and actual behavior.\n\n3. The issue includes specific version information for all relevant dependencies.\n\n4. The user clearly states what they expect (empty list and [-1,-1,-1]) versus what they're getting ([2] and [0,0,0]).\n\n5. The user explains why this is problematic for their use case - they need a reliable way to detect non-convergence.\n\nThe question is clear: Is this a documentation error or an implementation bug? The solution would involve either updating the documentation to match the actual behavior or fixing the implementation to match the documented behavior by properly indicating non-convergence.\n\nThis issue contains all the necessary information to investigate and fix the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15524": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected results, and actual error messages. The problem is clearly defined: when using KNeighborsClassifier with a precomputed metric in combination with cross_val_predict and GridSearchCV, an error occurs stating \"X should be a square kernel matrix\".\n\nThe issue provides:\n1. Complete code to reproduce the error\n2. A working example without the precomputed metric for comparison\n3. The full error traceback showing exactly where the problem occurs\n4. The specific version of scikit-learn being used (0.20.2)\n\nThe error occurs because when using cross-validation with a precomputed distance matrix, the validation splits need special handling. During cross-validation, the distance matrix is split into training and testing subsets, but the resulting matrices are no longer square, which violates the requirement for precomputed metrics.\n\nThis is a legitimate bug in scikit-learn's handling of precomputed metrics in nested cross-validation scenarios, and there's enough information to understand and attempt to fix the issue.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-15625": {
        "explanation": "This issue is well-specified. The requester is asking to add a 'normalize' parameter to the metrics.classification.confusion_matrix function to allow for direct normalization of the confusion matrix without having to do it manually afterward. \n\nThe request clearly:\n1. Identifies the specific function to modify (metrics.classification.confusion_matrix)\n2. Specifies the exact parameter to add ('normalize')\n3. Explains the purpose of the parameter (to get a normalized confusion matrix directly)\n4. Provides context for why this would be useful (the requester frequently uses this function and finds manual normalization unnecessary)\n5. Notes that similar functionality exists in other related functions like accuracy_score\n\nA developer could implement this feature without needing additional clarification. The implementation would involve adding a normalize parameter to the function that, when set to True, would normalize the confusion matrix before returning it. This is a common pattern in scikit-learn's API and follows existing precedent in other metric functions.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-24145": {
        "explanation": "This issue is well-specified with a clear request to add a sparse matrix output option to the SplineTransformer class. The description includes:\n\n1. A clear goal: Allow SplineTransformer to return a sparse matrix instead of a dense ndarray\n2. A specific implementation approach: Use the `design_matrix` function from scipy >= 1.8\n3. A code example showing the expected API with a `sparse=True` parameter\n4. The rationale: B-splines naturally have a sparse structure, making sparse matrices appropriate\n\nThe implementation path is straightforward - add a `sparse` parameter to SplineTransformer that, when True, returns a sparse matrix representation instead of a dense array. The PR would need to:\n1. Add the parameter to the class\n2. Modify the transform method to return sparse output when requested\n3. Add appropriate documentation\n4. Add tests for the new functionality\n\nThe only minor ambiguity is whether backward compatibility with older scipy versions should be maintained, but this could be handled with appropriate version checking or documentation.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-24677": {
        "explanation": "This issue is reasonably specified. It clearly identifies a bug in the `sklearn.metrics.silhouette_samples` function when working with sparse matrices. The issue references a specific GitHub issue (#18524) which would provide additional context. \n\nThe description identifies two specific problems:\n1. A failure in the pre-computed check for sparse matrices when getting diagonal elements\n2. An issue when trying to index a sparse matrix to pass weights to the np.bincount function\n\nWhile the exact implementation details aren't provided in the description itself, the reference to the GitHub issue and the clear identification of the specific function and problems make this reasonably actionable. An experienced developer familiar with scikit-learn could likely understand what needs to be fixed by examining the referenced issue and the current implementation of the silhouette_samples function.\n\nThe PR itself appears to be a fix for these issues, and the description is explaining what the PR implements rather than asking a question, which makes it more straightforward to understand what needs to be done.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25102": {
        "explanation": "This issue is well-specified with clear problem definition and proposed solution. The author:\n\n1. Clearly describes the problem: transformers that don't modify values are losing dtype information when outputting pandas DataFrames\n2. Provides a concrete example with code and output showing the issue\n3. Suggests a specific implementation approach by modifying `_SetOutputMixin` to add a `dtypes` parameter to `_wrap_in_pandas_container`\n4. Explains why this is important (preserving categorical and other dtype information for later analysis steps)\n5. Mentions alternatives considered\n\nThe issue includes all necessary context for a developer to understand the problem, reproduce it, and implement the proposed solution. The suggested implementation path is specific enough to guide development while leaving appropriate flexibility for the implementer to handle edge cases and integration details.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25232": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: `IterativeImputer` lacks a `fill_value` parameter when using `initial_strategy=\"constant\"`, despite the fact that `SimpleImputer` (which it references in its documentation) has this parameter.\n\n2. The proposed solution is specific: Add a `fill_value` parameter to `IterativeImputer` that works when `initial_strategy` is set to `\"constant\"`.\n\n3. There's a specific additional requirement: The parameter should accept `np.nan` as a valid value for compatibility with decision tree-based estimators.\n\nThe issue provides sufficient context by referencing the existing documentation and explaining the expected behavior based on the related `SimpleImputer` class. The implementation would involve adding the parameter to the class signature, updating the documentation, and modifying the initialization logic to use the provided fill value when the constant strategy is selected.\n\nThis is a straightforward feature addition with clear acceptance criteria.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25363": {
        "explanation": "The issue description provides context about a problem with configuration handling in the `delayed` function, particularly when used with `Parallel` across different threads. It mentions that this is an alternative solution to a previous PR (#25242) that had limitations.\n\nKey points from the description:\n1. The issue is about passing explicit configuration to the `delayed` function\n2. It's an alternative to PR #25242 that addresses a specific thread-related limitation\n3. The solution involves explicitly passing configuration obtained by the thread making the `Parallel` call\n4. A warning is raised if configurations don't match, which will help catch errors\n5. The code will still work if `joblib` provides a way to provide a `context` and `config`\n\nWhile the description provides good context about the problem and the proposed solution approach, it lacks some specific details:\n- It doesn't show the actual code changes needed\n- It doesn't explain exactly what configuration needs to be passed\n- It doesn't fully describe the original issue (#25242 and #25239)\n\nHowever, an experienced developer familiar with scikit-learn's codebase, particularly the parallel processing components and how they interact with joblib, would likely understand what needs to be done. The issue describes a specific technical problem and outlines a clear approach to solving it.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25500": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The bug description clearly identifies the problem: CalibratedClassifierCV with isotonic regression doesn't work when `set_config(transform_output=\"pandas\")` is set because IsotonicRegression returns a DataFrame instead of a numpy array, causing a broadcasting error in the `predict_proba` method.\n\nThe issue includes:\n1. A concise description of the bug\n2. Complete code to reproduce the issue\n3. The exact error traceback showing where the problem occurs\n4. Expected behavior (it should not crash)\n5. Detailed version information about the environment\n\nThe error is specifically identified as a ValueError in the broadcasting operation: `ValueError: could not broadcast input array from shape (20,1) into shape (20,)` which occurs when trying to assign DataFrame values to a numpy array.\n\nA developer would have all the information needed to diagnose and fix this issue without requiring additional clarification. The solution would likely involve ensuring that the calibrator's predict method returns a numpy array with the correct shape regardless of the global pandas output configuration.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25570": {
        "explanation": "This issue is well-specified with clear information about the bug, steps to reproduce, expected behavior, and actual results. The problem is clearly defined: ColumnTransformer with pandas output fails when a transformer applies to 0 features (empty feature list). The issue provides:\n\n1. A complete code example that reproduces the error\n2. The exact error message and traceback\n3. A clear explanation of when the issue occurs (with pandas output and when a transformer has no features)\n4. Alternative scenarios that work (removing .set_output or removing the empty transformer)\n5. The expected behavior (steps with 0 features should be ignored)\n6. Detailed version information\n\nThe issue description pinpoints the problem to the ColumnTransformer's handling of transformers with no features when using pandas output, specifically mentioning the relevant code lines in _column_transformer.py. This provides enough context for a developer to understand the issue and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25601": {
        "explanation": "This issue is well-specified with clear information about the bug, steps to reproduce, expected results, and actual results:\n\n1. The bug is clearly described: When using RandomForestClassifier with a very small value for max_samples (near zero) combined with class_weight='balanced_subsample', the model fails with an unhelpful error.\n\n2. Complete reproduction steps are provided with a minimal code example that triggers the error.\n\n3. The expected behavior is clearly stated with two acceptable alternatives: either prevent the error by enforcing a minimum sample size, or provide a more descriptive error message.\n\n4. The actual error output is fully provided with a complete stack trace.\n\n5. Version information for all relevant dependencies is included.\n\nThe issue is asking for one of two specific solutions:\n1. Enforce a minimum sample size when using these parameters together\n2. Provide a more descriptive error message when this edge case occurs\n\nThis is a well-defined edge case with a clear problem and proposed solutions. A developer would have all the information needed to reproduce the issue and implement a fix without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25694": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The bug is clearly described: when using MLPRegressor with early_stopping=True, the partial_fit method produces a TypeError due to an operation between NoneType and float. The issue includes:\n\n1. A clear description of the problem\n2. Complete code to reproduce the issue\n3. The full error traceback\n4. Expected behavior (either graceful handling or a fix)\n5. Environment details including Python and library versions\n\nThe error occurs in the _update_no_improvement_count method when trying to compare self.loss_curve_[-1] with self.best_loss_ - self.tol, where one of these values is None. The issue is likely related to a specific PR that the reporter mentions. This provides enough context for a developer to investigate and fix the issue without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25697": {
        "explanation": "This issue is well-specified and clear. It identifies a specific inconsistency in the API of scikit-learn: two models (`BayesianRidge` and `ARDRegression`) use the parameter name `n_iter` while other models in the library use `max_iter` for the same purpose. The requested change is straightforward - deprecate the `n_iter` parameter in these two models and replace it with `max_iter` to maintain consistency across the library. This is a common type of API improvement that follows standard deprecation practices in software development. The issue provides all the necessary information to implement the solution without ambiguity.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25744": {
        "explanation": "This issue is well-specified with clear details about the bug, steps to reproduce, expected vs. actual results, and relevant version information.\n\nThe problem is clearly defined: When setting `min_samples_split=1` in DecisionTreeClassifier, an exception should be raised according to the parameter's constraints (which require it to be \u2265 2 for integers), but no exception is raised.\n\nThe issue includes:\n1. A clear description of the bug\n2. A link to the relevant code in the repository\n3. A complete code snippet to reproduce the issue\n4. The expected error message\n5. The actual result (no exception)\n6. Detailed version information\n7. A hypothesis about the cause (overlapping constraints between Real and Integral types)\n\nThe solution would involve fixing the parameter validation in DecisionTreeClassifier to properly enforce the constraint that when `min_samples_split` is an integer, it must be \u2265 2. This is a well-defined task with clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25747": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The user has provided:\n\n1. A complete, minimal code example that reproduces the issue\n2. The exact error message with traceback\n3. Clear description of what works (using transform_output=\"default\") and what doesn't (using transform_output=\"pandas\")\n4. The expected behavior (no error when using pandas transform output)\n5. Detailed version information for all relevant packages\n\nThe issue appears to be that when using scikit-learn's FeatureUnion with a custom transformer that returns aggregated data (via groupby) and the transform_output is set to \"pandas\", there's a length mismatch error. The error occurs because the original DataFrame has 96 elements, but the transformed output has only 4 elements (due to the groupby aggregation), and scikit-learn is trying to assign the original index to the transformed data.\n\nA developer would have enough information to understand the problem and work on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25752": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected vs. actual results, and version information. The bug report indicates that the KMeans algorithm in scikit-learn is not properly using sample weights during initialization. \n\nThe user provides:\n1. A complete code example that reproduces the issue\n2. Clear expected results (centers_with_weight=[[1.],[5.]] and centers_no_weight=[[100.],[3.]])\n3. Actual results showing that sample weights are being ignored (both with and without weights produce the same result)\n4. Detailed version information for the environment\n\nThe issue is specifically about the KMeans initialization not respecting sample weights, which is evident from the example where heavily weighted points (1 and 5) should become the centers when weights are applied, but instead the algorithm produces the same centers as when no weights are used.\n\nA developer would have enough information to investigate why the sample_weight parameter isn't affecting the initialization of cluster centers and implement a fix to ensure weights are properly considered throughout the KMeans algorithm.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25805": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The user is trying to use CalibratedClassifierCV with a LightGBM model\n2. They're encountering a specific error: \"ValueError: Found input variables with inconsistent numbers of samples: [43364, 1]\"\n3. The error occurs when using the eval_set parameter in fit_params\n4. The shapes of all relevant arrays are provided (X_train, X_valid, Y_train, Y_valid)\n5. The code snippet shows exactly how they're trying to use the components\n6. The user explains that the format they're using is required by LightGBM's API\n\nThe issue is clear: CalibratedClassifierCV is not properly handling the eval_set parameter that LightGBM expects. The error comes from sklearn's validation.py in the check_consistent_length function.\n\nThe solution would involve finding a way to pass LightGBM-specific parameters through CalibratedClassifierCV to the underlying estimator without triggering sklearn's validation checks, or modifying how these parameters are handled. This is a well-defined compatibility issue between two libraries with clear constraints.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25931": {
        "explanation": "This issue is well-specified with clear details about the bug, steps to reproduce, expected vs. actual results, and environment information.\n\nThe issue describes a specific problem: when fitting an `IsolationForest` with a non-default `contamination` parameter using a pandas DataFrame, it incorrectly raises a warning that \"X does not have valid feature names, but IsolationForest was fitted with feature names\" even though X does have valid feature names.\n\nThe reporter has:\n1. Clearly identified the problem\n2. Provided a minimal reproducible example\n3. Explained the expected vs. actual behavior\n4. Included detailed version information\n5. Even suggested a likely cause - that when contamination != \"auto\", the estimator calls predict on the training data to determine the offset parameter\n\nThe issue points to a specific line in the scikit-learn codebase that is likely causing the problem. This gives a clear direction for investigation and fixing the issue.\n\nA developer could take this information and immediately start working on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25969": {
        "explanation": "This issue is reasonably specified for an experienced developer familiar with the scikit-learn codebase. The PR aims to add a `_get_response_values` method to a `CurveDisplayMixin` class, which appears to be part of a larger refactoring effort to standardize how prediction responses are handled across different visualization components.\n\nThe issue provides:\n1. Clear context that this is reviving a previous PR (#18589)\n2. A roadmap of subsequent changes that will follow this PR\n3. Details about what was done in the previous PR (in the collapsed section)\n4. The specific functions and classes that will be affected\n\nWhat makes it not fully \"well-specified\" is:\n- The exact implementation details of `_get_response_values` are not provided\n- The specific behavior differences or improvements aren't explicitly stated\n- There's an assumption that the developer is familiar with the existing codebase structure and the previous PRs\n\nHowever, an experienced scikit-learn developer would likely understand what needs to be done based on the context provided. The issue describes a refactoring to centralize response handling logic that was previously duplicated across different parts of the codebase, which is a common and understandable software engineering task.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-25973": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code example, expected results, actual error output, and version information. The user is reporting that the `SequentialFeatureSelector` class doesn't properly handle an iterable of splits passed to the `cv` parameter, despite the documentation suggesting it should. The error occurs when trying to fit the model with splits from `LeaveOneGroupOut()`. The code example is complete and can be run to reproduce the issue. The error message is clearly provided, showing an IndexError in the `_aggregate_score_dicts` function. This appears to be a legitimate bug report with all the necessary information to investigate and fix the issue.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26194": {
        "explanation": "This issue is well-specified with clear details about the problem and a potential solution:\n\n1. The problem is clearly identified: the `roc_curve` function in scikit-learn can return threshold values greater than 1 when working with probability estimates, which is mathematically inconsistent since probabilities should be in the range [0,1].\n\n2. The exact location of the issue in the code is provided: the line in `sklearn/metrics/_ranking.py` that adds 1 to the maximum score to create an additional point for (fpr=0, tpr=0).\n\n3. A reproducible test case is provided that demonstrates the issue.\n\n4. A potential solution is suggested: check if the maximum threshold is \u2264 1, and if so, clip the thresholds to not exceed 1.\n\nThe issue provides enough context for a developer to understand the problem, reproduce it, and implement a fix. The suggested solution is reasonable and addresses the mathematical inconsistency. While there might be alternative approaches to fixing this issue, the description provides sufficient information to make a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26242": {
        "explanation": "This issue is well-specified with clear information about the problem and how to fix it:\n\n1. The bug is clearly described: AdaBoostClassifier's `base_estimator` parameter was deprecated in favor of `estimator`, but code that explicitly sets `base_estimator=None` now fails with a validation error.\n\n2. A minimal reproducible example is provided with the exact code that triggers the error.\n\n3. The expected behavior is stated: no error should be thrown when `base_estimator=None` is used.\n\n4. The actual error message is included, showing exactly what's happening.\n\n5. The version information is provided (sklearn 1.2.2).\n\n6. The reporter even suggests a specific solution: adding `None` to the list of allowed values in `_parameter_constraints`.\n\nThis issue contains all the necessary information to understand the problem and implement a fix without needing additional clarification. The solution path is straightforward - modify the parameter validation to accept `None` as a valid value for the deprecated `base_estimator` parameter.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26289": {
        "explanation": "This issue is well-specified with a clear problem description. The user is experiencing an error when using `sklearn.tree.export_text` with feature names, while the similar function `tree.export_graphviz` works fine with the same input. The error message is provided: \"ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\".\n\nThe issue includes:\n1. The exact code that works (using export_graphviz)\n2. The exact code that fails (using export_text)\n3. The complete error traceback\n4. A sample of the feature_names array being used\n\nFrom this information, I can identify that the problem is likely in how export_text handles the feature_names parameter compared to export_graphviz. The error suggests that export_text is trying to evaluate the truthiness of the entire array rather than checking if it exists or is not None. This is a common Python error when using an array in a boolean context.\n\nA solution would involve examining the implementation of export_text in sklearn, understanding how it validates the feature_names parameter, and proposing a fix that properly handles numpy arrays as feature_names.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26318": {
        "explanation": "This issue is well-specified with clear details about the discrepancy between documentation and implementation. The issue:\n\n1. Clearly identifies the problem: The documentation for `warm_start` parameter in `BaseForest`-derived classes (like `RandomForestRegressor`) states that when `warm_start=False`, it will \"fit a whole new forest,\" but the actual implementation doesn't do this when `n_more_estimators == 0`.\n\n2. Provides specific references:\n   - Links to the official documentation\n   - Links to the exact line in the source code showing the discrepancy\n   \n3. Offers two concrete potential solutions:\n   - Update the documentation to match the actual behavior\n   - Modify the code to match the documented behavior\n\nThe issue provides all the necessary information to understand the problem and implement a solution. A developer could proceed with either fixing the documentation or modifying the code without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26323": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected vs. actual results, and a specific technical problem. The bug occurs when using `set_output` on a `ColumnTransformer` with a transformer defined in the `remainder` parameter - the output transformation is not properly applied to the remainder transformer.\n\nThe issue description:\n1. Clearly identifies the specific component affected (`ColumnTransformer.set_output` with `remainder` as an estimator)\n2. Provides a minimal reproducible code example\n3. Shows the expected and actual outputs side by side\n4. Points to the specific part of the code that's failing (line 853 in `_column_transformer.py`)\n5. Includes version information\n\nThe problem is that when using `set_output(transform=\"pandas\")`, the first example doesn't properly convert the boolean column back to boolean type because the `remainder` transformer's output isn't being properly handled. The second example works correctly because it uses explicit transformers instead of the `remainder` parameter.\n\nA developer could implement a solution by ensuring that the `set_output` method properly propagates to the transformer specified in the `remainder` parameter.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26400": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The bug is clearly defined: when using PowerTransformer with method='box-cox' on data containing an all-NaN column, it throws an unhelpful error message instead of either preserving the NaN column (like the default method does) or providing a more descriptive error message.\n\nThe issue includes:\n1. A clear description of the problem\n2. Complete code to reproduce the issue\n3. The exact error message and stack trace\n4. Expected behavior (two acceptable alternatives)\n5. System and dependency versions\n\nThe solution would need to modify the PowerTransformer implementation to handle all-NaN columns more gracefully when using the 'box-cox' method, either by preserving them (like the default method) or by providing a more informative error message that indicates the presence of an unfittable column.\n\nThis is a well-defined bug with a clear path to resolution, making it well-specified for a developer to implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-26644": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a problem: the `partial_dependence` function in scikit-learn's `inspect` module currently calculates arithmetic averages over predictions without considering sample weights, which is problematic for models fitted with sample weights.\n\nThe proposed solution is straightforward: add a new parameter `sample_weight=None` to the function that, when provided, would replace the arithmetic average with a weighted average. The issue also clarifies that this change would only affect the aggregation step, not the calculation of Individual Conditional Expectation (ICE) curves.\n\nWhat makes this reasonably specified rather than well-specified:\n1. It doesn't specify exactly which functions in the `inspect` module need to be modified (though it mentions `partial_dependence` functions, suggesting there might be multiple related functions).\n2. It doesn't provide implementation details about how the weighted average should be calculated.\n3. It doesn't address potential edge cases or validation requirements for the sample weights.\n\nHowever, these gaps can be filled by examining the codebase and following standard practices for handling sample weights in scikit-learn. The core requirement is clear enough to implement a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-7760": {
        "explanation": "This issue is reasonably well-specified. It describes a specific problem with the common tests for scikit-learn estimators, particularly related to how the tests handle properties vs. direct attributes in the context of `get_params` and `set_params`. \n\nThe issue points to a specific PR (#7477) and provides a code example that illustrates the problem: an estimator that stores parameters as private attributes (`_param`) and exposes them through properties (`param`). The issue states that this pattern should cause problems with `get_params` and `set_params` functionality, but the common tests are currently allowing it to pass when they should be failing.\n\nWhat makes this reasonably specified rather than well-specified:\n1. It doesn't explicitly explain why this pattern is problematic (though someone familiar with scikit-learn's parameter handling would understand)\n2. It doesn't specify exactly how the tests should be modified to catch this issue\n3. The reporter mentions they \"haven't looked into it\" which suggests some uncertainty\n\nHowever, for an experienced scikit-learn developer, there's enough information to:\n1. Understand the problem (properties vs direct attributes for parameters)\n2. Locate the relevant code (common tests for estimators, specifically `check_estimator`)\n3. Develop a solution (modify tests to detect and fail when parameters are exposed as properties rather than direct attributes)\n\nThe solution would involve enhancing the common tests to verify that parameters defined in `__init__` are stored as direct attributes rather than being accessed through properties.",
        "label": "REASONABLY_SPECIFIED"
    },
    "scikit-learn__scikit-learn-8554": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The user reports that `Isomap` and `LocallyLinearEmbedding` classes in scikit-learn do not accept sparse matrix inputs despite the documentation stating they should. The issue includes:\n\n1. Specific identification of the problem: The `check_array` function is called without `accept_sparse=True` parameter in multiple locations\n2. Evidence from the code: Grep results showing all instances of `check_array` in the manifold module\n3. Error traceback demonstrating the issue\n4. Specific files and line numbers where changes are needed\n5. A clear solution: Add `accept_sparse=True` to the `check_array` calls\n6. The user has offered to submit a PR\n\nThe issue is technically focused, provides sufficient context, and has a straightforward solution path. Anyone with knowledge of scikit-learn's codebase could implement the fix based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-9274": {
        "explanation": "This issue is well-specified with clear details about the problem and how to reproduce it:\n\n1. The problem is clearly defined: When training an MLP regressor/classifier using the l-bfgs solver, the iterations are capped at 15,000 regardless of the max_iter parameter value set by the user.\n\n2. The cause is identified: The MLP's max_iter parameter is being passed to the l-bfgs optimizer's \"maxfun\" parameter (maximum function calls) but not to the \"maxiter\" parameter (maximum iterations), which defaults to 15,000.\n\n3. A complete reproduction example is provided with code and a link to the necessary data file.\n\n4. Expected vs. actual results are clearly stated: Expected to run for 18,000 iterations but actually runs for 15,000.\n\n5. Version information is provided, though the issue is noted to exist across versions.\n\n6. There's even a partial analysis of a potential fix direction (using maxiter rather than maxfun).\n\nThe solution approach is straightforward: modify the code that calls the l-bfgs optimizer to pass the MLP's max_iter parameter to both maxfun and maxiter parameters of the optimizer, or at least to maxiter. This would allow the MLP to train for the number of iterations specified by the user.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-9288": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and actual results. The user has:\n\n1. Provided a complete, minimal code example that reproduces the issue\n2. Clearly stated the expected behavior (same inertia value regardless of n_jobs parameter)\n3. Shown the actual results with specific numerical differences\n4. Included version information for the relevant libraries\n\nThe issue is about KMeans clustering in scikit-learn producing slightly different results when run with n_jobs=1 versus n_jobs>1. The difference is small but consistent, with parallel execution (n_jobs>1) giving a different inertia value than single-threaded execution.\n\nThis is likely a numerical precision or implementation issue in the parallel version of the algorithm. A developer would have enough information to investigate the source code differences between the single-threaded and multi-threaded implementations of KMeans to identify where the discrepancy occurs.\n\nThe issue is specific, reproducible, and contains all necessary information to begin debugging.",
        "label": "WELL_SPECIFIED"
    },
    "scikit-learn__scikit-learn-9775": {
        "explanation": "The issue is reasonably specified, but could benefit from a bit more context. Here's what I understand:\n\n1. The issue is about the `trustworthiness` function in the `sklearn.manifold.t_sne` module.\n2. Currently, this function has a `precomputed` boolean parameter.\n3. The request is to replace this boolean parameter with a more standard approach: `metric='precomputed'`.\n\nThis follows a common pattern in scikit-learn where distance metrics can be specified as strings, with 'precomputed' being a special value indicating that the input is already a distance matrix rather than raw data points.\n\nWhile the issue doesn't provide extensive background or implementation details, the request is clear enough for someone familiar with scikit-learn's API conventions to understand what needs to be changed. The solution would involve:\n\n1. Modifying the function signature to accept a `metric` parameter instead of `precomputed`\n2. Updating the function implementation to handle `metric='precomputed'` appropriately\n3. Ensuring backward compatibility or documenting the breaking change\n4. Updating relevant documentation\n\nA developer familiar with scikit-learn would recognize this as aligning with the library's standard API patterns.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10021": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that two Sphinx configuration options don't work well together: `autodoc_unqualified_typehints` and `autodoc_typehints=\"description\"`. \n\nThe issue clearly states:\n1. The specific configuration settings that cause the problem\n2. The expected behavior: typehints in info-field-list should be changed to the short style when `autodoc_unqualified_typehints` is enabled\n3. The environment details: Python 3.10.1, Sphinx 4.x, and the autodoc extension\n\nWhat's missing is:\n1. A concrete example showing the actual output versus the expected output\n2. The specific documentation or code that exhibits this issue\n3. A more detailed explanation of what \"short style\" means in this context\n\nHowever, someone familiar with Sphinx autodoc would likely understand what the issue is referring to. The `autodoc_unqualified_typehints` option is meant to shorten fully-qualified type names (e.g., `module.submodule.Class` to just `Class`), and the user expects this to work in the description section when `autodoc_typehints=\"description\"` is set.\n\nWith knowledge of Sphinx autodoc's behavior, this issue provides enough information to investigate and potentially fix the problem.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10067": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: Sphinx documentation doesn't include the language attribute in HTML by default, causing accessibility issues identified by Lighthouse.\n\n2. The solution is explicitly stated: When users run sphinx-quickstart and enter a language, that value should be automatically set in the generated conf.py file.\n\n3. There's even a suggestion for an additional feature (prompting to set language for existing installations).\n\n4. The issue provides context about why this matters (accessibility for screen readers) and includes examples of the current behavior and desired outcome.\n\n5. The issue references the specific configuration value that needs to be modified and links to relevant documentation.\n\nA developer could implement this solution without needing additional clarification. The implementation would involve modifying the sphinx-quickstart tool to include the user-selected language in the generated conf.py file.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10097": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly described: when using LaTeX (specifically XeLaTeX) to build Sphinx documentation, long option names with square brackets overflow in the Index, as shown in the screenshot. The example provided demonstrates the issue with a specific RST directive (option directive with a long parameter list in square brackets).\n\nThe issue includes:\n1. A clear code example that reproduces the problem\n2. A screenshot showing the overflow issue\n3. The environment details (OS, Python version, Sphinx version)\n\nWhat's missing:\n1. The \"Expected behavior\" section is not filled out, but it's reasonably implied that the expected behavior is for the text not to overflow in the Index\n2. No specific Sphinx extensions are mentioned, but this may not be relevant to the issue\n3. No specific solution approach is suggested\n\nDespite these gaps, a developer familiar with Sphinx and LaTeX would likely understand what needs to be fixed - the LaTeX output needs to be modified to handle long option names with square brackets properly in the Index. The solution would involve modifying how Sphinx generates LaTeX code for the Index entries of option directives.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10137": {
        "explanation": "This issue is reasonably well-specified. It describes a specific problem with the extlink replacement suggestion feature in Sphinx, where the system is suggesting to replace a hardcoded GitHub URL with an extlink that doesn't actually make sense in context.\n\nThe issue provides:\n1. Clear context about the feature (added in PR #9800)\n2. A specific configuration example in conf.py\n3. A concrete example of problematic text\n4. The exact suggestion message that's causing the problem\n5. A potential solution direction (bailing out the check when there's a '/' in the extlink)\n\nWhat makes it not fully well-specified:\n1. It doesn't explicitly state what the expected behavior should be - though it implies that URLs with certain patterns (like paths with '/') should be excluded from suggestions\n2. It asks for opinions from specific contributors rather than definitively stating the required solution\n3. It doesn't specify exactly how to determine when to bail out (just mentions the '/' character as a possibility)\n\nHowever, there's enough information for a developer to understand the issue and implement a reasonable solution by modifying the extlink suggestion logic to avoid suggesting replacements in cases where the URL structure doesn't match the extlink pattern appropriately.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-10191": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a complete, minimal code example in `index.rst` that demonstrates the bug\n2. Specified the exact command to reproduce the issue (`make latexpdf`)\n3. Included a screenshot showing the problem (footnotes rendered as \"?\" instead of proper numbers)\n4. Clearly stated the expected behavior (footnotes should render correctly)\n5. Provided relevant environment information (Mac OS, Python 3.8.7, Sphinx 4.4.0 and 4.5.0+)\n\nThe issue describes a specific bug in Sphinx's PDF output where alternating references to the same footnotes are incorrectly rendered as question marks instead of proper footnote numbers. The reproduction steps are complete and should allow a developer to recreate the issue and verify a fix. The problem appears to be in how Sphinx handles multiple references to the same footnote when generating LaTeX/PDF output.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10207": {
        "explanation": "This issue description appears to be a combination of multiple unrelated issues, which makes it confusing to understand what exactly needs to be solved. Let me analyze the main components:\n\n1. The first part mentions adding a keyboard shortcut `/` to focus on search in Sphinx templates, similar to GitHub and Gmail.\n2. The second part is about a bug in the sphinx-napoleon extension where instance attributes are incorrectly prefixed with class names.\n3. The third part is about making autodoc abbreviate long dictionaries.\n\nThese are three completely different feature requests/bug reports that have been combined into a single issue description. The title only refers to the first one (keyboard shortcut), but most of the content is about the second issue (napoleon prefixing).\n\nWithout clarification on which of these issues needs to be addressed, it would be difficult to create a meaningful solution. The issues themselves are reasonably well-described individually, but combining them creates confusion about the actual task at hand.\n\nIf we were to focus only on the first issue (as suggested by the title), then it's reasonably specified - implement a keyboard shortcut `/` to focus on the search box in Sphinx templates. However, the lack of context about the current search implementation in Sphinx and how keyboard shortcuts are currently handled makes it somewhat vague.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-10323": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and context. The user is reporting a problem with Sphinx's `literalinclude` directive when used with the `prepend` option - specifically that leading whitespace is removed from the prepended content, resulting in incorrect indentation in code examples.\n\nThe issue includes:\n1. Clear reproduction steps with example files (index.rst and pom.xml)\n2. The actual output showing the indentation problem\n3. The expected output showing how it should look\n4. Environment details (OS, Python version, Sphinx version, extensions)\n5. A potential workaround that was attempted using `dedent` and why it doesn't fully solve the problem\n\nThe user has provided enough information to understand the problem: when using `literalinclude` with `prepend`, the indentation of the prepended content doesn't match the indentation of the included code, making the resulting code block look poorly formatted. The user wants the leading whitespace to be preserved in the prepended content to match the indentation of the included code.\n\nA developer would have enough information to investigate and attempt to fix this issue in the Sphinx codebase, specifically in how the `literalinclude` directive processes the `prepend` and `append` options in relation to indentation.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10325": {
        "explanation": "This issue is well-specified with clear requirements. The user is requesting an enhancement to the `:inherited-members:` option in Sphinx documentation to accept a list of classes that should be ignored when documenting inherited members. \n\nThe problem is clearly described with two specific use cases:\n1. A class inherits from multiple base classes, and the user wants to selectively document members from some base classes while ignoring others.\n2. A module contains multiple classes inheriting from built-in types (list, set, tuple) that should be ignored while documenting other inherited members.\n\nThe requested solution is specific: enhance the `:inherited-members:` option to accept a list of classes to ignore. The user has also considered alternatives and explained why they are insufficient.\n\nThis issue provides enough context for a developer to understand what needs to be implemented, why it's needed, and what the expected behavior should be. The scope is well-defined and the functionality requested is a logical extension of an existing feature.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10435": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: unwanted whitespace is being added at the start and end of inline code in LaTeX output after the enhancement from PR #10251.\n\n2. The issue provides:\n   - Clear reproduction steps with a code example\n   - Screenshots showing the current behavior with unwanted spaces\n   - Screenshots showing the desired behavior when spaces are manually removed\n   - A screenshot of how it looked before PR #10251\n\n3. The issue suggests a potential solution: adding % characters at the beginning and end of the code block in the LaTeX output to prevent unwanted spaces.\n\n4. The environment details are provided (OS, Python version, Sphinx version).\n\nThe issue is specific, reproducible, and includes visual evidence of the problem. A developer would have all the necessary information to understand the issue and implement a fix by modifying the LaTeX writer to prevent the insertion of whitespace at the beginning and end of inline code blocks.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10449": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that when using `autodoc_typehints = \"description\"` in Sphinx configuration, the `autoclass` directive incorrectly includes a return type annotation for class definitions. \n\nThe issue includes:\n1. A clear code example showing the problematic class definition\n2. Detailed reproduction steps with all necessary files and their contents\n3. Instructions for setting up the environment and building the documentation\n4. A link to a live example of the issue\n5. Expected behavior (no return type should be listed for the class)\n6. Environment details (OS, Python versions, Sphinx version)\n\nThe issue is specific enough that a developer could reproduce it and understand what needs to be fixed - the `autoclass` directive should not display return type information for class definitions when `autodoc_typehints = \"description\"` is set. This is a well-defined bug with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10457": {
        "explanation": "This issue is well-specified with clear information about the bug, how to reproduce it, and the expected behavior:\n\n1. The bug is clearly identified: The function `sphinx.domain.python.filter_meta_fields` only removes the first meta-field from a field list, but fails to remove subsequent meta-fields.\n\n2. A clear reproduction case is provided with a code example showing how to trigger the bug (a class with a docstring containing multiple meta-fields).\n\n3. The expected behavior is explicitly stated: no meta-fields should appear in the generated documentation.\n\n4. The environment is well-specified (Python 3.6, Sphinx 5.x and 4.5.x, using sphinx.ext.autodoc).\n\n5. The issue even points to the specific function that contains the bug and references the commit that introduced it (b968bb91e9).\n\nThe submitter has identified the root cause as a coding bug where the function stops processing after finding the first meta-field. This gives a clear direction for the fix: modify the function to continue processing and remove all meta-fields, not just the first one.\n\nA developer could implement a solution without needing additional information or clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10466": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The issue describes:\n\n1. The exact problem: duplicate location entries in .pot files when running `make clean; make gettext` in the Blender documentation project\n2. Specific examples of the duplications with screenshots\n3. The suspected source of the problem (in sphinx/builders/gettext.py)\n4. A proposed solution with code snippets\n5. Clear reproduction steps\n6. Expected behavior\n7. Environment details (OS, Python version, Sphinx version)\n\nThe issue reporter has even done debugging work and identified where the problem likely occurs, suggesting a specific fix by adding a uniqueLocation method to deduplicate the locations list. They've also noted other files that might need similar changes.\n\nWith this information, a developer could understand the problem, reproduce it, and implement a solution without needing additional clarification. The proposed solution provides a good starting point, though a developer would need to verify it's the correct approach.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10481": {
        "explanation": "This issue is well-specified with clear details about the problem and desired solution:\n\n1. The problem is clearly identified: In Sphinx 5, when a project has \"language = None\" in conf.py, it causes a build error with a specific exception: \"'NoneType' object has no attribute 'startswith'\".\n\n2. The error message is provided verbatim, making it easy to identify where the problem occurs.\n\n3. The issue provides context about when this problem started occurring (after a specific commit) and how widespread it might be (affecting older projects).\n\n4. The desired solution is explicitly stated: When Sphinx encounters NoneType for language, it should set the language to English and log a message instead of erroring out.\n\n5. The issue explains why this solution makes sense: it's consistent with how Sphinx already handles projects with no language setting at all.\n\n6. The issue even mentions alternatives that have been considered.\n\nA developer would have enough information to locate the code that handles the language configuration, understand the problem, and implement the requested change without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10492": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The bug report identifies:\n\n1. The exact location of the typo in the code (with a direct GitHub link to the specific lines)\n2. Two specific typos: \"langugae\" and \"langauge\" instead of the correct \"language\"\n3. Where the issue was observed (Read the Docs build)\n4. The expected behavior (correct spelling of \"language\")\n\nThe solution is straightforward - fix the typos in the identified file. There's no ambiguity about what needs to be done or how to verify the fix. A developer can easily locate the issue, make the correction, and verify that the warning message now uses the correct spelling.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10504": {
        "explanation": "This issue is well-specified with clear error information and reproduction steps. The issue describes:\n\n1. A specific error: \"IndexError: list index out of range\" occurring during the build process of Linux kernel documentation\n2. Detailed reproduction steps using the Arch Build System\n3. Expected behavior (the packages that should be generated)\n4. Complete error logs showing the Python traceback\n5. Environment details including OS, Python version, Sphinx version, and other relevant tools\n\nThe error occurs in the Sphinx documentation build process, specifically in the HTML builder's transforms when trying to access a node's children. The traceback points to a specific line in the Sphinx HTML transforms code where it's trying to split a node that doesn't have the expected content.\n\nThis is a well-defined bug with clear reproduction steps and error information that would allow a developer to investigate and fix the issue. The problem appears to be related to Sphinx 5.0.0 having compatibility issues with the Linux kernel documentation build process.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10551": {
        "explanation": "This issue is well-specified with clear examples and reproduction steps. The bug is precisely described: when a function parameter has a default value that is a negative number, the HTML output renders with an unwanted space between the minus sign and the first digit (e.g., `axis=-1` appears as `axis=- 1`). \n\nThe reporter provides:\n1. A clear description of the problem\n2. Multiple examples across different projects (SciPy, NumPy, Pandas, Matplotlib) with direct links\n3. Visual evidence of the issue (through the links)\n4. The expected behavior is implied (no space between minus sign and digit)\n\nWhile the reporter doesn't know exactly which component is causing the issue (Sphinx itself or the PyData Sphinx theme), this uncertainty doesn't prevent a developer from investigating and fixing the problem. The issue can be reproduced by examining the HTML output of the provided examples and comparing it with the source code.\n\nA developer can:\n1. Examine the HTML rendering code in Sphinx that handles parameter default values\n2. Check how negative numbers are processed specifically\n3. Create a test case with a negative default parameter\n4. Fix the rendering to properly handle negative numbers without adding the space\n\nThis is a straightforward rendering bug with clear examples and a well-defined expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10614": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a detailed description of the bug: SVG inheritance diagrams have broken links when created in non-root directories\n2. Included a reproducible test case with a zip file\n3. Outlined specific steps to reproduce the issue\n4. Explained the technical cause: links in SVG files are relative to the SVG file but are written as if relative to the embedding page\n5. Provided examples of correct and incorrect links with detailed explanations\n6. Specified the expected behavior\n7. Included environment details (OS, Python version, Sphinx version, extensions)\n\nThe issue is clearly a path resolution problem with SVG inheritance diagrams in nested directories. The solution would need to ensure that links in SVG files are correctly generated relative to their location in the documentation structure. This is a well-defined technical problem with a clear success criterion: links in SVG inheritance diagrams should work correctly regardless of the file's location in the directory structure.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10673": {
        "explanation": "This issue is well-specified with a clear problem description and desired solution. The problem is that users are trying to add special Sphinx-generated pages (genindex, modindex, search) to the toctree directive, but this generates warnings because these are not actual document files. The issue includes:\n\n1. Clear error messages that users encounter\n2. Multiple references to Stack Overflow questions showing this is a common problem\n3. A specific code example showing what users are trying to do\n4. A clear description of the desired solution - that these special pages should be allowed in the toctree directive without generating warnings\n\nThe solution would involve modifying Sphinx to recognize these special pages when they appear in a toctree directive and handle them appropriately instead of treating them as missing documents. This is a well-defined enhancement request with a clear scope and acceptance criteria.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-10807": {
        "explanation": "This issue is well-specified with clear details about the problem and potential solution. The user has:\n\n1. Clearly described the bug: when using `automodule` with `:members:`, Sphinx places members under the bottommost header of the module docstring, which affects both display and TOC generation.\n\n2. Provided a reproducible example with:\n   - A GitHub repository (sphinx-automodule-test)\n   - A live build demonstrating the issue\n   - Specific examples showing the problem in real-world documentation\n\n3. Explained the expected behavior: module docstrings should be content of the `module` directive rather than standalone RST.\n\n4. Suggested a potential solution with a code diff showing where the fix might be implemented.\n\n5. Included all relevant context about their environment (OS, Python version, Sphinx version, extensions).\n\nThe issue is specific, reproducible, and includes enough technical detail that a developer could understand the problem and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11109": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: The current formatting of `Union`, `Optional`, and `Literal` types in Python annotations is verbose and should be improved.\n\n2. The desired solution is explicitly described:\n   - Convert `Union[X, Y, Z]` to `X | Y | Z` (PEP 604 syntax)\n   - Convert `Optional[X]` to `X | None`\n   - Strip \"Literal\" from `Literal[\"X\"]` and display as `\"X\"` with syntax highlighting\n\n3. A reference implementation is provided (from the tensorstore project), giving a concrete example of how this might be implemented.\n\n4. The implementation approach is suggested: add a config option in Sphinx to support these formatting improvements.\n\nThe issue provides enough context, examples, and direction for a developer to understand what needs to be done and how to approach it. The scope is well-defined and the expected outcome is clear.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11192": {
        "explanation": "This issue is well-specified with clear reproduction steps and context:\n\n1. The problem is clearly defined: Using `sphinx-build -j auto` results in a smaller searchindex.js file that doesn't return search results properly, while building without the `-j` flag works correctly.\n\n2. Specific version information is provided: The issue started with Sphinx 6.1.0 and wasn't present in 6.0.0.\n\n3. Clear reproduction steps are provided:\n   - Command that fails: `sphinx-build -j auto -n -b html -d build/doctrees -q -E . build/html`\n   - Command that works: `sphinx-build -n -b html -d build/doctrees -q -E . build/html`\n\n4. Complete environment information is provided, including Python version, Sphinx version, and other relevant dependencies.\n\n5. A link to the documentation files is provided for further investigation: https://github.com/rogerbinns/apsw/tree/master/doc\n\nThe issue is specific to the `-j auto` parallel build option in Sphinx 6.1.0+ causing search index generation problems. This is a concrete, reproducible bug with clear expected behavior (search should work properly) and a clear failure mode (search returns nothing due to smaller searchindex.js).",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11266": {
        "explanation": "This issue is well-specified with clear information about what's wrong and how to reproduce it:\n\n1. The problem is clearly stated: In French language documentation, there's a missing space before the colon after \"Voir aussi\" (the French translation of \"See also\") for the seealso directive in LaTeX output.\n\n2. A screenshot is provided showing the issue visually.\n\n3. Steps to reproduce are provided with a code example.\n\n4. The issue includes version information (started in Sphinx 6.1.0).\n\n5. The reporter even identifies the specific PR (#11080) that caused this regression and links to the related issue (#6744).\n\nThe solution would require adding the appropriate space before the colon in the French translation of the seealso directive when generating LaTeX output. This is a straightforward localization fix with clear acceptance criteria - the space should appear before the colon in the French output as it did before version 6.1.0.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11311": {
        "explanation": "This issue is well-specified with clear technical details and proposed solutions. The issue:\n\n1. Clearly identifies the current implementation of `SigElementFallbackTransform.fallback` and its behavior\n2. Provides specific links to relevant code sections\n3. Outlines the current disadvantages of the implementation\n4. Proposes detailed alternative solutions with three specific conditions for custom translator classes\n\nThe issue author demonstrates deep understanding of the codebase and provides concrete suggestions for implementation:\n- Using `__init_subclass__` hook with a meta-keyword to automatically add classes to `SIG_ELEMENTS`\n- Detailed handling options for custom translator classes\n- Specific proposal to preserve node type information via attributes\n\nWhile implementing this would require familiarity with Sphinx's internals, the issue provides sufficient technical detail and clear requirements for a developer to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11312": {
        "explanation": "This issue is reasonably well-specified, describing a bug where the ordering of elements in a set nested within another collection (specifically a tuple) is inconsistent when displayed using util.inspect.object_description, leading to non-reproducible documentation output.\n\nThe issue provides:\n1. A clear description of the problem - inconsistent ordering of set elements in documentation output\n2. A specific example from the alembic codebase showing the issue\n3. Examples of the different outputs that can occur\n4. Links to the relevant code and build logs\n5. Context about where this is occurring (Sphinx documentation generation)\n\nWhat makes this not fully \"well-specified\" is:\n1. The exact conditions that trigger the inconsistent ordering are not yet identified, as acknowledged in the \"How to Reproduce\" section\n2. There's no clear reproduction case outside of the full build environment\n3. The issue references a possible connection to another issue (#4834) but doesn't elaborate on that connection\n\nHowever, there's enough information for a developer to investigate the issue in the util.inspect.object_description function, understand how it handles sets nested in other collections, and develop a solution that ensures consistent ordering of set elements. The core problem is clear even if the exact triggering conditions aren't fully understood yet.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-11316": {
        "explanation": "This issue is well-specified with clear reproduction steps and environment information. The bug description clearly identifies:\n\n1. The exact problem: A warning \"Inline interpreted text or phrase reference start-string without end-string\" occurs when using Napoleon extension with a specific docstring pattern\n2. The specific conditions: Only happens with references on the first line of a dataclass attribute docstring\n3. Clear reproduction code examples: Two code snippets showing the problematic case and the working case\n4. Complete environment information: Python version, Sphinx version, and relevant extensions\n5. The expected behavior is implied: The warning should not appear since the reference is actually valid\n\nThe issue provides all necessary information to understand, reproduce, and potentially fix the problem. The bug appears to be in how the Napoleon extension processes docstrings in dataclass attributes when references appear on the first line.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11445": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description clearly identifies the problem: when `rst_prolog` is set, headings containing domain directives (like `:mod:`) don't render correctly and aren't included in the toctree.\n\nThe issue provides:\n1. Complete reproduction steps with exact commands to run\n2. All necessary file contents (index.rst, mypackage.rst, and the conf.py modification)\n3. Clear explanation of when the issue occurs and when it doesn't\n4. Environment information including Sphinx version\n5. A way to verify the issue (grep command to check output)\n\nThe issue also clarifies that this is a regression that has existed for some time, as it can be reproduced in Sphinx v4.0.0.\n\nA developer would have all the information needed to reproduce the issue, investigate the cause, and implement a fix. The issue is about a specific rendering problem with a clear test case that demonstrates both the failing and working scenarios.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11489": {
        "explanation": "This issue is well-specified with a clear problem statement and proposed solution. The user is requesting a feature to disable linkcheck anchor checks for specific URLs based on a regex pattern.\n\nThe problem is clearly defined: GitHub's Markdown renderer requires JavaScript, which breaks linkcheck anchor checks in Sphinx. The user provides a specific example of how this affects their documentation.\n\nThe proposed solution is concrete and includes a code example of how the configuration might look:\n```python\nlinkcheck_anchors_disabled = [\n   # Requires JavaScript\n   r'https://github.com'\n]\n```\n\nThe issue includes:\n1. A clear problem statement\n2. A specific solution with example code\n3. Visual evidence of the problem (screenshot)\n4. A reference to a related issue\n\nA developer would have enough information to implement this feature without needing additional clarification. The solution involves adding a new configuration option to Sphinx's linkcheck builder that would allow disabling anchor checks for URLs matching specific patterns while still validating that the base URL exists.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11510": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes a problem with Sphinx's source-read event not properly modifying included files in documentation. The issue includes:\n\n1. A clear description of the problem: the source-read event in a custom Sphinx extension doesn't properly modify text in included files\n2. Complete reproduction code with all necessary files (conf.py, index.rst, something-to-include.rst, my-extension.py)\n3. Clear reproduction steps with commands to run\n4. The actual output showing the problem\n5. The expected output showing what should happen\n6. Environment details (OS, Python version, Sphinx version)\n7. Additional context with links to the project and related commits\n\nThe issue demonstrates that while the source-read event is triggered for included files and the replacement happens in the source variable, the changes don't appear in the final HTML output. This is a specific, reproducible bug with all the information needed to investigate and fix the issue in Sphinx's handling of included files with the source-read event.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-11550": {
        "explanation": "This issue is well-specified with clear reproduction steps and detailed context. The bug description clearly identifies the problem: when using Sphinx's autodoc with `autodoc_preserve_defaults = True`, an exception occurs when documenting a property that uses a lambda on a different line from the `property()` call.\n\nThe issue includes:\n1. A complete minimal reproducible example with all necessary files (mod.py, conf.py, index.rst)\n2. The exact command to reproduce the issue\n3. The full error output\n4. Environment information (OS, Python version, Sphinx version, etc.)\n5. Additional context showing workarounds that suppress the error (removing line breaks, using traditional @property decorator, or replacing lambda with a named function)\n\nThe issue is specific to a particular syntax pattern and has clear conditions for reproduction. A developer would have all the information needed to investigate and fix this bug in Sphinx's autodoc functionality, specifically how it handles multiline lambda expressions in property definitions when `autodoc_preserve_defaults = True` is enabled.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7234": {
        "explanation": "The issue is requesting support for documenting Python's `@functools.singledispatch` functions in a way that shows the main function documentation along with links to all available overloads.\n\nWhile the request is clear in its general intent (documenting singledispatch functions and their overloads together), there are several details missing:\n\n1. The context of where this documentation should appear is not explicitly stated, though it's likely referring to a documentation generator tool.\n2. It doesn't specify how the overloads should be detected or how the links should be formatted.\n3. There's no mention of how to handle nested overloads or edge cases.\n4. No specific examples of desired output beyond a very basic sketch.\n\nHowever, the core request is understandable: create a mechanism to automatically document singledispatch functions by showing the main function documentation and providing links to all its overloads. This is a reasonable feature request for a documentation tool, and a developer familiar with Python's singledispatch mechanism could implement a solution based on this description, even if some implementation details would need to be determined during development.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7268": {
        "explanation": "The issue is requesting that the `sphinx.ext.autodoc.typehints` extension be loaded automatically when the `autodoc` extension is used. The issue references another issue (#6418) which likely provides additional context.\n\nHowever, the description is quite brief and lacks several important details:\n\n1. It doesn't explain the current behavior (presumably users need to explicitly load the typehints extension)\n2. It doesn't specify what \"matured\" means in this context - what criteria determine that typehints are mature enough?\n3. It doesn't provide any implementation guidance or acceptance criteria\n4. It doesn't explain the rationale beyond a vague reference to typehints being \"matured\"\n\nWhile I can infer that the task involves modifying the autodoc extension to automatically load the typehints extension, the lack of specifics about implementation details, backward compatibility concerns, or configuration options makes this issue somewhat vague. A developer would likely need to research issue #6418 and the codebase to fully understand what's needed.\n\nThat said, there is a clear action item (make autodoc automatically load typehints) that could be implemented with some research into the existing code.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7305": {
        "explanation": "This issue is well-specified with clear reproduction steps and error messages. The user is reporting that when using the power operator (`**`) in default parameter values within Sphinx method documentation (in RST format), they receive \"could not parse arglist\" errors. The issue includes:\n\n1. Exact code snippets that reproduce the error\n2. The exact error messages received\n3. Environment information (OS, Python version, Sphinx version)\n4. Clear indication that this happens in Sphinx 3.x and master branches but not in 2.x\n\nThe problem is clearly defined: Sphinx cannot parse method signatures that contain the power operator (`**`) in default parameter values. The expected behavior is implied - Sphinx should be able to parse these expressions correctly.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Identify where in the Sphinx codebase the argument parsing is failing\n3. Implement a fix to handle the power operator in default values\n\nThe only minor omission is that the \"Expected behavior\" section doesn't explicitly state what should happen, but it's obvious from context that Sphinx should parse these expressions without errors.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7350": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Clearly described the bug: Napoleon's `Attributes:` directive ignores the `:noindex:` option\n2. Provided specific reproduction steps:\n   - Included the exact reST code that demonstrates the issue\n   - Mentioned using `example_google.py` from the official documentation\n   - Listed the exact environment setup (Ubuntu 18.04, packages installed)\n   - Described the process (sphinx-quickstart, conf.py edits, make html)\n3. Stated the expected behavior: the document index should be empty\n\nThe issue is focused on a specific technical problem with the `:noindex:` option not working for Napoleon's `Attributes:` directive while working for other directives. A developer could reproduce this issue and verify a fix based on the information provided. The problem scope is well-defined and the expected outcome is clear.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7351": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is clearly defined: when there are multiple files with the same name but different extensions (e.g., a.md and a.rst), Sphinx silently chooses one to parse (a.rst) even when the other is explicitly referenced in the toctree (a.md). The issue provides:\n\n1. Clear reproduction steps with a minimal example\n2. The expected behavior: either a config option to specify extension preference or a warning when this situation occurs\n3. Environment information including OS, Python version, Sphinx version, and relevant extensions\n4. Context about where the issue was first noted\n\nThe solution requirements are clear: modify Sphinx's behavior to either respect the explicitly referenced file extension, provide a configuration option for extension preference, or at minimum log a warning when this ambiguity occurs. This is a specific technical issue with a well-defined scope that can be addressed without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7356": {
        "explanation": "This issue is well-specified with clear details about what the problem is. The issue describes:\n\n1. A specific breaking change: Python domain IDs changed from using underscores to dashes in fragment identifiers (anchors)\n2. Clear reproduction steps with a code example\n3. The expected behavior is clearly stated - previously `example_python_function` would create `#example_python_function` but now creates `#example-python-function`\n4. The impact of the change is explained - it breaks links from nbsphinx and external sites to Python API docs\n\nThe issue provides enough context to understand the problem and what needs to be fixed. A developer would need to modify how Python domain IDs are generated, likely reverting to using underscores instead of dashes or providing backward compatibility. The issue references specific pull request #7236 which introduced the change, which would help in understanding the exact code that needs to be modified.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7374": {
        "explanation": "This issue is well-specified with clear details about what the problem is. The issue describes:\n\n1. A specific breaking change: Python domain IDs changed from using underscores to dashes in fragment identifiers (anchors)\n2. Clear reproduction steps with a code example\n3. The expected behavior is clearly stated - previously `example_python_function` would create `#example_python_function` but now creates `#example-python-function`\n4. The impact of the change is explained - it breaks links from nbsphinx and external sites to Python API docs\n\nThe issue provides enough context to understand the problem and what needs to be fixed. A developer would need to modify how Python domain IDs are generated, likely reverting to using underscores instead of dashes or providing backward compatibility. The issue references specific pull request #7236 which introduced the change, which would help in understanding the exact code that needs to be modified.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7380": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes a specific problem with the C++ domain in Sphinx where using parentheses in template parameter packs fails to render correctly. The user has:\n\n1. Provided clear code examples showing the problematic syntax\n2. Included a complete reproduction case with a GitHub repository\n3. Specified the expected behavior (that parentheses should work in template parameter packs)\n4. Included environment information (OS, Python version, Sphinx version)\n5. Attached build output for further diagnosis\n6. Provided a real-world use case showing why this functionality is needed\n\nThe issue is specifically about the C++ domain parser in Sphinx failing to handle parentheses in template parameter packs, particularly when used with casts. This is a well-defined parsing problem with clear boundaries. A developer would be able to reproduce the issue, locate the relevant code in the C++ domain parser, and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7395": {
        "explanation": "This issue is well-specified with clear details about the problem. The description:\n\n1. Clearly identifies the inconsistency between index entries generated by the \"index\" directive with \"builtin\" type versus the \"function\" directive\n2. Provides concrete examples of both directives and their outputs\n3. Explicitly lists the four differences between the generated outputs\n4. Includes a link to see the actual implementation in the Python documentation\n5. States the desired outcome: to unify and merge index entries generated by these two directives\n\nThe issue provides enough context for a developer to understand the problem and what needs to be fixed. The solution would involve modifying the documentation generation system (likely Sphinx) to ensure consistent formatting of index entries regardless of which directive is used to generate them.\n\nA developer would need to:\n1. Locate the code responsible for generating index entries from these directives\n2. Understand how each directive currently formats its entries\n3. Modify the code to standardize the output format\n4. Test the changes to ensure consistency\n\nThis is a straightforward documentation formatting issue with clear examples and a well-defined goal.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7440": {
        "explanation": "This issue is well-specified with clear information about what's happening:\n\n1. The problem is clearly stated: there's a warning about duplicate glossary terms in the documentation, specifically for \"mysql\" where there appears to be another entry with a different case (\"MySQL\").\n\n2. Reproduction steps are provided with exact commands to clone the repository, navigate to the documentation directory, install Sphinx, and build the documentation.\n\n3. The expected behavior is implied through the question \"MySQL != mysql term right?\" suggesting that the reporter believes these should be treated as different terms due to case differences.\n\n4. The exact file location is provided (glossary.rst) with a link to the specific line in the repository.\n\n5. Environment information is included (OS, Python version, Sphinx version).\n\n6. A Travis CI build link is provided showing where the error occurred.\n\nThe issue is about Sphinx documentation treating differently-cased versions of the same term as duplicates, and the solution would involve either:\n1. Merging the two glossary entries\n2. Renaming one of them to be distinctly different\n3. Configuring Sphinx to treat case-sensitive terms differently (if possible)\n\nThis is a straightforward documentation issue with all the necessary information to solve it.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7454": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and environment information. The bug is precisely described: when using `autodoc_typehints='signature'`, the `None` return type in function signatures doesn't generate a clickable link to Python's documentation, while it does when using `autodoc_typehints='description'`. The issue includes:\n\n1. A complete reproduction script that demonstrates the problem\n2. The exact output showing the inconsistency\n3. The expected behavior (None should link to documentation in both modes)\n4. Environment details (OS, Python version, Sphinx version)\n5. Additional context about related fixes\n\nA developer could easily reproduce this issue and understand what needs to be fixed: the code that handles type hints in signature mode needs to be updated to create links for the `None` type similar to how it works in description mode. The solution would involve modifying Sphinx's autodoc extension to ensure consistent handling of the `None` type across both modes.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7462": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information. The bug occurs when using an empty tuple type annotation (`Tuple[()]`) in Python code that is being documented with Sphinx. The error is clearly identified as an `IndexError: pop from empty list` in the Sphinx Python domain parser.\n\nThe issue includes:\n1. A clear code example that triggers the bug\n2. The exact error message with traceback location\n3. Complete reproduction steps including dependencies and versions\n4. Expected behavior (docs should build with valid type annotations)\n5. Environment information (OS, Python version, Sphinx version)\n6. A link to the GitHub repository where the issue can be reproduced\n\nThe root cause appears to be in Sphinx's handling of empty tuple type annotations, specifically in the `unparse` method in `sphinx/domains/python.py`. This is specific enough that a developer could locate the problematic code and understand what needs to be fixed without requiring additional information.\n\nThe solution would likely involve modifying Sphinx's Python domain parser to properly handle empty tuple type annotations without attempting to pop from an empty list.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7578": {
        "explanation": "This issue is reasonably well-specified with clear information about the bug:\n\n1. The issue identifies a specific problem: broken autosummary links in Sphinx-Gallery builds\n2. It provides the exact commit where the problem started (e5192ba48b45576e636e7dce82ad9183051443ed)\n3. It includes a specific error message showing what's failing\n4. It provides a link to a test case repository that reproduces the issue\n5. It includes steps to reproduce the issue locally\n6. It mentions a potential related issue (#7549) but explains why that might not be the cause\n\nThe only aspects that make this less than perfectly specified:\n- The reporter mentions they're still working on creating a more minimal example\n- There's no explicit mention of what the expected behavior should be (though it's implied that the links should work)\n- There's no mention of what versions of Python or other dependencies are being used\n\nHowever, with the provided reproduction steps and error message, a developer should be able to investigate and understand the issue without needing additional clarification. The issue is specific enough to begin meaningful work on a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7590": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: C++ User Defined Literals (UDLs) are not supported in Sphinx documentation\n2. A specific example that demonstrates the issue with the error message\n3. The location in the codebase where the limitation exists (with a link to the relevant file)\n4. A clear request to add support for UDLs in Sphinx\n\nThe issue provides enough context for a developer to understand what needs to be implemented. The developer would need to:\n1. Understand how C++ UDLs work (e.g., the `q_J` and `q_s` suffixes in the example)\n2. Modify the C++ domain parser in Sphinx to recognize and properly handle these literals\n3. Update the relevant code in the specified file (sphinx/domains/cpp.py)\n\nThe solution path is clear, and the requirements are specific enough to implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7593": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly defined: Currently, Sphinx converts `:kbd:` markup to a single `<kbd>` HTML element, but the user wants it to parse key combinations and convert them to individual or nested `<kbd>` elements.\n\n2. The desired solution is explicitly described with concrete examples of the expected HTML output (both individual and nested options).\n\n3. The issue includes references to MDN documentation that supports the proposed approach.\n\n4. Alternative solutions are acknowledged (users manually formatting with escaped characters).\n\n5. Related issues and PRs are linked for context.\n\nThe issue provides enough technical detail about both the current behavior and the desired behavior to implement a solution. The developer would need to modify how Sphinx parses the `:kbd:` role to detect key combinations (likely using hyphens and spaces as delimiters) and generate the appropriate nested HTML structure.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7615": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and environment information. The bug is precisely described: Sphinx incorrectly renders four or more consecutive backslashes compared to Docutils, which follows the RST spec correctly. The issue includes:\n\n1. A clear code snippet to reproduce the problem\n2. Specific expected behavior (how many backslashes should render as what)\n3. A screenshot showing the incorrect rendering\n4. Complete environment information (OS, Python version, Sphinx version)\n5. A comparison to the correct behavior in Docutils\n\nThe issue provides all the necessary information for a developer to understand the problem, reproduce it, and implement a fix by making Sphinx's backslash rendering match Docutils' behavior according to the RST specification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7670": {
        "explanation": "The issue is requesting support for C++20 \"requires clauses\" which are part of C++ concepts and constraints. The description provides:\n\n1. A clear feature request: Add support for C++20 requires clauses\n2. A link to documentation explaining what requires clauses are\n3. Context about the requester's project (mp-units) that would benefit from this feature\n4. A link to the project's documentation\n\nHowever, the issue lacks specific details about:\n- What tool/software this request is for (presumably some kind of C++ IDE, compiler, or analysis tool)\n- What specific functionality related to requires clauses is needed (syntax highlighting, code completion, error checking, etc.)\n- Any examples of how the current system fails to handle requires clauses\n- What the expected behavior would be when implemented\n\nWithout knowing what software this feature request is for and what specific functionality is needed, it would be difficult to implement a complete solution. A developer would need to make assumptions about the scope and requirements of the implementation.",
        "label": "VAGUE"
    },
    "sphinx-doc__sphinx-7738": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description clearly identifies the problem: when using the Napoleon extension with Sphinx, attribute names ending with an underscore (like `hello_`) are incorrectly rendered with an escaped backslash (`hello\\_`) in the HTML output.\n\nThe issue includes:\n1. Complete reproduction steps with code examples\n2. Clear expected behavior (no backslash should appear)\n3. Environment information (OS, Python version, Sphinx version, extensions)\n4. A note that older versions (2.4.3) don't have this issue, suggesting a regression\n\nThe solution would involve fixing how Napoleon processes attribute names with trailing underscores to prevent the unnecessary escaping of the underscore character in the HTML output. This is a specific, technical issue with a clear definition of success: the attribute name should render without the visible backslash.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7748": {
        "explanation": "The issue describes a specific problem with the `autodoc_docstring_signature` feature when working with overloaded methods in SWIG-wrapped C++ classes. The issue clearly states:\n\n1. The current behavior: `autodoc_docstring_signature` only picks up the first signature from the docstring\n2. The desired behavior: It should pick up all signatures for overloaded methods\n3. The context: This happens when using SWIG to wrap C++ classes with overloaded methods for Python\n\nThe issue provides enough information to understand the problem and what a solution would entail - modifying the `autodoc_docstring_signature` functionality to handle multiple signatures in docstrings generated by SWIG.\n\nHowever, there are some details that would be helpful but are missing:\n- Examples of the current behavior and expected behavior\n- Information about the specific format of SWIG-generated docstrings\n- Details about how the signatures are formatted in the docstring\n\nDespite these missing details, a developer familiar with Sphinx's autodoc functionality and SWIG's documentation conventions should be able to investigate and implement a solution based on this description. The core problem and desired outcome are clear.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7757": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that when documenting a Python function with positional-only parameters (using the \"/\" syntax introduced in Python 3.8) that have default values, Sphinx is not displaying those default values in the generated documentation.\n\nThe issue includes:\n1. Clear reproduction steps with a specific code example\n2. A screenshot showing the actual output\n3. A clear statement of expected behavior (default values should be shown)\n4. Environment details including OS, Python version (3.8.2), and Sphinx version (3.1.0dev)\n\nThe problem is specific and focused: the default value for parameter 'b' (which is a positional-only parameter marked by the '/' syntax) is not being displayed in the documentation, while it should be shown as \"b=0\" similar to how other parameters with default values are displayed.\n\nA developer would have enough information to locate the issue in Sphinx's code that handles the rendering of function signatures, particularly the part that deals with positional-only parameters, and fix the display of default values for these parameters.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7760": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The user wants to add a configuration option to the Sphinx coverage extension that would print missing coverage as warnings to stdout/stderr.\n2. The purpose is to make CI builds fail when documentation coverage is incomplete, similar to how the linkcheck builder reports broken links.\n3. The user has provided a specific name for the configuration option: `coverage_print_missing_as_warning`.\n4. The user has shown their current tox configuration and explained why the current behavior doesn't meet their needs.\n5. They've also considered an alternative approach (writing a script to check the output files).\n\nThe issue provides enough context about the current behavior, the desired behavior, and a specific implementation suggestion. A developer familiar with Sphinx's coverage extension would have sufficient information to implement this feature without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7762": {
        "explanation": "This issue is well-specified with clear problem description and solution requirements:\n\n1. The problem is clearly identified: The Sphinx linkcheck builder has a hardcoded HTTP Accept header that causes issues with certain servers (specifically GitHub in this case).\n\n2. The issue provides concrete evidence of the problem:\n   - Links to the specific code in the Sphinx repository where the Accept header is hardcoded\n   - Shows exact curl commands demonstrating the issue\n   - Provides HTTP response details showing how different Accept headers produce different results\n\n3. The requested solution is clear: Make HTTP headers configurable in the linkcheck builder.\n\n4. The issue includes an alternative solution that's currently being used (adding URLs to linkcheck_ignore) but explains why a better solution is needed.\n\n5. The issue provides sufficient context for a developer to understand and implement the feature:\n   - The specific file that needs modification (linkcheck.py)\n   - The exact line number where the change is needed\n   - Clear examples of how different headers affect the response\n\nA developer could implement this feature without needing additional clarification. The solution would involve adding configuration options to allow users to specify custom HTTP headers for the linkcheck builder.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7814": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and even a potential solution path. The reporter has:\n\n1. Clearly described the bug: Sphinx autodoc raises warnings for non-builtin type annotations, causing builds with nitpicking to fail\n2. Provided complete reproduction steps with code examples\n3. Specified the expected behavior\n4. Included environment information (OS, Python version, Sphinx version)\n5. Provided additional context with a potential root cause analysis\n6. Even included a potential patch that partially solves the issue\n\nThe reporter has also identified a limitation in their proposed solution (it doesn't work with custom classes), which gives further clarity about the scope of the problem.\n\nA developer could take this information and:\n1. Reproduce the issue using the provided steps\n2. Understand the root cause (the change in commit 88e8ebbe199c)\n3. Test the proposed patch and extend it to handle custom classes\n4. Implement a complete solution\n\nThe issue contains all necessary information to understand, reproduce, and begin working on a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7859": {
        "explanation": "This issue is well-specified with clear reproduction steps and detailed error information. The bug report describes:\n\n1. The exact problem: Using `autodoc_typehints = 'description'` in Sphinx configuration causes a KeyError: 'module'\n2. Complete reproduction steps:\n   - Git repository to clone and specific commit hash\n   - Instructions to modify the conf.py file\n   - Commands to run to reproduce the error\n3. Environment details:\n   - OS: Linux\n   - Python version: 3.5.6\n   - Sphinx version: 3.0.4\n   - Sphinx extensions being used\n4. Full error message and traceback\n\nThe error is clearly related to Sphinx's autodoc extension when using the 'description' value for autodoc_typehints. The traceback points to the specific issue in sphinx/ext/autodoc/typehints.py where it's trying to access a 'module' key that doesn't exist.\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification. They can reproduce the exact environment, see the full error stack trace, and understand the configuration that triggers the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7889": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information. The reporter describes a specific bug where Sphinx's Autodoc extension throws a TypeError when building documentation for generically-typed classes. The error occurs because `mock._make_subclass` attempts to concatenate a string to a TypeVar.\n\nThe issue includes:\n1. Clear description of the bug\n2. Complete reproduction steps with a specific repository to clone\n3. Expected behavior\n4. Environment details (OS, Python version, Sphinx version, extensions)\n5. Error log reference\n\nThe reporter has provided all necessary information to understand and reproduce the issue. The problem is specifically in Sphinx's handling of generic typing in the autodoc extension. A developer would be able to investigate this issue by following the reproduction steps, examining the error in the mock module, and implementing a fix that properly handles TypeVar objects when building documentation for generic classes.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7906": {
        "explanation": "This issue is well-specified with clear examples and expected behavior. The reporter:\n\n1. Clearly describes the current behavior: when `:noindex:` is added to a directive, it prevents cross-referencing and no permalink is generated\n2. Provides specific ReST input code examples\n3. Shows the actual HTML output generated\n4. Explains the expected behavior: `:noindex:` should only prevent index entries, not cross-referencing or permalinks\n5. Provides a specific example of what the expected HTML output should look like\n6. Includes environment details (OS, Python version, Sphinx version)\n\nThe issue has a clear problem statement and a specific expected solution. A developer would understand exactly what needs to be fixed: modify the behavior of the `:noindex:` flag so it only prevents index entries but still allows cross-referencing and permalinks. The implementation details are clear from the examples provided.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7910": {
        "explanation": "This issue is well-specified with clear technical details about the problem. The user has:\n\n1. Clearly identified the issue: decorated `__init__` methods don't show up in Sphinx documentation despite having `napoleon_include_init_with_doc = True` set.\n\n2. Provided the root cause analysis: They've traced the problem to a specific Sphinx commit and identified exactly why it's happening - when `__init__` is decorated, its `__globals__` no longer contains the class reference, causing Sphinx to incorrectly determine it's not a method.\n\n3. Included relevant code snippets and debug information showing the variables at the point of failure.\n\n4. Provided environment details including OS, Python version (3.6), and Sphinx version (1.6.5).\n\nThe issue has enough information for a developer to understand the problem, reproduce it, and develop a solution. The fix would likely involve modifying how Sphinx determines if a method belongs to a class when dealing with decorated methods.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7930": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter:\n\n1. Clearly describes the problem: When using `sphinx.ext.autodoc` with `nitpicky = True` and dataclasses with custom type variables, they get a warning.\n2. Provides a complete reproduction package (sphinx-example.zip) with instructions on how to reproduce the issue.\n3. Specifies the exact environment (macOS, Python 3.7.7, Sphinx 3.1.2).\n4. Notes that the issue is reproducible in Sphinx 3.1.0 but not in 3.0.4, indicating a regression.\n5. States the expected behavior: no warning or a clear message on how to avoid it.\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and work on a solution. The regression information is particularly helpful as it narrows down where the issue was introduced. The only minor limitation is that the specific warning message isn't included in the description, but this can be easily determined by following the reproduction steps.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7961": {
        "explanation": "The issue is requesting support for MathJax 3.0 in the Sphinx MathJax extension. It provides clear context about why this is important (80% decrease in load time and other improvements) and links to relevant documentation including the MathJax 3.0 release notes and upgrading guide. \n\nThe issue acknowledges that the upgrade won't be trivial based on the upgrading documentation, which shows awareness of the potential complexity. However, it lacks specific technical details about:\n1. What exact changes would be needed in the Sphinx MathJax extension\n2. Any specific compatibility issues that need to be addressed\n3. A clear definition of what \"success\" would look like beyond just upgrading to MathJax 3.0\n\nWhile the general goal is clear (upgrade to MathJax 3.0), a developer would need to research the specific technical requirements by examining both the current Sphinx MathJax extension implementation and the MathJax upgrade documentation to determine the exact changes needed. The developer would need to make decisions about implementation details that aren't specified in the issue.\n\nThe issue ends with \"What do folks think?\" which suggests it's partly a discussion starter rather than a fully specified task, but there's enough information to begin meaningful work on the upgrade.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-7975": {
        "explanation": "This issue is well-specified and provides clear information about the problem:\n\n1. The issue describes a specific bug in the HTML output of an index feature.\n2. It clearly identifies the problem: when using index entries with special characters (_@_, _\u00a3_, and _\u2190_), two separate sections called \"Symbols\" are generated in the HTML output.\n3. It explains the exact behavior: one section contains _@_ entries (before normal words) and another contains _\u00a3_ and _\u2190_ entries (after normal words).\n4. It describes the navigation problem: both sections have the same HTML anchor, causing both \"Symbols\" links at the top of the index page to lead to the first section only.\n\nThe issue provides enough detail to understand the problem, reproduce it, and identify what a successful solution would look like: the index should have a single \"Symbols\" section containing all special character entries, or if they remain separated, they should have distinct anchors so links navigate to the correct sections.\n\nA developer can work on this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-7985": {
        "explanation": "This issue is well-specified with clear information about what is needed. The user wants the `linkcheck` feature in Sphinx to check local (internal) links in addition to external links. \n\nThe issue includes:\n1. A clear problem statement: linkcheck doesn't check local links but should\n2. A reproducible example with code to demonstrate the issue\n3. The current output showing that external links are checked but local links are not\n4. The expected behavior: local links should also be checked\n5. Environment information including OS, Python version, and Sphinx version\n\nThe solution would involve modifying the linkcheck builder in Sphinx to validate internal document references in addition to external URLs. The implementation would need to check if local references like \"doesntexist\" actually point to valid targets within the documentation. This is a well-defined enhancement request with a clear scope.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8007": {
        "explanation": "This issue is well-specified. The user is requesting a feature for autodoc to have an option to not unfold type aliases in the documentation. They clearly explain:\n\n1. The current behavior: When using a type alias (e.g., `Position = int`), autodoc unfolds it to show the underlying type (e.g., `pos: int`) in the documentation.\n2. The desired behavior: An option to preserve the alias in the documentation (e.g., `pos: Position`).\n3. The rationale: Preserving aliases maintains information about how the program is structured.\n\nThe request includes a concrete example that illustrates both the current behavior and the desired behavior. The solution would involve adding a configuration option to autodoc that controls whether type aliases are unfolded or preserved in the generated documentation.\n\nThis is a straightforward feature request with clear requirements and a well-defined scope.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8035": {
        "explanation": "This issue is well-specified and clear about what needs to be implemented. The user wants to enhance the `:private-members:` option in Sphinx's autodoc extension to accept specific member names as arguments, similar to how the `:members:` directive already works. \n\nThe issue provides:\n1. A clear problem statement: Currently `:private-members:` is an all-or-nothing flag that includes all private members when enabled\n2. A specific requested solution: Make `:private-members:` accept arguments to specify which private members to document\n3. Context about current workarounds and limitations\n4. A reference to prior discussion on a related issue\n\nThe implementation would likely involve modifying the autodoc extension to parse arguments provided to the `:private-members:` directive and filter private members accordingly, similar to how `:members:` already works. This is a focused, well-defined feature request with a clear path to implementation.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8037": {
        "explanation": "This issue is well-specified with clear information about the bug and how to reproduce it. The user is reporting that Sphinx's C++ parser is failing to handle valid C++14 syntax, specifically in a template parameter list. The error message is provided in detail, showing exactly where the parser is failing: at a template parameter with std::enable_if.\n\nThe reproduction steps are complete and specific:\n1. Clone a specific branch of a repository\n2. Navigate to a specific directory\n3. Install requirements\n4. Run a specific command\n\nThe expected behavior is clearly stated: the build should succeed instead of failing with the reported error.\n\nThe issue contains all the necessary information for a developer to understand the problem, reproduce it, and work on a fix. The error is specific to Sphinx's C++ parser handling of template parameter lists with std::enable_if, which is a common C++14 pattern. A developer would need to investigate how the C++ parser in Sphinx handles template parameter lists and fix the parsing logic to properly handle this syntax.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8056": {
        "explanation": "This issue is well-specified with clear details about the problem and expected behavior:\n\n1. The problem is clearly defined: When documenting multiple parameters on the same line in a docstring using numpydoc format, the rendering in HTML is incorrect.\n\n2. The issue includes specific examples of the docstring syntax being used and a screenshot showing the problematic rendering.\n\n3. The expected behavior is explicitly stated with a clear example of how it should look.\n\n4. The environment information is comprehensive, including:\n   - OS version\n   - Python version\n   - Sphinx version\n   - Browser information\n   - List of Sphinx extensions being used\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and work toward a solution. A developer would be able to identify which component of the documentation system needs to be modified to fix the rendering of multiple parameters in docstrings.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8075": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and environment details. The user has:\n\n1. Provided a complete test document that demonstrates the problem\n2. Included the exact error messages and warnings\n3. Shared screenshots of both HTML and LaTeX outputs showing the issues\n4. Specified their environment (OS, Python version, Sphinx version)\n5. Clearly articulated three specific expected behaviors they want to see implemented\n\nThe issue is about references to figures without captions causing errors in both HTML and LaTeX outputs. The user wants:\n- Valid LaTeX output without undefined references\n- Uncaptioned figures to be referenceable in LaTeX (possibly with an optional setting)\n- More meaningful warnings about uncaptioned figures\n\nThe reproduction steps are detailed enough that a developer could recreate the issue and understand what needs to be fixed. The expected behavior is specific enough to guide implementation. This is a well-documented bug report with all the necessary information to attempt a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8117": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user is experiencing a problem with Sphinx documentation where a C function declaration with a trailing macro (`noreturn_function`) is causing a build error, even though this attribute is defined in the `c_id_attributes` configuration.\n\nThe issue provides:\n1. The exact function declaration causing the problem: `void up_exit(int status) noreturn_function;`\n2. The relevant configuration showing that `noreturn_function` is included in `c_id_attributes`\n3. The exact error message received\n4. The expected behavior: \"The macro should be ignored while parsing the function declaration\"\n5. Environment details including OS, Python version, and Sphinx version\n6. A link to the project repository\n\nWith this information, a developer could understand the issue, reproduce it, and work on a solution. The problem appears to be in how Sphinx's C domain parser handles function declarations with trailing attributes, even when those attributes are properly configured in `c_id_attributes`.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8120": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user is trying to override Sphinx's default translations by adding a custom `locale/da/LC_MESSAGES/sphinx.po` file to change caption labels for figures and code blocks in Danish. They've provided:\n\n1. A clear description of the problem: custom translations in `locale/da/LC_MESSAGES/sphinx.po` are being ignored in favor of official translations\n2. A specific test repository with exact checkout instructions\n3. Detailed reproduction steps including setup commands\n4. Clear expected behavior (custom labels \"Foobar 1\" and \"Whatever 1\" instead of default Danish translations)\n5. A screenshot showing the current behavior\n6. Complete environment information\n\nThe issue includes all necessary context to understand and attempt to solve the problem. The user has demonstrated they understand the Sphinx documentation about locale_dirs and has made a reasonable attempt to implement the feature. The problem is specific to how Sphinx handles custom translations in the locale directory, which is a well-defined technical issue.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8125": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution. The user explains:\n\n1. The problem: When a Python module has `__all__` defined, Sphinx's autodoc only passes the identifiers listed in `__all__` to the autodoc-skip-member function, preventing private members from being documented even when the user has a custom skip function to include them.\n\n2. Current behavior: The autodoc-skip-member function never sees identifiers not listed in `__all__`, as confirmed by the user's debugging with print statements.\n\n3. Desired behavior: Sphinx should pass all members to autodoc-skip-member, regardless of `__all__` definitions, possibly with an indicator in the options parameter that a member would have been excluded by `__all__`.\n\n4. Workaround: The user removed `__all__` from their module as a temporary solution.\n\nThe issue provides enough context to understand the problem, the current behavior, and the expected behavior. A developer familiar with Sphinx's autodoc system would be able to locate where in the code the filtering by `__all__` happens and modify it to pass all members to the skip function, potentially with additional metadata as suggested.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8202": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a detailed description of the bug: a warning appears when reassigning an attribute from a Generic class in a child class with an __init__ method.\n2. Included complete reproduction steps with a git repository to clone, installation commands, and how to trigger the error.\n3. Provided a minimal code example demonstrating the issue with both Python code and RST documentation.\n4. Clearly stated the expected behavior: \"The warning should not appear!\"\n5. Included environment information (OS, Python version, Sphinx version, and extensions).\n6. Noted that it's a specific corner case that might be sensitive to code modifications.\n7. Referenced a related issue.\n\nThe issue is specific to Sphinx documentation generation for Python code using generics and class inheritance. The warning occurs during documentation building, and the exact warning message is provided. A developer would have all the necessary information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8264": {
        "explanation": "This issue is reasonably well-specified. The user is encountering a TypeError: 'type' object is not iterable when trying to build documentation using Sphinx. They've provided:\n\n1. A clear error message and traceback\n2. The versions of their software (Sphinx 3.2.1, Python 3.8.5)\n3. The context (first time using Sphinx with a Miniconda virtual environment)\n4. The full stack trace showing the error occurs in Sphinx's typing module\n\nThe issue appears to be related to how Sphinx is handling type annotations in the documentation. The error occurs specifically in the `_stringify_py37` function in `sphinx/util/typing.py` when trying to iterate over `annotation.__args__`.\n\nWhile the user hasn't provided their actual documentation source files or the specific code that's causing the issue, the error is specific enough that a solution can likely be found by:\n1. Examining how they're using type annotations in their docstrings\n2. Checking for compatibility issues between their Python version and Sphinx version\n3. Looking for known issues in Sphinx related to handling certain types of annotations\n\nA developer with access to the codebase could identify the problematic type annotation pattern and suggest a fix or workaround.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8265": {
        "explanation": "This issue is well-specified with clear information about what's wrong and how to reproduce it:\n\n1. The problem is clearly defined: when rendering docstrings in HTML documentation, tuple default arguments (specifically color=(1, 1, 1)) are incorrectly displayed as separate arguments.\n\n2. Reproduction steps are provided with a complete Dockerfile that allows anyone to reproduce the issue.\n\n3. The expected behavior is clearly stated: the tuple default argument should be rendered correctly with parentheses.\n\n4. Links to the project and specific documentation page are provided.\n\n5. A screenshot shows the actual incorrect rendering.\n\n6. Environment information is complete.\n\nThe issue provides all necessary information for a developer to understand the problem, reproduce it, and work on a solution. The problem is specific to how Sphinx or the documentation system renders tuple default arguments in method signatures, and the fix would likely involve modifying how these are processed during documentation generation.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8269": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description explains that when `linkcheck_anchors` is set to `True`, Sphinx's `linkcheck` command reports anchor-related errors even when the primary issue is an HTTP error (like 404 or 500). \n\nThe issue includes:\n1. Clear description of the problem\n2. Exact reproduction steps with commands to run\n3. Actual output vs expected output comparison\n4. Environment information (OS, Python version, Sphinx version)\n\nThe solution would need to modify the linkcheck functionality to prioritize reporting HTTP errors over anchor-related errors when both occur. The expected behavior is clearly defined - it should report the HTTP error status code rather than \"Anchor not found\" when the server returns an error response.\n\nThis is a straightforward bug fix with a well-defined scope and clear success criteria.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8273": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. Problem: Currently, all generated man pages are placed in a single directory (`<build-dir>/man`), which doesn't work with the Unix `MANPATH` environment variable because the `man` program expects section directories (like `man/man1`, `man/man3`, etc.).\n\n2. Requested solution: Modify Sphinx to automatically create section directories (e.g., `man/man1/`, `man/man3/`, etc.) and place each generated man page in the appropriate section directory.\n\n3. The issue acknowledges potential backward compatibility concerns for users who expect the current behavior.\n\nThe technical context is clear - this relates to man page generation in Sphinx and how it interacts with standard Unix man page conventions. The solution approach is straightforward: restructure the output directory hierarchy to match standard man page conventions.\n\nThe only minor detail not specified is exactly how Sphinx would determine which section a particular man page belongs to, but this is likely determinable from existing metadata in the documentation or could follow standard conventions based on content type.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8278": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The bug is clearly described: Sphinx 3.2.1 converts hexadecimal default arguments to decimal in the rendered HTML documentation.\n\n2. There's a specific example provided showing the function definition with a hexadecimal default argument (color=0xffffff) and a screenshot showing how Sphinx incorrectly renders it as decimal (16777215).\n\n3. The expected behavior is explicitly stated: Sphinx should preserve the number system/radix used in the original code when displaying default arguments.\n\n4. The issue explains why this matters: hexadecimal is more intuitive for certain use cases like RGB colors and file permissions.\n\n5. Complete environment information is provided (OS, Python version, Sphinx version).\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed - Sphinx needs to be modified to preserve the original number format (hexadecimal in this case) when displaying default arguments in documentation.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8282": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that the `autodoc_typehints = 'none'` setting in conf.py is not being applied to overloaded functions defined with the `@overload` decorator from the typing module. \n\nThe issue includes:\n1. Complete reproduction code with all necessary files (conf.py, index.rst, and example.py)\n2. Clear expected behavior: the `autodoc_typehints` setting should apply to overloaded callables\n3. Environment information including OS, Python version, and Sphinx version\n4. The specific Sphinx extension being used (sphinx.ext.autodoc)\n\nA developer would be able to reproduce this issue and understand what needs to be fixed: the autodoc extension needs to be modified to respect the `autodoc_typehints` setting for functions decorated with `@overload`. The solution would involve examining how the autodoc extension processes overloaded functions and ensuring the typehint rendering respects the global configuration.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8284": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. Problem: Currently, all generated man pages are placed in a single directory (`<build-dir>/man`), which doesn't work with the Unix `MANPATH` environment variable because the `man` program expects section directories (like `man/man1`, `man/man3`, etc.).\n\n2. Requested solution: Modify Sphinx to automatically create section directories (e.g., `man/man1/`, `man/man3/`, etc.) and place each generated man page in the appropriate section directory.\n\n3. The issue acknowledges potential backward compatibility concerns for users who expect the current behavior.\n\nThe technical context is clear - this relates to man page generation in Sphinx and how it interacts with standard Unix man page conventions. The solution approach is straightforward: restructure the output directory hierarchy to match standard man page conventions.\n\nThe only minor detail not specified is exactly how Sphinx would determine which section a particular man page belongs to, but this is likely determinable from existing metadata in the documentation or could follow standard conventions based on content type.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8435": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that the `autodoc_type_aliases` configuration option in Sphinx is not being applied to variables and attributes, only showing its effect on function parameters and return types. \n\nThe issue includes:\n1. A clear description of the problem\n2. Complete code examples showing how to reproduce the issue\n3. The expected behavior (that type aliases should be applied to module variables and class attributes)\n4. Environment information including OS, Python version, and Sphinx version\n\nThe solution would need to modify Sphinx's autodoc extension to ensure that the type alias replacement also works for variables and attributes, not just function signatures. This is a specific, technical issue with a clear path to resolution.\n\nA developer would be able to understand what needs to be fixed and could verify the solution by using the provided reproduction steps.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8459": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter has:\n\n1. Provided a complete code example showing the issue (types.py with a function using a type alias)\n2. Included the relevant Sphinx configuration (conf.py with autodoc_typehints and autodoc_type_aliases settings)\n3. Shown the actual output they're getting with autodoc_typehints = 'description'\n4. Shown the output when removing that setting, demonstrating the difference\n5. Clearly stated the expected behavior: \"types.JSONObject instead of Dict[str, Any] in both cases\"\n6. Included environment information (OS, Python version, Sphinx version, and extensions)\n\nThe issue is that when autodoc_typehints is set to 'description', the type aliases defined in autodoc_type_aliases aren't being used - instead, the original types are shown. This is a specific, reproducible bug with a clear expectation of what correct behavior should be. A developer could implement a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8474": {
        "explanation": "This issue is reasonably well-specified. The user has provided:\n\n1. Clear context: They upgraded to Sphinx 3.3 and are now seeing specific warnings\n2. The exact warning message: \"WARNING: no number is assigned for table:\"\n3. When the warnings appear: during builds of 'singlehtml' or 'latex' formats\n4. What they've already tried: Looking through the changelog for numref-related changes\n\nWhat's missing:\n1. Specific examples of the tables causing the warnings\n2. The exact configuration they're using\n3. The complete build output showing the context of the warnings\n\nHowever, this is enough information to start investigating. A developer could:\n- Check Sphinx 3.3 release notes and code changes related to table numbering\n- Look for changes in how numref handles tables between versions\n- Create a minimal reproduction case to test the behavior\n- Examine how table numbering is implemented in Sphinx 3.3\n\nThe issue is specific enough to narrow down the investigation to table numbering functionality in Sphinx 3.3, making it reasonably specified for someone familiar with the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8475": {
        "explanation": "This issue is well-specified with clear details about the problem and the required solution:\n\n1. The problem is clearly identified: The linkchecker fails when HEAD requests result in \"Too Many Redirects\" errors, and doesn't fall back to GET requests in these cases.\n\n2. The specific scenario is described: Some websites enter infinite redirect loops with HEAD requests, causing the linkchecker to report valid links as broken.\n\n3. A concrete example URL is provided: https://idr.openmicroscopy.org/webclient/?show=well-119093\n\n4. The expected behavior is clear: The linkchecker should fall back to GET requests when HEAD requests result in \"Too Many Redirects\" errors, similar to how it handles other types of failures.\n\n5. The historical context is provided: This used to work before Sphinx 3.2.0 when HEAD requests were enforced (with reference to issue #7936).\n\nThe solution approach is straightforward: extend the existing except clause in the linkchecker to also catch TooManyRedirects exceptions and retry with a GET request in those cases. This is a targeted, specific change with a clear success criterion (the example URL should pass linkcheck after the fix).",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8506": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies the specific version (Sphinx 3.2) where the problem occurs\n2. It provides the exact error message being encountered\n3. It includes a concrete code example showing what worked in previous versions but now fails\n4. It explains the expected behavior (that Sphinx should accept a wider range of option syntax patterns)\n5. It provides context for why this is important (compatibility with different Linux distro sphinx-build versions)\n\nThe solution requirements are clear: modify Sphinx to accept more flexible option syntax patterns like \"[enable=]PATTERN\" that were previously accepted in older versions. The issue describes a regression in behavior that needs to be fixed to maintain backward compatibility.\n\nA developer would have enough information to locate the relevant code in Sphinx that handles option directive validation and modify it to be more permissive with the syntax patterns it accepts.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8509": {
        "explanation": "This issue is well-specified. The request is clear: change the default LaTeX engine for Japanese documentation from platex to uplatex. The rationale is provided - uplatex supports Unicode characters directly without character encoding conversion, while maintaining compatibility with the current default (platex). The issue references relevant GitHub issues and pull requests that provide context. The change appears to be a straightforward configuration update to make uplatex the default LaTeX engine specifically for Japanese documentation in Sphinx. There's enough information for a developer to implement this change without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8539": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The reporter describes a problem with Sphinx's autodoc functionality where using `autodoc_typehints='description'` together with `autoclass_content='class'` causes duplicate parameter documentation. When type hints from the `__init__` method are used, they appear both in the class documentation and in the `__init__` method documentation, creating redundancy.\n\nThe issue includes:\n1. Clear configuration settings that cause the problem\n2. Detailed reproduction steps with code examples\n3. A sample project attached as a tarball\n4. Expected behavior (no duplicate \"Parameters\" section)\n5. Environment information (OS, Python versions, Sphinx version)\n\nThe reporter also includes a related issue about `__init__` type hints not being used with `autodoc_typehints=\"description\"` but being used with `autodoc_typehints=\"signature\"`, which provides additional context.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed: the autodoc extension should not generate duplicate parameter documentation when using these specific configuration options together.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8548": {
        "explanation": "The issue describes a specific problem with the autodoc feature in Sphinx, where it fails to document inherited attributes (data members) properly. The issue explains that autodoc searches for cached docstrings using (namespace, attrname) as the search key but doesn't check for baseclass-namespace.\n\nWhile the issue identifies the problem clearly, it lacks some important details:\n1. No code examples demonstrating the issue\n2. No expected vs. actual behavior description\n3. No specific files or components mentioned where the fix should be applied\n4. No suggested solution approach\n\nHowever, the core problem is well-defined: autodoc is not properly handling inherited attributes because it's not checking the baseclass namespace for docstrings. This provides enough information for a developer familiar with the Sphinx codebase to locate the autodoc implementation and modify how it searches for docstrings of inherited attributes to include checking the baseclass namespace.\n\nA developer would need to:\n1. Understand how autodoc currently handles docstring caching\n2. Modify the search mechanism to also check baseclass namespaces\n3. Test with classes that have inherited attributes\n\nThe issue is specific enough to understand what's broken and what general approach is needed to fix it.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8551": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and environment information. The bug is precisely described: when using `:type:` and `:rtype:` fields in Sphinx documentation, unqualified class names are not properly resolved based on the current module context (set by `py:currentmodule`), leading to ambiguous class lookup warnings. \n\nThe issue provides:\n1. A complete code example that reproduces the problem\n2. The exact warning messages that appear\n3. Clear identification of where the bug occurs (marked with \"BUG\" comment)\n4. The expected behavior (no warnings, and proper resolution to `mod.submod.A`)\n5. Environment information (Sphinx versions tested)\n\nThe problem is that unqualified names in `:type:` and `:rtype:` fields are searching across all modules rather than respecting the current module context, unlike explicit xref roles. This is specific enough that a developer could understand the issue, reproduce it, and work on a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8552": {
        "explanation": "The issue is well-specified with a clear objective: add support for the \"Receives\" section of NumpyDoc guidelines to the Napoleon extension. The description includes:\n\n1. A clear reference to what needs to be added (the \"Receives\" section)\n2. A link to the official documentation explaining the format\n3. An explanation of what the \"Receives\" section is for (documenting parameters passed to a generator's .send() method)\n4. A note that it's related to the \"Yields\" section which is already supported\n5. A constraint that if \"Receives\" is included, \"Yields\" must also be included\n\nThe implementation would likely follow the pattern of how the \"Yields\" section is currently handled in the codebase. A developer would need to:\n1. Add parsing for the \"Receives\" section\n2. Format it similarly to the \"Parameters\" section (as specified in the NumpyDoc guidelines)\n3. Add validation to ensure \"Yields\" is present when \"Receives\" is used\n\nThis is a straightforward feature addition with clear requirements and existing patterns to follow.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8579": {
        "explanation": "This issue is well-specified with clear information to reproduce and diagnose the problem:\n\n1. The bug is clearly described: Sphinx's linkcheck builder crashes with a specific error message when running on Weblate documentation.\n\n2. Complete reproduction steps are provided:\n   - Git repository to clone\n   - Commands to run\n   - Dependencies to install\n\n3. The error message is fully provided, showing two exceptions:\n   - A ValueError: 'task_done() called too many times'\n   - A TypeError: '<' not supported between instances of 'int' and 'NoneType'\n\n4. Environment information is complete:\n   - OS: Linux\n   - Python version: 3.8.6\n   - Sphinx version: 3.4.0\n\n5. Expected behavior is clear: no crash should occur when running linkcheck.\n\n6. Additional context includes a link to the CI failure.\n\nThe issue appears to be a bug in Sphinx 3.4.0's linkcheck functionality, specifically related to queue handling in a threaded environment. With the provided repository, commands, and error messages, a developer should be able to reproduce and debug this issue without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8593": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user is reporting that the `:meta public:` directive is not working for variables in Sphinx's autodoc extension. They provide:\n\n1. A clear code example showing a private variable `_foo` with the `:meta public:` directive\n2. The RST configuration they're using\n3. The expected behavior (that `_foo` should be shown in the documentation)\n4. The actual behavior (it's not shown)\n5. Relevant environment information (OS, Python version, Sphinx version)\n\nThe issue is specific to the autodoc extension's handling of the `:meta public:` directive for variables, which appears to not be working as expected. This is a concrete, testable bug with clear reproduction steps that would allow a developer to verify the issue and implement a fix. The solution would involve modifying the autodoc extension to properly recognize and handle the `:meta public:` directive for variables, similar to how it presumably works for other Python objects.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8595": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that when a Python module has an empty `__all__` list (which should restrict what is exported/documented), Sphinx's autodoc extension is ignoring this empty list and documenting all functions anyway. \n\nThe reproduction case is complete with:\n1. A sample Python file (example.py) with an empty `__all__` list and three functions\n2. A sample RST file showing how to reference the module\n3. A clear description of what happens (all functions are shown) vs. what should happen (no functions should be shown)\n\nThe environment information is provided, including OS, Python version, and Sphinx version. The issue is specifically about the behavior of the sphinx.ext.autodoc extension when encountering an empty `__all__` list in a module.\n\nThis is a straightforward bug that can be investigated and fixed without needing additional information. The expected behavior is clearly defined: when `__all__` is empty, no members should be documented.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8599": {
        "explanation": "This issue is well-specified with clear requirements and a proposed implementation. The user wants to:\n\n1. Add a configuration option `html_add_permalinks_html` to customize the HTML content of permalink links\n2. Add a configuration option `viewcode_source_html` to customize the HTML content of source code links generated by the `viewcode` extension\n\nThe issue includes:\n- Current behavior with code examples\n- Desired behavior with code examples\n- A clear explanation of why this change would be beneficial\n- A proposed implementation approach\n- Example configuration values\n- Concerns about the implementation approach that could be addressed\n\nThe user has already created a patch implementing these features and is seeking feedback. The requirements are specific and the scope is well-defined. A developer could implement this feature based on the information provided without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8611": {
        "explanation": "This issue is well-specified with clear details about the problem. The user is experiencing an error in Sphinx documentation generation where an inherited attribute (`staticMetaObject` from `QObject`) is being flagged as undocumented in a class (`Reenter`) that inherits from `QObject`. \n\nThe issue description provides:\n1. Clear error message: \"attribute qtrio.Reenter.staticMetaObject is undocumented\"\n2. Specific code showing the class implementation\n3. Detailed reproduction steps with exact commands\n4. Full session output showing the error\n5. Environment information including OS, Python version, and Sphinx version\n6. Information about when the issue started occurring (with Sphinx 3.4.0)\n7. Link to the specific code in the repository\n\nThe expected behavior is also clearly stated: no warnings/errors should be generated for attributes that are only inherited.\n\nA developer would have all the necessary information to investigate and fix this issue, which appears to be related to how Sphinx 3.4.0+ handles documentation for inherited attributes.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8621": {
        "explanation": "This issue is well-specified with clear examples of the bug, expected behavior, and reproducible steps. The problem is clearly defined: the `:kbd:` role in Sphinx incorrectly processes compound-key separators (-, +, ^) when they are used as keystrokes themselves.\n\nThe issue provides:\n1. Specific examples of problematic input (`:kbd:'-'`, `:kbd:'+'`, `:kbd:'Shift-+'`)\n2. The exact incorrect HTML output currently being generated\n3. Clear explanation of the expected behavior\n4. A screenshot showing the visual result of the bug\n5. Environment information\n\nThe solution would need to modify the parsing algorithm for the kbd role to distinguish between these characters when they're used as separators versus when they're used as keystrokes themselves. The issue description provides enough context to understand the problem and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8633": {
        "explanation": "This issue is well-specified with clear reproduction steps and error messages. The bug description provides:\n\n1. Exact error message and traceback\n2. The specific versions where the issue occurs (Sphinx 3.4.1) and doesn't occur (Sphinx 3.3.1)\n3. A minimal reproducible example in the Python console\n4. The full error log as an attachment\n5. Clear context that this happens when building documentation for the psf/requests project\n\nThe root cause is identifiable: Sphinx's `restify` function in `sphinx/util/typing.py` is trying to access the `__name__` attribute of a `LookupDict` object (from the `requests.codes` module), but this object doesn't have that attribute. This is causing documentation builds to fail.\n\nA developer would have all the necessary information to diagnose and fix this issue, likely by modifying Sphinx to handle `LookupDict` objects properly in its type documentation system.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8638": {
        "explanation": "This issue is well-specified with clear details about the problem, steps to reproduce, and expected behavior:\n\n1. The bug is clearly described: When using autodoc (especially via apidoc), instance variables in a class that share the same name with variables elsewhere in the project are incorrectly auto-linked to those other variables, even though they are unrelated.\n\n2. Complete reproduction steps are provided:\n   - A specific GitHub repository with the relevant branch is provided\n   - Clear commands to clone, build, and serve the documentation\n   - Instructions to observe the issue\n\n3. Expected behavior is clearly stated: Class variable documentation should not automatically link to other variables of the same name elsewhere in the project.\n\n4. A screenshot is provided to illustrate the issue.\n\n5. Environment information is included, with the relevant components (Sphinx version 1.8.3 and extensions like autodoc) highlighted.\n\n6. The issue author has even created a dedicated example repository to demonstrate the problem.\n\nThis is a well-defined issue with all the necessary information to understand the problem and attempt a solution. The solution would likely involve modifying how Sphinx's autodoc extension handles references to variables with the same name across different scopes.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8674": {
        "explanation": "This issue is reasonably specified but lacks some details. The core request is clear: enable passing options to Pygments (a syntax highlighter) when used in Sphinx documentation. The issue specifically mentions the need for the `:startsinline:` option for PHP code snippets.\n\nThe issue suggests two implementation approaches:\n1. Allow default options to be set in conf.py (the Sphinx configuration file)\n2. Allow options to be specified directly in sourcecode blocks\n\nHowever, the issue lacks specific details about:\n- The exact syntax for how these options should be passed\n- Which Pygments options should be supported\n- How the feature should handle conflicts between global and local options\n- Any backward compatibility concerns\n\nDespite these missing details, there's enough information for a developer familiar with Sphinx and Pygments to make a reasonable implementation. The core requirement is clear, and the suggested implementation approaches provide a good starting point. A developer could research the Pygments API to understand available options and implement a solution that follows Sphinx's existing patterns for configuration.\n\nThe reference to issue #175 and discussions on #python-docs suggests there may be additional context available elsewhere, but the core request stands on its own.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8679": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description clearly explains that when creating a reference to a rubric directive that contains text with backtick quotes (``quoted``), the reference text is broken/incomplete in the rendered output. \n\nThe issue includes:\n1. A clear code example showing how to reproduce the bug\n2. A screenshot demonstrating the problem\n3. A link to a live example site reproducing the issue\n4. The expected behavior (reference text should be displayed entirely)\n5. Environment information (OS, Python version, Sphinx version)\n\nThe issue also links to the official Sphinx documentation for the rubric directive and provides a complete test project that demonstrates the issue. A developer would have all the necessary information to understand, reproduce, and fix this bug without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8684": {
        "explanation": "The issue is requesting an enhancement to the `:dedent:` functionality in what appears to be a documentation or code display tool, likely Sphinx (based on the directive syntax). The request is to make it behave like Python's `textwrap.dedent()` function, specifically allowing for automatic full dedentation when no explicit dedent value is provided.\n\nThe issue provides:\n1. A clear reference to the Python functionality they want to mimic (with a link)\n2. A specific suggestion for implementation (automatic full dedent when no value is provided)\n3. The context is about the `dedent` option for what appears to be Sphinx directives like `code-block` and `literalinclude`\n\nHowever, the issue lacks:\n1. Examples of current behavior vs. desired behavior\n2. Information about the current implementation\n3. Details about edge cases or potential challenges\n\nDespite these gaps, there is enough information to understand what is being requested and how it should work, especially for someone familiar with both Python's `textwrap.dedent()` and the Sphinx documentation system. The solution would involve modifying the existing `:dedent:` option to detect when no value is provided and then apply full dedentation in that case.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8707": {
        "explanation": "The issue description has some inconsistencies but provides enough information to understand the core problem. The title states \"viewcode does not work when `make singlehtml html`\" but the description says \"viewcode does not work when `make clean html`\". Then the reproduction steps show `make clean singlehtml html`. \n\nDespite this inconsistency, the expected behavior is clearly stated: viewcode should be \"Always enabled for HTML builds (except singlehtml and epub)\". This implies that the issue is that viewcode is not working for regular HTML builds when they are combined with singlehtml in the same make command.\n\nThe environment information is well-specified, including OS, Python version, Sphinx version, and the relevant extension (sphinx.ext.viewcode). The issue appears to be that when running a combined build command that includes both singlehtml and html targets, the viewcode extension doesn't work properly for the html output, even though it should work for html (but not for singlehtml).\n\nWhile there's some ambiguity in the exact command that's causing the issue, the core problem and expected behavior are clear enough that a developer familiar with Sphinx could investigate and attempt to fix the issue.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-8721": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description states that when building both HTML and EPUB formats with `make html epub`, the viewcode extension is creating module pages for EPUB even when the configuration option `viewcode_enable_epub=False` is set. \n\nThe issue provides:\n1. Clear reproduction steps: `make html epub`\n2. Expected behavior: module pages should not be created for EPUB when the option is disabled\n3. Environment details: Mac OS, Python 3.9.1, Sphinx version (HEAD of 3.x), and the specific extension causing the issue (sphinx.ext.viewcode)\n\nThe problem is clearly defined - the viewcode extension is ignoring the `viewcode_enable_epub=False` configuration when building multiple formats simultaneously. A developer would be able to reproduce this issue and understand what needs to be fixed: ensure that the viewcode extension respects the `viewcode_enable_epub` setting when building EPUB format, even when building multiple formats in one command.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8729": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes a problem with the `sphinx-apidoc` command when used with the `--implicit-namespaces` option. The issue is that the behavior changes depending on whether there are Python files directly in the root namespace or only in subpackages.\n\nThe reporter has:\n1. Clearly described the problem behavior\n2. Provided specific reproduction steps with commands\n3. Explained the expected vs. actual behavior\n4. Included a test project (sphinx-apidoc-bug.zip) to reproduce the issue\n5. Provided environment details (OS, Python version, Sphinx version)\n6. Suggested a potential solution (option to not search recursively or disable recursive search with implicit namespaces)\n\nA developer could take this information and:\n- Reproduce the issue using the provided steps and test project\n- Understand the expected behavior\n- Implement a fix based on the suggested approach or another solution\n- Test that the fix resolves the issue as described\n\nThe issue is specific to the `sphinx-apidoc` command's handling of Python namespace packages and is well-defined enough to work on without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8771": {
        "explanation": "This issue is well-specified. The user is requesting a feature for Sphinx's autodoc functionality to preserve variable names in function signatures rather than showing their literal values.\n\nThe issue clearly:\n1. Describes the current behavior (Sphinx replaces constants with their literal values in documentation)\n2. Provides a concrete example showing both the code and the current output\n3. Specifies the desired behavior (keeping the constant name in the signature)\n4. Explains why this is important (to avoid manually writing signatures)\n\nA developer would understand exactly what needs to be implemented: modify Sphinx's autodoc feature to detect when default parameters are constants and preserve their names rather than their values in the generated documentation. The scope is clear and the expected outcome is well-defined.\n\nThe only minor detail not specified is whether this should be an optional feature (configurable) or the default behavior, but this is a reasonable implementation decision that could be made during development.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8801": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes a specific problem with Sphinx's autodoc extension where annotated attributes in a parent class are not being properly documented when using the `:inherited-members:` directive in a child class.\n\nThe issue includes:\n1. A clear code example showing the parent class `Foo` with an annotated attribute `attr1` and a child class `Bar`\n2. The exact RST configuration being used\n3. The observed behavior (parent class attribute not showing up)\n4. The expected behavior (it should be shown)\n5. Environment details including Python and Sphinx versions\n\nThe problem is specific and reproducible - when using autodoc with `:inherited-members:`, type-annotated attributes from parent classes are being treated as \"undocumented\" and only appear when the `:undoc-members:` option is added. This is clearly a bug since the attribute has both a type annotation and a docstring comment.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed in the autodoc extension's handling of inherited annotated attributes.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-8969": {
        "explanation": "This issue is well-specified with clear details about the problem. The user is reporting that absolute paths in the `:file:` option of the `csv-table` directive are treated as truly absolute paths (starting from the root of the filesystem), whereas they expected them to be treated as \"source-relative\" paths (relative to the Sphinx source directory) similar to how the `figure` directive handles absolute paths.\n\nThe issue includes:\n1. Clear description of the current behavior\n2. Specific code examples demonstrating the problem\n3. Error messages showing what happens\n4. Clear explanation of the expected behavior\n5. Complete environment information\n\nThe user has provided enough context to understand the problem and what a solution would entail: modifying how the `csv-table` directive handles paths that begin with a forward slash to make them relative to the source directory rather than the filesystem root. This is a well-defined scope for a fix or enhancement to the Sphinx codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9015": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug in the autodoc documentation generation where `TracebackType` is incorrectly rendered as `traceback` and links to a local property instead of the external Python type.\n\n2. The reporter provides:\n   - A clear code example showing the class with type annotations\n   - A screenshot showing the actual incorrect rendering\n   - A clear explanation of the expected behavior (showing as `TracebackType` or linking to the Python documentation)\n   - The project information (pytest)\n   - Detailed environment information including OS, Python version, Sphinx version, and extensions\n\n3. The issue is specific to Sphinx autodoc's handling of type annotations, particularly how it renders and links external types versus local properties with similar names.\n\nThe problem is well-defined, reproducible, and the expected outcome is clear. A developer would have enough information to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9104": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly described: there are Sphinx documentation warnings about \"py:class reference target not found: ..\" occurring in the documentation for the `trio_main` method. The issue reporter has:\n\n1. Provided the exact warning messages\n2. Included the relevant code snippet showing the method signature with the `...` ellipsis in the type annotation\n3. Provided detailed reproduction steps with complete output\n4. Shared their hypothesis that it might be related to the ellipsis notation in Python type hints\n5. Suggested a possible solution (that it should just be plain `...` in the output or possibly link to Python's Ellipsis documentation)\n\nThe issue is related to Sphinx documentation generation and how it handles the ellipsis (`...`) in Python type annotations. The reporter believes the warning occurs because Sphinx is trying to interpret the ellipsis as a class reference rather than as the Python ellipsis literal.\n\nWhile the exact solution isn't specified, there's enough information for a developer to investigate and implement a fix, which would likely involve adjusting how the docstring or type annotation is written to properly handle the ellipsis notation in Sphinx documentation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9128": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes a problem where Sphinx's autodoc extension generates a duplication warning when documenting the same object that has been imported in two different ways (directly from its original module and through an import in another module).\n\nThe reproduction steps are complete and include:\n1. Example code showing the import of StringIO from io\n2. RST file content showing how the object is being documented twice\n3. The exact warning message being generated\n4. The expected behavior (no warning)\n5. Environment details including Python version (3.9.4) and Sphinx version (4.0.x)\n\nThe issue is clear: when documenting an object that is imported in multiple places (in this case, StringIO from io being documented both as example.StringIO and io.StringIO), Sphinx generates a duplication warning, but the user expects no warning since it's the same object.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - Sphinx's autodoc extension needs to recognize when the same object is being documented through different import paths and avoid generating duplication warnings in such cases.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9155": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is clearly defined: when using C builtin types like `int` in Sphinx documentation with `:param <type> <name>:` or `:type <name>: <type>:` syntax, it generates a warning about unparseable C cross-references. The user has provided:\n\n1. A clear description of the problem\n2. Complete reproduction steps with a downloadable example\n3. The exact error message received\n4. The expected behavior (that builtin types should be treated like other unresolved references without warnings)\n5. Detailed environment information\n\nThe issue is specific to Sphinx's C domain and how it handles builtin types in parameter documentation. A developer would have enough information to investigate and fix this issue without needing additional clarification. The solution would likely involve modifying how Sphinx's C domain parser handles keywords that are also builtin types.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9171": {
        "explanation": "This issue is well-specified. The user is describing a clear problem with the Sphinx `autoclass` directive in the Python domain. When using `autoclass`, Sphinx automatically adds constructor parameters (`*args`) to the class heading, which the user finds confusing because:\n\n1. It mixes the class declaration concept with constructor call syntax\n2. The user wants to document the constructor separately using `automethod`\n3. The class has an overloaded `__call__` method, making the automatic signature even more confusing\n\nThe user is specifically asking how to suppress the automatic addition of the constructor signature in the class heading when using the `autoclass` directive. This is a clear, specific feature request with a well-defined expected outcome: a way to generate class documentation headers without the constructor parameters.\n\nThe issue provides enough context about the current behavior, the desired behavior, and the rationale for the change. A developer familiar with Sphinx's Python domain implementation would have sufficient information to investigate and implement a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9207": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a detailed bug description - after upgrading to Sphinx 4.0.0, they're getting a warning about \"more than one target found for cross-reference\" that didn't appear in 3.5.4\n2. Created a minimal reproduction repository with exact steps to reproduce the issue\n3. Included the exact error output\n4. Specified the expected behavior (no warning)\n5. Provided environment details (OS, Python version, Sphinx version)\n6. Even bisected the issue to a specific commit in the Sphinx repository\n\nThe issue is related to how Sphinx 4.0 handles cross-references when there are multiple targets with the same name in different modules. The reproduction repository allows for direct testing and verification of the issue. The problem is clearly defined and the solution would involve addressing how Sphinx resolves ambiguous cross-references after the identified commit changed this behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9229": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user reports inconsistent behavior with type alias documentation in Sphinx, where docstrings for some type aliases are correctly displayed while others show only the default \"alias of...\" text.\n\nThe issue includes:\n1. A clear description of the problem\n2. Complete, step-by-step reproduction instructions with code examples\n3. Expected behavior clearly stated\n4. Environment information (OS, Python version, Sphinx version)\n5. Screenshots showing the issue\n6. A reference to a possibly related issue (#4422)\n\nThe reproduction steps are detailed enough to recreate the environment and observe the problem. The user has provided a minimal example that demonstrates the issue with three type aliases (ScaffoldOpts, FileContents, and FileOp), where only one displays the docstring correctly while the others show the default text.\n\nA developer would have enough information to investigate why some type alias docstrings are being properly rendered while others are not, and to implement a fix that ensures consistent behavior across all type aliases.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9230": {
        "explanation": "This issue is well-specified with clear information about what's wrong and what the expected behavior should be:\n\n1. The problem is clearly defined: When documenting a parameter with type `dict(str, str)` in a docstring, Sphinx renders it incorrectly as `str) opc_meta (dict(str,)` instead of the expected `opc_meta (dict(str,str))`.\n\n2. Steps to reproduce are provided: Create any method with a docstring containing the specified parameter format.\n\n3. Expected behavior is clearly stated: The parameter should be rendered as `opc_meta (dict(str,str)) \u2013 (optional)`.\n\n4. A test project (sphinxTest.zip) is provided to reproduce the issue.\n\n5. Screenshots showing both the docstring and the incorrect rendering are included.\n\n6. Complete environment information is provided (OS, Python version, Sphinx version, extensions).\n\nThis issue provides all the necessary information for a developer to understand the problem, reproduce it, and work on a solution. The issue is specifically about how Sphinx parses and renders docstrings with certain parameter types, which is a well-defined scope for investigation and fixing.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9231": {
        "explanation": "This issue is well-specified with clear details about the problem and its impact. The issue describes:\n\n1. A specific change in behavior: `man_make_section_directory` was enabled by default in a previous PR (#8284), which is causing problems.\n\n2. The exact nature of the problem: It breaks projects that rely on the previous behavior, causing man pages to be misplaced and unusable in Linux distributions.\n\n3. A specific technical issue: The implementation generates paths like `<builddir>/1` instead of `<builddir>/man1`, which doesn't work with MANPATH.\n\n4. Concrete examples of breakage: The issue provides two specific examples (fish-shell and llvm-project) with detailed explanations of how the change affects them.\n\n5. A proposed solution: Keep the option disabled by default.\n\nWith this information, a developer would have a clear understanding of what needs to be fixed (revert the default setting of `man_make_section_directory` to disabled) and why it's important. The issue provides sufficient context and examples to understand the impact of the current behavior and verify that a solution works correctly.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9233": {
        "explanation": "This issue is well-specified for the following reasons:\n\n1. The requester clearly states what they want to achieve: customize the formatting of the base list for classes, specifically to provide information about parameterized types.\n\n2. They reference a specific existing issue (agronholm/sphinx-autodoc-typehints#8) that demonstrates the use case.\n\n3. They propose a specific solution with a detailed API signature: `autodoc-process-bases(app, what, name, obj, options, formatted_bases)`.\n\n4. They explain how the new hook would work, including the parameters it would take and how it would modify the output.\n\n5. They provide two alternative approaches for the hook's behavior: either modifying the list in-place or returning a replacement string.\n\n6. The requester offers to implement the solution themselves, indicating they have a clear understanding of what needs to be done.\n\nThe issue provides enough context and specificity that a developer could implement the requested feature without needing additional clarification. The proposed hook follows the pattern of existing hooks in the system, making it consistent with the codebase's design.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9234": {
        "explanation": "This issue is well-specified with clear requirements for a feature enhancement to the link checker functionality. The problem is clearly articulated: the current link checker cannot distinguish between \"good\" redirects (to related content) and \"bad\" redirects (to unrelated pages). \n\nThe requested solution is specific: allow users to define pairs of URLs that represent acceptable redirects, while reporting errors for any redirects not in this allowlist. The issue provides concrete examples of both acceptable redirects (sphinx-doc.org to sphinx-doc.org/en/master/) and problematic redirects (a Windows blog post that now redirects to a general page instead of the specific article).\n\nThe issue also mentions an alternative approach that has been considered (post-processing the output file), which helps clarify what the user is trying to achieve.\n\nWith this information, a developer could implement a solution that:\n1. Allows configuration of allowed redirect pairs\n2. Checks if a redirect matches an allowed pair\n3. Reports errors for redirects not in the allowed list\n\nThe requirements are clear enough to begin implementation without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9246": {
        "explanation": "This issue is well-specified with clear reproduction steps, error messages, and context. The user has:\n\n1. Provided complete reproduction steps with exact versions of dependencies\n2. Included all relevant code samples (Python files and configuration)\n3. Shared the full error traceback showing the \"Unknown node: pending_xref_condition\" error\n4. Identified the likely source of the problem in the sphinx-qt-documentation plugin\n5. Explained the technical context, referencing a similar issue (#8996)\n6. Analyzed the root cause - the plugin's missing_reference implementation doesn't handle pending_xref_condition nodes\n7. Suggested a potential solution approach (resolving contnode when the missing-reference event is created)\n\nThe issue describes a specific technical problem with Sphinx documentation generation when using the sphinx-qt-documentation plugin, and provides enough information for a developer to understand and address the problem. The user has even done preliminary debugging to identify where the issue might be occurring.\n\nA developer could work on this issue without needing additional clarification, as the reproduction steps are complete and the technical context is well explained.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9258": {
        "explanation": "This issue is well-specified. It clearly describes:\n\n1. The feature request: Support for union types specification using the pipe symbol (|)\n2. The specific syntax being requested: `bytes | str` format for specifying multiple acceptable types\n3. A concrete example showing how it should work in docstrings\n4. Reference to existing implementation in PyCharm, indicating this is an established pattern\n\nThe issue provides enough context for a developer to understand what needs to be implemented - adding support for the pipe symbol syntax in type specifications within docstrings. The implementation would involve updating the type parsing logic to recognize the pipe symbol as a union type operator.\n\nThe only minor detail not specified is which specific tool or library this feature request is for (likely a Python documentation or type checking tool), but this would be obvious from the repository context.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9281": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a complete description of the problem: Python Enum values in function signatures are rendered in an undesirable way in Sphinx documentation\n2. Created a minimal reproducible example in a GitHub repository with clear steps to reproduce the issue\n3. Clearly stated the expected output (`ugly_enum_func(e: ugly_enum.MyEnum = MyEnum.ValueA) \u2192 None`) versus the actual output (`ugly_enum_func(e: ugly_enum.MyEnum = <MyEnum.ValueA: 10>) \u2192 None`)\n4. Included relevant environment information (Python 3.9.5, Sphinx 4.0.2, using autodoc extension)\n\nThe issue is focused on a specific rendering problem in Sphinx documentation where enum values show their internal representation (`<MyEnum.ValueA: 10>`) rather than just their symbolic name (`MyEnum.ValueA`). This is a clear cosmetic issue that could be addressed by modifying how Sphinx's autodoc extension formats enum values in function signatures.\n\nA developer would have all the necessary information to investigate and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9289": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior:\n\n1. The bug is clearly described: `python_use_unqualified_type_names` configuration doesn't work when combined with `autodoc_typehints='description'` - type names aren't shortened even though links are created.\n\n2. Complete reproduction steps are provided:\n   - Specific versions mentioned (Sphinx 4.0.2, Python 3.9)\n   - Full configuration file (conf.py)\n   - Sample code files (package.py and index.rst)\n   - Visual evidence of the issue (screenshot)\n\n3. The expected behavior is explicitly stated: type links should be shortened to just `A` instead of the fully qualified name, similar to how it works with `autodoc_typehints='signature'`.\n\nThe issue provides all necessary information to understand, reproduce, and fix the problem. A developer could immediately start working on this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9320": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user provides:\n\n1. A clear description of the bug: When running `sphinx-quickstart` in a folder with an existing conf.py file and pressing Enter at the prompt \"Please enter a new root path name (or just Enter to exit)\", it returns an error message instead of exiting.\n\n2. Simple reproduction steps: Run `sphinx-quickstart` twice in the same directory.\n\n3. Expected behavior: Pressing Enter should exit the program.\n\n4. A screenshot demonstrating the issue.\n\n5. Complete environment information: Ubuntu 20.04, Python 3.8.5, Sphinx 3.2.1.\n\nThe issue is focused on a specific behavior that doesn't match the documented/expected functionality. A developer would have all the information needed to reproduce the issue, locate the relevant code in the sphinx-quickstart command handling, and fix the validation logic that prevents exiting when pressing Enter.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9367": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed:\n\n1. The problem is clearly defined: 1-element tuples like `(1,)` are being incorrectly rendered as `(1)` without the trailing comma.\n\n2. The expected behavior is explicitly stated: the rendering should preserve the trailing comma for 1-element tuples.\n\n3. The issue provides context by referencing related issues (#7964 and #8265) that have already addressed similar problems.\n\n4. The issue includes specific instructions on how to reproduce the problem and where to add a test case, pointing to the exact file and line number in the codebase (https://github.com/sphinx-doc/sphinx/blob/e0b1e1002b500acc63dfd0806f8095dd6b27037b/tests/test_pycode_ast.py#L57).\n\n5. The test case format is provided: `(\"(1,)\", \"(1,)\")`, which clearly shows the input and expected output.\n\nWith this information, a developer can understand the issue, locate the relevant code, add the test case, and implement a fix to ensure that 1-element tuples maintain their trailing comma when rendered.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9386": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that when `autodoc_typehints` is set to `none`, type hints are still being rendered for properties in the documentation output. The issue includes:\n\n1. Clear description of the problem: properties still show type hints despite the setting\n2. Reproduction steps: download the provided zip file, go to docs directory, run `make html`\n3. Expected behavior: no type hints should appear in the output\n4. A screenshot showing the issue\n5. Complete environment information\n6. A sample project to reproduce the issue\n7. A real-world example where the issue was encountered\n\nThe issue is specific to Sphinx's autodoc extension and how it handles the `autodoc_typehints` setting for properties. With the provided reproduction materials and clear expected outcome, a developer would have all the necessary information to investigate and fix this bug.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9461": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is precisely defined: methods decorated with both `@classmethod` and `@property` are not being documented by Sphinx, while regular `@property` methods are documented correctly.\n\n2. The user provides a complete reproduction case with:\n   - A GitHub repository to clone\n   - Exact steps to reproduce the issue\n   - A list of specific methods that are not being documented correctly\n\n3. The expected behavior is clearly stated: methods with both decorators should be documented appropriately.\n\n4. The user has done investigation work and provided insights about the root cause:\n   - Explained how `type(BaseClass.baseclass_class_property)` returns the type of the returned object\n   - Noted that this will be fixed in Python 3.10 (with reference to a Python bug report)\n\n5. All relevant environment details are provided:\n   - OS: Ubuntu 20.04.2 LTS\n   - Python version: 3.9.6\n   - Sphinx version: 4.0.3\n   - Sphinx extensions being used\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and work on a solution. A developer could immediately start working on this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9464": {
        "explanation": "This issue is well-specified with clear information about the bug, reproduction steps, and a suggested solution:\n\n1. The problem is clearly defined: PEP 585 type hints (like `list[str]`) are not rendering correctly in Sphinx autodoc, showing only `list` instead of the full `list[str]`.\n\n2. Reproduction steps are provided with a simple code example and the corresponding RST directive.\n\n3. The expected behavior is explicitly stated: `list[str]` should render as `list[str]`.\n\n4. A specific solution is suggested: modify the code at a specific location in the Sphinx codebase to check for `__args__` attribute and use `repr(annotation)` in that case.\n\n5. All relevant environment details are provided: Python version (3.9.3), Sphinx version (4.1.1), OS (Mac), and the extension being used (autodoc).\n\n6. A screenshot is included showing the issue.\n\n7. A test repository is linked for further investigation.\n\nThis issue provides all the necessary information for a developer to understand the problem, reproduce it, and implement the suggested fix or an alternative solution.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9467": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug: GitHub linkcheck in Sphinx 4.1.0 is failing for certain types of links that worked in previous versions.\n\n2. It provides a concrete example of the problematic link format:\n```rst\n.. _`OpenSSL's test vectors`: https://github.com/openssl/openssl/blob/97cf1f6c2854a3a955fd7dd3a1f113deba00c9ef/crypto/evp/evptests.txt#L232 \n```\n\n3. Clear reproduction steps are provided:\n   - Clone the repository\n   - Run a specific command (tox -e docs-linkcheck)\n\n4. The expected behavior is clearly stated: the linkcheck should pass.\n\n5. The issue includes the specific Sphinx version (4.1.0) where the problem occurs.\n\n6. It even points to the specific commit in Sphinx that caused the change: https://github.com/sphinx-doc/sphinx/commit/92335bd6e67dec9d8cadfdfb6d441a440e8dc87e\n\n7. Failing test logs are provided for reference.\n\nWith this information, a developer could investigate how the GitHub linkcheck behavior changed in Sphinx 4.1.0 and implement a solution to make the links work with the new behavior or adapt the documentation to comply with the new requirements.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9547": {
        "explanation": "This issue is well-specified with clear examples of the problem and potential solutions. The user is reporting that Sphinx's C domain parser doesn't support GNU C extensions like `_Complex` and `_Fract` types, providing specific error messages and code examples that demonstrate the issue. \n\nThe issue includes:\n1. Clear examples of the C declarations that are failing to parse\n2. The exact error messages being generated\n3. References to relevant code in the Sphinx codebase that handles similar special cases\n4. Links to documentation for the GNU extensions that need support\n5. Suggestions for possible approaches to fix the issue (either adding special handling for these types or exposing the domain parser via an API)\n\nA developer would have enough information to understand the problem, locate the relevant code, and implement a solution that adds support for these GNU C extension types in the Sphinx C domain parser. The issue is specific and actionable without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9591": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that cross-references in property type annotations don't work properly in Sphinx documentation. The issue includes:\n\n1. A clear code example demonstrating the problem with property type annotations not being cross-referenced\n2. Complete reproduction steps including how to clone the repository, install dependencies, and build the documentation\n3. A link to a live example of the generated documentation showing the issue\n4. Expected behavior clearly stated: \"I expected the documented type in property's type annotation to be cross-referenced\"\n5. Environment details including OS, Python versions, Sphinx version, and extensions used\n\nThe issue creator has provided a minimal reproducible example in a dedicated repository, making it straightforward to understand and investigate the problem. The issue is specific to Sphinx's autodoc extension not properly handling cross-references in property return type annotations. A developer would have all the necessary information to work on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9602": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug description clearly identifies the problem: when using `Literal` type annotations (like `Literal[True]`), Sphinx with nitpick enabled flags these values as missing `py:class` references, causing build failures when using `-n -W` flags.\n\nThe issue provides:\n1. A clear code example demonstrating the problem\n2. A link to a complete reproduction repository\n3. Instructions on how to reproduce the issue (`./doc.sh`)\n4. The expected behavior (that literal values should not trigger nitpick warnings)\n5. Relevant environment details (OS, Python versions, Sphinx version, extensions)\n\nThe issue author has done thorough work in isolating and documenting the problem, making it straightforward to understand what needs to be fixed. A developer would be able to reproduce the issue and work on a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9654": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user describes a problem with Sphinx autodoc where it incorrectly identifies the base class in documentation for classes in an inheritance chain. Instead of showing the direct parent class (C), it shows a distant ancestor (A).\n\nThe issue includes:\n1. Clear description of the problem (incorrect base class detection in documentation)\n2. Specific reproduction steps with a GitHub repository, installation commands, and how to observe the issue\n3. Expected behavior (direct superclass should be shown)\n4. Environment details (OS, Python version, Sphinx version, extensions)\n5. A concrete example from their codebase (Landsat class showing torch.utils.data.Dataset instead of RasterDataset)\n\nThe issue is specific to Sphinx's autodoc extension and how it handles class inheritance documentation. With the provided information, a developer could investigate the issue in the specified codebase, understand the inheritance chain, and work on fixing how Sphinx determines which base class to display in the documentation.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9658": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue is precisely defined: When documenting classes that inherit from mocked classes (specifically torch.nn.Module), the \"Bases\" section in Sphinx documentation shows incomplete information (\"torch.nn.\" instead of \"torch.nn.Module\").\n\n2. The issue description includes:\n   - Exact Sphinx version dependencies and behavior differences\n   - Clear reproduction steps with git clone commands and build instructions\n   - Expected vs. actual behavior with screenshots\n   - Links to example builds showing both correct and incorrect behavior\n   - Environment details (OS, Python version, Sphinx version)\n   - Complete list of Sphinx extensions being used\n   - Links to a demo PR and readthedocs build demonstrating the issue\n\n3. The issue is clearly tied to Sphinx versions >=3.4.2 and specifically affects how mocked classes are documented in the inheritance chain.\n\nA developer would have all the necessary information to reproduce the issue and understand what needs to be fixed - the documentation system needs to correctly display the full class name of mocked base classes rather than truncating them.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9665": {
        "explanation": "This issue is reasonably specified, though it requires understanding of the context from previous issues (#4944, #3104, #9233). \n\nThe core problem is clear: The user wants to control how base classes are displayed in Sphinx documentation, specifically the text of hyperlinks for base classes. Currently, Sphinx can automatically insert references to documented base classes, but the user wants more control over the link text.\n\nThe proposed solution is also clear: Allow the `autodoc-process-signature` event to return a list of strings that will be appended to the \"Bases: \" section, which would enable users to return formatted strings like `:class:telegram.TelegramObject`.\n\nWhile the issue provides enough information to understand what needs to be done, a developer would need to:\n1. Review the related issues (#4944, #3104, #9233) to fully understand the context\n2. Understand how the `autodoc-process-signature` event currently works\n3. Determine how to modify it to accept string returns that would be used for base class references\n\nThis requires some investigation into the existing codebase and understanding of Sphinx's autodoc extension, but the goal and desired outcome are clear enough to work on a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sphinx-doc__sphinx-9673": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user is reporting that when using the Sphinx configuration option `autodoc_typehints_description_target = \"documented\"` together with the Napoleon extension (Google style), return types are missing from the generated documentation.\n\nThe issue includes:\n1. Clear configuration settings being used\n2. A sample docstring that demonstrates the problem\n3. Expected behavior (return type should be present in documentation)\n4. Environment details (OS, Python version, Sphinx version, extensions)\n5. A screenshot showing the issue\n6. The specific project where this is occurring\n\nA developer would be able to reproduce this issue by setting up a test environment with the specified configuration and docstring format. The problem appears to be that the `autodoc_typehints_description_target = \"documented\"` setting is not working correctly with Napoleon's Google-style docstrings for return types.\n\nThe solution would likely involve investigating how Napoleon processes return type information and how it interacts with the autodoc typehints configuration, then fixing the integration between these two features.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9698": {
        "explanation": "This issue is well-specified with clear information about what's happening and what should happen instead:\n\n1. The problem is clearly defined: When using a `py:method` directive with the `:property:` option, an index entry is created with parentheses, but it shouldn't have them.\n\n2. A clear reproduction case is provided with the exact RST code that triggers the issue.\n\n3. The expected behavior is explicitly stated: \"An index entry for the property should not have parens.\"\n\n4. A screenshot is provided showing the current incorrect behavior, which helps visualize the issue.\n\n5. The environment details are provided (OS, Python version, Sphinx version).\n\nThe issue is essentially about fixing how Sphinx generates index entries for Python methods that are marked as properties. When a method is marked with the `:property:` option, it should be treated like a property in the index (without parentheses), not like a method (with parentheses). This is a clear, specific bug with a well-defined expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9711": {
        "explanation": "This issue is well-specified with clear details about the bug, reproduction steps, and expected behavior. The problem is that the `needs_extensions` check in Sphinx is comparing version numbers as strings rather than as semantic versions, causing incorrect version compatibility checks. \n\nThe issue provides:\n1. A clear description of the bug: version comparison is done as strings, causing '0.6' to be considered greater than '0.10'\n2. Complete reproduction steps with a specific example repository\n3. The exact error message encountered\n4. Expected behavior: sphinx-gallery 0.10.0 should be accepted when 0.6.0 is the minimum specified\n5. Environment details including OS, Python version, and Sphinx version\n\nThe solution would require modifying how Sphinx compares version numbers in the `needs_extensions` check to use proper semantic versioning comparison rather than string comparison. This is a well-defined task with a clear success criterion.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9799": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The bug report describes that when using Sphinx's autodoc feature on a function with hexadecimal default arguments (like `0xFFFFFFFF`), these values are being converted to their decimal representation (`4294967295`) in the generated documentation. The reporter wants either to preserve the hexadecimal format in the documentation or to have an option to control this behavior.\n\nThe issue includes:\n1. A clear code example showing the function with a hexadecimal default value\n2. The actual output showing the decimal conversion\n3. The expected behavior (preserve hexadecimal format or provide an option to do so)\n4. Environment details including OS, Python version, Sphinx version, and relevant extensions\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided code example\n2. Understand what needs to be fixed (the conversion of hexadecimal values to decimal in autodoc)\n3. Implement a solution that either preserves the original format or adds a configuration option\n\nThis is a focused, specific bug with clear reproduction steps and a well-defined expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9828": {
        "explanation": "This issue is well-specified with clear problem definition and proposed solution. The issue explains:\n\n1. The current problem: \"fuzzy\" translations in PO files are not used in MO files, defaulting to source language instead\n2. Why this matters: fuzzy translations (like machine translations) could be useful if users had the option to include them\n3. A specific proposed solution: add a new configuration parameter \"use_fuzzy_translations\"\n4. The exact code change needed: modify line 72 in sphinx/util/i18n.py to pass the new parameter to write_mo()\n5. Confirmation that the solution is technically feasible: the babel package already supports this functionality\n\nThe issue provides the exact file path, line number, and code change required. It also references external documentation confirming the technical feasibility. A developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9829": {
        "explanation": "This issue is well-specified with clear technical details and proposed solutions. The issue describes a problem with configuring MathJax 3 to work with Sphinx, specifically:\n\n1. The problem is clearly defined: MathJax 3 expects configuration to be present before loading, but Sphinx loads MathJax asynchronously.\n2. The current limitations are explained in detail: the `mathjax3_config` parameter only accepts dictionaries that get JSON-serialized, which doesn't support functions.\n3. The issue provides code snippets showing the relevant parts of the codebase.\n4. Three specific solution options are proposed:\n   - Allow arbitrary strings for mathjax3_config without JSON serialization\n   - Change `async` to `defer` when loading MathJax\n   - Make it possible for users to change `async` to `defer` themselves\n\nThe reporter even indicates a preference for the latter two options and explains why they would be beneficial. This gives a clear direction for implementing a solution. A developer would have enough information to understand the problem and implement one of the proposed solutions without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9902": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user is reporting that when using the Sphinx `py:data` directive with the `:type:` option, it doesn't correctly create a link to the type when using an imported name rather than the fully qualified name.\n\nThe issue includes:\n1. Clear description of the problem: imported types in `:type:` option don't create clickable links\n2. Complete reproduction steps with code examples\n3. Expected behavior with specific output examples\n4. Environment details (OS, Python version, Sphinx version, extensions)\n5. Project repository link\n\nThe solution would need to modify Sphinx's handling of the `:type:` option in the `py:data` directive to resolve imported names within the module context rather than requiring fully qualified paths. This is a specific, technical issue with a clear definition of success: making the imported type name clickable in the documentation output.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9931": {
        "explanation": "This issue is well-specified with clear details about the problem and desired solution:\n\n1. The problem is clearly described: When using `add_module_names = False` in Sphinx configuration, it only affects class/function/attribute header names but not type hints in function arguments.\n\n2. The issue includes complete code examples showing:\n   - The Python code being documented\n   - The Sphinx configuration\n   - The RST file\n   - The expected vs. actual output\n\n3. The desired solution is explicitly stated with three acceptable options:\n   - Extend the existing `add_module_names` to affect type annotations\n   - Add a new `add_type_module_names` configuration option\n   - Add a new `autodoc_add_module_names` configuration option\n\n4. The issue even addresses an alternative solution found on StackOverflow and explains why it's not viable.\n\nA developer could implement this feature without needing additional clarification. The scope is well-defined, the expected behavior is clear, and the implementation options are reasonable.",
        "label": "WELL_SPECIFIED"
    },
    "sphinx-doc__sphinx-9997": {
        "explanation": "This issue is well-specified with clear reproduction steps, expected behavior, and context:\n\n1. The problem is clearly defined: autodoc typehints generate links for simple types (like `int`) but not for parametrized types (like `Literal[\"a\", \"b\"]`).\n\n2. Complete reproduction steps are provided with:\n   - Exact code to create a test file\n   - Commands to run Sphinx with specific configuration\n   - Where to look for the output\n\n3. Expected behavior is explicitly stated: `Literal` should link to typing.Literal in CPython's docs, just like `int` does.\n\n4. A screenshot is included showing the current behavior.\n\n5. Environment details are provided (OS, Python version, Sphinx version, extensions).\n\nThe issue is focused on a specific technical problem with autodoc typehints in Sphinx, and provides all the necessary information to understand, reproduce, and potentially fix the issue. A developer could work on this without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11384": {
        "explanation": "This issue is well-specified and clear. It has two distinct parts:\n\n1. The main issue: The `fps` (formal power series) function doesn't print in a way that reflects its nature as a formal power series. Instead, it prints like a normal series (same as `series`). The user wants it to print in a way that shows the actual formula representation it's using internally, which is visible when using `print(fps(sin(x)))`.\n\n2. A side question: When `fps` is given a function it can't compute (like `tan(x)`), it silently returns the original function instead of raising an exception. The user is questioning if this is intentional behavior or if it should raise an exception instead.\n\nBoth parts of the issue are clearly described with specific examples showing the current behavior and the expected behavior. The issue provides code snippets demonstrating the problem, making it easy to understand and reproduce. The desired outcome is also clear: the `fps` function should print in a way that shows its formal power series representation rather than looking like a normal series.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11400": {
        "explanation": "The issue is reasonably well-specified. It describes a problem with the `ccode()` function in SymPy not properly handling the `sinc(x)` function when converting to C code. The user provides:\n\n1. A clear example showing the current behavior (returning a comment that sinc is not supported)\n2. An acknowledgment that `math.h` doesn't have a built-in `sinc` function\n3. A suggested alternative implementation using `Piecewise` that works correctly\n\nThe issue implies that `ccode(sinc(x))` should be automatically translated to the piecewise equivalent shown in the second example, rather than returning \"not supported\". This is a reasonable interpretation of what's needed.\n\nThe only minor ambiguity is whether the solution should specifically use the exact piecewise form shown or if there are other acceptable implementations. However, this doesn't prevent understanding what a successful solution would look like: modifying the C code printer to properly handle the `sinc` function by implementing it as a piecewise function that handles the special case at x=0.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-11438": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the diophantine equation solver where equations with mixed even powers (like x\u00b2+y\u00b2+z\u2074) are being incorrectly classified as 'general_sum_of_even_powers' when they should not be. The issue provides:\n\n1. A specific code example that reproduces the bug\n2. The current incorrect output (classification as 'general_sum_of_even_powers')\n3. A clear explanation of what's wrong: \"A check should be made that all powers are the same (not only that they are even)\"\n\nThe solution would involve modifying the classification logic to ensure that for an equation to be classified as 'general_sum_of_even_powers', not only should all powers be even, but they should also be the same value. This would prevent equations with mixed powers (like x\u00b2+y\u00b2+z\u2074) from being incorrectly classified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11618": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the distance calculation between Point objects where one point has 3 dimensions and the other has 2 dimensions. The issue provides:\n\n1. A specific code example showing the problematic behavior\n2. The current incorrect output (1)\n3. An explanation of what's happening: the 3rd dimension is being ignored when calculating the distance\n4. The expected calculation: sqrt(5) instead of sqrt((2-1)^2 + (0-0)^2)\n\nFrom this description, I can understand that:\n- There's a Point class that can be initialized with 2 or 3 dimensions\n- The distance method is incorrectly handling cases where points have different dimensions\n- The fix would need to ensure all dimensions are considered in the distance calculation, even when points have different numbers of dimensions\n\nThis is sufficient information to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11788": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that singularity function powers do not print correctly in the qtconsole when using SymPy's continuum mechanics module. \n\nThe issue includes:\n1. A clear code example that reproduces the problem\n2. A screenshot showing the incorrect output\n3. The context (using qtconsole with SymPy's Beam class)\n\nWhat makes it not fully well-specified:\n1. It doesn't explicitly state what the correct output should look like\n2. It doesn't mention which version of SymPy is being used\n3. It doesn't specify if this happens in other environments (regular Python console, Jupyter notebook, etc.)\n\nHowever, a developer familiar with SymPy would likely understand that the issue relates to the display of singularity functions in the qtconsole, where the negative powers (-1, -2) in the apply_load method are not being rendered properly. The screenshot shows that the output is displaying the raw representation rather than a properly formatted mathematical notation.\n\nWith the provided code example and understanding of how SymPy's pretty printing should work, a developer could make a reasonable attempt at fixing this issue.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-11794": {
        "explanation": "This issue is quite vague and lacks critical details needed for implementation. The description only states that \"ASCII printing for Singularity Functions is needed\" without specifying:\n\n1. What Singularity Functions are in this context\n2. What the expected ASCII printing format should look like\n3. Any examples of desired output\n4. Where in the codebase this functionality should be implemented\n5. How this feature relates to existing code\n6. Any specific requirements or constraints for the implementation\n\nWithout understanding what Singularity Functions are in this codebase, what ASCII printing means in this context, and how the feature should be integrated with existing code, it would be very difficult to implement a solution that meets the actual requirements. A developer would need to make too many assumptions or request significant clarification before being able to work on this issue.",
        "label": "VAGUE"
    },
    "sympy__sympy-11822": {
        "explanation": "This issue is well-specified with a clear error trace and reproduction steps. The problem occurs when trying to print `Symbol('')` (a Symbol with an empty string name) in an interactive environment, which causes a TypeError. The error trace shows exactly where the problem occurs - in the pretty printing functionality, specifically in the `split_super_sub` function in `conventions.py` where it tries to match a regex pattern against an empty string. The user also provides a workaround (`pprint(Symbol(''), use_unicode=False)` works), which gives additional insight into the issue.\n\nFrom the traceback, we can see that:\n1. The error happens in the pretty printing system\n2. It's specifically related to handling empty symbol names\n3. The regex match in `split_super_sub` is failing when given an empty string\n4. The issue only occurs with unicode pretty printing (not with plain printing)\n\nThis is enough information to locate the bug and implement a fix, which would likely involve adding a check for empty strings in the `split_super_sub` function or in the calling code.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11862": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user has identified that when using lambdify with mpmath module, there appears to be precision loss when working with high-precision floating-point numbers (more than 16 digits).\n\nThe issue demonstrates:\n1. Creating symbolic expressions with sqrt(2)\n2. Evaluating one expression to 64-digit precision\n3. Using lambdify with module='mpmath' on both expressions\n4. Showing that the lambdified functions produce different results when they should be equivalent\n\nThe expected behavior is that when using mpmath as the module, the high precision should be preserved throughout the calculation. Instead, it appears that somewhere in the lambdify process, the high-precision float is being converted to double precision.\n\nThe issue provides all necessary code to reproduce the problem, clear expected vs. actual behavior, and enough context to understand what's happening. A developer could immediately start investigating where in the lambdify implementation the precision loss occurs.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11870": {
        "explanation": "This issue is well-specified. The user is asking for a way to simplify complex exponential expressions into their trigonometric equivalents. They provide two clear examples:\n\n1. The first example shows `f = 1/2 * (-I*exp(I*k) + I*exp(-I*k))` which should simplify to `sin(k)` using the Euler's formula relationship.\n\n2. The second example shows `f = 1/2/k * (-I*exp(I*k) + I*exp(-I*k))` which should simplify to `sinc(k)`.\n\nThe user is asking if there's a way to make `trigsimp()` or some other function perform these simplifications automatically. This is a clear feature request with specific input and expected output examples. The mathematical relationships are well-defined (these are standard complex exponential to trigonometric conversions), and the desired functionality is clearly articulated. A developer familiar with the symbolic mathematics capabilities of the library would understand what needs to be implemented.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11897": {
        "explanation": "This issue is well-specified with clear examples demonstrating the inconsistency between LaTeX printer and pretty printer outputs. The issue provides:\n\n1. Specific code examples showing the discrepancy between `latex()` and `pprint()` outputs\n2. Clear examples of the actual outputs that are inconsistent\n3. A clear statement of the expected behavior: \"The LaTeX printer should always give the same output as the pretty printer, unless better output is possible from LaTeX\"\n\nThe first example shows how `exp(-x)*log(x)` is rendered differently between the two printers, with LaTeX showing `\\frac{1}{e^{x}} \\log{\\left (x \\right )}` while pretty print shows `\u212f^(-x)\u22c5log(x)`.\n\nThe second example shows how `1/(x + y)/2` is rendered differently, with LaTeX showing `\\frac{1}{2 x + 2 y}` while pretty print shows `1/(2\u22c5(x + y))`.\n\nA developer can clearly understand what needs to be fixed: make the LaTeX printer's output consistent with the pretty printer's output in these cases. The issue is specific enough to identify the problem and test if a solution works correctly.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-11989": {
        "explanation": "This issue is well-specified with clear examples of the problem. The user reports that the `diophantine` function (presumably from a mathematical library like SymPy) is not finding all solutions for specific Diophantine equations.\n\nThe issue provides:\n1. A specific equation where no solutions are found: `2*x**2+y**2-16`\n2. The expected solutions for this equation: `{x=0,y=4}` and `{x=0,y=-4}`\n3. A second example equation: `10*x**2 + 12*x*y + 12*y**2 - 34`\n4. The partial solutions that are found for the second equation: `{x=-1,y=2}` and `{x=1,y=1}`\n5. The missing solutions for the second equation: `{x=-1,y=-1}` and `{x=1,y=-2}`\n6. A hypothesis about the cause: \"some issue with the solving of the elliptic case\"\n\nThe problem is clearly reproducible with specific test cases and expected outcomes. A developer would be able to investigate the implementation of the diophantine solver, particularly focusing on the elliptic case handling, and verify why these specific solutions are being missed. The issue provides enough information to test, debug, and fix the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12088": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes a precision inconsistency in the Poly class when working with mpmath values. Specifically:\n\n1. The problem is clearly demonstrated with reproducible code examples showing that Poly doesn't respect the precision set in mpmath.mp.dps.\n2. The issue provides specific test cases showing the expected vs. actual behavior.\n3. It identifies a likely cause: confusion between base-10 precision (dps) and base-2 precision (prec) in the codebase.\n4. It references related issues that provide additional context.\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided examples\n2. Understand the expected behavior (Poly should respect the precision set in mpmath.mp.dps)\n3. Investigate the specific area of code that handles precision conversion between mpmath and Poly\n4. Implement a fix that correctly handles the precision conversion\n\nThe issue provides all necessary context and examples to understand and address the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12096": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue explains that when evaluating a composition of implemented functions using evalf(), the inner function's implementation is not being called recursively. \n\nThe example demonstrates that individual implemented functions (f(2) and g(2)) evaluate correctly with evalf(), but a composition f(g(2)) fails to evaluate and just returns the symbolic expression. \n\nThe issue even points to the specific location of the problem in the codebase: the `Function._eval_evalf` method, which is not calling evalf recursively on the return value of `_imp_`.\n\nWith this information, a developer would know:\n1. The exact problem (evalf not recursively evaluating implemented functions)\n2. How to reproduce the issue (with the provided code example)\n3. Where to look in the codebase (Function._eval_evalf)\n4. What needs to be fixed (make _eval_evalf call evalf recursively on the result of _imp_)\n\nThis is sufficient information to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12108": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly stated: The string representation of logical expressions currently uses function notation (And, Not, Or) rather than symbolic operators.\n\n2. The desired outcome is explicitly defined: Use symbolic operators (`~`, `&`, `|`) instead of function notation, with proper parenthesization.\n\n3. An example is provided showing the current behavior and implying the expected output.\n\n4. The scope is well-defined: Change the string printer (and non-Unicode pretty printer) for logical expressions.\n\n5. The issue even provides implementation guidance, noting that parenthesization needs to be handled correctly.\n\nThe developer has enough information to implement this feature without needing additional clarification. They know which components to modify (the string printer), what the current behavior is, and what the desired behavior should be.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12171": {
        "explanation": "This issue is well-specified with clear problems and solutions:\n\n1. The issue clearly identifies two specific bugs in the MathematicaCodePrinter:\n   - Incorrect handling of derivatives: Derivative(f(t), t) is not properly converted to D[f[t],t]\n   - Incorrect handling of floating-point numbers with exponents: 1.0e-4 is not converted to 1.0*^-4 (Mathematica's syntax)\n\n2. The issue provides concrete examples of the incorrect behavior.\n\n3. The issue even provides the exact code fix needed to resolve both problems:\n   - Adding a _print_Derivative method to handle derivatives correctly\n   - Adding a _print_Float method to handle floating-point exponent notation correctly\n\nA developer could implement this fix directly as specified, test it with the provided examples, and verify the solution works. The context is clear (MCodePrinter class), the problems are specific, and the solution is provided.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12194": {
        "explanation": "This issue is well-specified. The user is requesting a feature to return prime factors as a list including multiplicity (e.g., [2, 2, 2, 3] for 24) rather than the current dictionary format provided by sympy.factorint. The user clearly explains:\n\n1. The current behavior (sympy.factorint returns a dictionary)\n2. The desired behavior (a list of prime factors with repetition for multiplicity)\n3. The workaround they currently use (a lambda function to convert the dictionary)\n4. Specific implementation suggestions (either an option to factorint like aslist=True or a new function)\n\nThe request is straightforward, has a clear use case, and provides concrete examples of both the current and desired functionality. A developer could implement this feature without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12214": {
        "explanation": "This issue is reasonably well-specified. The user is encountering a problem with B-splines of degree > 1 in the `spline_basis` function. They provide specific code examples showing what works (degree 1) and what doesn't work (degree 2), along with the knot vectors they're using. They also mention their approach to generating knots for B-spline basis matrices.\n\nWhat makes this reasonably specified rather than well-specified:\n1. The exact error or unexpected behavior isn't explicitly stated - they just say it \"breaks\" without specifying how.\n2. The context of which library they're using isn't completely clear (though `sy.bspline_basis_set` suggests it's SymPy).\n3. They don't explicitly state what the \"desired basis matrix\" should look like.\n\nHowever, there's enough information to understand the core issue: there appears to be a problem with the knot vector configuration for B-splines of degree > 1. The user shows their understanding of B-spline theory (with the need for repeated knots at endpoints) and provides a specific formula they've been using. A developer familiar with the library should be able to identify the issue based on the provided code examples and knot vectors.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12227": {
        "explanation": "This issue is well-specified with clear requirements for implementation. The problem is clearly articulated: the Float constructor only allows setting decimal precision, but there's a need to directly set binary precision. The issue explains:\n\n1. The current problem: Float constructor only accepts decimal precision which gets converted to binary precision\n2. The limitation: not all binary precisions can be directly specified\n3. The proposed solution: modify the Float constructor to accept both `dps` (decimal precision) and `prec` (binary precision) parameters\n4. The backward compatibility concerns: changing the meaning of the `prec` parameter requires careful handling\n\nThe issue even provides a suggested API design:\n```\nFloat(s, dps=15, prec=53)\n```\n\nThe implementation would need to:\n1. Change the current `prec` parameter to `dps` to align with naming conventions\n2. Add a new `prec` parameter for binary precision\n3. Handle the case where both are specified (raise an error)\n4. Implement a deprecation path for the parameter name change\n\nThis is a well-defined task with clear requirements and constraints that a developer could implement without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12236": {
        "explanation": "This issue is well-specified with a clear demonstration of the bug. The user provides:\n\n1. A complete code example that reproduces the issue\n2. The exact version of SymPy being used (1.0)\n3. The expected behavior is implied by comparing two different approaches:\n   - When applying `apart(t)` directly to the expression with the symbol `a`, it gives `-a*t`\n   - When first substituting `a=1` and then applying `apart()` or `apart(t)`, it gives `-t + 1/2 - 1/(2*(2*t - 1))`\n\nThe inconsistency is clear: the partial fraction decomposition (`apart()`) gives different results depending on whether the substitution happens before or after applying the function. This is a well-defined mathematical operation that should yield consistent results regardless of the order of operations.\n\nThe issue is specific to the `apart()` function in SymPy and includes all the necessary context to reproduce and understand the problem. A developer would have enough information to investigate why the partial fraction decomposition behaves differently in these two cases and fix the inconsistency.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12270": {
        "explanation": "This issue is well-specified with clear examples demonstrating the bug. The user provides:\n\n1. Specific code examples showing the expected behavior vs. the problematic behavior\n2. The exact function that's failing: `extract_multiplicatively(-2)` on negative expressions\n3. A detailed explanation of why they believe it's a bug, pointing to the specific implementation in `core/expr.py`\n4. The root cause: when `primitive()` returns a tuple with a negative expression, and then creates a non-evaluated `Mul`, the `extract_multiplicatively` function fails to work with negative multipliers\n5. Even references a specific commit that might be related to the issue\n\nThe issue provides enough context for a developer to understand the problem, reproduce it, and investigate the implementation in the codebase. The expected behavior is clear: `(-2*x - 4*y - 8).extract_multiplicatively(-2)` should return `x + 2*y + 4`, similar to how other extraction operations work with negative numbers.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12307": {
        "explanation": "This issue is well-specified with clear information about the problem and a suggested solution:\n\n1. The problem is clearly stated: Sympy's Fortran code generation for the sign function produces invalid Fortran syntax (`sign(x)` instead of the correct Fortran syntax).\n\n2. The issue includes a reproducible example with specific versions (Sympy 1.0, Python 3.6) and code that demonstrates the problem.\n\n3. The expected behavior is explained with reference to the official Fortran documentation for the sign function.\n\n4. A specific solution is suggested: generate `sign(1, x)` or `sign(1d0, x)` depending on the context.\n\n5. The only minor uncertainty mentioned is how to handle the case when x=0, but this is a detail that can be addressed during implementation by referring to Sympy's current behavior.\n\nThe issue provides all the necessary information for a developer to understand the problem, verify it, and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12419": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible code example. The user has identified what appears to be a bug in SymPy's handling of symbolic matrix operations. \n\nThe issue describes:\n1. The expected behavior: The sum of all elements in an identity matrix of size n\u00d7n should be n (since there are n ones on the diagonal and zeros elsewhere)\n2. The actual behavior: SymPy returns 0 for this sum\n3. A complete code example that reproduces the issue\n4. Verification steps showing that the matrix is correctly identified as an identity matrix (diagonal elements are 1, off-diagonal are 0, sum of diagonal is n)\n\nThe problem is clearly defined and the expected outcome is unambiguous. A developer would have all the information needed to investigate and fix this issue without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12454": {
        "explanation": "This issue is well-specified with clear information about the bug and its cause. The description:\n\n1. Clearly identifies the problem: `Matrix.is_upper()` raises an IndexError for tall matrices (specifically a 4x2 matrix)\n2. Provides a complete error traceback showing exactly where the error occurs\n3. Shows the relevant code for the `is_upper()` method\n4. Demonstrates the issue with a concrete example, showing the indices that are being generated\n5. Identifies the specific problematic index (3,2) that causes the error because it's out of bounds for a 4x2 matrix\n\nThe issue is that the `is_upper()` method is generating indices that are out of bounds for tall matrices (where rows > columns). The method is trying to access elements that don't exist in the matrix.\n\nA developer would have all the information needed to fix this issue - they would need to modify the `is_upper()` method to properly handle tall matrices by ensuring it doesn't try to access elements beyond the matrix dimensions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12472": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `sqrt` function where it incorrectly splits non-real factors that are nonnegative. The issue provides a concrete example showing the current incorrect behavior where `sqrt((3 + 4*I)/(3 - 4*I))` returns `sqrt(-1/(3 - 4*I))*sqrt(-3 - 4*I)` instead of keeping the expression under a single square root. \n\nThe issue also mentions that a fix already exists in PR #12472, and this ticket serves as a reminder to ensure proper testing is added for this case. The expected behavior is clearly implied: non-real factors, even if nonnegative, should remain inside the square root rather than being split out.\n\nA developer would understand exactly what needs to be fixed and tested based on this description.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12481": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: The `Permutation` constructor raises a `ValueError` when given non-disjoint cycles (e.g., `Permutation([[0,1],[0,1]])`).\n\n2. The expected behavior: When non-disjoint cycles are provided, they should be applied in left-to-right order to construct the resulting permutation, rather than raising an error.\n\n3. A specific example: `Permutation([[0,1],[0,1]])` should construct the identity permutation.\n\n4. The rationale: Non-disjoint cycles should be allowed as there's no reason to forbid them, and computing the result is straightforward.\n\nThe issue provides enough information for a developer to understand what needs to be fixed and how the corrected behavior should work. The solution would involve modifying the `Permutation` constructor to handle non-disjoint cycles by applying them sequentially rather than rejecting them with an error.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12489": {
        "explanation": "This issue is well-specified with clear technical details about the problem. The user explains:\n\n1. The specific component affected: `combinatorics.Permutation`\n2. The exact nature of the problem: subclassing is impossible because `Permutation.__new__` uses `_af_new` which always returns instances of `Permutation` instead of the subclass\n3. A proposed solution: use Python's standard instance creation mechanisms with classmethods and use the class reference for instance creation\n4. Evidence they've tested their solution: they monkeypatched locally and ran tests successfully\n\nThe user has provided enough technical detail to understand the issue, reproduce it, and consider the proposed solution. They've even done preliminary testing to verify their approach works. The only minor uncertainty is about potential side effects, but this is a reasonable concern when modifying core behavior and doesn't prevent understanding what needs to be fixed.\n\nA developer could take this information and implement a solution that modifies the instance creation mechanism in `Permutation` to properly support subclassing without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12798": {
        "explanation": "The issue description is quite minimal but provides enough context to understand what needs to be done. It states that the definitions of \"ff\" and \"rf\" were changed in SymPy 1.0 through a specific pull request (#8941), and the requirement is to revert this change.\n\nThe issue includes a link to the specific pull request that made the change, which would allow a developer to:\n1. Review exactly what changes were made to the \"ff\" and \"rf\" definitions\n2. Understand the rationale behind those changes\n3. Implement a reversion of those specific changes\n\nWhile the description doesn't explain why the change should be reverted or provide details about what \"ff\" and \"rf\" are (likely falling factorial and rising factorial functions in mathematics), a developer with access to the codebase and the referenced PR would have sufficient information to create a PR that reverts the specified changes.\n\nThe task is straightforward: revert the changes made to \"ff\" and \"rf\" definitions that were introduced in PR #8941. This is a common and well-understood operation in software development.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-12881": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `free_symbols` method of polynomial objects: when a generator is repeated in the polynomial's generator list but doesn't actually appear in any term of the polynomial expression, that generator's free symbols are incorrectly included in the result.\n\nThe issue provides:\n1. A clear example: `Poly(x,x,y).free_symbols` returns `{x, y}` when it should return just `{x}`\n2. A precise description of the expected behavior: free symbols should only include symbols from generators that actually appear in the polynomial expression\n3. A proposed implementation fix with code that shows how to check if a generator is actually used in any monomial\n\nThe only minor uncertainty is about the domain part in the proposed solution, but this doesn't affect understanding the core issue. The proposed solution logic is clear: iterate through generators, check if each appears in any monomial, and only include free symbols from generators that are actually used.\n\nThis is sufficient information for a developer to implement a fix without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12906": {
        "explanation": "This issue is well-specified and clear. It identifies a specific bug in the `canonical` method of the `Relational` class in SymPy. The issue demonstrates that calling `canonical` on a relational expression doesn't produce a truly canonical form, as evidenced by the fact that applying `canonical` twice yields a different result than applying it once. \n\nThe issue includes a concrete, reproducible example showing the problem: `r = x**2 > -y/x` and then demonstrates that `r.canonical == r.canonical.canonical` returns `False`, which indicates that the `canonical` method is not idempotent as would be expected for a true canonicalization function.\n\nA successful solution would need to fix the `canonical` method so that applying it multiple times yields the same result as applying it once, making it truly idempotent. This is a well-defined mathematical property that a canonicalization function should have.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-12977": {
        "explanation": "This issue is well-specified with clear information about what's not working and what needs to be fixed. The issue description includes:\n\n1. A clear problem statement: \"nullary functions should be allowed\" (functions with no arguments)\n2. A detailed error trace showing exactly where and how the code fails\n3. The specific error: `ValueError: max() arg is an empty sequence` when trying to call a function with no arguments\n4. The root cause is identifiable: in the `__new__` method of the function class, it's trying to call `max()` on an empty sequence (when there are no arguments)\n5. Additional context that it should also work with explicit subclasses of Function\n\nThe issue is straightforward - the code needs to be modified to handle the case where a function is called with no arguments. The error occurs because the code is trying to find the maximum value in an empty list of arguments. A developer would know exactly what to fix and where to look in the codebase (the function.py file in the core module).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13001": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue shows that when using the `cse` (Common Subexpression Elimination) function in SymPy, it's leaving behind a \"hollow Mul\" - specifically, the `-4*a**2` term is being transformed into `Mul(-1, 4, a**2, evaluate=False)` which causes equality comparison to fail between the original expression and the CSE-processed expression.\n\nThe example provides:\n1. The exact input expression: `a/(-4*a**2 + s**2)`\n2. The code to reproduce the issue: `cse_eq = cse(eq)[1][0]`\n3. The unexpected behavior: `cse_eq == eq` returns `False`\n4. The root cause: `-4*a**2` is transformed to `Mul(-1, 4, a**2, evaluate=False)`\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Understand what's happening (unevaluated multiplication)\n3. Fix the issue by ensuring CSE properly evaluates these multiplication expressions or handles equality comparisons correctly\n\nThis is a specific, technical issue with clear expectations for a solution - the CSE function should preserve the mathematical equivalence of expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13031": {
        "explanation": "This issue is well-specified with clear examples demonstrating a regression in behavior between SymPy 1.0 and 1.1. The issue specifically shows that:\n\n1. In SymPy 1.0, horizontally stacking matrices with zero rows (M1, M2, M3, M4 where each has 0 rows) correctly resulted in a matrix with shape (0, 6) - the columns were properly summed (0+1+2+3=6).\n\n2. In SymPy 1.1, the same operation incorrectly results in a shape of (0, 3) - only taking the column count from the last matrix (M4) rather than summing all columns.\n\n3. The issue also demonstrates that when matrices have at least 1 row, the behavior works correctly in SymPy 1.1, resulting in shape (1, 6).\n\nThis is clearly a regression bug in the Matrix.hstack method when handling empty matrices (those with 0 rows). The expected behavior is well-defined (sum the columns of all matrices), and the actual behavior is clearly shown. A developer would have enough information to locate the issue in the code and fix the regression.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13043": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with the `decompose()` function in the `intpoly` module, where when `separate=True` is used, it returns a list of values from a dictionary in arbitrary order. This is causing test failures for the reporter.\n\nThe issue identifies:\n1. The specific function (`decompose()` in `intpoly`)\n2. The specific parameter condition (`separate=True`)\n3. The problem (returning values in arbitrary order)\n4. The impact (causing test failures)\n\nThe issue also suggests potential solutions:\n- Sort the returned list in some way\n- Return a set instead\n- Have callers use the dictionary directly\n\nWhile the issue doesn't specify exactly how the list should be sorted (what criteria to use), this is a reasonable question for the implementer to decide based on the function's purpose and usage patterns. The mention of test failures suggests that deterministic ordering is needed for consistent test results.\n\nA developer with access to the codebase should be able to:\n1. Locate the `decompose()` function in the `intpoly` module\n2. Understand how it's currently implemented\n3. See how it's used in tests\n4. Implement a solution that provides deterministic ordering\n\nThe issue has tagged specific developers who might have context, which is helpful.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13091": {
        "explanation": "This issue is well-specified with clear information about what needs to be changed and why. The issue describes:\n\n1. The problem: SymPy comparison methods return `False` instead of `NotImplemented` when comparing with unknown types, which breaks the Python comparison protocol.\n2. The expected behavior: Return `NotImplemented` to properly delegate to the reflected method on the other object.\n3. The rationale: This would allow custom classes to properly implement comparisons with SymPy objects and maintain symmetric relations.\n4. A specific example demonstrating the issue with a custom `Foo` class.\n5. A specific line to edit (line 316 in basic.py) with the exact change needed (replace `return False` with `return NotImplemented`).\n\nThe issue even includes links to relevant documentation and acknowledges there might be other similar places in the codebase that need the same change. This is a very clear and actionable issue that a developer could immediately start working on without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13146": {
        "explanation": "This issue is well-specified. The user has provided:\n\n1. A clear code example that reproduces the problem\n2. The current output they're getting (`-0.5*x**2.5 + 0.5*x**2.5`)\n3. The expected output they want (simplification to `0`)\n\nThe issue is about SymPy not fully simplifying an expression that should mathematically evaluate to zero. The expressions `expr1 = S(1)/2*x**2.5` and `expr2 = S(1)*x**(S(5)/2)/2` are mathematically equivalent, but when subtracted and simplified, SymPy doesn't recognize them as identical and reduce the result to zero.\n\nThe problem is clearly defined, and a solution would involve finding the appropriate SymPy function or approach to properly simplify this expression to zero. There's enough information to understand the issue and attempt a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13173": {
        "explanation": "This issue is well-specified and clearly describes a problem with the `degree()` function when handling multivariate expressions. The issue explains that:\n\n1. When `degree()` is used on a multivariate expression, it returns the degree with respect to the first generator, which is chosen canonically but arbitrarily.\n2. This creates ambiguity because the result depends on the order of symbols, as demonstrated in the examples where `degree(a + b**2)` returns 1 while `degree(b + a**2)` returns 2.\n3. The issue implies that the function should require explicit specification of the generator to avoid silent ambiguity.\n\nThe problem is clearly demonstrated with code examples showing the inconsistent behavior. The issue description provides enough context to understand what's happening and what needs to be fixed. A solution would likely involve modifying the `degree()` function to either require an explicit generator parameter for multivariate expressions or to provide a warning when the generator is ambiguous. The expected behavior change is clear from the description.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13185": {
        "explanation": "This issue is reasonably well-specified. The user has provided a clear example demonstrating the problem with the `cse()` function when working with MatrixSymbol indexing. The issue shows that when using common subexpression elimination (cse) with MatrixSymbols, the function creates unnecessary intermediate variables (x0 and x2) that are just copies of the original matrices a and b.\n\nThe example code is complete and demonstrates the issue clearly. The output shows how the cse function is generating redundant assignments in the C code output, where x0 and x2 are assigned all the elements from a and b respectively, before those elements are used.\n\nWhile the user doesn't explicitly state what the expected behavior should be, it's reasonable to infer that they want the cse function to handle MatrixSymbol indexing more efficiently by not creating these redundant intermediate variables. A successful solution would likely modify the cse function to recognize when it's dealing with MatrixSymbol indexing and handle it more efficiently.\n\nThe only minor ambiguity is whether the issue is specifically about the C code generation or about the cse function's behavior in general, but the focus appears to be on the cse function itself based on the title and the example.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13236": {
        "explanation": "This issue is well-specified. It clearly describes a mathematical property that should be implemented in the factorial function: factorial(n) should automatically simplify expressions like factorial(n) % n to 0 (and ideally factorial(n) % k to 0 for any integer k where 1 \u2264 k \u2264 n).\n\nThe issue provides:\n1. A clear description of the expected behavior\n2. A concrete example showing the current behavior\n3. The mathematical property being referenced (factorial(n) is always divisible by any integer from 1 to n)\n4. A note about a related issue (#8531) and a fix for part of the problem (#8687)\n\nThe task is to implement this mathematical knowledge into the factorial function so that symbolic expressions involving factorial(n) % k (where k is in the range 1 to n) automatically simplify to 0. This is a well-defined mathematical property with clear implementation requirements.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13259": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue describes a bug in SymPy's simplification of trigonometric functions with complex arguments. \n\nThe problem is clearly demonstrated:\n1. The user defines an expression `a` containing a complex trigonometric function\n2. When evaluating `a` numerically, they get `-0.333333333333333 + 1.0*I`\n3. After simplification to expression `b`, the numerical value changes to `1.6818694524863*I`\n4. The user identifies the specific error: `cos(pi/6 -I*asinh())` is incorrectly converted to `cosh(pi/6 + asinh())` instead of `cosh(I*pi/6 + asinh())`\n5. They provide additional evidence showing that simplification behaves differently when the expression is isolated\n\nThe issue contains:\n- The exact SymPy version (1.0)\n- Complete code to reproduce the issue\n- The expected behavior (numerical equivalence between original and simplified expressions)\n- The actual behavior (different numerical values)\n- A specific hypothesis about what's going wrong in the simplification process\n\nA developer would have all the information needed to investigate and fix this bug in the simplification of trigonometric functions with complex arguments.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13264": {
        "explanation": "This issue is well-specified with clear details about the problem and a proposed solution:\n\n1. The user clearly describes a specific issue where calling `simplify()` once doesn't fully simplify an expression, but calling it twice (`simplify(simplify(expr))`) produces a more simplified result.\n\n2. The user provides concrete examples with images showing the initial expression (675 operations), the result after one simplification (23 operations), and the result after a second simplification (7 operations).\n\n3. The user has identified a potential cause: the order of simplification strategies matters, and some strategies may be called too early.\n\n4. The user proposes a specific solution: adding an optional recursive mode to the `simplify()` function with a parameter `max_rec_steps` to control recursion depth.\n\n5. The user even provides the exact code modification needed to implement this feature.\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and implement the proposed solution. The user has done thorough analysis and provided a clear path forward.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13265": {
        "explanation": "This issue is well-specified with a clear problem description. The user demonstrates that when calculating the exponential of a specific matrix, SymPy's simplification functions correctly identify and simplify some trigonometric expressions (cos(1) and -sin(1)) but fail to recognize another expression that should also simplify to sin(1).\n\nSpecifically, the expression:\n```\n     \u2148      -\u2148\n  \u2148\u22c5\u212f    \u2148\u22c5\u212f\n- \u2500\u2500\u2500\u2500 + \u2500\u2500\u2500\u2500\u2500\n   2       2\n```\nshould be recognized as sin(1) based on Euler's formula (e^(ix) - e^(-ix))/(2i) = sin(x).\n\nThe issue provides:\n1. A clear reproducible example with the exact matrix input\n2. The expected vs actual output\n3. Shows attempts with different simplification functions (simplify, sqrtdenest, trigsimp)\n4. Points out exactly which part of the output is problematic\n\nA developer would have all the information needed to reproduce the issue and understand what needs to be fixed - the simplification functions need to be enhanced to recognize this specific form of sin expressed using exponentials.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13286": {
        "explanation": "The issue description is reasonably specified. It clearly identifies two problems:\n\n1. The `periodicity(Abs(sin(x)),x)` function returns `2*pi` when it should return `pi`. This is demonstrated with a concrete example showing the current behavior.\n\n2. There's a recursion error when calling `periodicity(x > 2, x)` (issue #13205).\n\nThe description provides specific test cases that demonstrate the issues, which helps in understanding the problem. It also mentions that the solution should fix the infinite recursion for relational expressions and improve the periodicity calculation for absolute trigonometric functions.\n\nHowever, there are some aspects that could be clearer:\n- The exact implementation details of how to fix these issues are not provided\n- There's no explanation of why `Abs(sin(x))` should have a period of `pi` rather than `2*pi`\n- The relationship between the two issues (#13205 and #13207) could be better explained\n\nDespite these gaps, a developer familiar with SymPy's periodicity function would likely have enough information to investigate and implement a solution based on the provided examples and expected behavior.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13301": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a problem with the `AccumBounds` class in that its string representation (`str`) and S-expression representation (`srepr`) are currently not recreatable - meaning you cannot copy the output and use it to recreate the original object.\n\nThe issue provides a concrete example showing that both `str(AccumBounds(-1, 1))` and `srepr(AccumBounds(-1, 1))` currently return `'<-1, 1>'`, which is not valid Python syntax to recreate the object.\n\nThe requirement is clear: modify the string representation methods to return recreatable strings (likely in the form of `'AccumBounds(-1, 1)'`), while keeping the pretty-printed form (presumably `'<-1, 1>'`) only for the pretty printer.\n\nWhat's missing is:\n1. The exact format expected for the recreatable string\n2. Which module/file contains the `AccumBounds` class\n3. Whether there are any other considerations or edge cases to handle\n\nHowever, these gaps can be filled by examining the codebase to locate the `AccumBounds` class and understanding the standard string representation patterns used in the project. The core requirement is clear enough to implement a solution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13361": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue provides a specific code example that reproduces the bug\n2. It shows the actual output (`Float('16.0', precision=1)`) and implies the expected output (20)\n3. It identifies the likely cause of the problem: an incorrect `precision=1` attribute being attached to the result of `floor(0.5)`\n4. It suggests a specific solution: make `expr.evalf(subs=dict)` behave the same as `expr.subs(dict).evalf()`\n5. It provides version information (Python 3.6.1, SymPy 1.1.1)\n6. It references a Stack Overflow post for additional context\n\nThe issue is about a mathematical expression evaluation bug in SymPy where using `evalf` with the `subs` argument doesn't correctly handle expressions with the `floor` function. The problem and proposed solution are clear enough that a developer could implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13364": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly identified: SymPy's implementation of `__pow__` needs to support a third optional argument to work with ternary pow() as specified in Python's documentation.\n2. The issue provides a specific error message that occurs when trying to use the ternary pow() with SymPy objects.\n3. It specifies that at minimum, this should be implemented for Integer objects.\n4. It mentions a stretch goal of implementing it for arbitrary expressions, with dependencies on another issue (#5589) for the Mod functionality.\n5. There's a clear example of the current failing behavior.\n\nThe issue provides enough context to understand what needs to be fixed, why it needs to be fixed, and what a successful implementation would look like. A developer would need to modify the `__pow__` method in SymPy to handle the third argument (modulo) for at least Integer objects, ensuring it works with Python's built-in `pow()` function.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13369": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue provides a specific code example that demonstrates the problem: trying to compute eigenvalues of a 3x3 symbolic matrix using `.eigenvals()` fails with a specific error message.\n\n2. The user has already done troubleshooting and shows that:\n   - The characteristic polynomial can be solved explicitly using `solve(det(lam*eye(3) -M), lam)`\n   - But `roots(M.charpoly())` returns an empty dictionary `{}`\n   - Declaring x as a real symbol doesn't help\n\n3. The issue includes the exact error message received.\n\n4. The user has referenced a related Stack Overflow post, showing they've done research.\n\nThe problem is clearly defined: there's an inconsistency in SymPy where one method can find the eigenvalues of this symbolic matrix while another method (which should work) fails. A successful solution would need to either fix the `.eigenvals()` method to work with this type of symbolic matrix or explain why this limitation exists and provide a workaround.\n\nThe issue contains all the necessary information to reproduce the problem and understand what needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13372": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. There's a specific error (UnboundLocalError) that occurs in the evalf function when evaluating a Mul expression with Max as the first argument, but not when Max is the second argument.\n\n2. The exact traceback is provided, showing where the error occurs (local variable 'reprec' referenced before assignment).\n\n3. The reporter has identified the likely cause - the elif clauses that define reprec and imprec variables don't have an else clause to handle cases that don't match any of the conditions.\n\n4. A potential fix is suggested (adding an `else: raise NotImplementedError`).\n\nThe issue provides enough context to understand the problem, reproduce it, and implement a solution. The reporter even mentions they found this after changing the order of Mul args in a specific PR, which gives additional context. The suggested fix is reasonable and directly addresses the identified problem.\n\nA developer could take this information and implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13429": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user has:\n\n1. Provided the exact version of sympy (1.1.1) and Python environment (3.6.0, Anaconda, Ubuntu 16.04)\n2. Given a specific reproducible example with a rational number that shows inconsistent comparison behavior with \u03c0\n3. Demonstrated the logical contradiction: `r < sympy.pi` is False, `r == sympy.pi` is False, and `sympy.pi < r` is False, yet `r > sympy.pi` is True\n4. Identified a likely cause: the use of `evalf()` with default precision (15) for comparisons between rationals and reals\n5. Provided evidence for this theory by showing that `r == sympy.pi.evalf(15)` is True but `r == sympy.pi.evalf(16)` is False\n\nThe issue is clearly about fixing the comparison logic between rational and irrational numbers in sympy to ensure consistent results. The expected behavior would be that comparisons should be mathematically sound and not lead to contradictions like the one demonstrated.\n\nA developer would have enough information to investigate and fix this issue without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13441": {
        "explanation": "This issue is well-specified with a clear problem description. The issue reports that the `count_ops` function in SymPy is slow for large expressions, and provides a detailed reproduction case with a stack trace showing where the code is hanging. \n\nThe issue includes:\n1. A specific example that demonstrates the problem (using n_link_pendulum_on_cart model)\n2. The exact code that triggers the issue (calculating the Jacobian of a large expression)\n3. A stack trace showing where the code is hanging (in the `count_ops` function during pattern matching)\n4. Environment information (Python 3.5.1, IPython 4.1.2)\n\nThe problem is clearly defined: when working with large expressions, the `count_ops` function becomes extremely slow, causing operations like pattern matching to hang. The stack trace shows that this happens during the `matches` method when it calls `count_ops`.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed - either optimizing the `count_ops` function for large expressions or potentially modifying how it's used in pattern matching operations.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13471": {
        "explanation": "This issue is well-specified with a clear problem description and a minimum working example that reproduces the error. The issue describes a specific compatibility problem when pickle-serializing a SymPy expression containing a float in Python 2 and then deserializing it in Python 3.\n\nThe error message is provided, showing that the problem occurs in the `__new__` method of a class in SymPy's `numbers.py` file, specifically when trying to convert a string with a Python 2 long integer literal ('1L') to an integer in Python 3, where the 'L' suffix no longer exists.\n\nThe exact versions of Python (2.7 and 3.6) and the SymPy commit (3546ac7) are specified, making it easy to reproduce the issue. The error is clearly related to the Python 2 to 3 transition and how SymPy handles numeric types during deserialization.\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided example\n2. Locate the problematic code in the SymPy codebase (numbers.py, line 1045)\n3. Understand that the issue is related to the 'L' suffix for long integers in Python 2 that doesn't exist in Python 3\n4. Implement a fix that properly handles the conversion between Python 2 and 3 numeric representations during pickle deserialization",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13480": {
        "explanation": "This issue is well-specified with clear information to diagnose and fix the problem:\n\n1. The issue provides a specific error message with a traceback pointing to the exact location of the error in the code (hyperbolic.py, line 590).\n\n2. The error is clearly identified as a NameError where 'cotm' is not defined.\n\n3. The issue provides a complete, reproducible code example that triggers the error.\n\n4. The issue specifies multiple input values (2, 3, 5, 6, etc.) that cause the error, establishing a pattern.\n\nWith this information, a developer can:\n1. Locate the specific file and line where the error occurs\n2. Understand that there's a variable reference issue ('cotm' not defined)\n3. Reproduce the error using the provided example\n4. Test the fix with the provided list of problematic values\n\nThis is sufficient information to diagnose and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13551": {
        "explanation": "This issue is well-specified with clear information about what's wrong and what the expected behavior should be:\n\n1. The issue clearly identifies a specific mathematical function (Product) that is producing incorrect results.\n2. It provides a concrete example with code that can be run to reproduce the issue.\n3. It shows the current incorrect output (9/2 for n=2).\n4. It explicitly states what the correct result should be (15/2 for n=2).\n5. It even points to the mathematical concept that should be used for the correct implementation (q-Pochhammer symbol).\n\nA developer would have all the necessary information to:\n1. Reproduce the issue\n2. Verify the correct expected output\n3. Understand what mathematical concept needs to be implemented or fixed\n4. Test their solution with the provided example\n\nThe issue is therefore well-specified with clear criteria for a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13581": {
        "explanation": "This issue is reasonably well-specified. It describes a mathematical simplification that should be implemented in the SymPy library for modular arithmetic expressions.\n\nThe core request is clear: `Mod(Mod(x + 1, 2) + 1, 2)` should simplify to `Mod(x, 2)`. This is a specific mathematical identity that can be verified and implemented.\n\nThe issue also mentions a more general case: `Mod(foo*Mod(x + 1, 2) + non_mod_terms + 1, 2)` should be simplified recursively. While this is less specific, it provides a direction for implementing a more general simplification rule for nested modulo operations.\n\nThe link to a Stack Overflow question provides additional context about the problem.\n\nWhat's missing is:\n1. Specific implementation details or approach suggestions\n2. Test cases beyond the two examples\n3. Information about where in the codebase this simplification should be implemented\n\nHowever, an experienced developer familiar with SymPy could reasonably determine where and how to implement this simplification rule based on the given information and by examining the existing code for similar simplification patterns.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13615": {
        "explanation": "This issue is well-specified with a clear problem description and expected behavior:\n\n1. The problem is clearly stated: The Complement operation doesn't work correctly when the input set contains a mixture of symbols and numbers.\n\n2. A specific reproducible example is provided showing:\n   - Input: Complement(FiniteSet(x,y,2), Interval(-10,10))\n   - Current output: {x, y}\n   - Expected output: {x, y} \\ [-10,10]\n\n3. The issue is that the Complement operation is incorrectly removing the number 2 from the result, but it should keep the symbols x and y while showing that they are being complemented with the interval [-10,10].\n\nThe issue provides enough information for a developer to:\n1. Reproduce the problem\n2. Understand the expected behavior\n3. Locate the relevant code in the codebase (likely in the Complement or FiniteSet implementation)\n4. Implement a fix that correctly handles mixed symbol and number inputs\n\nThis is a well-defined bug with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13647": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user provides:\n\n1. A complete, reproducible example with code snippets\n2. The expected behavior is implied (the identity matrix should maintain its structure)\n3. The actual behavior is clearly described (the 3x3 identity matrix is shifted from bottom to top)\n4. The version of SymPy being used (1.1.1)\n\nThe issue describes a bug in the `col_insert()` method of the Matrix class in SymPy, where inserting columns at a specific position (position 3 in this case) causes unexpected shifting of the identity matrix elements. The problem is clearly visible in the output shown.\n\nThe only minor ambiguity is that the expected output isn't explicitly shown, but it can be inferred from the description that the identity matrix structure should be preserved properly after column insertion. A developer familiar with the Matrix class would understand what the correct behavior should be.\n\nThis is sufficient information for a developer to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13678": {
        "explanation": "This issue is well-specified with clear examples of the problem. The issue describes that various trigonometric and hyperbolic functions are incorrectly returning `False` for their `is_real` property when they should be returning `None` (indicating uncertainty) when given non-real symbols as input.\n\nThe description includes specific test cases that are currently failing, showing exactly which functions are affected (sin, cos, sinh, cosh, tanh, etc.) and what the expected behavior should be. The assertions clearly demonstrate that when a symbol with `real=False` is passed to these functions, the `is_real` property should return `None` rather than `False`.\n\nThe issue also mentions that the reporter intends to submit a pull request, indicating they understand the problem well enough to propose a solution.\n\nA developer would be able to:\n1. Locate the implementations of these trigonometric and hyperbolic functions\n2. Examine how the `is_real` property is currently determined\n3. Modify the logic to return `None` when the input is not real\n4. Verify the fix using the provided test cases\n\nThe issue is specific, testable, and has clear acceptance criteria in the form of the assertions that should pass after the fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13682": {
        "explanation": "This issue is reasonably well-specified. It clearly outlines the need for implementing ordinal arithmetic in SymPy and provides two potential approaches:\n\n1. Creating a new `OrdinalNumber` object that inherits from `Symbol` or `Basic`\n2. Adding new assumptions to allow current symbols to be ordinals\n\nThe issue also details:\n- How it could work with assumptions (with specific examples)\n- What operations should be supported (addition, multiplication, exponentiation)\n- Additional methods that should be implemented (relationals, Cantor normal form, prime ordinals, etc.)\n- A reference to a non-symbolic Python implementation that could serve as a basis\n\nWhile the issue provides good direction, there are some aspects that would need further clarification during implementation:\n- The exact interface for the operations\n- How to handle edge cases in ordinal arithmetic\n- Performance considerations\n- How deeply to integrate with existing SymPy functionality\n\nHowever, these details could reasonably be worked out during implementation based on SymPy's existing patterns and the referenced implementation. The issue provides enough information to start a meaningful implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13744": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a stack overflow error occurring in the `coset_table` doctest in the SymPy library. The error trace shows a recursive call pattern in the `eliminate_word` function (line 687) in the `free_groups.py` file that's causing the stack overflow. \n\nThe issue provides:\n1. The specific file with the problem (sympy/combinatorics/coset_table.py)\n2. A detailed stack trace showing the recursive call chain\n3. The specific function causing the problem (eliminate_word in free_groups.py)\n4. The environment details (Python 3.5.4)\n5. A link to the Travis CI build that failed\n\nWhat's missing is:\n1. The specific doctest that's failing\n2. The exact input that causes the infinite recursion\n3. Any expected behavior or proposed solution\n\nHowever, with the information provided, a developer familiar with the codebase should be able to:\n1. Locate the problematic doctest in coset_table.py\n2. Identify the recursive function in free_groups.py that needs fixing\n3. Implement a solution to prevent infinite recursion (likely by adding a recursion depth check or restructuring the algorithm)\n\nThis is a classic infinite recursion bug that has a clear path to resolution, even without all the details.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-13757": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue shows that when a polynomial object (Poly) is multiplied by an expression, the behavior is inconsistent depending on the order of multiplication:\n\n1. When Poly(x) is on the left side of multiplication (Poly(x)*x), it correctly evaluates to Poly(x**2).\n2. When Poly(x) is on the right side (x*Poly(x)), it doesn't evaluate and just returns the unevaluated expression.\n3. The issue also shows that multiplication with numeric constants works correctly when the constant is on the left (-2*Poly(x)), but not when using symbolic constants (S(-2)*Poly(x)).\n4. Multiplication works correctly when Poly is on the left side (Poly(x)*S(-2)).\n\nThe expected behavior is clearly implied: multiplication should evaluate consistently regardless of the order of operands. The issue provides specific test cases that can be used to verify a fix. The code examples are complete and reproducible, showing both the current behavior and implying the expected behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13761": {
        "explanation": "This issue is well-specified with a clear error message and reproducible steps. The user is reporting that when trying to simplify an expression containing `x + csch(sinc(1))` using SymPy's `simplify` function, they encounter a KeyError for 'sinc'. The error occurs in the `fu.py` file at line 433, where it seems the code is trying to apply a transformation but doesn't have a mapping for the 'sinc' function.\n\nThe issue includes:\n1. The exact code that produces the error\n2. The specific error message with traceback\n3. A clarification that they're not expecting actual simplification, just for the function to work without errors\n\nA developer would be able to:\n1. Reproduce the issue using the provided code\n2. Locate the problem in the `fu.py` file at the specified line\n3. Understand that the issue is related to handling the 'sinc' function in the simplification process\n\nThe solution would likely involve adding proper handling for the 'sinc' function in the relevant mapping dictionary in the simplification code.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13773": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The problem is clearly stated: Currently, the `@` operator (matrix multiplication, `__matmul__`) in the Matrix class behaves the same as `__mul__`, but it should only work for actual matrix multiplication.\n\n2. The expected behavior is explicitly defined: The `@` operator should fail if one of the arguments is not a matrix (e.g., when it's a scalar).\n\n3. Examples are provided showing both the current behavior and the desired behavior:\n   - Current behavior: `2@B` works like scalar multiplication\n   - Desired behavior: `2@B` should fail with an error (similar to NumPy's behavior)\n\n4. A reference implementation (NumPy) is provided to demonstrate the expected behavior, including the exact error message.\n\nThe issue provides all the necessary information to implement a solution: modify the `__matmul__` method to check if both operands are matrices, and raise an appropriate error if one is a scalar. The solution path is clear and unambiguous.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13798": {
        "explanation": "This issue is well-specified. The user clearly explains:\n\n1. The current functionality: The `latex()` function accepts a `mul_symbol` parameter with only 4 predefined choices\n2. The desired change: Allow users to specify custom multiplication symbols, specifically wanting to use `\\,` (thin space)\n3. The context and rationale: Thin spaces are already used by SymPy in integrals to separate differentials from integrands\n4. A concrete example of the desired output: `3 \\, x^{2} \\, y`\n\nThe user even offers to create a PR themselves and is asking if there's a technical reason for the current limitation or if it's just a historical artifact. This provides all the necessary information to understand what needs to be changed and why. The solution would involve modifying the `latex()` function to accept arbitrary strings for the `mul_symbol` parameter while maintaining backward compatibility with the existing four choices.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13806": {
        "explanation": "The issue is about adding support for the LaTeX notation \\[{90^^\\circ }\\] which represents an angle of 90 degrees. The user wants this notation to be properly rendered in the system.\n\nWhile the core request is clear (support for degree notation in LaTeX), there are several important details missing:\n1. What software/system is this issue referring to? Is it a LaTeX rendering engine, a math library, or something else?\n2. What is the current behavior when this notation is used? Does it fail to render, render incorrectly, or cause an error?\n3. What is the expected behavior? Should it render as a superscript degree symbol?\n4. Is this a standard LaTeX notation or a custom one? The typical LaTeX for degrees is usually `^\\circ` or `^{\\circ}`, not `^^`.\n\nWithout knowing which software this applies to and what the current implementation looks like, it would be difficult to implement a solution. The double caret (`^^`) in the notation is also unusual and may indicate a misunderstanding of LaTeX syntax or a specific requirement that needs clarification.",
        "label": "VAGUE"
    },
    "sympy__sympy-13840": {
        "explanation": "The issue description is quite vague and lacks critical details needed to understand the problem and implement a solution. The description only mentions \"Max & Min converting using SymPy\" and asks why languages like JavaScript and R \"cannot be converted from Max & Min\" without explaining:\n\n1. What specific conversion functionality is being attempted\n2. What code was tried that failed\n3. What error messages were encountered\n4. What the expected behavior should be\n5. What the actual behavior was\n\nThe screenshot included might provide some context, but the description alone doesn't explain what the user is trying to accomplish with SymPy's Max and Min functions in relation to JavaScript and R. Without understanding the specific conversion issue, the expected output, or the context of the problem, it would be very difficult to implement a meaningful solution.\n\nA well-specified issue would include code examples, specific error messages, expected vs. actual behavior, and a clear description of what functionality is needed.",
        "label": "VAGUE"
    },
    "sympy__sympy-13852": {
        "explanation": "This issue is well-specified and provides clear information about what needs to be fixed:\n\n1. The first part describes a specific bug with the `polylog(2, Rational(1,2))` function, which doesn't expand correctly. The expected result is clearly stated as `-log(2)**2/2 + pi**2/12`, and there's even verification code showing this is the correct answer.\n\n2. The second part identifies another issue with `polylog(1, z)` expansion, where it unnecessarily includes `exp_polar(-I*pi)` terms. The issue provides:\n   - Clear examples of the problematic behavior\n   - Mathematical reasoning for why this is incorrect\n   - Evidence that the functions should be equivalent (derivative comparison)\n   - Demonstration that the current implementation creates inconsistencies\n\nThe issue provides sufficient context, expected behavior, and mathematical justification for both problems. A developer would have a clear understanding of what needs to be fixed in the `polylog` function's expansion implementation without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13877": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information. The issue describes a problem with the Matrix determinant calculation in SymPy when using symbolic entries. The user provides:\n\n1. A complete code example that reproduces the issue\n2. The exact error message and traceback\n3. The expected behavior (calculating determinants for symbolic matrices)\n4. A hypothesis about the root cause (Bareiss algorithm potentially not being valid for symbolic matrices)\n\nThe error occurs when calculating the determinant of a matrix with symbolic entries using the function `f(n) = det(Matrix([[i + a*j for i in range(n)] for j in range(n)]))`. For n=5, it returns \"nan\" and for n=6, it raises a \"TypeError: Invalid NaN comparison\" exception.\n\nThe traceback clearly shows where the error occurs in the code, pointing to issues in the Bareiss algorithm implementation when handling symbolic entries. This provides enough information for a developer to investigate the issue, understand the problem, and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13878": {
        "explanation": "This issue is well-specified with clear objectives and test cases. The problem is clearly defined: several continuous probability distributions in SymPy have CDFs that are currently computed by integrating the PDF, but this approach doesn't work well for many distributions. The solution is to implement internal `_cdf` methods with precomputed CDFs for these distributions.\n\nThe issue provides:\n1. A clear explanation of the problem and the proposed solution\n2. Specific distributions that need to be fixed\n3. Concrete test cases for each distribution showing the current problematic behavior\n4. References to where the correct CDF formulas can be found (Wikipedia)\n5. A suggested approach for testing the implementations (differentiating the CDF and comparing with PDF)\n\nThe task is straightforward: implement the `_cdf` method for each listed distribution using the known mathematical formulas rather than relying on integration. The issue even provides guidance on how to test the implementations.\n\nA developer with knowledge of probability distributions and SymPy's codebase structure would have all the information needed to implement a solution without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13895": {
        "explanation": "This issue is well-specified and clearly demonstrates a bug in SymPy's simplification function. The description provides:\n\n1. A clear example of the problem: `(-x/4 - S(1)/12)**x - 1` simplifies to an inequivalent expression\n2. Complete code to reproduce the issue\n3. Evidence that the simplified expression is incorrect:\n   - The original expression `e` and simplified expression `f` produce different numerical results when evaluated at x = 9/5\n   - The original expression evaluates to a real number (-1.32255...)\n   - The simplified expression incorrectly produces a complex number (-0.73905... - 0.18959...i)\n\nThe issue is clearly a bug in SymPy's simplification algorithm that's causing mathematical equivalence to be broken. The solution would involve fixing the simplification logic to maintain mathematical equivalence between the original and simplified expressions.\n\nThe reproduction steps are complete, the expected behavior is clear (mathematical equivalence should be preserved), and the actual incorrect behavior is well-documented with numerical examples.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13915": {
        "explanation": "This issue is well-specified and provides clear information about the problem:\n\n1. The issue demonstrates a specific mathematical expression `r = (1/(a+b) + 1/(a-b))/(1/(a+b) - 1/(a-b))` where substituting `b=a` leads to an incorrect result.\n\n2. The expected behavior is clearly stated: when `b=a`, the expression should be undefined because it creates division by zero in the subexpressions `1/(a-b)`.\n\n3. The issue points out that while `r.limit(b,a)` correctly gives `-1`, the direct substitution `r.subs(b,a)` incorrectly returns `1`.\n\n4. The environment is well-specified (Python 3.6.4, SymPy 1.1.1) and a complete reproducible example is provided.\n\nThe problem is clear: SymPy is not correctly handling the case where a substitution leads to an undefined expression (division by zero). The solution would involve modifying how SymPy evaluates substitutions to properly handle cases where the substitution creates undefined subexpressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13962": {
        "explanation": "This issue is well-specified with clear requirements and context:\n\n1. The problem is clearly defined: Currently, SymPy prints quantities using their full names (e.g., \"meter\") rather than abbreviations (e.g., \"m\"), which the reporter finds user-unfriendly.\n\n2. The specific code locations are provided:\n   - The test that expects full names: https://github.com/sympy/sympy/blob/8e962a301d7cc2d6fc3fa83deedd82697a809fd6/sympy/physics/units/tests/test_quantities.py#L87\n   - Where the change would need to be made: https://github.com/sympy/sympy/blob/8e962a301d7cc2d6fc3fa83deedd82697a809fd6/sympy/printing/str.py#L713\n\n3. The expected behavior is clearly described with examples:\n   - Current behavior: `print meter` returns \"meter\"\n   - Desired behavior: `print meter` should return \"m\"\n\n4. The issue acknowledges that tests will need to be updated and asks if there's a reason for the current behavior.\n\n5. The PR description indicates that the change has already been implemented, modifying the printing of quantities to use abbreviations when available.\n\nThe solution approach is straightforward: modify the printing functionality to use abbreviations instead of full names for quantities, and update the relevant tests. There's enough information to implement this change without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13971": {
        "explanation": "This issue is well-specified and clear. It describes a specific problem with the display of `SeqFormula()` in Jupyter notebooks where the output has backslash-escaped square brackets (`\\left\\[0, 1, 4, 9, \\ldots\\right\\]`) which don't render properly when copied to a markdown cell. The issue provides:\n\n1. A clear code example to reproduce the problem\n2. The exact output that's problematic\n3. A comparison with what works correctly\n4. A clear question about what should be fixed (either remove the backslash escaping of square brackets or make `\\]` render properly)\n\nThe issue is specific to SymPy's rendering in Jupyter notebooks and provides enough context for a developer to understand the problem, reproduce it, and implement a solution. The fix would likely involve modifying how SymPy's pretty printing handles sequence notation in Jupyter environments.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13974": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible examples. The issue is about the inability to evaluate powers of tensor product expressions using either the `expand(tensorproduct=True)` method or the `tensor_product_simp` function.\n\nThe description provides:\n1. A complete code example that demonstrates the issue\n2. Clear expected behavior (shown in examples [5] and [9])\n3. Clear actual behavior (shown in examples [3], [4], [7], and [8])\n\nThe problem is that when tensor products are raised to powers (like `tp(1,1)**2` or `tp(1,Pauli(3))**2`), they are not being simplified correctly. However, when equivalent expressions are constructed by substitution, they work as expected.\n\nA successful solution would need to modify either the `expand` method with the `tensorproduct=True` parameter or the `tensor_product_simp` function to correctly handle powers of tensor products. The expected behavior is clearly demonstrated in the examples, making it straightforward to verify if a solution works correctly.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-13978": {
        "explanation": "This issue is reasonably well-specified. The user is reporting that SymPy generates Octave code for imaginary numbers in the format `sqrt(3)i`, which causes errors in Octave 4.0. They suggest that the format should be changed to `sqrt(3)*i` to make it compatible with Octave.\n\nThe issue clearly identifies:\n1. The problem: SymPy generates imaginary number representations that cause errors in Octave 4.0\n2. A specific example: `sqrt(3)i` vs the suggested `sqrt(3)*i`\n3. A proposed solution: Add a multiplication symbol between the coefficient and the imaginary unit\n\nWhile the issue doesn't provide the exact error message from Octave or point to the specific part of the SymPy codebase that handles Octave code generation, a developer familiar with SymPy would likely be able to locate the relevant code for Octave printers and modify how imaginary numbers are formatted. The fix would involve ensuring that imaginary numbers are represented with an explicit multiplication operator between the coefficient and the imaginary unit 'i'.\n\nThe issue is specific enough to understand what needs to be fixed, though some investigation would be needed to find the exact location in the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14024": {
        "explanation": "This issue is well-specified with a clear demonstration of an inconsistency in SymPy's simplification behavior. The issue shows two cases:\n\n1. When `a` is a symbolic positive integer (`Symbol('a', integer=True, positive=True)`), the simplification of `(-a)**x * a**(-x)` to `(-1)**x` produces numerically consistent results.\n\n2. When `a` is a specific positive integer value (`S(2)`), the same simplification produces numerically inconsistent results - the original expression `(-2)**x*2**(-x)` evaluates differently from the simplified expression `(-1)**x` when `x = -10/3`.\n\nThe issue clearly demonstrates the bug with concrete examples, showing the exact expressions, their simplifications, and the numerical evaluations that don't match. The expected behavior would be for the simplification to maintain numerical equivalence in both cases.\n\nThis is a well-defined mathematical inconsistency that can be investigated and fixed by examining how SymPy handles powers of negative numbers and simplification rules for these types of expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14031": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible steps. The issue describes a specific error that occurs when trying to integrate a particular expression involving exponential functions. The error message is provided, and there's a reference to a previous related issue (#13970) which suggests this is a continuation of a known problem.\n\nThe code snippet provided is complete and can be run to reproduce the error:\n1. Define a variable x\n2. Define a function f with exponential terms\n3. Try to integrate f with respect to x\n4. Observe the CoercionFailed error\n\nThe error message clearly indicates the problem: the system is unable to convert an expression containing E and exp to a field element. The reference to a previous issue (#13970) provides additional context that this is a known class of problems.\n\nA developer familiar with SymPy's integration capabilities and the previous issue would have enough information to investigate and attempt a fix for this coercion failure.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14038": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in SymPy's `product` function when evaluating the infinite product of (1 - a\u00b2/(n\u03c0)\u00b2) from n=1 to infinity. The issue states that the current implementation incorrectly returns 0, while the mathematically correct result should be `sinc(a)` (where sinc(a) = sin(a)/a). \n\nThe issue provides:\n1. A clear code example demonstrating the problem\n2. The expected output (sinc(a))\n3. The incorrect current output (0)\n\nThis is a well-defined mathematical problem with a known correct answer. The issue is asking for the product function to correctly evaluate this specific infinite product to the known closed-form expression. A developer would have enough information to investigate why the current implementation fails and implement a fix to correctly handle this case.\n\nThe solution would involve modifying SymPy's product evaluation logic to recognize this specific infinite product pattern and return the sinc function instead of 0.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14070": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific function that's not working: `logcombine(log(3) - log(2))`\n2. The expected output: `log(3/2)`\n3. The current behavior: \"does nothing\" (implying it returns the input unchanged)\n4. Historical context: \"This used to work in 0.6.7\"\n5. Reference to original issue: #2851\n\nThe problem is straightforward - the logcombine function is not properly combining logarithmic expressions of the form log(a) - log(b) into log(a/b). This is a clear mathematical transformation that should be implemented in the function. A developer would know exactly what needs to be fixed and could verify the solution by checking if the function returns log(3/2) for the given input.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14082": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The exact problem: `integrate(1/(x**2 + y**2), x)` returns 0 when it should return `atan(x/y) / y`\n2. A reproducible test case with the exact code that demonstrates the issue\n3. The expected result: `atan(x/y) / y`\n4. A reference to a similar issue (#8246) which could provide additional context\n\nThe issue provides all the necessary information to understand the problem and verify a solution. A developer would need to:\n1. Investigate why the integration function returns 0 for this specific expression\n2. Fix the integration algorithm to correctly handle this case\n3. Verify the fix by ensuring the integration returns the expected result `atan(x/y) / y`\n\nThe mathematical correctness of the expected result can be verified independently, and the issue is specific enough to be addressed without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14085": {
        "explanation": "This issue is well-specified with a clear problem description and expected behavior. The issue describes that:\n\n1. When trying to use `sympify()` or the shorthand `S()` with a Unicode Greek letter (specifically \"\u03b1\"), it fails with a SympifyError.\n2. The error message indicates that it cannot parse the Greek letter.\n3. The expected behavior is that `sympify()` should be able to parse Greek letters, since they are used in the pretty printing of symbols with those names (as demonstrated in the example where `Symbol('alpha')` is printed as `\u03b1`).\n\nThe issue includes a clear reproduction case with the exact error message and demonstrates the inconsistency between how symbols are displayed and how they cannot be parsed back. The context is clear, and the desired solution is implied: make `sympify()` able to handle Greek letters as input.\n\nA developer would have enough information to investigate and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14180": {
        "explanation": "The issue is well-specified. It clearly identifies a problem with the LaTeX conversion function where `ln(10)` is being incorrectly converted to `log{\\left(10\\right)}` instead of using the proper LaTeX command `\\ln{\\left(10\\right)}`. \n\nThe issue explains:\n1. The current behavior: `latex(ln(10))` produces `log{\\left(10\\right)}`\n2. The problem: In some fields, `log` without a base specified could be interpreted as log base 10 or log base 2\n3. The expected behavior: `ln(10)` should be converted to `\\ln{\\left(10\\right)}` since LaTeX has a specific command for natural logarithm\n\nThis provides enough information to understand what needs to be fixed - the LaTeX conversion function needs to be modified to use `\\ln` instead of `log` when converting natural logarithm expressions. The solution would involve finding where this conversion happens in the codebase and updating the logic to use the correct LaTeX command.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14207": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible code example, expected output, and actual output. The issue is about incorrect parenthesis placement when printing multiplication expressions involving division in SymPy.\n\nSpecifically:\n1. The problem is clearly demonstrated: when printing `-2*u/(a*a)` with `evaluate=False`, SymPy outputs `-2*u/a*a` which is mathematically different due to operator precedence.\n2. The context is explained: this is being used for code generation where the exact form of the expression matters.\n3. The expected output is explicitly stated: `-2*u/(a*a)` instead of `-2*u/a*a`.\n4. The issue mentions that the problem also occurs in C and Python code generation printers.\n5. Version information is provided: Python 3.6 and SymPy master (sympy-1.1.1-2784-g98d5dd9).\n\nThe solution would involve modifying how SymPy's printing system handles parentheses in multiplication expressions involving division, particularly when `evaluate=False` is used. This would likely require changes to the `print_Mul` method and similar methods in the code generation printers.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14248": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue is that when subtracting MatrixSymbols, the output is displayed as a sum with (-1) coefficients rather than using the more natural subtraction notation. The user provides:\n\n1. A specific code example that demonstrates the problem\n2. The actual output from three different printers (str, pretty, and latex)\n3. The expected behavior (differences should print like \"a-b\" instead of \"a+(-1)*b\")\n\nThe issue is focused on a specific component (MatrixSymbols) and a specific behavior (how subtraction is displayed). The solution would involve modifying how MatrixSymbols are printed when they appear in expressions with negative coefficients, to match the behavior of other SymPy objects.\n\nThis is a clear display/formatting issue with a well-defined expected outcome, making it straightforward to understand what a successful solution would look like.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14308": {
        "explanation": "The issue is reasonably well-specified. It identifies two specific problems with the pretty printing of vectors in SymPy:\n\n1. The first problem is clearly demonstrated with a reproducible code example showing how expressions with vectors break pretty printing, resulting in a malformed output where the vector notation (e_j) appears twice and the expression structure is incorrect.\n\n2. The second problem states that even when pretty printing works correctly, the baseline alignment is wrong - specifically that vector components should be centered but aren't.\n\nThe issue provides enough information to understand what's wrong and what needs to be fixed. A developer familiar with SymPy's pretty printing system would be able to investigate and fix these issues based on this description. The only minor ambiguity is that it doesn't specify exactly what the correct output should look like, but this can be reasonably inferred from understanding SymPy's pretty printing conventions and the statement about centering the baseline.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-14317": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue states that when printing a polynomial using the LaTeX printer, the order of monomials differs from the order used by the str and pretty printers. \n\nThe example clearly demonstrates the problem:\n1. The str and pretty printers show monomials in descending order of degree: a*x^5, x^4, b*x^3, 2*x^2, c*x, 3\n2. The LaTeX printer shows a different order: a*x^5, b*x^3, c*x, x^4, 2*x^2, 3\n\nThe expected behavior is that the LaTeX printer should maintain the same monomial order as the other printers (from highest to lowest degrees). The solution would involve modifying the LaTeX printer to ensure consistent ordering across all printing methods.\n\nThe issue provides all necessary information to understand and reproduce the problem, and the expected behavior is clearly implied.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14333": {
        "explanation": "This issue is well-specified and provides clear information about the current behavior of the `mod_inverse` function in SymPy and how it differs from the expected mathematical behavior and other implementations.\n\nThe issue clearly identifies three problems:\n1. SymPy's `mod_inverse(a,m)` assigns the same sign to the result as `a`, which differs from mathematical convention and other implementations like Wolfram Alpha\n2. SymPy doesn't support negative modulo values due to a `m>1` check\n3. The expected behavior is provided with specific examples and references to how other systems (C/C++/Java vs Python vs Wolfram Alpha) handle modular arithmetic\n\nThe issue includes concrete examples of current behavior and expected behavior, making it clear what needs to be fixed. The solution would involve:\n1. Modifying the `mod_inverse` function to handle negative modulo values\n2. Ensuring the sign assignment follows the mathematical convention where modulo m has range [0,m) for m>0 and (m,0] for m<0\n3. Making the function consistent with the expected behavior demonstrated by the Wolfram Alpha examples\n\nThis is sufficient information for a developer to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14396": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible error. The issue shows that:\n\n1. When creating a polynomial with `Poly(1.2*x*y*z, x)`, SymPy automatically detects the domain as 'RR[y,z]' (real coefficients with y,z as parameters)\n2. However, when explicitly specifying this same domain with `Poly(1.2*x*y*z, x, domain='RR[y,z]')`, it fails with an `OptionError`\n3. The error message indicates that 'RR[y,z]' is not recognized as a valid domain specification\n\nThe issue also mentions that the error message could be improved.\n\nThis is a clear bug report with a reproducible example and expected behavior (the domain that SymPy auto-detects should be valid when explicitly specified). A developer would need to:\n1. Fix the domain parsing to accept 'RR[y,z]' format when explicitly specified\n2. Improve the error message to be more helpful\n\nThe solution path is clear and the issue provides all necessary information to understand and address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14531": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes how the `sympy_integers=True` setting in the `sstr` function is not being respected by certain subexpressions like `Eq` and `Limit`. The examples clearly show the current incorrect output versus the expected output. \n\nThe issue also mentions a related problem with code generation where the `python()` function doesn't generate the expected Symbol declarations. The examples provide enough context to understand what's happening and what needs to be fixed.\n\nThe problem is consistent and reproducible based on the examples provided. A developer would be able to identify the relevant code in the StrPrinter class and its subclasses to fix the issue where certain subexpressions are not respecting the printer settings.\n\nThe mention that \"A fix is forthcoming\" suggests that the reporter may already have a solution in mind, but this doesn't detract from the clarity of the issue description.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14564": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problematic behavior of the ConditionSet class in SymPy. The issue identifies several specific problems:\n\n1. The documentation is ambiguous about what ConditionSet actually represents.\n2. There are inconsistencies in how substitution (subs) works with ConditionSet.\n3. The __contains__ method raises errors when it should return boolean values.\n4. There's a question about whether ConditionSet should automatically evaluate to EmptySet when the condition cannot be satisfied.\n5. Specific examples of incorrect behavior are provided with expected corrections.\n\nThe issue provides concrete code examples with current outputs and suggestions for what the correct behavior should be. The examples cover various edge cases and demonstrate the inconsistencies clearly. The reporter has also suggested specific fixes for some of the problems.\n\nThe issue is detailed enough that a developer could implement a solution without needing additional clarification. The expected behavior is clear from the examples and explanations provided.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14627": {
        "explanation": "This issue is well-specified and clear. It identifies a specific problem: after a certain commit (76adb16333dffd42635f50f935a4a1badbe0a814), the `binomial(n, n)` expression no longer automatically simplifies to 1, which is mathematically correct (the binomial coefficient \"n choose n\" equals 1). The issue provides:\n\n1. A clear example demonstrating the problem\n2. The exact commit that introduced the issue\n3. A workaround (using `simplify()`)\n4. A clear expectation: `binomial(n, n)` should automatically simplify to 1\n\nThe issue is asking whether this behavior was intentional and whether it should be fixed so that `binomial(n, n)` automatically simplifies to 1 without requiring an explicit call to `simplify()`. This is a well-defined mathematical property that can be implemented in code.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14699": {
        "explanation": "This issue is well-specified and clear. The problem is that the `orientnew` method in the `ReferenceFrame` class from `sympy.physics.mechanics` does not support setting indices, while the direct constructor of `ReferenceFrame` does support this feature.\n\nThe issue provides:\n1. Clear example of what currently works: `A = ReferenceFrame('A', indices=('1', '2', '3'))`\n2. Clear example of what doesn't work but should: `B = A.orientnew('B', 'Axis', [theta, A['1']], indices=('1', '2', '3'))`\n3. A suggestion for a potential additional feature (global setting for indices)\n\nThe solution would require modifying the `orientnew` method to accept and pass along the `indices` parameter to the `ReferenceFrame` constructor. This is a straightforward enhancement that can be implemented by examining the current implementation of `orientnew` and adding the missing parameter. The issue is specific, reproducible, and has a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14711": {
        "explanation": "This issue is well-specified with a clear problem description. The user provides a complete code snippet that demonstrates the error, along with the full error traceback. The issue is that when trying to use Python's `sum()` function with a list containing a vector and a zero-multiplied vector (`sum([N.x, (0 * N.x)])`), it raises a TypeError because the `sum()` function in Python starts with an initial value of 0, which is not a Vector object.\n\nThe error occurs in the `__add__` method of the Vector class when it tries to add the initial 0 value to the first vector in the list. The error message \"A Vector must be supplied\" clearly indicates the problem.\n\nA solution would need to modify how Vector addition handles scalar values (particularly 0) or provide an alternative way to sum vectors that properly handles the initial value in Python's sum() function. The issue is specific, reproducible, and the expected behavior is clear (the sum operation should work with vectors).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14774": {
        "explanation": "This issue is well-specified with clear information about:\n\n1. The problem: The LaTeX printer doesn't properly support full inverse trig function names for acsc and asec when using inv_trig_style=\"full\"\n2. Examples demonstrating the issue: Shows working example with asin(x) and non-working example with acsc(x)\n3. Expected vs actual output: Expected '\\arcsin' vs '\\operatorname{acsc}' instead of '\\operatorname{arccsc}'\n4. Proposed solution: Specific code change to line 743 of sympy/printing/latex.py to add \"acsc\" and \"asec\" to the inv_trig_table list\n\nThe issue provides enough context to understand the problem, reproduce it, and implement a solution. The proposed fix is specific and reasonable, making it straightforward to create a PR that addresses the issue.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14817": {
        "explanation": "This issue is well-specified with a clear problem description and error trace. The issue shows:\n\n1. A specific error when trying to pretty print a matrix addition expression containing a MatrixSymbol with a special character ('*') in its name\n2. The exact traceback showing where the error occurs in the code\n3. The root cause: the code is using sympify to handle string arguments from MatrixSymbol, which fails when the symbol name contains special characters like '*'\n4. A specific question about the implementation logic (why it omits the '+' when the first argument is negative)\n\nThe issue provides enough context to understand the problem and locate the relevant code in the pretty printing module. The fix would involve:\n1. Modifying how the MatAdd pretty printer handles MatrixSymbol names with special characters\n2. Avoiding the use of sympify for processing the MatrixSymbol arguments\n3. Potentially reviewing the logic for handling negative terms in matrix addition\n\nThis is a well-defined bug with a clear reproduction case and enough information to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14821": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: When generating Octave/MATLAB code for the two-argument zeta function using `octave_code(zeta(x,n))`, the arguments are in the wrong order.\n\n2. The expected behavior: The generated code should be `zeta(n, x)` instead of `zeta(x, n)`.\n\n3. A reference to documentation: The issue includes a link to MATLAB's documentation for the zeta function, which confirms that in MATLAB/Octave, the argument order is indeed (n, x) rather than (x, n).\n\nThe issue is specific, identifies the current incorrect behavior, specifies the expected correct behavior, and provides a reference to verify the correct implementation. A developer would be able to understand what needs to be fixed without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-14976": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes that when using `lambdify` with the 'mpmath' module, rational numbers (like 1/3) are not properly wrapped, resulting in reduced precision in numerical calculations.\n\nThe issue provides:\n1. A specific code example showing the problem with `lambdify(modules='mpmath')` not properly handling rational numbers\n2. The actual output showing that the rational `S(1)/3` becomes `232/3` in the generated function\n3. A demonstration of how this affects precision in `nsolve` calculations\n4. A reference to a previous PR where this was originally reported\n\nThe expected behavior is implied but clear: rational numbers should be properly handled when using 'mpmath' as the module in lambdify to maintain full precision. The issue is specific, reproducible, and the problem is well-defined with concrete examples showing both the issue and its impact.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15011": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue describes that `lambdify` in SymPy fails when trying to use a `MatrixSymbol` with curly braces in its name, even when the `dummify=True` option is used. The user provides a complete code example that demonstrates the problem:\n\n1. They show that using curly braces in regular symbols works fine\n2. They show that using `MatrixSymbol` with normal names works fine\n3. They demonstrate the specific failure case: using `MatrixSymbol` with curly braces in the name\n4. They show that even with `dummify=True`, the error persists\n\nThe error is clearly a `SyntaxError`, and the expected behavior would be for `dummify=True` to handle special characters in the name. This is enough information to investigate and fix the issue in the SymPy codebase, specifically in the `lambdify` function's handling of `MatrixSymbol` names when `dummify=True` is set.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15017": {
        "explanation": "This issue is well-specified with clear examples and expected behavior. The problem is precisely defined: the `__len__` method of `sympy.tensor.array.NDimArray` returns 0 for rank-0 arrays (scalars), but the reporter believes it should return 1 to match NumPy's behavior and to be consistent with the number of elements in the array's iterator.\n\nThe issue includes:\n1. Clear reproduction steps with code examples\n2. The current behavior (`len(a)` returns 0)\n3. The expected behavior (should return 1, like `len(list(a))`)\n4. A comparison with NumPy's behavior for the same case\n5. Version information (sympy 1.2-rc1 on Python 3.6.6)\n\nA developer would know exactly what needs to be fixed: modify the `__len__` method in the `NDimArray` class to return 1 for rank-0 arrays instead of 0. The solution would be straightforward - find the implementation of `__len__` in the `sympy.tensor.array.NDimArray` class and modify it to handle the rank-0 case correctly.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15085": {
        "explanation": "This issue is well-specified with a clear problem description and a minimal working example (MWE) that reproduces the error. The issue describes a regression in the `lambdify` function when working with `sympy.Matrix` objects.\n\nThe problem is clearly stated: when trying to create a lambda function from a sympy expression involving a dot product with a `sympy.Matrix`, a `NameError` occurs stating that 'Matrix' is not defined.\n\nThe MWE provides all necessary code to reproduce the issue, including:\n1. All required imports\n2. Definition of a custom `dot` function\n3. Creation of a symbolic expression using `Matrix`\n4. Attempt to lambdify the expression\n5. Attempt to evaluate the lambdified function\n6. The exact error message received\n\nThere are also commented-out lines that might suggest a potential solution approach (custom modules for lambdify), which provides additional context.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Understand the expected behavior\n3. Investigate why the Matrix object is not properly handled by lambdify\n4. Implement a fix that ensures Matrix objects are properly recognized when lambdifying expressions\n\nThe issue is specific enough to identify the root cause and develop a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15198": {
        "explanation": "This issue description is vague and lacks critical details needed for a meaningful solution:\n\n1. It mentions a regression in codegen for octave/julia/jscode but doesn't clearly explain what the expected behavior should be across all these languages.\n\n2. It specifically mentions that Octave codegen prints `laguerre` but should error on `assoc_laguerre`, but doesn't explain:\n   - Why this behavior is expected\n   - What the current incorrect behavior is exactly\n   - What the proper implementation should do\n\n3. The issue references a specific commit (e99b756df3291a666ee2d2288daec4253014df40) but doesn't explain what changes in that commit caused the regression.\n\n4. It mentions the behavior is \"untested, apparently\" which suggests uncertainty about the expected behavior.\n\n5. There are no steps to reproduce the issue, no code examples showing the problem, and no clear description of what a successful fix would look like.\n\nWithout more specific information about the expected behavior, the current behavior, and how to reproduce the issue, it would be difficult to implement a meaningful solution.",
        "label": "VAGUE"
    },
    "sympy__sympy-15222": {
        "explanation": "The issue description shows a potential bug in the `Subs` class in SymPy, specifically related to substitution and hashing. The problem is clearly demonstrated with code examples:\n\n1. When creating a `Subs` object with `Subs(x+y,(a,),(4,))` and then applying `.subs(a,z)`, the result remains unchanged, which appears to be expected behavior.\n\n2. However, when creating `Subs(x+y,(a,),(a,))` and then applying `.subs(a,z)`, the result is `Subs(x + y, (a,), (4,))` instead of the expected `Subs(x + y, (a,), (z,))`.\n\nThe issue is well-defined with concrete examples showing the current behavior and the expected behavior. The problem appears to be that substitution is not working correctly when the substitution variable appears in the third argument of the `Subs` constructor.\n\nA developer would be able to reproduce this issue and understand what needs to be fixed: the substitution mechanism for the `Subs` class needs to be corrected to properly handle cases where the substitution variable appears in the tuple of values.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15225": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user has identified what appears to be a bug in SymPy's `bool_map` function when comparing a logical XOR expression with its negation (XNOR). \n\nThe issue shows:\n1. A complete code example that can be run to reproduce the problem\n2. The actual output showing that `bool_map` is returning mappings between expressions that are logically different\n3. The user's understanding that the simplified functions are correct but `bool_map` is incorrectly suggesting they are equivalent under some mapping\n\nThe expected behavior would be that `bool_map` should return `None` when comparing logically different expressions (XOR and XNOR), as they cannot be mapped to each other. This appears to be a bug in the `_finger` fingerprint mechanism used by `bool_map` to determine equivalence.\n\nA developer would have enough information to investigate the issue in the `bool_map` function and its underlying fingerprinting mechanism to identify and fix the bug.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15231": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user has:\n\n1. Provided a minimal code example that demonstrates the issue\n2. Included the exact error message received\n3. Shown the generated Fortran code that contains the problem\n4. Identified the specific issue: the `Mod` function is being declared twice in the generated Fortran code\n\nThe problem is clear: when using SymPy's `autowrap` function with expressions containing multiple instances of the `Mod` function, the generated Fortran code incorrectly declares the `Mod` variable multiple times, causing a compilation error.\n\nThe solution would involve modifying the code generation in SymPy's autowrap functionality to ensure that the `Mod` function is only declared once in the generated Fortran code, regardless of how many times it appears in the expression.\n\nThis is a well-defined bug with a clear path to resolution - modify the code generation to avoid duplicate declarations of functions like `Mod` in the generated Fortran code.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15241": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with the canonicalization of variables in the `Derivative` class, particularly in the `_sort_variable_count` method. The issue clearly states what the expected behavior should be: when sorting variables in a derivative expression, any symbols that appear after functions but are not in the free symbols of the function should be moved before the functions.\n\nThe example provided illustrates this expected behavior: `Derivative(f(x, y), x, f(y), x)` should be canonicalized to `Derivative(f(x, y), x, x, f(y))`. This shows that the 'x' that appears after 'f(y)' should be moved before 'f(y)' in the canonical form.\n\nWhile the issue doesn't provide implementation details or the current algorithm, it gives enough information for a developer familiar with the codebase to understand what needs to be changed. The developer would need to modify the `_sort_variable_count` method in the `Derivative` class to implement this new canonicalization rule.\n\nThe only potential ambiguity is whether there are edge cases or additional requirements not mentioned in the issue, but the core problem and expected solution are clear enough to make a meaningful attempt at solving it.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15286": {
        "explanation": "This issue is reasonably well-specified, though it contains multiple components that need to be addressed:\n\n1. The main issue appears to be about recognizing elliptical integrals in SymPy's integration functionality, as the current implementation of ellipse circumference calculation is slow compared to a custom implementation.\n\n2. The PR description mentions several additional features being implemented:\n   - A function for finding the equation of an ellipse using slope as a parameter\n   - A faster method for calculating the circumference of an ellipse\n   - A method for checking if points are collinear\n   - Pluralizing certain method names\n\nThe issue provides:\n- A clear demonstration of the performance problem\n- A reference implementation for a faster algorithm\n- Links to related issues being fixed\n- A mathematical reference for the approach being used\n- Release notes indicating what's being added\n\nWhat makes this not fully \"well-specified\" is that it combines multiple features in one PR, and it's not entirely clear how they all relate to the original issue about elliptical integrals. The implementation details for some of the additional features (like the collinearity check) aren't fully explained.\n\nHowever, there is enough information for a developer familiar with SymPy to understand what needs to be done and implement a solution. The core functionality (faster ellipse circumference calculation and equation using slope) is clearly described with examples and references.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15308": {
        "explanation": "The issue is well-specified and clear. It describes a problem with LaTeX printing for Matrix Expressions in SymPy, specifically when using the `trace()` function on matrix expressions. The issue shows:\n\n1. A concrete example with code that demonstrates the problem\n2. The current output: 'Trace(A**2)'\n3. Two specific issues:\n   - The 'Trace' function is not properly recognized by the LaTeX printer\n   - The inner expression A**2 is not properly formatted in LaTeX (should be A^2)\n\nThe expected behavior is implied: the LaTeX output should properly render both the trace function and the inner matrix expression in proper LaTeX notation. This would likely mean something like '\\text{Tr}(A^2)' or similar standard LaTeX notation for matrix traces.\n\nA developer would have enough information to locate the LaTeX printer code for matrix expressions, identify why the trace function isn't properly handled, and implement a fix that ensures both the trace function and its arguments are properly rendered in LaTeX.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15320": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with the RootOf function in SymPy where the generator (variable) might be ignored or inconsistently handled when substitutions are made.\n\nThe issue provides clear examples demonstrating two problematic cases:\n1. When substituting tan(x) for x in a polynomial and passing it to RootOf, the function ignores the substitution and returns a result in terms of x, which could lead to incorrect interpretations of the roots.\n2. When directly using tan(x) as a generator in one case and x in another with the same polynomial form, RootOf inconsistently chooses which generator to use in the result.\n\nThe issue doesn't explicitly state what the expected behavior should be, but it's reasonably clear that the function should maintain consistency with the generator that was provided. The warning comments in the examples (marked with \"/!\\\") indicate the problematic behavior.\n\nWhile the issue doesn't provide details about the implementation or a specific fix, it provides enough information for a developer familiar with the codebase to investigate and address the inconsistent behavior of RootOf with respect to generators.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15345": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific function that has a problem (`mathematica_code`)\n2. The exact input that causes the problem (`Max(x,2)`)\n3. The current output (`'Max(2, x)'`)\n4. The expected output (`'Max[x,2]'`)\n5. The reason why the current output is problematic (it's not valid Mathematica code)\n\nThe issue is describing a bug where the `mathematica_code` function is not correctly translating Python's `Max` function to Mathematica's syntax. In Mathematica, the `Max` function uses square brackets, not parentheses, and the order of arguments appears to be different.\n\nA developer would have enough information to reproduce the issue and understand what needs to be fixed: the `mathematica_code` function needs to be updated to properly handle the `Max` function by using square brackets instead of parentheses and potentially preserving the original argument order.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15346": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user is reporting that while SymPy can simplify trigonometric expressions with symbolic variables (e.g., sin(x)*sin(y) + cos(x)*cos(y) simplifies to cos(x-y)), it fails to simplify the same expression when using Rational numbers as inputs.\n\nThe issue includes complete code examples showing:\n1. Working simplification with symbolic variables\n2. Working simplification with basic Rational arithmetic\n3. Failing simplification when using Rational numbers in trigonometric functions\n\nThe expected behavior is clear: the expression sin(Rational(1, 50))*sin(Rational(1, 25)) + cos(Rational(1, 50))*cos(Rational(1, 25)) should simplify to cos(Rational(1, 50) - Rational(1, 25)) or cos(-Rational(1, 50)), following the same trigonometric identity that works with symbolic variables.\n\nThe environment is also specified (Python 3 on Windows with latest cloned SymPy). This is a well-defined bug that can be reproduced and addressed without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15349": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The user has:\n\n1. Identified a specific function with an error (`Quaternion.to_rotation_matrix()`)\n2. Provided the exact file and line number where the issue occurs\n3. Included a complete code example that demonstrates the problem\n4. Specified the expected behavior (one of the `sin(x)` terms should be negative)\n5. Included version information (Sympy 1.2)\n\nThe issue is related to quaternion rotation matrices in mathematical physics/computer graphics, where the correct rotation matrix for a quaternion representing rotation around the x-axis should have opposite signs for the sine terms in the matrix. This is a well-known mathematical property that can be verified against standard references.\n\nA developer can reproduce this issue, verify the mathematical correctness against references, and implement a fix without needing additional clarification. The only minor question asked (\"What was the reference of the original equations?\") is not essential for solving the problem, as the correct mathematical form of rotation matrices is well-established.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15446": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue is that when printing the product of a matrix symbol and a negated matrix symbol using LaTeX, the output is ambiguous because it omits necessary parentheses. \n\nThe example code demonstrates the problem:\n1. Create two matrix symbols x and y\n2. Form an expression x*y\n3. Substitute y with -y\n4. Print the LaTeX representation of the result\n\nThe output is \"x -y\" which could be misinterpreted as \"x minus y\" rather than \"x times negative y\". The expected behavior would be to include parentheses around the negative y, resulting in something like \"x(-y)\" to make it clear that it's a product with a negative matrix.\n\nThe issue has a clear scope, a reproducible example, and the expected behavior can be inferred. A solution would involve modifying the LaTeX printer to properly handle this case by adding parentheses around negated matrix symbols in products.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15523": {
        "explanation": "This issue is well-specified with clear error messages and context. The reporter has:\n\n1. Identified specific test failures in Python 3.7.1 that pass in other versions\n2. Provided exact error messages with stack traces\n3. Specified the failing test files: `test_implicit_multiplication_application.py` and `test_sympify.py`\n4. Included the exact error messages showing:\n   - A TypeError in factorial function calls: \"factorial takes exactly 1 argument (0 given)\"\n   - A TypeError in sympify with keywords: \"__new__() missing 1 required positional argument: 'expr'\"\n\nThe issue is clearly about compatibility problems with Python 3.7.1 that weren't caught by the existing test infrastructure. The error messages provide enough detail to trace the problem to specific functions and code paths. A developer would be able to reproduce the issue by running the specified tests under Python 3.7.1 and then investigate the root cause of the failures, which appear to be related to function argument handling that changed in Python 3.7.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15555": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible error. The bug report shows that the `primepi()` function in SymPy doesn't work with symbolic arguments, specifically when trying to use it within a `limit()` function. The error occurs because `primepi()` tries to convert its argument to an integer using `int(n)`, but when a symbolic argument (like a Symbol 'x') is passed, this conversion fails with the error \"int() argument must be a string or a number, not 'Symbol'\".\n\nThe issue includes:\n1. A clear description of the problem\n2. A specific use case (demonstrating the prime number theorem)\n3. A reproducible code example showing the error\n4. The exact error message and traceback\n5. The specific line in the code causing the issue (line 129 in sympy/ntheory/generate.py)\n\nThe solution would require modifying the `primepi()` function to handle symbolic arguments appropriately, likely by adding a conditional check before attempting to convert to an integer. This is a common pattern in SymPy for functions that need to work with both numeric and symbolic inputs.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15586": {
        "explanation": "This issue is well-specified with clear information about what the user is trying to accomplish and the exact error they're encountering. The issue includes:\n\n1. A complete code snippet that reproduces the problem\n2. The exact error message with traceback\n3. A clear goal: inverting a float matrix using sympy and numpy\n4. A hint that using integer matrices works but float matrices fail\n5. Links to additional resources (StackOverflow discussion and live code)\n\nThe problem is clearly about the incompatibility between sympy's matrix inversion via lambdify and numpy's float matrices. The error occurs specifically when trying to apply the lambdified symbolic inverse function to a float matrix. The user even notes that integer matrices work fine, which provides valuable context for understanding and solving the issue.\n\nWith this information, a developer could investigate why the lambdify function fails with float matrices but works with integer matrices, and propose a solution to make the code work with float matrices as intended. The issue contains all the necessary information to reproduce and diagnose the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15596": {
        "explanation": "The issue is reasonably well-specified. The user is reporting that when they call the `degree()` function on a rational function (a fraction with polynomials in the numerator and denominator), they get a result of 1, which appears to be the degree of just the numerator. \n\nThe user is questioning whether:\n1. SymPy should be able to compute degrees of rational functions\n2. If it's not implemented, whether the function should fail with an error instead of returning what appears to be just the degree of the numerator\n\nThe issue provides a clear example with code and expected behavior questions. While the user doesn't explicitly state what the correct behavior should be (e.g., should it return a tuple of numerator/denominator degrees, or should it follow the mathematical definition where degree of a rational function is numerator degree minus denominator degree), they are clearly asking for clarification on the intended behavior.\n\nA developer would be able to:\n1. Check the documentation for the intended behavior of the degree() function with rational functions\n2. Determine if the current implementation is correct or needs modification\n3. Either implement proper handling of rational functions or add appropriate error handling\n\nThe issue is specific enough to investigate and address without requiring significant additional information.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15599": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The issue describes that `Mod(3*i, 2)` should reduce to `Mod(i, 2)` but currently doesn't. The description includes:\n\n1. A clear problem statement with an example\n2. A reference to a related Stack Overflow post\n3. A proposed diff showing exactly what code changes are needed\n4. A test case to verify the fix\n5. Before and after examples showing the expected behavior\n6. Release notes indicating where the change belongs\n\nThe proposed solution includes modifying the `doit` method in `sympy/core/mod.py` to handle this specific case and adding a test case in `test_arit.py`. The diff shows exactly what lines need to be changed and what new code needs to be added.\n\nWith this information, a developer could implement the fix without needing additional clarification. The issue provides all necessary context and a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15625": {
        "explanation": "This issue is well-specified with clear reproduction steps, error messages, and a proposed solution. The problem is clearly defined: when converting Jupyter notebooks with SymPy expressions to LaTeX/PDF using nbconvert, the LaTeX output breaks because SymPy's LaTeX representation is wrapped in both `$$` delimiters and `\\begin{equation*}...\\end{equation*}` environment, causing a \"Bad math environment delimiter\" error.\n\nThe issue provides:\n1. Complete reproduction steps with code examples\n2. The exact error message\n3. An explanation of why the error occurs (nesting math environments incorrectly)\n4. A proposed solution (changing from `$$..$$` to `$\\displaystyle...$`)\n5. References to related issues in other repositories\n6. Visual examples showing the before and after states\n7. Release notes indicating where the change should be made\n\nA developer would have all the information needed to locate the problematic code in the SymPy codebase that handles LaTeX output formatting and implement the suggested fix. The solution approach is clear: modify how SymPy wraps its LaTeX output to avoid the incompatibility with nbconvert's processing.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15635": {
        "explanation": "This issue is well-specified with clear examples of the problems and expected solutions. The reporter has identified three specific issues with string representation (`str`) and symbolic representation (`srepr`) of sets in SymPy:\n\n1. The `str(Interval)` representation produces invalid Python code or code that doesn't recreate the original object\n2. The `srepr(S.Integers)` uses a name that isn't imported from sympy by default\n3. The `str(Union)` produces invalid Python syntax with the \"U\" operator\n\nFor each issue, the reporter provides:\n- Exact code examples showing the problematic output\n- Clear expectations of what the correct output should be\n- The underlying principles that should guide the fix (e.g., \"str printer should always generate valid Python\")\n\nThe issue also mentions that a general audit of printing in the sets module would be worthwhile, suggesting there might be similar issues elsewhere, but the three specific examples provide enough concrete information to start working on a solution.\n\nA developer would be able to locate the relevant code in the sets module, understand the printing mechanisms, and implement fixes that follow the stated principles without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15678": {
        "explanation": "This issue is well-specified with clear examples of the problems and expected behavior:\n\n1. The first problem is that `idiff()` doesn't support equations using `Eq()` - it throws an IndexError when trying to solve the differential equation.\n\n2. The second problem is that `idiff()` doesn't support using function notation like `f(x)` as the dependent variable - it throws a ValueError saying it expects x-dependent symbols.\n\n3. A working example is provided showing the correct output when using a standard variable `y`.\n\nThe issue description includes specific error messages and test cases that demonstrate both problems. The statement that \"both should be easy to correct\" suggests that the fix is likely straightforward - probably involving modifying the function to handle these additional input formats.\n\nThe code path is even identified (sympy/geometry/util.py), making it clear where the changes need to be made. With this information, a developer could locate the `idiff` function, understand its current limitations, and modify it to handle both `Eq` objects and function notation.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15685": {
        "explanation": "This issue is well-specified with clear information about the problem and expected behavior:\n\n1. The issue clearly identifies a specific problem: dividing a voltage quantity by a current quantity yields \"ohm/1000\" instead of the expected \"ohm\" in the SI system.\n\n2. The issue provides a complete, reproducible code example that demonstrates the problem.\n\n3. The root cause is identified: the `scale_factor` property for ohm is 1000, which conflicts with the SI definition where 1 V / 1 A should equal 1 \u03a9.\n\n4. The issue explains that this is related to the \"kilogram workaround\" and that `scale_factor` should be consistent with the unit system definition.\n\n5. The proposed solution is clear: make `.scale_factor` private in the units module to hide this implementation detail from users.\n\nA developer would have enough information to understand the problem, locate the relevant code in the units module, and implement a solution that makes the `scale_factor` property private while ensuring the correct conversion behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15809": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The current behavior: `Min()` and `Max()` with no arguments raise a `ValueError` with the message \"The Max/Min functions must have arguments.\"\n\n2. The proposed change: Make `Min()` return positive infinity (`oo`) and `Max()` return negative infinity (`-oo`) when called with no arguments.\n\n3. The rationale: This aligns with mathematical conventions for empty sets in extended real numbers, with a reference to Wikipedia for justification.\n\nThe issue provides all the necessary information to implement the solution:\n- Which functions need to be modified (Min and Max)\n- What the current behavior is\n- What the new behavior should be\n- Why the change makes sense mathematically\n\nA developer could implement this change without needing additional clarification. The solution would involve modifying the Min and Max functions to handle the case of zero arguments by returning the appropriate infinity values instead of raising an error.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15875": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue states that:\n\n1. The `is_zero` method should return `None` if it cannot decide, but should never give the wrong answer\n2. There's a specific example where `is_zero` returns `False` for an expression, but after simplification, it returns `True` - indicating an inconsistency\n3. The issue is causing errors in determining the rank of a matrix\n4. The problem appears to be related to complex numbers within an `Add` expression\n\nThe issue includes a concrete code example that demonstrates the bug, making it easy to reproduce. It also references related issues (#15872 and #15873) which can provide additional context. The release notes section clearly indicates that this is a bug fix for the core functionality of `is_zero` when dealing with expressions containing `Add`.\n\nA developer would have enough information to:\n1. Reproduce the issue using the provided example\n2. Understand the expected behavior (is_zero should never give wrong answers)\n3. Focus on the specific area of the code (is_zero implementation for complex numbers in Add expressions)\n4. Test their solution against the provided example\n\nThe issue is therefore well-specified for a meaningful attempt at a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-15948": {
        "explanation": "This issue is reasonably well-specified. The user has provided:\n\n1. A clear description of the problem: substituting values into a matrix equation gives different results depending on whether evaluation happens before or after substitution\n2. Detailed debug code showing the issue\n3. Output showing the unexpected behavior\n4. A proposed fix in the PR description that identifies the root cause: the `Mul.flatten` function not returning the correct type for matrix expressions\n\nThe issue provides enough context to understand what's happening - there appears to be a bug in how SymPy handles matrix expressions during substitution operations. The PR description suggests that `Mul(-1, Mul(A, B))` is not preserving the matrix type correctly.\n\nWhile the issue doesn't provide the complete codebase context or the exact implementation of the fix, it gives enough information to understand the problem and the general approach to solving it. A developer familiar with SymPy's internals would likely be able to locate the `Mul.flatten` function and modify it to handle matrix expressions correctly.\n\nThe only missing piece is the exact implementation details of how to fix the `flatten` function, but this is a reasonable gap that could be filled by examining the codebase.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15970": {
        "explanation": "The issue description is quite brief but provides enough information to understand what needs to be done:\n\n1. The issue is about changing the LaTeX representation of lists, tuples, and dicts.\n2. It specifically requests replacing '\\quad' with '\\ ' in the LaTeX output.\n3. There's a reference to a Twitter discussion that provides context (though the link might be outdated or the discussion might not be fully accessible).\n\nWhile the description is minimal, it's clear what change is being requested - a specific LaTeX formatting change for certain data structures. For an experienced developer with access to the codebase, this would be enough to:\n1. Locate where LaTeX representations for these data structures are defined\n2. Replace '\\quad' with '\\ ' in those representations\n3. Ensure the change works as expected\n\nThe issue is focused on a specific technical change with a clear before and after state. The only potential ambiguity is whether there are any edge cases or considerations not mentioned, but the core task is well-defined.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15971": {
        "explanation": "This issue is reasonably well-specified. It clearly states the task: adding pretty printing functionality for the lerchphi function in the pretty.py file. The issue also references a larger issue (#6013) that it partially fixes, which provides context.\n\nWhat makes it not fully well-specified:\n1. It doesn't provide details about how the pretty printing should be implemented or what the expected output format should be.\n2. It doesn't explain what the lerchphi function is or its mathematical representation.\n3. It doesn't specify any test cases or examples of expected output.\n\nHowever, a developer familiar with SymPy's codebase would likely understand:\n- Where to add the code (pretty.py)\n- What functionality to implement (pretty printing for lerchphi)\n- The general pattern to follow (by looking at how other special functions are pretty printed in the same file)\n\nWith access to the full codebase, a developer could examine the existing pretty printing implementations for similar special functions and the definition of the lerchphi function to determine the appropriate implementation approach.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-15976": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a complete code example that reproduces the issue\n2. Shown the actual output with screenshots\n3. Demonstrated a working case (using 'x' instead of 'x2') for comparison\n4. Specified their environment details (OS, Sympy version, Python version)\n\nThe problem is clearly defined: when using a symbol that ends with a number (like 'x2') in Sympy's MathML presentation printer, the symbol becomes invisible in the rendered output. This is a specific bug with a clear expected behavior - the symbol should be visible just like when using 'x' without a number.\n\nThe issue contains all the necessary information to understand, reproduce, and potentially fix the problem. A developer would be able to investigate why the MathML printer is handling symbols with trailing numbers differently and implement a fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16003": {
        "explanation": "This issue is well-specified with clear requirements for improving the MathML presentation printing of multiple derivatives. The problem is clearly described with visual examples showing the current incorrect output and the desired output format. \n\nThe main requirements are explicitly stated:\n1. Print all derivative variables on a single line\n2. Get the correct power in the numerator\n3. Preferably place the actual function on a separate line\n4. If possible, implement logic to group adjacent identical terms\n\nThe issue also provides references to existing code and discussions (#15975) that can guide the implementation, and mentions that there are already `_print_derivative2` methods in PR #3926 that could be used or adapted.\n\nThe issue includes visual examples (images) that clearly show the current incorrect behavior and the desired output, making it easy to understand what needs to be fixed and what the expected result should look like.\n\nWith this information, a developer would have a clear understanding of what needs to be fixed and how to approach the solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16056": {
        "explanation": "This issue is well-specified and provides clear information about what needs to be done:\n\n1. The problem is clearly identified: there are two different LaTeX commands (`\\mathrm` and `\\text`) being used inconsistently throughout the codebase for printing text in LaTeX output.\n\n2. The pros and cons of each approach are explicitly listed, showing understanding of the tradeoffs.\n\n3. The scope is quantified: 123 instances of `\\mathrm` and 46 instances of `\\text` in .py files.\n\n4. A specific solution is proposed: create a helper function (suggested name `print_text`) that would make the choice configurable, rather than having to modify each instance individually.\n\n5. The desired outcome is clear: make the choice between `\\mathrm` and `\\text` configurable, with a standard default.\n\nThe issue provides enough context for a developer to understand the problem, the desired solution approach, and the expected outcome. While implementing this would require some design decisions (like where to put the configuration option, what the default should be), these are normal engineering decisions that can be made during implementation based on the clear requirements provided.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16088": {
        "explanation": "This issue is well-specified with clear examples of the current behavior and the desired behavior. The issue describes that the `simplify` method should pull out constant terms from `Integral` expressions, similar to how it already works for `Sum` expressions. \n\nThe description includes:\n1. A clear problem statement: `simplify` doesn't pull out constant terms from `Integral` expressions\n2. Examples showing the current behavior with `Sum` vs `Integral`\n3. Examples showing the desired behavior after the fix\n4. References to related issues (#15965, #7971)\n5. Specific information about what needs to be implemented\n\nThe implementation would involve modifying the `simplify` function to handle `Integral` expressions by factoring out terms that are independent of the integration variable. The examples provided give clear test cases for verifying the solution.\n\nThe only minor ambiguity is the mention of \"adding `doit` method in simplify\" which the author admits they don't have ideas about, but this doesn't affect the clarity of what needs to be implemented.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16221": {
        "explanation": "The issue description is quite vague and lacks important details needed for implementation. While it states that support for printing matrices and arrays in Wolfram Mathematica should be added, it doesn't provide:\n\n1. The current implementation details or code structure\n2. What specific printing functionality is missing (e.g., pretty printing, export to specific formats)\n3. Expected behavior or output format for matrices and arrays\n4. Any specific requirements or constraints for the implementation\n5. Information about the existing printer architecture\n6. Examples of what currently happens vs. what should happen\n\nWithout these details, a developer would need to make many assumptions about the requirements and implementation approach. They would likely need to explore the codebase extensively to understand the current printing system before being able to implement a solution. The issue lacks concrete acceptance criteria to determine when the task is successfully completed.",
        "label": "VAGUE"
    },
    "sympy__sympy-16281": {
        "explanation": "The issue is reasonably well-specified. It describes a concrete problem with the current pretty printing of the `Product` function in the library, with clear examples of the current output and specific points for improvement:\n\n1. Remove the empty line at the bottom of the product symbol\n2. Make the product symbol less wide/fat\n3. Consider extending the top bar for better aesthetics\n\nThe issue provides examples of the current output and even suggests an alternative visualization for comparison. While there is some subjectivity in what would constitute a \"better\" appearance (the author mentions being \"unsure\" about extending the top bar), the core problems are clearly identified.\n\nA developer could implement these changes without needing additional clarification, though they might need to make some aesthetic judgments about the exact dimensions and spacing. The issue is focused on improving the visual representation of a specific mathematical notation, which is a well-defined task even if there's room for some creative interpretation in the implementation.\n\nThe issue doesn't specify which file(s) need to be modified, but this could be determined by examining the codebase to find where the pretty printing for the `Product` function is implemented.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16331": {
        "explanation": "This issue is well-specified with a clear problem description and error message. The issue is that when using the `ufuncify` function with two arguments and the Cython backend, it fails with a specific error related to variable redeclaration. The error message shows that there's a name collision where `_y` is being redeclared in the generated Cython code:\n\n```\ncdef np.ndarray[np.double_t, ndim=1] _y = np.empty((_m))\n```\n\nThis line is trying to redefine `_y` which was already defined as a function parameter. The issue also provides important context that:\n1. It works with a single argument function\n2. It works with other backends\n3. It only fails with the specific combination of two arguments and the Cython backend\n\nThis gives a clear reproduction case (`ufuncify((x, y), x + y, backend='Cython')`) and enough information to understand the root cause - a variable naming conflict in the generated Cython code. The solution would involve fixing the code generation in the autowrap.py module to avoid reusing the same variable name for both the input parameter and the output array.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16334": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in SymPy's handling of expressions involving zero raised to a non-zero real power. The issue provides:\n\n1. A specific test case that demonstrates the problem\n2. The expected behavior: `(q**p).is_positive` should return `False` because 0 raised to any non-zero real power should be either 0 or complex infinity (`zoo`), neither of which is positive\n3. The current behavior: the expression doesn't return `False` as expected\n\nThe issue is about the mathematical property that 0 raised to any non-zero real power should not be positive, and this property is not correctly implemented in the current code. The fix would involve ensuring that when a symbol known to be zero is raised to a power that's known to be real and non-zero, the `is_positive` property correctly returns `False`.\n\nThe issue provides enough context and a clear test case that can be used to verify a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16437": {
        "explanation": "This issue is well-specified. It clearly identifies a bug where the `Float` function incorrectly handles string representations of floating-point numbers that contain underscores after the decimal point. The issue provides specific examples showing the correct behavior (when underscores are only before the decimal point) and multiple incorrect behaviors (when underscores appear after the decimal point). The decimal point is being misplaced based on the number of underscores after the decimal point.\n\nThe issue also points to a potential root cause - an upstream bug in the mpmath library - and provides a link to the relevant issue in that repository. This gives a clear direction for investigation and resolution.\n\nA successful solution would ensure that `Float()` correctly parses string representations of floating-point numbers with underscores after the decimal point, maintaining the correct decimal point position regardless of underscore placement.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16449": {
        "explanation": "The issue is reasonably well-specified. It clearly requests adding a function to calculate the Cumulative Distribution Function (CDF) for the Maxwell distribution, and provides a specific reference (Wikipedia) for implementation details. \n\nFrom the reference, I can see that the Maxwell distribution (also known as Maxwell-Boltzmann distribution) is a well-defined probability distribution in physics and statistics. The Wikipedia page contains the mathematical formulas needed to implement the CDF.\n\nWhat's missing from the issue description:\n1. No specific programming language or framework is mentioned, though this is likely clear from the repository context\n2. No details about function signature, parameter names, or return types\n3. No mention of any existing related functions that might need to be modified or extended\n4. No test requirements or edge cases to consider\n\nHowever, given the mathematical nature of the task and the reference provided, a competent developer familiar with the codebase should be able to implement this function without additional clarification. The Wikipedia page provides the necessary mathematical formulas for the CDF of the Maxwell distribution.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16450": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `posify` function where the `finite=True` assumption is not preserved when transforming a symbol. The issue provides:\n\n1. A clear reproduction case with code examples showing the behavior\n2. The expected behavior (preserving the finiteness assumption)\n3. A suggestion that other assumptions might need to be preserved as well\n\nThe problem is straightforward: when `posify` transforms a symbol, it's not carrying over the `finite` assumption from the original symbol to the transformed one. The solution would involve modifying the `posify` function to preserve this assumption and potentially other assumptions as mentioned.\n\nA developer could implement a fix by examining the `posify` function's implementation and ensuring it properly copies all relevant assumptions from the original symbol to the transformed one.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16474": {
        "explanation": "The issue is reasonably specified but has some areas that could benefit from more detail:\n\n1. It clearly identifies the need to add LaTeX, MathML, and pretty printers for the HadamardPower class.\n2. It also mentions extending HadamardProduct to support division symbol.\n3. The issue includes a checklist of specific tasks to be completed.\n\nHowever, there are some aspects that could be more detailed:\n- It doesn't specify the exact syntax or format expected for the LaTeX, MathML, and pretty printers.\n- It doesn't explain how the division symbol should be implemented in HadamardProduct.\n- There's no context about where these classes exist in the codebase or what they represent.\n\nDespite these gaps, an experienced developer familiar with the codebase would likely understand what needs to be done. The tasks involve implementing standard printing methods for mathematical expressions, which follows established patterns in mathematical libraries. The division symbol extension for HadamardProduct is a logical addition to complement existing operations.\n\nWith access to the codebase, a developer could examine existing implementations of similar printers and extend the functionality accordingly.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16503": {
        "explanation": "This issue is well-specified. It clearly identifies a problem with the alignment in the pretty printing of Sum expressions. The issue provides:\n\n1. A concrete example showing the current output\n2. A clear description of what's wrong: the `x` and the `+ 3` are not aligned\n3. The expected behavior: these elements should be aligned\n\nWhile the reporter mentions they're not sure whether the `x` should be lower or the `+ 3` should be higher, this is a reasonable design decision that can be made during implementation by examining the existing code and determining the most consistent approach with the rest of the codebase.\n\nThe issue provides enough information for a developer to reproduce the problem, understand what needs to be fixed, and implement a solution that aligns these elements properly in the pretty printer.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16527": {
        "explanation": "This issue is well-specified with clear information about what the problem is and what the expected behavior should be:\n\n1. The issue clearly identifies that `collect_const()` cannot collect rational numbers in SymPy 1.3.\n2. A specific example is provided with the current output and the expected output.\n3. The user shows that when trying to collect terms with the rational constant 1/2, the function doesn't properly group them.\n4. The expected result is clearly stated: `a + b + 1/2 * (c + d)` instead of the current output `a + b + (c/2 + d/2)`.\n5. The issue even references a potentially related issue (#13107), showing awareness of the context.\n\nThe problem is reproducible with the provided code snippet, and the expected behavior is unambiguous. A developer would have enough information to investigate and implement a fix for the `collect_const()` function to properly handle rational numbers.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16597": {
        "explanation": "This issue is reasonably well-specified. The user is pointing out that when a Symbol is created with the property `even=True`, the `is_finite` property is not automatically set to True, which seems counterintuitive since a number should logically be finite before it can be even.\n\nThe issue provides:\n1. A clear code example demonstrating the problem\n2. The current behavior (m.is_finite returns None)\n3. The expected behavior (m.is_finite should be True when even=True)\n\nThe underlying assumption is logical - evenness is a property that applies to integers, which are finite numbers. So if a symbol is marked as even, it should implicitly be finite.\n\nWhile the issue doesn't specify exactly how to fix this (should there be an automatic inference system? should the even=True parameter automatically set is_finite=True?), the core problem is clear enough that a developer familiar with the codebase could implement a reasonable solution. The solution would likely involve modifying how properties like \"even\" relate to other properties like \"finite\" in the Symbol class.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16601": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue shows that when a Parabola is declared symbolically in SymPy (using symbolic variables), attempting to access the `vertex` property results in a TypeError. The error occurs because the code tries to make a comparison (`x < self.focus.args[0]`) with symbolic values, which cannot be evaluated to a boolean.\n\nThe traceback provides the exact location of the error in the code:\n1. In the `vertex` property, it calls `self.p_parameter`\n2. In the `p_parameter` method, it attempts to compare `x < self.focus.args[0]`\n3. This comparison fails with symbolic values\n\nThe solution would need to modify the Parabola class to handle symbolic parameters properly, likely by avoiding direct boolean comparisons with symbolic values or by using SymPy's symbolic comparison capabilities instead. The issue provides all the necessary information to understand and address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16632": {
        "explanation": "This issue is well-specified with a clear problem statement. The user is experiencing an issue with SymPy's expression parsing where two mathematically equivalent expressions (`2**n * 3**n` and `2**n3**n` with implicit multiplication) are being parsed differently:\n1. The first expression is being simplified to `6**n`\n2. The second expression remains as `3**n*2**n`\n\nWhen subtracting these expressions, the result should be zero since they are mathematically equivalent, but SymPy is not recognizing this equivalence.\n\nThe user has already tried several approaches to resolve this issue:\n- Using simplify\n- Using powsimp with force=True\n- Using powdenest\n- Specifying variable properties (positive=True, integer=True)\n- Trying different transformations\n\nThe issue is clear, reproducible with the provided code, and has a well-defined expected outcome (getting zero when subtracting the expressions). A solution would involve finding the correct SymPy function or approach to make these expressions evaluate to the same form or to properly simplify their difference to zero.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16637": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue describes a specific bug: when finding the intersection between a Plane and a Segment3D, the function returns points that are outside the segment.\n\n2. A clear reproducible example is provided with code that demonstrates the issue.\n\n3. The expected behavior is explicitly stated: \"The expected result is of course that `Plane.intersections` returns an empty list in this case.\"\n\n4. A workaround is provided, showing the desired behavior.\n\n5. Version information is included.\n\n6. The issue even includes a proposed fix direction by mentioning that the intersection method in the Plane class needs to be updated to check if the intersection points lie within both objects.\n\nThe release notes at the end suggest this is actually a PR description that fixes the issue #15069, but the issue itself is very clear. A developer would have all the information needed to implement a solution without requiring further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16766": {
        "explanation": "This issue is well-specified with a clear problem statement and a proposed solution. The issue describes:\n\n1. The specific problem: PythonCodePrinter doesn't properly support the Indexed type in SymPy\n2. A reproducible test case showing the issue\n3. The current output that demonstrates the problem\n4. A specific proposed solution with the exact method implementation needed\n\nThe issue provides all the necessary information for a developer to understand the problem and implement a fix. The proposed solution is straightforward - adding a _print_Indexed method to the PythonCodePrinter class to properly handle Indexed objects. The implementation is provided and appears to be complete and correct, converting an Indexed object into the proper Python syntax for indexing.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16781": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: The dot diagram in the tutorial shows the wrong order for the expression x**2, displaying it as if it were 2**x.\n2. The expected behavior: The diagram should show Symbol('x') as the base and Integer(2) as the exponent.\n3. The location of the issue: Both in the tutorial documentation and in the dot printer implementation.\n4. The specific URL where the error can be found.\n\nThe issue provides enough context to understand what's wrong and what needs to be fixed. A developer would need to:\n1. Examine the dot printer implementation to understand why it's generating the incorrect order\n2. Fix the implementation to correctly represent the expression structure\n3. Update the tutorial documentation with the corrected diagram\n\nThere's no ambiguity about what constitutes a successful solution - the dot printer should generate diagrams that correctly represent the expression structure, with the base and exponent in the proper positions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16792": {
        "explanation": "This issue is well-specified with a clear problem description, reproducible example, and expected behavior. The user has:\n\n1. Provided a minimal code example that demonstrates the bug\n2. Shown the exact error message received\n3. Identified the root cause (incorrect C function signature generation)\n4. Demonstrated a working case to contrast with the failing case\n5. Explained why this is a legitimate use case (interfacing with external libraries)\n6. Even examined the generated C code to pinpoint where the problem occurs\n\nThe issue is with the Cython backend of SymPy's autowrap utility, which fails to properly handle array arguments that don't appear in the wrapped expression. The expected behavior is clear: the function should return the constant value regardless of the array input, but instead it's generating an incorrect C function signature that treats matrix symbols as scalars when they don't appear in the expression.\n\nThe user has already identified the problem area (in the `codegen` module) and intends to submit a PR. This gives a clear direction for solving the issue.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16840": {
        "explanation": "The issue description is quite minimal but still provides enough information to understand the problem. It states that `S(2)//S.Half` results in a ZeroDivisionError, while in Python the operation `2//.5` evaluates to 4. \n\nThis suggests that there's an inconsistency between how the floor division operator (`//`) works with the `S` class (presumably a symbolic math or numeric class in the codebase) compared to Python's native numeric types. Specifically, when using `S.Half` (which likely represents 0.5) as the divisor in a floor division operation, it's causing a ZeroDivisionError instead of returning the expected result of 4.\n\nWhile the description is brief, it clearly identifies:\n1. The specific operation that's failing: `S(2)//S.Half`\n2. The error being encountered: ZeroDivisionError\n3. The expected behavior by comparison to Python's native behavior: `2//.5 -> 4`\n\nThis is sufficient information for a developer familiar with the codebase to locate the issue in the implementation of floor division for the `S` class and fix it to handle division by `S.Half` correctly. The fix would likely involve ensuring that floor division with `S.Half` behaves consistently with Python's native floor division.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-16862": {
        "explanation": "This issue is well-specified with clear information about the bug and its location:\n\n1. The exact file and line number where the bug occurs is provided (sympy/sets/fancysets.py line 240)\n2. The specific problem is identified: using `args[0]` on an empty set causes an IndexError\n3. A test case that demonstrates the issue is referenced (test failing for (1, 0) but passing for (0, 0))\n4. The issue explains the root cause: when linsolve returns an empty set, accessing args[0] raises an IndexError\n\nThe issue also asks a relevant question about why the code is type-casting a set to a list, which might be part of the solution.\n\nWith this information, a developer could:\n1. Examine the referenced code\n2. Understand the bug's cause\n3. Implement a fix that handles empty sets properly\n4. Verify the fix using the mentioned test case\n\nThe issue provides enough context to understand and solve the problem without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16886": {
        "explanation": "This issue is very well-specified. It clearly identifies:\n\n1. The exact problem: The Morse code mapping for the number \"1\" is incorrect in the simpy.crypto.crypto module\n2. The current incorrect mapping: `\"----\": \"1\"`\n3. The expected correct mapping: `\".----\": \"1\"`\n\nThe solution would simply require locating the Morse code mapping in the simpy.crypto.crypto module and changing the encoding for \"1\" from four dashes to one dot followed by four dashes. This is a straightforward bug fix with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16943": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is reporting that the `fps()` function in SymPy doesn't always return a formal power series object, but sometimes returns other types (like `Pow` in this case) that don't support the expected interface (like indexing). \n\nThe issue includes:\n1. A specific code example that demonstrates the problem\n2. The actual error message received\n3. The expected behavior (that fps should return an object that supports indexing)\n4. A reference to the documentation that doesn't mention this behavior\n\nThe issue is pointing out an inconsistency in the API where `fps()` sometimes returns a formal power series object and sometimes returns a different type of object that doesn't support the same interface. This is clearly a bug that needs fixing to ensure consistent behavior.\n\nA successful solution would ensure that `fps()` always returns a formal power series object with a consistent interface, regardless of whether the input is already a polynomial or not.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16963": {
        "explanation": "This issue is well-specified with a clear problem statement and reproducible example. The issue describes that `sympify()` does not convert Python dictionaries to SymPy's `Dict` class, which causes hashing problems. The example code clearly demonstrates the issue:\n\n1. A Python dictionary `d = {1:1}` remains a Python `dict` after calling `sympify(d)`\n2. But using `Dict(d)` directly creates a `sympy.core.containers.Dict` object\n\nThe issue also references a specific line in a pull request that provides additional context about the problem. The user is asking why dictionaries aren't converted to SymPy's `Dict` class during sympification and requesting this behavior to be implemented.\n\nThe second part of the issue description appears to be an unrelated issue about the `N()` function and decimal precision, which seems to have been accidentally included. However, this doesn't detract from the clarity of the main issue about dictionary conversion.\n\nA developer would have a clear understanding of what needs to be fixed: modify the `sympify()` function to convert Python dictionaries to SymPy's `Dict` class, similar to how other Python types are converted to their SymPy equivalents.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-16988": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `Intersection` class where duplicate sets in the arguments are not being handled correctly. The issue provides:\n\n1. A concrete example showing the current incorrect behavior: `Intersection({1},{1},{x})` returns `EmptySet()` when it should not.\n2. A comparison example showing expected behavior for a similar case: `Intersection({1},{x})` returns `{1}`.\n3. The expected output for the problematic case: `Piecewise(({1}, Eq(x, 1)), (S.EmptySet, True))` or leaving it unevaluated.\n4. A suggested solution approach: remove duplicates at instantiation time and order them for canonical processing.\n\nThe issue is describing a mathematical set operation where the intersection of sets {1}, {1}, and {x} should be handled differently than it currently is. The duplicate {1} sets should be treated as a single instance, and the intersection with {x} should yield a conditional result based on whether x equals 1. This is a clear, specific problem with a well-defined expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17010": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue identifies a specific bug: infinite recursion in `could_extract_minus_sign()` usage in polynomial functions.\n2. It provides a concrete example in the `chebyshevu` function where this occurs.\n3. It explains the exact mechanism causing the infinite recursion: when both `n.could_extract_minus_sign()` and `(-n - 2).could_extract_minus_sign()` are True.\n4. It references a specific file (sympy/functions/special/polynomials.py) and includes a code snippet.\n5. It mentions a related issue that was fixed (#13102) and a PR where this issue was discovered (#13059).\n6. It suggests a direction for the solution: \"remove the minus sign from the highest order term.\"\n\nThe issue provides enough context and specificity to understand the problem and begin working on a solution. The only minor ambiguity is in the exact implementation of the solution (how to \"remove the minus sign from the highest order term\"), but this is a reasonable technical challenge for someone familiar with the codebase to solve.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17038": {
        "explanation": "This issue is very well-specified with clear examples of the bug, detailed analysis of the root cause, and even suggestions for a solution. The description provides:\n\n1. Clear reproduction steps with specific inputs that demonstrate the bug\n2. Expected vs actual outputs\n3. Version information (Python 3.7.3, SymPy 1.4, macOS 10.14.5)\n4. A link to the problematic code in the codebase\n5. Detailed technical analysis of why the bug occurs (the bound used for determining when to use math.sqrt is too large)\n6. Mathematical explanation of the limitations of floating-point calculations\n7. Suggestions for potential fixes\n\nThe issue is about the `isqrt` function in SymPy returning incorrect integer square root values for certain large numbers due to floating-point precision limitations. The problem occurs because the current implementation uses `math.sqrt` for values below a certain threshold, but that threshold is too high given the precision limitations of IEEE 754 binary64 floating-point numbers.\n\nA developer would have all the information needed to implement a fix, which would likely involve either:\n1. Reducing the threshold for when to use the floating-point math.sqrt approach\n2. Adding a verification step to check if the result is correct before returning it\n3. Using the int(_sqrt(n + 0.5)) approach with appropriate bounds\n\nThis is a classic numerical computing issue with a well-defined scope and clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17067": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue provides a specific mathematical expression that produces incorrect results when simplified using SymPy's `simplify()` function.\n\n2. The expected result is clearly stated: `-cos(alpha)*cot(beta)`.\n\n3. The actual incorrect result is provided: `-2*cos(alpha)*cos(beta)/sin(2*beta)`.\n\n4. The issue includes verification methods - the result was verified using Mathematica and by direct calculation using trigonometric identities.\n\n5. A complete minimal working example (MWE) is provided with the exact code to reproduce the issue.\n\n6. The issue even includes a mathematical explanation of why the expected result is correct, showing the steps that should be taken to simplify the expression.\n\n7. The version information is provided (SymPy 1.4 and previously 1.3).\n\nThis is a clear bug report with all the necessary information to understand the problem and attempt a solution. A developer would be able to reproduce the issue, verify the expected behavior, and work on fixing the simplification algorithm for trigonometric expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17115": {
        "explanation": "This issue is reasonably well-specified with clear information about the problem:\n\n1. The issue provides a specific code example that reproduces the error\n2. It includes the full traceback showing exactly where the error occurs\n3. The reporter provides a screenshot confirming the issue exists in SymPy Live as well\n4. There's a hypothesis about the cause: the error might be in `c = c.as_set().as_relational(x)` in `Piecewise.eval`\n5. The reporter suggests a potential fix direction (allowing Logic to pass through a specific loop)\n6. There's context from other developers who couldn't reproduce the issue\n\nThe only missing elements are:\n- The exact version of SymPy being used\n- A clearer explanation of what the expected behavior should be\n\nHowever, these gaps don't prevent understanding the core issue: the Piecewise function is failing when given a condition that includes the `S.Integers.contains(x)` constraint along with other conditions. The error occurs because something in the condition handling can't be properly converted to a Set.\n\nA developer with access to the codebase should be able to reproduce this issue and work on a solution based on the information provided.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17139": {
        "explanation": "This issue is well-specified with a clear error case and stack trace. The problem occurs when trying to simplify the expression `cos(x)**I` in SymPy, which results in a TypeError: \"Invalid comparison of complex I\". The error happens in the fu.py file during the simplification process when it attempts to compare a complex exponent.\n\nThe stack trace provides detailed information about where the error occurs:\n1. The error happens in the `__lt__` method in expr.py when comparing a complex number\n2. This is triggered by the condition `(rv.exp < 0) == True` in fu.py\n3. The issue occurs specifically when dealing with a complex exponent (I)\n\nThis is a clear bug report with:\n- A reproducible example\n- The exact error message\n- A complete stack trace showing the execution path\n- The specific problematic expression (cos(x)**I)\n\nA developer would have enough information to locate the issue in the code and understand that the simplification algorithm needs to be modified to properly handle complex exponents.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17150": {
        "explanation": "This issue is well-specified with clear details:\n\n1. The problem is precisely defined: `log(Rational(408,499),2)` incorrectly produces `zoo` instead of the expected `log(Rational(51,499))/log(2) + 3`.\n\n2. The location of the issue is identified: line 531 in `sympy/functions/elementary/exponential.py` in the code that extracts base powers.\n\n3. The specific bug is explained: `arg // den` is evaluating to `0` when it should be `Rational(51,499)`.\n\n4. Two potential solutions are proposed:\n   - Remove the conditional and keep only the else branch\n   - Modify the conditional to check if both arg and den are integers\n\nThe issue includes the relevant code snippet and explains the mathematical reasoning behind the expected result. The submitter even acknowledges that the issue might be more complex since the code hasn't been changed recently.\n\nWith this information, a developer could reproduce the issue, understand the problem, and implement one of the suggested fixes or investigate further if needed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17176": {
        "explanation": "This issue is well-specified with clear examples demonstrating two related problems:\n\n1. The `bool_map` function incorrectly maps `Xor(A1,A2,A3)` to its negation `~(Xor(A1,A2,A3))`, showing they are equivalent when they should not be.\n\n2. The `_finger` fingerprint routine in the Boolean algebra module produces identical fingerprints for expressions that should be different, specifically for XOR and XNOR (negated XOR) with 3 and 4 inputs.\n\nThe issue provides concrete code examples that reproduce both problems, shows the incorrect output, and references a related issue (#15171). The reporter has also verified the problem exists for 4 inputs.\n\nA successful solution would need to:\n1. Fix the `bool_map` function to correctly distinguish between XOR and XNOR operations\n2. Fix the `_finger` fingerprint routine to generate distinct fingerprints for different boolean expressions\n\nThe issue is specific enough to understand the problem and has clear test cases that can be used to verify a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17194": {
        "explanation": "This issue is well-specified. The user has clearly identified three specific bugs in the MathML printing functionality of SymPy. For each bug, they have:\n\n1. Provided the exact function call that produces the error (`sympy.printing.mathml(sympy.acoth(x))`, etc.)\n2. Shown the current incorrect output (e.g., `'<apply><acoth/><ci>x</ci></apply>'`)\n3. Specified the expected correct output (e.g., `'<apply><arccoth/><ci>x</ci></apply>'`)\n\nThe pattern is consistent across all three examples - the issue is that the inverse trigonometric/hyperbolic functions are being printed with abbreviated prefixes (a-) instead of the full prefix (arc-) in MathML. This is a straightforward bug fix that would involve modifying the MathML printer in SymPy to use the correct element names for these functions.\n\nThe solution would involve locating the MathML printer implementation in the SymPy codebase and updating the relevant function name mappings for acoth, acsc, and asec to use the proper \"arc\" prefix versions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17223": {
        "explanation": "This issue is well-specified with a clear problem description and a concrete example demonstrating the bug. The issue explains that the `match` function in SymPy is not respecting the non-commutativity of matrix multiplication when matching expressions.\n\nThe example clearly shows:\n1. The setup with matrix symbols and wild symbols\n2. Two expressions `e1` and `e2` where the matrix multiplication order is different\n3. The current incorrect behavior: `e1.match(e2)` returns `{w_: a*b}` \n4. The expected behavior: `e1.match(e2)` should return `None` since matrix multiplication is non-commutative\n\nThe issue provides all the necessary context to understand the problem and what a correct solution would look like. A developer would need to modify the `match` function to properly account for the non-commutativity of matrix multiplication when comparing expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17239": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue provides a complete, reproducible code example showing how different code printers handle the `Eq` relation.\n2. It clearly demonstrates the expected vs. actual output for multiple language printers.\n3. It identifies which printers are working correctly (C and Fortran) and which ones are not (glsl, javascript, julia, mathematica, octave, rust).\n4. It provides a specific diagnosis of the problem: the non-working printers are \"false-positively looking up for `StrPrinter._print_Relational`\" instead of properly implementing their own handling.\n5. The solution direction is implied - the non-working printers need to properly override `_print_Relational` like the working C and Fortran printers do.\n\nA developer could take this information and directly investigate the code for these printers, compare the working implementations with the non-working ones, and implement the necessary fixes without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17271": {
        "explanation": "This issue is reasonably well-specified. The problem is clearly identified: calling `frac(zoo)` results in a TypeError due to an operation between NoneType and Zero. The issue includes:\n\n1. A clear reproduction case with the exact error message\n2. The specific code location that's causing the problem (lines 311-312 in integers.py)\n3. Context about what the user is trying to do (cover those lines in a test)\n\nWhat's missing is a clear specification of what the expected behavior should be. The user states \"Not sure what should happen, but not this\" and \"I do not really want an exception,\" but doesn't specify what the correct return value should be for `frac(zoo)`.\n\nHowever, since this is a mathematical function dealing with the complex infinity symbol (zoo), a developer familiar with SymPy's mathematical conventions could determine the appropriate behavior by:\n1. Examining how `frac()` handles other special values\n2. Understanding the mathematical definition of the fractional part when applied to complex infinity\n3. Looking at how similar functions in the codebase handle zoo\n\nThe issue provides enough information to investigate and propose a sensible solution, even though the exact expected behavior isn't explicitly stated.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17273": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific problem: The value of hbar (Planck's reduced constant) in sympy.physics.units is incorrect\n2. The expected correct value: 1.054571817e-34 J s\n3. The source of the correct value: CODATA recommended value with a link to the official reference\n\nA developer would be able to:\n1. Locate the hbar constant in the sympy.physics.units module\n2. Verify its current value\n3. Update it to match the CODATA recommended value\n4. Add appropriate documentation referencing the source\n\nThere's no ambiguity about what needs to be fixed or what the correct value should be.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17288": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The user has:\n\n1. Clearly identified the problem: derivatives of second-or-more order with superscripted symbols produce incorrect LaTeX output\n2. Provided a specific code example that demonstrates the issue\n3. Explained what's wrong with the current output (`\\frac{d^{2}}{d x^{*}^{2}} x^{*}`)\n4. Identified the specific LaTeX error (it should be `x^{*2}` instead of `x^{*}^{2}`)\n5. Specified the environment where the issue occurs (IPython)\n\nThe issue is related to LaTeX formatting in SymPy's derivative printing, specifically when handling superscripted symbols in higher-order derivatives. A developer would have enough information to reproduce the issue and understand what needs to be fixed in the LaTeX printer for derivatives.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17313": {
        "explanation": "The issue description is vague and lacks critical context. It appears to be discussing mathematical ceiling and floor functions and their behavior with positive and negative numbers, but it doesn't provide:\n\n1. The programming language or framework context\n2. Where these functions are implemented or used\n3. What specific code is failing\n4. What the current behavior is versus the expected behavior\n5. No code samples or test cases\n6. No file references or locations in the codebase\n\nWithout knowing which codebase this refers to, what the current implementation looks like, or what specific bug is being observed, it would be impossible to create a meaningful PR. The description is just stating mathematical properties that should be true but doesn't explain how they relate to the actual code that needs fixing.",
        "label": "VAGUE"
    },
    "sympy__sympy-17318": {
        "explanation": "This issue is well-specified with clear information about the problem and expected solution:\n\n1. The problem is clearly identified: The `sqrtdenest` function raises an IndexError when processing certain expressions with complex numbers.\n\n2. A specific reproducible example is provided with the exact expression that causes the error: `sqrtdenest((3 - sqrt(2)*sqrt(4 + 3*I) + 3*I)/2)`\n\n3. The full error traceback is included, showing exactly where the error occurs (in `_split_gcd` function when trying to access `a[0]` with an empty tuple).\n\n4. The expected behavior is clearly stated: \"If an expression cannot be denested it should be returned unchanged.\"\n\n5. The issue even includes the expected output after the fix, showing that the original expression should be returned unchanged when it can't be denested.\n\n6. The issue references another issue (#12420) which likely provides additional context.\n\nWith this information, a developer could locate the problem in the code (specifically in the `_split_gcd` function in `radsimp.py`), understand why it's happening (likely the `surds` parameter is empty), and implement a fix to handle this edge case by returning the original expression when denesting isn't possible.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17340": {
        "explanation": "The issue description is quite vague and lacks important context. It states that \"Permutation args are now subtypes of Basic\" and that the submitter changed a \"list\" subtype in Permutation's args to \"Tuple\", but it doesn't provide:\n\n1. The purpose or motivation for this change\n2. Any specific problems that need to be solved\n3. The expected behavior or outcome\n4. The context of what \"Permutation\" and \"Basic\" refer to in the codebase\n5. Whether this is reporting a bug, requesting a feature, or just informing about a change already made\n\nWithout this information, it's unclear what action is being requested. Is this a notification of a change already made? A request to review and merge a change? A report of an issue with the change? The description doesn't specify what needs to be done or what problem needs to be solved.\n\nThe description is too brief and lacks the necessary details for someone to understand what work is required without significant additional context about the codebase and the specific components mentioned.",
        "label": "VAGUE"
    },
    "sympy__sympy-17394": {
        "explanation": "This issue is well-specified with clear requirements for implementation:\n\n1. The issue identifies a specific problem: when using lambdify with 'scipy' modules, the SymPy functions lowergamma and uppergamma are not correctly translated to their SciPy equivalents (gammainc and gammaincc).\n\n2. It provides concrete examples demonstrating the current incorrect behavior.\n\n3. It specifies that SciPy implements the *regularized* incomplete gamma function, which means some scaling will be needed for proper implementation (and references issue #16533 for context).\n\n4. It includes additional requirements: lambdify with 'numpy' should fail appropriately for these functions.\n\n5. It references related issues (#16535, #15134) that provide additional context.\n\nThe issue provides all the necessary information to understand what's wrong and what needs to be fixed. A developer would need to:\n1. Modify the lambdify translation for 'scipy' modules to map lowergamma/uppergamma to the appropriate SciPy functions\n2. Handle the scaling required since SciPy implements regularized versions\n3. Ensure proper error handling for unsupported modules\n\nThis is a well-defined task with clear acceptance criteria.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17512": {
        "explanation": "This issue is well-specified with clear problems and expected solutions:\n\n1. The issue identifies a specific inconsistency: `Range(range(10))` works but creates problems because:\n   - It allows Python's `range` objects to be converted to SymPy's `Range` objects\n   - This creates inconsistencies when trying to use `Range(Range(10))` which fails\n   - The resulting object would have nonsensical attributes (`.start` would be `range(10)`)\n\n2. The reporter provides clear examples demonstrating the issue, including code snippets with outputs and error messages.\n\n3. The expected solution is clearly implied:\n   - SymPy objects should not act as converters beyond sympification\n   - `Range(range(10))` should not be allowed\n   - Instead, users should use automatic sympification or explicitly call `sympify(range(10))`\n\n4. The issue also identifies a related problem with `ImageSet` not sympifying its arguments, with a clear example and expected behavior.\n\nThe issue provides enough context, examples, and reasoning to understand what needs to be fixed and why. A developer could implement a solution without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17630": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a complete code example that reproduces the error\n2. Shown the exact error traceback\n3. Identified the specific problem: when multiplying a BlockMatrix containing ZeroMatrix blocks twice, it fails because the first multiplication converts ZeroMatrix objects to Zero objects, which lack the 'cols' attribute needed for subsequent multiplications\n4. Included version information (Python 3.7.4 and SymPy 1.4)\n\nThe issue is clearly a bug in SymPy's handling of BlockMatrix multiplication when ZeroMatrix blocks are involved. The solution would need to ensure that Zero objects resulting from matrix operations maintain the necessary matrix attributes or are properly converted to ZeroMatrix objects. The problem occurs specifically in the colblocksizes method when it tries to access the 'cols' attribute of what has become a Zero object rather than a ZeroMatrix object.\n\nA developer with access to the codebase would be able to understand and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17653": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue describes a specific bug: `IndexedBase.__new__` is converting subclasses of Symbol back to Symbol, causing loss of type information.\n\n2. The issue provides complete reproduction steps:\n   - Clone a specific repository\n   - Install dependencies\n   - Run tests with current SymPy master (fails)\n   - Run tests with SymPy 1.4 (works)\n\n3. The issue identifies the exact location of the problem in the code:\n   - The file path is provided (sympy/tensor/indexed.py)\n   - The specific line causing the issue is identified (the commented out `label = Symbol(label.name)` line)\n   - The issue explains that when this line is commented out, everything works fine\n\n4. The issue explains why this is a problem: it causes their `TypedSymbol` (a subclass of Symbol) to lose its type information.\n\n5. The issue includes a clear question about whether there's a reason for the problematic line.\n\nA developer would have all the information needed to understand the issue, reproduce it, and implement a fix (likely by removing or modifying the line that's causing the problem).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17655": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible steps. The user has:\n\n1. Provided a complete code example that demonstrates the issue\n2. Shown the exact error message received\n3. Clearly stated the expected behavior (both expressions should give the same result)\n4. Identified the specific inconsistency: `point1 + point2 * sympy.sympify(2.0)` works, but `point1 + sympy.sympify(2.0) * point2` fails\n\nThe issue appears to be related to the order of operations when multiplying a SymPy Point object with a number. When the Point is on the left side of the multiplication, it works correctly, but when it's on the right side, it raises a GeometryError.\n\nThis is a clear bug in the SymPy library's handling of Point multiplication, and the solution would involve modifying the relevant code in the Point class to handle commutative multiplication properly. A developer would have enough information to locate the issue in the codebase and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17696": {
        "explanation": "This issue is well-specified with clear information about what's wrong and what needs to be fixed:\n\n1. The problem is clearly identified: the `refine` function in SymPy doesn't properly simplify `sign(x)` expressions when given certain assumptions.\n2. Specific examples are provided showing the current behavior (all expressions remain as `sign(x)`) and the expected behavior (at least `expr3` and `expr4` should be simplified to `1`).\n3. A solution direction is suggested: implement a fix similar to PR #17019.\n4. Related issues are referenced (#8326 and #17052) which can provide additional context.\n\nThe task is to enhance the `refine` function to properly handle `sign(x)` expressions when given assumptions about `x`. The expected behavior is clearly defined, and there's even a reference to a similar PR that can guide the implementation. A developer with knowledge of SymPy would have enough information to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17770": {
        "explanation": "The issue description is somewhat vague and lacks specific details about what exactly needs to be corrected in the `_eval_is_real` and `_eval_is_finite` functions for subclasses of `HyperbolicFunction`. \n\nWhile it mentions that these functions are \"still sort of incomplete\" and that they can be used to define properties like `is_real` and `is_finite` for `cosh`, `sinh`, and `tanh`, it doesn't specify:\n\n1. What is currently wrong or incorrect with these functions\n2. What specific improvements are needed\n3. What the expected behavior should be\n4. Any test cases or examples demonstrating the issue\n\nThe description reads more like a comment on a PR that has already been submitted rather than a clear issue description. It acknowledges incompleteness but doesn't provide clear direction on what needs to be fixed or how to determine if a solution is successful.\n\nWithout more specific information about the current implementation issues and expected behavior, it would be difficult to create a targeted solution.",
        "label": "VAGUE"
    },
    "sympy__sympy-17809": {
        "explanation": "This issue is well-specified with clear requirements and examples. The problem is that `cosh(x).is_positive` should return `True` when x is real, and `abs(cosh(x))` should simplify to `cosh(x)` for real x. The issue includes:\n\n1. A clear description of the expected behavior\n2. Concrete examples showing the current behavior vs. expected behavior\n3. A reference to a related issue (#11721)\n4. Specific test cases that demonstrate the problem\n5. Clear indication of what's working and what's not working\n\nThe solution would involve:\n1. Modifying the `is_positive` method for the `cosh` function to return `True` when the argument is real\n2. Updating the `Abs` function to return the function itself when the function is known to be positive\n\nThe examples provided give sufficient context to understand both the problem and the expected solution. The issue also shows the current behavior in SymPy 1.0 that needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-17821": {
        "explanation": "This issue is reasonably specified with two main tasks:\n\n1. Implement `S.Catalan.rewrite(Sum)` - This is a clear, specific task to add functionality to rewrite the Catalan constant in terms of a sum.\n\n2. Demonstrate using doctest-generated LaTeX equations instead of manually typing them - This is more of a proof-of-concept request to show how SymPy could generate LaTeX equations automatically for documentation.\n\nThe issue provides enough context to understand what needs to be done, though there are some details that would need to be filled in:\n- The exact format for the doctest-generated LaTeX is not fully specified\n- The extent of the demonstration (how many examples to include) is not specified\n- The optimization suggestions for the LaTeX printer are mentioned but not required for this PR\n\nDespite these minor gaps, there is a clear path forward for implementation. The first task is very specific, and the second task is asking for a demonstration of a concept with some flexibility in implementation details.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-17845": {
        "explanation": "The issue is reasonably specified, though it could benefit from more clarity on the desired solution. \n\nThe problem is clearly stated: the string representations of `Interval` and `FiniteSet` objects don't follow the principle that `str(object)` should produce valid code to recreate the object. Currently:\n- `str(Interval(0,1))` produces `\"[0, 1]\"` (not valid Python to recreate the object)\n- `str(FiniteSet(1,2,3))` produces `\"{1, 2, 3}\"` (not valid Python to recreate the object)\n\nThe issue acknowledges a challenge: changing the string representation to follow this principle would make the output less readable (e.g., `\"Interval(0, 1, True, False)\"`).\n\nWhat's missing is a clear directive on what approach to take. Should we:\n1. Change `str()` to produce valid Python code and accept the less readable output?\n2. Keep `str()` as is and modify something else?\n3. Create a different method for getting valid Python code?\n\nDespite this ambiguity, a developer could make a reasonable attempt at a solution by:\n1. Understanding the codebase's conventions\n2. Examining how similar objects handle this issue\n3. Implementing a solution that balances readability with the principle mentioned\n\nThe issue provides enough context to understand the problem and explore potential solutions.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18033": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. The task is to remove the `Permutation.print_cyclic` flag from the Permutation object.\n2. The printing behavior should be moved to the SymPy printers instead of being on the object itself.\n3. The flag needs to be deprecated following the documented deprecation policy.\n4. There's a specific requirement for the `str` printer to print a Python-valid representation, while pretty printers (pprint/latex) should print the cyclic notation like \"(1 2 3)\".\n5. A reference to a previous discussion is provided for additional context.\n\nThe issue provides clear guidance on what needs to be changed, why it needs to be changed, and how to handle backward compatibility through deprecation. It also specifies the expected behavior for different printer types. A developer would have enough information to implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18057": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The problem is that SymPy's equality comparison (`__eq__` method) is attempting to evaluate string representations of objects using `eval()`, which is a security risk and causes unexpected behavior.\n\nThe issue provides:\n1. Clear reproduction code showing the problematic behavior\n2. The exact error message and stack trace\n3. The specific line in the code causing the problem\n4. A related issue showing another manifestation of the same problem\n\nThe solution would need to modify SymPy's equality comparison logic to avoid using `eval()` on string representations of unknown objects. This is a well-defined security and functionality issue with a clear path to resolution - stop using `eval()` in the equality comparison and implement a safer alternative.\n\nThe issue is critical (marked as \"should be fixed ASAP\") because using `eval()` on arbitrary string representations is a known security vulnerability that could potentially allow code execution if exploited.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18062": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `imageset` function when dealing with complex numbers. The issue provides a specific example showing that when using `imageset` with a Lambda function that returns a complex expression (1 + I*n), the real part (1) is incorrectly dropped from the result, leaving only the imaginary part (I*n). The expected behavior would be to preserve the entire expression (1 + I*n) in the output set.\n\nThe issue includes:\n1. A clear reproduction case with code\n2. The actual output that demonstrates the bug\n3. An implicit expectation of what the correct behavior should be (preserving the full expression)\n\nWith this information, a developer can reproduce the issue, understand what's wrong, and work on fixing the `imageset` function to properly handle complex number expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18087": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific function that has a bug: `trigsimp`\n2. The exact expression that's incorrectly simplified: `cos(x)+sqrt(sin(x)**2)`\n3. The incorrect result: it's being simplified as if it were `cos(x)+sin(x)`\n4. The scope of the issue: happens for general complex x, but works correctly for real x\n5. The affected versions: \"various versions, including 1.5\"\n\nThe issue describes a mathematical error in the trigonometric simplification function where it's not correctly handling the square root of a squared sine function in the complex domain. The square root of sin(x)\u00b2 should be |sin(x)| (the absolute value), not sin(x), when x is complex.\n\nA developer would have a clear understanding of what needs to be fixed: the `trigsimp` function needs to be modified to correctly handle the expression `cos(x)+sqrt(sin(x)**2)` for complex values of x, ensuring it doesn't simplify it as if it were `cos(x)+sin(x)`.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18109": {
        "explanation": "The issue is well-specified with a clear problem statement and a reproducible code example. The user is asking why the pretty printing of the expression \"oo - oo\" (infinity minus infinity) results in \"nan\" when evaluation is disabled using the `evaluate(False)` context manager.\n\nThe issue includes:\n1. A clear question about specific behavior\n2. A complete code snippet that reproduces the issue\n3. The expected vs. actual behavior is implied (the user expects \"oo - oo\" to be printed literally when evaluation is disabled, but instead gets \"nan\")\n\nThis is a focused question about the behavior of SymPy's pretty printing functionality when combined with the evaluate(False) context manager. A developer familiar with SymPy would understand what needs to be investigated and fixed without needing additional information.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18116": {
        "explanation": "The issue description is somewhat vague and lacks specific details about what needs to be changed. Here's what I can understand:\n\n1. There appears to be a conceptual issue where \"Relational\" objects are being treated as \"Boolean\" objects in the SymPy codebase.\n2. There are \"a few places\" where \"Expr methods\" are called on Relational objects, which seems to be problematic.\n3. There's a reference to an original issue (#4986 and a Google Code issue #1887), which might contain more details.\n\nHowever, the description lacks:\n- Specific examples of the problematic code\n- Clear explanation of what the desired behavior should be\n- Details about which files or components need to be modified\n- Any error cases or unexpected behaviors that are occurring\n\nWithout access to the referenced original issue or the codebase, it would be difficult to understand exactly what needs to be fixed. A developer would likely need to research the original issue, understand the relationship between Boolean and Relational classes in the SymPy codebase, and identify the specific places where Expr methods are being incorrectly called on Relational objects.",
        "label": "VAGUE"
    },
    "sympy__sympy-18130": {
        "explanation": "This issue is well-specified with a clear bug demonstration. The problem is that when creating an ImageSet of n\u00b2-1 where n is from the integers, and then intersecting it with integers, the result is incorrectly an empty set. This is obviously wrong since n\u00b2-1 will always be an integer when n is an integer, so the intersection should be the entire ImageSet.\n\nThe issue provides a specific code example that reproduces the bug, showing the exact input and the incorrect output. The expected behavior is implied but clear: the intersection of ImageSet(Lambda(n, n**2 - 1), S.Integers) with S.Integers should return the ImageSet itself, not an empty set.\n\nThis is a specific bug in the diophantine equation solver or in the set intersection logic of SymPy that can be investigated and fixed with the information provided.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18168": {
        "explanation": "The issue is reasonably well-specified. It points out a specific problem with the `is_closed` and `is_open` properties of the `S.Rationals` class in the SymPy library. The issue includes:\n\n1. A clear description of the current behavior (returning `True, None`)\n2. A specific code example demonstrating the issue\n3. A suggestion for what the correct behavior should be (`is_open=False` and `is_closed=False`) based on mathematical principles (since the rational numbers Q are neither open nor closed in the real numbers R)\n4. An alternative suggestion (raising a `NotImplementedError`)\n\nThe issue is asking for a determination on whether the current implementation is valid and implicitly requesting a fix. The mathematical context is clear - in topology, the set of rational numbers is neither open nor closed in the real number line, which is a well-established mathematical fact.\n\nThe only minor ambiguity is that the issue repeats itself, but this doesn't affect understanding the problem. A developer familiar with SymPy and basic topology would have enough information to implement a solution without needing further clarification.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18189": {
        "explanation": "This issue is well-specified. It clearly demonstrates a bug in the `diophantine` function where the results depend on the order of symbols provided in the `syms` parameter when `permute=True` is set. \n\nThe issue provides:\n1. Two concrete examples showing different outputs for the same equation\n2. The exact function call with all parameters\n3. The expected behavior is implied - when `permute=True`, the order of symbols should not affect the completeness of results\n\nThe problem is reproducible with the given code snippets. The first call with `syms=(m,n)` returns 8 solutions, while the second call with `syms=(n,m)` only returns 1 solution. Since both calls use `permute=True`, they should return the same set of solutions regardless of symbol order.\n\nA developer can clearly understand what needs to be fixed: ensure the `diophantine` function produces consistent and complete results regardless of the order of symbols when `permute=True` is specified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18191": {
        "explanation": "This issue is well-specified with clear information about the problem and a potential solution. The issue describes:\n\n1. A specific error case: `sqrt(1 / tan(1 + I))` causing a RecursionError\n2. A complete error traceback showing the recursion path\n3. The exact location in the code where the problem occurs (power.py line 373-374)\n4. A suggestion that an additional check is needed at that specific location\n\nThe error occurs in the `_eval_power` method where there appears to be an infinite recursion when computing powers of complex expressions. The issue points to the specific lines where a check needs to be added to prevent this recursion.\n\nWith this information, a developer could:\n1. Reproduce the exact error\n2. Understand the recursion path\n3. Examine the specific code location\n4. Implement an appropriate check to prevent the infinite recursion\n\nThe issue provides all the necessary context to understand and fix the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18198": {
        "explanation": "This issue is reasonably well-specified. The author is suggesting two main changes to the codebase:\n\n1. Rename the `core.evaluate` module to `core.parameters` to better reflect its purpose of handling global parameters (not just `evaluate` but also `distribute` and potentially future parameters).\n\n2. Create a dict-like handler called `global_parameters` to manage all global parameters with specific functionality:\n   - Access parameters via dictionary syntax\n   - Access/set values via properties\n   - Use parameters as context managers\n\nThe author has even provided clear examples of the desired API and mentioned they've already implemented parts 1 and 2 of their suggestion.\n\nWhat makes this reasonably specified rather than well-specified:\n- The implementation details of the context manager functionality (point 3) aren't fully explained\n- There's no mention of backward compatibility concerns or migration strategy\n- The exact location and structure of the existing code isn't provided\n- It's not clear if this is intended as a replacement or addition to existing functionality\n\nHowever, an experienced developer familiar with the codebase should be able to understand what's being proposed and implement a solution based on this description, especially since the author has already implemented parts of it.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18199": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `nthroot_mod` function where it fails to include x = 0 as a solution when a \u2261 0 (mod p). The issue provides:\n\n1. A clear description of the problem: when solving x^n \u2261 a (mod p) where a \u2261 0 (mod p), the function should include x \u2261 0 (mod p) as a solution but currently doesn't.\n\n2. A specific test case: `nthroot_mod(17*17, 5, 17)` should include 0 as a root modulo 17, but it doesn't.\n\nThe fix would involve modifying the `nthroot_mod` function to check if a \u2261 0 (mod p), and if so, include 0 in the set of solutions. This is a straightforward mathematical correction with a clear acceptance criterion.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18200": {
        "explanation": "This issue is well-specified with a clear error case and reproduction steps. The issue describes an AttributeError that occurs when trying to intersect an ImageSet with the set of integers. The error trace is provided, showing exactly where the error occurs: in the intersection_sets function when it tries to access free_symbols on an int object. \n\nThe issue also provides context by referencing related issues (#17568, #18081) and a specific comment in another issue (#9616), which helps understand the problem domain (diophantine equations and set intersections).\n\nFrom this information, a developer would be able to:\n1. Reproduce the exact error using the provided code snippet\n2. Locate the problematic code in the codebase (specifically in sympy/sets/handlers/intersection.py)\n3. Understand that the error occurs because the code is trying to access free_symbols on an integer value when it should be checking if the value is an integer first\n4. Develop a fix that properly handles the case when a solution is an integer rather than a symbolic expression\n\nThe issue provides all the necessary information to diagnose and fix the problem without requiring additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18211": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The problem: `solveset` raises a `NotImplementedError` when trying to solve a specific equation involving trigonometric functions.\n2. The expected behavior: Instead of raising an error, `solveset` should return a `ConditionSet` representing the solution.\n3. A concrete example: The equation `Eq(n*cos(n) - 3*sin(n), 0)` is provided, along with the expected output `ConditionSet(n, Eq(n*cos(n) - 3*sin(n), 0), Reals)`.\n\nThe issue provides enough context for a developer to understand what's happening and what needs to be fixed. The solution would involve modifying the `solveset` function to handle this type of equation by returning a `ConditionSet` instead of raising an error. This is a clear, actionable task with a well-defined expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18256": {
        "explanation": "This issue is well-specified. The user is requesting a change in how SymPy renders LaTeX for expressions where a symbol with a superscript is raised to a power. \n\nThe issue clearly:\n1. Describes the current behavior (SymPy adds parentheses: `\\left(x^{i}\\right)^{2}`)\n2. Shows the desired behavior (nested superscripts without parentheses: `{x^{i}}^{2}`)\n3. Distinguishes this from a different case (powered superscripts: `x^{i^{2}}`)\n4. Provides a real-world example from Wikipedia showing the conventional notation\n\nThe request is specific and unambiguous - modify SymPy's LaTeX printer to use the nested superscript notation `{x^{i}}^{2}` instead of the parenthesized notation `\\left(x^{i}\\right)^{2}` when a superscripted symbol is raised to a power. This would involve modifying the LaTeX printing code in SymPy to detect this specific case and change the output format accordingly.\n\nA developer would have a clear understanding of what needs to be changed and what the expected output should be.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18273": {
        "explanation": "This issue is well-specified with a clear error reproduction case. The problem occurs when using the `cse` (common subexpression elimination) function with expressions containing `CRootOf` objects. The error trace shows an `IndexError` occurring in the `rootoftools.py` file, specifically when trying to rebuild an expression with `CRootOf` that has an index of 1, which is apparently out of range.\n\nThe issue provides:\n1. A complete code snippet to reproduce the error\n2. The exact error message and traceback\n3. The specific function (`cse`) that's failing\n4. The specific error type (`IndexError`)\n5. The context of the error (working with `CRootOf` objects)\n\nFrom this information, a developer can understand that there's likely a bug in how `cse` handles `CRootOf` objects, particularly when rebuilding expressions. The error suggests that during the rebuilding process, the degree of the polynomial is incorrectly calculated or handled, causing valid indices to be rejected. This is enough information to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18351": {
        "explanation": "This issue is well-specified with clear objectives. It describes adding NumPy printer support for a specific list of matrix expressions in SymPy. The issue:\n\n1. Clearly identifies the problem (implementing NumPy printer support for additional matrix expressions)\n2. Provides a specific, itemized list of matrix expressions that need support\n3. References the original issue (#17013) that this PR aims to fix\n4. Includes appropriate release notes\n5. Uses a checklist format that makes it clear what needs to be implemented\n\nThe implementation would involve extending the NumPy printer in SymPy to handle these specific matrix expression types. Each matrix expression is a well-defined mathematical concept in SymPy's codebase, so a developer familiar with SymPy would understand what needs to be done. The task is straightforward: for each matrix expression type, add the appropriate printing method to convert it to NumPy-compatible code.\n\nThe only minor ambiguity is that the PR is marked as \"WIP\" (Work in Progress), suggesting that it's not complete yet, but this doesn't affect the clarity of what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18477": {
        "explanation": "The issue is reasonably well-specified, though it could benefit from some additional details. Here's what I understand:\n\n1. The mpmath printer has `min_fixed` and `max_fixed` settings that control floating-point number formatting.\n2. Currently, only the `strip_zeros` option is exposed to the printers.\n3. The issue requests exposing these additional settings (`min_fixed` and `max_fixed`) to the printers.\n4. There's also a request to unify the Float printer behavior across different printers (specifically mentioning that the LaTeX printer behaves differently from the string printer).\n\nWhat's missing:\n- Specific details about what `min_fixed` and `max_fixed` actually do\n- Which specific printers need these settings exposed\n- Exact examples of the inconsistencies between printers\n- Expected behavior after the changes\n\nHowever, an experienced developer familiar with the codebase should be able to:\n1. Identify where the mpmath printer settings are defined\n2. See how `strip_zeros` is currently exposed to printers\n3. Follow the same pattern to expose `min_fixed` and `max_fixed`\n4. Identify the differences in Float printing between printers and standardize them\n\nThe task is clear enough to make a meaningful attempt at a solution, even if some details need to be discovered during implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18478": {
        "explanation": "The issue is well-specified and clearly identifies a potential inconsistency in SymPy's `subs` method when evaluating expressions at infinity. The user provides two specific examples:\n\n1. `(x+cos(x)).subs(x,oo)` returns `oo`\n2. `(exp(x)+cos(x)).subs(x,oo)` returns `AccumBounds`\n\nThe user is asking why there's a difference in behavior between these two cases. This is a concrete, reproducible issue with clear examples. The expected behavior isn't explicitly stated, but the question implies that the user expects consistent behavior between similar expressions when substituting infinity.\n\nThe issue is specific enough that a developer could:\n1. Reproduce the behavior\n2. Investigate why the substitution mechanism handles these cases differently\n3. Determine if this is a bug or intended behavior\n4. Implement a fix if needed\n\nThe solution would likely involve examining how SymPy evaluates limits and handles oscillating functions (like cosine) when combined with different types of growing functions (linear vs. exponential).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18587": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user has identified that when creating a Permutation object in SymPy's combinatorics module, if the length of the provided list is greater than the specified 'size' parameter, the size parameter is ignored without raising an exception. The user provides a complete code example demonstrating the issue, showing that when creating a Permutation with a list of length 4 but specifying size=2, the resulting Permutation has size 4 instead of raising an exception.\n\nThe issue also includes a clear suggestion for improvement: raising an exception when the provided list is longer than the specified size. The user is asking if there's a reason for the current behavior, implying that they believe it should be changed.\n\nA developer would be able to:\n1. Locate the Permutation class in the sympy.combinatorics module\n2. Examine the constructor to understand how it handles the size parameter\n3. Implement a validation check to raise an exception when the list length exceeds the specified size\n4. Add appropriate tests\n\nThe issue is specific, reproducible, and has a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18605": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible examples. The issue identifies a specific inconsistency in the `Idx` class behavior:\n\n1. When given a tuple of bounds `(m, n)` where `m` and `n` are non-integer symbols, the code accepts it without error.\n2. However, when given a single non-integer symbol `m`, it correctly raises a `TypeError`.\n\nThe expected behavior is clearly stated: the `Idx` class should check that both the upper and lower bounds are integers when a tuple is provided, just as it does when a single value is provided.\n\nThe issue includes complete code examples demonstrating both the problematic behavior and the expected behavior, along with the error message that should be triggered. The solution would involve modifying the `Idx` class to perform integer validation on both bounds when a tuple is provided.\n\nThis is a straightforward bug fix with a clear path to resolution - add validation for both bounds in a tuple, similar to the validation already implemented for the single bound case.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18630": {
        "explanation": "This issue is well-specified with clear information about the problem. The issue describes an AttributeError that occurs when trying to integrate a specific expression: `1 / (1 + x**4)**(S(1)/4)` from 0 to infinity. \n\nThe error traceback is provided in detail, showing that the problem occurs in the series expansion process during integration. Specifically, a 'TupleArg' object doesn't have the required 'compute_leading_term' attribute. \n\nThe issue also includes a second example showing that the same error occurs when trying to compute the series expansion of a hypergeometric function directly.\n\nFrom this information, I can clearly identify:\n1. The specific function call that's failing\n2. The exact error message and traceback\n3. The underlying issue (TupleArg missing compute_leading_term)\n4. A related example showing the same issue in a different context\n\nThis provides enough information to locate the problem in the codebase and implement a solution - likely by adding the missing method to the TupleArg class or modifying how series expansions are handled for these types of expressions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18650": {
        "explanation": "This issue is well-specified. It clearly identifies:\n\n1. The specific mathematical expression that's not simplifying correctly: `sqrt(8)**Rational(2, 3)`\n2. The current output: `2**(1/3)*2**(2/3)`\n3. The expected output: `2`\n\nThe problem is straightforward - the symbolic math system isn't properly simplifying an expression that mathematically equals 2. The expression `sqrt(8)` equals `2*sqrt(2)`, and when raised to the power of `2/3`, it should simplify to `2`.\n\nMathematically:\n- `sqrt(8) = 2*sqrt(2) = 2^1 * 2^(1/2) = 2^(3/2)`\n- `(2^(3/2))^(2/3) = 2^((3/2)*(2/3)) = 2^1 = 2`\n\nA developer would need to investigate why the simplification engine isn't combining the exponents properly in this specific case. This is a concrete, reproducible issue with a clear expected outcome.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18667": {
        "explanation": "The issue describes adding a new feature called \"Schur_Number\" to the combinatorics module. It provides:\n\n1. A clear definition of what a Schur number is: \"The Schur number S(k) is the largest integer n for which the interval [1,n] can be partitioned into k sum-free sets.\"\n2. A reference to external documentation: http://mathworld.wolfram.com/SchurNumber.html\n3. Mentions that the implementation includes the partition which can be proven by induction\n4. States that test cases have been added\n\nHowever, there are some missing details:\n- The issue doesn't specify the exact function signature or API design\n- It doesn't clarify the expected behavior for edge cases\n- It doesn't provide examples of expected inputs and outputs\n- It's unclear if this is a feature request or a PR that has already been implemented\n\nDespite these gaps, there is enough information to understand what the Schur number is mathematically and what functionality is being added. A developer familiar with the codebase could reasonably implement this feature based on the mathematical definition provided and the external reference.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18698": {
        "explanation": "The issue is reasonably well-specified. It describes a bug in the `sqf_list` function (square-free factorization list) where the output is inconsistent. The issue provides two examples:\n\n1. In the first example, the function returns `(1, [(x**2 + 1, 1), (x - 1, 2), (x - 3, 3), (x - 2, 3)])` for the input `(x**2 + 1) * (x - 1)**2 * (x - 2)**3 * (x - 3)**3`. The issue states this is incorrect, as it should have combined the two factors of multiplicity 3 into a single factor.\n\n2. In the second example, the function correctly returns `(1, [(x - 2, 1), (x**2 - 1, 2)])` for the input `x**5 - 2*x**4 - 2*x**3 + 4*x**2 + x - 2`.\n\nThe issue is clear that the problem is about consistency in how factors with the same multiplicity are represented. However, there's a slight ambiguity in the expected output format for the first example - the description mentions \"(x*_2 - 5_x + 6, 3)\" which seems to contain typos or formatting issues. Despite this minor confusion, a developer familiar with the codebase should be able to understand that the issue is about combining factors with the same multiplicity, and the expected behavior can be inferred from the second example.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18728": {
        "explanation": "This issue is well-specified with clear details about what needs to be fixed. The issue describes:\n\n1. The specific problem: `Pow.is_zero` and `is_positive` methods are not correctly handling cases where values might be infinite.\n2. Clear examples of the current incorrect behavior on the master branch.\n3. Examples of the expected behavior after the fix.\n4. The specific cases that need to be addressed (e.g., when `a` is infinite and `b` is negative, etc.).\n5. Additional context about related issues (#9532) that should be fixed.\n6. Concerns about the current implementation and potential alternative approaches.\n\nThe issue provides enough information for a developer to understand what's wrong, what the expected behavior should be, and how to test if the fix is working correctly. The examples are clear and demonstrate both the current incorrect behavior and the expected behavior after the fix.\n\nWhile the author mentions some uncertainty about the best approach to take (whether to merge as is or build new PRs on top), this doesn't make the issue vague - it's just acknowledging that there might be different ways to solve the problem. The core requirements are still clear.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18763": {
        "explanation": "This issue is well-specified. It clearly identifies a problem with the LaTeX printing of Subs expressions in SymPy when they are multiplied by a coefficient. The issue provides:\n\n1. A concrete example showing the current behavior (3*Subs(-x+y, (x,),(1,)))\n2. The current LaTeX output: '3 \\\\left. - x + y \\\\right|_{\\\\substack{ x=1 }}'\n3. The desired LaTeX output: '3 \\\\left. \\\\left(- x + y\\\\right) \\\\right|_{\\\\substack{ x=1 }}'\n4. Visual examples of both the current and desired output\n\nThe problem is that the current LaTeX representation doesn't properly parenthesize the substitution expression when it's multiplied by a coefficient, which could lead to ambiguity in the mathematical interpretation. The solution would involve modifying the LaTeX printing code for Subs objects to add parentheses around the expression when it appears in a product.\n\nThis is a straightforward issue with a clear problem statement and expected outcome, making it well-specified for implementation.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18765": {
        "explanation": "This issue is well-specified with a clear problem statement and expected solution. The issue describes:\n\n1. The current inconsistency between `Add`/`Mul` and their matrix counterparts `MatAdd`/`MatMul` regarding the `evaluate` parameter\n2. Provides concrete examples showing how `Add` respects the `evaluate` parameter while `MatAdd` does not\n3. Clearly states the expected behavior: `MatAdd` and `MatMul` should support an `evaluate` parameter that controls whether expressions are canonicalized (e.g., `MatAdd(A, A, evaluate=True)` should return `2*A`)\n\nThe solution would involve:\n1. Adding the `evaluate` parameter to `MatAdd` and `MatMul` classes\n2. Implementing the canonicalization logic when `evaluate=True`\n3. Preserving the unevaluated form when `evaluate=False`\n\nThis is a straightforward feature enhancement with clear acceptance criteria and examples.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18810": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `generate_derangements` function where it incorrectly includes the original permutation in its output when the input is not sorted. The issue provides:\n\n1. A clear example demonstrating the bug: `list('TRUMP') in generate_derangements('TRUMP')` returns `True`, which is incorrect because a derangement should not include the original permutation.\n\n2. A contrasting example showing that the function works correctly when the input is sorted: `list('MPRTU') in generate_derangements('MPRTU')` returns `False` as expected.\n\nThe problem is clearly defined: the `generate_derangements` function is incorrectly assuming that the input permutation is sorted, but this shouldn't be a requirement. The function should work correctly regardless of whether the input is sorted or not.\n\nTo fix this issue, we would need to modify the `generate_derangements` function to ensure it properly excludes the original permutation from its output, regardless of the input order. This is a straightforward bug fix with clear acceptance criteria.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18835": {
        "explanation": "This issue is well-specified with a clear problem description and suggested solution. The issue describes that the `uniq` function in the codebase doesn't detect when its input list is modified during iteration, which can lead to silent errors and incorrect results. The issue provides:\n\n1. A clear example of the problem: When iterating over `uniq(f)` and modifying `f` during iteration, the output is incorrect.\n2. A comparison with Python's built-in behavior for dictionaries and sets, which raise a `RuntimeError` when modified during iteration.\n3. A specific suggestion for how to fix the issue: record the size of the list at the start of iteration and check if it changes during iteration, raising a `RuntimeError` if it does.\n\nThe issue provides enough context to understand what the `uniq` function does (it appears to iterate over unique elements in a list), what the problem is (modifying the input list during iteration causes incorrect results), and what a successful solution would look like (detecting and raising an error when the input list is modified during iteration).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18903": {
        "explanation": "The issue is well-specified and clear. It describes a problem with the symbolic evaluation of nested floor and ceiling functions in SymPy. The issue provides concrete examples showing that when evaluating expressions like `floor(floor(x) + 1)`, the system doesn't fully simplify the nested floor functions as expected. \n\nThe first example shows that `floor(floor(x) + 1)` evaluates to `x + floor(floor(x)) + 1` instead of the mathematically equivalent and simpler `x + floor(x) + 1`. The second example shows that a further simplification step does correctly evaluate `x + floor(floor(x)) + 1` to `x + floor(x) + 1`.\n\nThe issue also mentions that the same problem occurs with the `ceiling` function, though it doesn't provide explicit examples for that case.\n\nThis is a clear bug report with reproducible examples and an implicit expected behavior (that nested floor functions should be fully simplified in a single evaluation). A developer would understand what needs to be fixed: ensure that nested floor and ceiling functions are properly simplified in a single evaluation step.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-18908": {
        "explanation": "The issue is asking to add more SciPy special functions to the SciPyPrinter, which appears to be a code printer component in the codebase. The issue provides a link to the SciPy documentation for special functions, indicating which functions should be added.\n\nWhile the issue clearly identifies what needs to be done (add more SciPy special functions to the SciPyPrinter), it lacks specific details about:\n1. Which specific functions from the SciPy special module need to be added\n2. What the current implementation of SciPyPrinter looks like\n3. Any priority order for adding these functions\n4. Any specific requirements for how these functions should be implemented\n\nHowever, an experienced developer familiar with the codebase could:\n1. Review the current SciPyPrinter implementation\n2. Compare the currently supported SciPy special functions against the full list in the documentation\n3. Identify the missing functions\n4. Implement them following the existing patterns in the codebase\n\nThe task is clear enough to start working on, but the developer would need to make some decisions about which functions to prioritize and how to implement them based on existing code patterns.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18922": {
        "explanation": "The issue description provides a clear problem statement with a screenshot showing the incorrect behavior. The user is reporting that when they try to plot a horizontal line at y=3, the x-axis itself is being moved to y=3 instead. The screenshot helps visualize the problem.\n\nWhile the description is brief, it clearly identifies:\n1. The expected behavior: a horizontal line at y=3\n2. The actual behavior: the x-axis is moved to y=3\n3. Visual evidence through a screenshot\n\nThe issue appears to be related to plotting constants in a graphing library (likely matplotlib or a similar plotting library). This is a specific enough problem that a developer familiar with the codebase should be able to reproduce the issue and understand what needs to be fixed.\n\nThe only missing information is the exact code that produced this plot, but the issue is still clear enough that a developer could test different ways of plotting constants to reproduce and fix the problem.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-18961": {
        "explanation": "This issue is well-specified with clear requirements:\n\n1. Add a new optional parameter `bits` to the `digits` function in `sympy/ntheory/digits.py`\n2. The parameter should pad the output with leading zeros to reach the specified bit length\n3. The implementation is provided in the form of a diff, showing exactly what code changes are needed\n4. The issue mentions that tests should be added\n\nThe diff clearly shows:\n- Where to add the parameter in the function signature\n- The logic for padding the result with zeros when `bits` is specified\n- How to handle the base indicator in the first position of the returned list\n\nThe only minor detail not explicitly specified is what kind of tests should be added, but this is a reasonable expectation for an experienced developer to determine based on the functionality being added. The function behavior is clearly defined, making it straightforward to write appropriate tests.\n\nOverall, this is a well-specified issue with clear implementation guidance that would allow a developer to create a successful PR without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19007": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user provides:\n\n1. A complete code snippet that demonstrates the issue\n2. The actual output they're getting\n3. An explanation of why the output is incorrect\n4. An implicit expectation of what the correct behavior should be\n\nThe issue is about the BlockMatrix class in SymPy incorrectly handling element access when using symbolic indices. When accessing C[i, 0] where i is a symbolic integer, it's always returning an element from the first block (A) rather than conditionally returning elements from either A or B depending on the value of i.\n\nThe expected behavior is that C[i, 0] should return a conditional expression or some other representation that accounts for the fact that the element could come from either A or B depending on the value of i.\n\nThis is a specific bug with a clear reproduction case and expected behavior, making it well-specified for a solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19016": {
        "explanation": "This issue is well-specified with clear implementation requirements:\n\n1. The primary task is clearly defined: implement the `is_finite_set` property for the `Range` class in SymPy, with a specific implementation suggestion provided (`return self.size.is_finite`).\n\n2. The issue includes a concrete example demonstrating the current behavior (property not implemented) and the expected behavior (should return a boolean indicating if the range is finite).\n\n3. The submitter has even provided the exact code implementation they're planning to use, asking for feedback before submitting a PR.\n\n4. A secondary issue is also well-described regarding `sup` and `inf` not working for ranges with symbolic integers, with a specific error trace and example.\n\nBoth issues have clear reproduction steps, expected outcomes, and even suggested implementation approaches. A developer could immediately start working on these issues without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19040": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue reports that when using SymPy's `factor()` function with the `extension=[I]` parameter, it incorrectly drops a factor of `(y-1)` from the factorization of `(x-1)*(y-1)`. \n\nThe issue includes:\n1. A clear title describing the problem\n2. A reproducible code example showing the expected behavior (without extension) and the problematic behavior (with extension)\n3. The actual output vs. expected output\n4. A reference to a related issue (#5786)\n5. A link to a StackOverflow question with the same problem\n\nThe issue is also properly formatted as a PR that fixes issue #18895.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Understand what's going wrong (factor with extension dropping a term)\n3. Implement a fix to ensure that `factor(z, extension=[I])` correctly returns `(x-1)*(y-1)` instead of just `x-1`\n\nThe problem is specific and well-defined, making it suitable for a meaningful solution attempt.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19091": {
        "explanation": "This issue is well-specified and provides clear information about what's wrong and why it needs to be fixed:\n\n1. The problem is clearly identified: tensor contractions are being handled before applications of the metric, leading to incorrect results.\n\n2. The specific location of the issue is pinpointed in the code (`tensor/tensor.py` in the `_extract_data` method of the `Tensor` class).\n\n3. The technical explanation is provided with mathematical context: tensor contraction should follow the rule T^\u03bc_\u03bc = g_\u03bc\u03bd T^\u03bc\u03bd for pseudo-riemannian manifolds.\n\n4. The issue reporter has attempted to fix it themselves but found that the current design makes the bug \"essentially unavoidable\" without refactoring.\n\n5. There's a reference to a previous issue (#17328) that demonstrates the problem.\n\nThe only minor ambiguity is around the exact implementation details of the solution - how exactly the tensor module should be refactored. However, this is a reasonable area for the developer to exercise judgment based on the clear problem statement. The issue provides enough information for a competent developer familiar with the codebase to understand what needs to be fixed and why.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19093": {
        "explanation": "This issue is well-specified with clear information about what's not working. The description includes:\n\n1. A specific regression issue - functionality that worked in version 0.7.3 but no longer works\n2. A complete code example that reproduces the error\n3. The exact error message received: \"TypeError: unhashable type: 'MutableDenseMatrix'\"\n\nThe problem is clearly that operations between BlockDiagMatrix objects containing mutable matrices are failing with a TypeError. The expected behavior is that these operations should work as they did in version 0.7.3. \n\nA developer would be able to:\n1. Reproduce the issue using the provided code\n2. Compare behavior with version 0.7.3\n3. Identify why mutable matrices are causing hashability problems in BlockDiagMatrix operations\n4. Implement a fix that allows operations between BlockDiagMatrix objects with mutable matrices\n\nThe issue provides all necessary information to understand and address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19201": {
        "explanation": "This issue is well-specified with clear examples and proposed changes. The author provides:\n\n1. A detailed table comparing current behavior vs. proposed behavior for various matrix slice notations\n2. Specific examples of problematic cases\n3. Clear rationale for the changes (consistency in notation, avoiding dimension hints)\n4. Identification of a specific bug with composite matrices (missing brackets)\n\nThe issue covers multiple printers (str/pretty/latex) but is focused on a single coherent problem: improving the display of symbolic matrix slices. The examples are concrete and the expected outcomes are clearly defined.\n\nA developer could implement these changes without needing additional clarification, as the issue provides sufficient context and examples to understand both the current behavior and the desired behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19254": {
        "explanation": "This issue is reasonably well-specified. It clearly identifies:\n\n1. The specific methods that need to be improved: `dup_zz_mignotte_bound(f, K)` and `dmp_zz_mignotte_bound(f, u, K)`\n2. The proposed solution: replace the current Mignotte bound implementation with the Knuth-Cohen bound\n3. The rationale: research conducted with Prof. Ag.Akritas showing better performance across various polynomial types\n\nHowever, there are some details missing that would make implementation more straightforward:\n- No specific reference to the Knuth-Cohen bound formula or implementation details\n- No code examples or pseudocode for the proposed implementation\n- No specific performance metrics or benchmarks showing the improvement\n- No mention of backward compatibility concerns\n\nDespite these gaps, an experienced developer familiar with SymPy's polynomial factorization could likely implement this change by researching the Knuth-Cohen bound and applying it appropriately to the existing codebase. The issue provides enough context to understand what needs to be done, even if some research is required to determine exactly how to implement it.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19346": {
        "explanation": "This issue is well-specified. It clearly identifies a problem with the `srepr` function in SymPy, which is not properly handling dictionary and set data structures the same way it handles lists and tuples.\n\nThe issue provides:\n1. Clear examples of the current behavior for lists and tuples (which work correctly)\n2. Clear examples of the problematic behavior for dictionaries and sets\n3. Expected behavior (implied to be similar to how lists and tuples are handled, showing the symbolic representation)\n\nThe question about whether this is intended behavior is reasonable, but even if it is, the reporter suggests that fixing it would be \"an easy job\" - implying that the expected behavior should be consistent across all collection types.\n\nA developer would be able to:\n1. Locate the `srepr` function in the SymPy codebase\n2. Understand how it currently handles different data structures\n3. Extend the functionality to properly handle dictionaries and sets in a similar way to lists and tuples\n4. Implement and test the fix\n\nThe issue is specific, reproducible, and has a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19487": {
        "explanation": "This issue is well-specified. It clearly describes:\n\n1. The current implementation of the `sign` function in SymPy\n2. The desired feature: adding a way to rewrite the `sign` function in terms of `Abs` using the `.rewrite(Abs)` method\n3. A specific example of the expected output\n4. A potential edge case (when x=0) that needs to be addressed\n\nThe issue also provides two possible approaches for handling the edge case:\n- Either change `sign(0)` to return `nan` to match the mathematical definition\n- Or make the rewrite method handle the zero case specially\n\nThis gives a clear understanding of what needs to be implemented and what considerations need to be made. A developer can proceed with implementing the `.rewrite(Abs)` method for the `sign` function and decide on one of the suggested approaches for handling the zero case.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19495": {
        "explanation": "This issue is well-specified with clear examples demonstrating unexpected behavior when using the `subs` method on a `ConditionSet` containing an `ImageSet`. The reporter provides:\n\n1. A complete code example showing the unexpected behavior\n2. The expected behavior (demonstrated with `FiniteSet`)\n3. A comparison with working substitution on plain `ImageSet`\n4. Alternative approaches that work correctly (`xreplace` and `subs` with `simultaneous=True`)\n\nThe issue appears to be that when using `subs` on a `ConditionSet` containing an `ImageSet`, the bound variable `x` of the `ConditionSet` is being confused with the substitution variable `y`. This results in an incorrect output where the condition becomes malformed.\n\nThe problem is clearly reproducible with the provided code snippets, and the expected behavior can be inferred from the working examples. A developer would have enough information to investigate and fix the issue in the substitution handling for `ConditionSet` objects.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19601": {
        "explanation": "This issue is reasonably well-specified. The user is asking about plans for a SymPy 1.6.1 release, similar to how 1.5.1 followed 1.5. They explain their situation - they're currently using 1.5.1 and have submitted patches that have been merged into SymPy master. They're looking forward to upgrading to version 1.6 but seem to be asking if there will be a 1.6.1 release.\n\nWhile the issue doesn't provide technical details about what specifically needs to be fixed, it's clear that the request is about release planning information. The maintainers would understand what's being asked - whether there are plans for a 1.6.1 release and potentially when it might happen. This is a straightforward question about project management and release scheduling that maintainers should be able to answer without needing additional technical details.\n\nThe only minor ambiguity is whether the user is asking about a 1.6.1 release because they need specific fixes that would be in 1.6.1 but not in 1.6, or if they're just asking about the general release schedule. However, this doesn't prevent the maintainers from providing a meaningful response about release plans.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-19637": {
        "explanation": "The issue is well-specified and provides clear information about the problem:\n\n1. It shows the exact code that's causing the error: using `kernS` from `sympy.core.sympify` to parse a mathematical expression.\n2. It includes the specific error message: \"UnboundLocalError: local variable 'kern' referenced before assignment\"\n3. It even includes a hint about where the error occurs: \"hit = kern in s\"\n\nFrom this information, I can understand that there's a bug in the `kernS` function where it's trying to use a variable named `kern` before it's been assigned a value. This is a clear programming error that can be located in the sympy codebase and fixed.\n\nThe issue provides enough context to:\n1. Locate the `kernS` function in the sympy.core.sympify module\n2. Identify where the variable `kern` is being used before assignment\n3. Implement a fix to properly initialize the variable before use\n\nThis is a straightforward bug fix with a clear reproduction case and error message.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19783": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible code example. The issue is that when multiplying a Dagger operator with an IdentityOperator, the expression is not being simplified as expected. \n\nSpecifically:\n1. When multiplying a regular Operator with IdentityOperator, it correctly simplifies to just the Operator (A * Identity = A)\n2. However, when multiplying a Dagger(Operator) with IdentityOperator, it doesn't simplify and returns \"A^\\dagger I\" instead of just \"A^\\dagger\"\n\nThe expected behavior is clear: Dagger(A) * IdentityOperator() should simplify to just Dagger(A), similar to how A * IdentityOperator() simplifies to A.\n\nThe issue provides all necessary context, imports, and a minimal code example that demonstrates the problem. A developer can understand what's happening, reproduce the issue, and identify what needs to be fixed in the simplification logic for Dagger operators.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19885": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The user has provided a specific code example that reproduces the issue\n2. They've included the exact input (a system of linear equations)\n3. They've shown the expected output (from `solve()`) and the incorrect output (from `linsolve()`)\n4. They've attached a test file for reproduction\n5. The issue is clearly stated: `linsolve()` is returning incorrect results compared to `solve()` for the same system of equations\n\nThe problem is concrete and reproducible. A developer can run the provided code, compare the outputs from both functions, and investigate why `linsolve()` is producing different results than `solve()`. The issue appears to be a genuine bug in the `linsolve()` function's implementation.\n\nThe only minor detail missing is the actual output values from both functions, but these can be easily obtained by running the provided code. The screenshot likely shows these results, and the test file is available for download.\n\nThis is sufficient information for a developer to investigate and fix the issue without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-19954": {
        "explanation": "This issue is well-specified with clear reproduction steps and error information. The user has:\n\n1. Provided the exact versions of their environment (SymPy 1.6.1, NumPy 1.18.5, SciPy 1.4.1, Python 3.8.5)\n2. Included a complete, minimal code example that reproduces the issue\n3. Shared the full error traceback showing where the error occurs\n4. Demonstrated that the issue occurs in multiple cases (DihedralGroup(18) and DihedralGroup(2*25))\n\nThe error is clearly an IndexError in the minimal_blocks method when trying to delete elements from lists called num_blocks and blocks. The traceback points to the exact line in the SymPy codebase where the error occurs.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Locate the problematic code in the SymPy library\n3. Debug why the index is out of range\n4. Implement a fix for the sylow_subgroup method when used with certain DihedralGroup parameters\n\nNo additional information is needed to begin working on a solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20049": {
        "explanation": "This issue is well-specified with a clear problem statement and expected behavior. The user provides:\n\n1. A complete code example demonstrating the issue\n2. The exact error message received\n3. The expected output that should be produced\n4. A clear explanation of why they believe this should work (drawing a parallel to how angular velocity works)\n\nThe issue is asking for the `Point.vel()` method to automatically calculate the velocity of a point when possible, similar to how `ReferenceFrame.ang_vel_in()` calculates angular velocity. Currently, when a point's position is defined relative to another point using `set_pos()`, calling `vel()` fails with an error instead of calculating the velocity based on the position relationship.\n\nThe solution would involve modifying the `vel()` method in the Point class to check if the position has been defined relative to another point, and if so, calculate the velocity by taking the time derivative of the position vector in the specified reference frame.\n\nThis is a straightforward feature request with clear requirements and expectations.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20139": {
        "explanation": "The issue description is reasonably specified, though it could benefit from more context. Here's what we can understand:\n\n1. The issue involves changing the type of the first argument of `MatrixSymbol` from Symbol to Str.\n2. This change is related to the \"matrices\" component of the SymPy library.\n3. The release notes clearly state that \"MatrixSymbol will store Str in its first argument.\"\n\nWhat's missing:\n- The rationale behind this change is not explained (why is this change needed?)\n- There's no explanation of potential impacts or side effects\n- No specific implementation details are provided\n\nHowever, the task itself is clear enough: modify the `MatrixSymbol` class to accept and store a Str object instead of a Symbol for its first argument. This is a straightforward refactoring task that an experienced developer familiar with the SymPy codebase could implement without additional clarification. The developer would need to:\n1. Locate the `MatrixSymbol` class\n2. Change the type handling for the first argument from Symbol to Str\n3. Update any related tests or documentation\n4. Ensure backward compatibility if needed\n\nWhile more context would be helpful, the core task is defined well enough to proceed with implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-20154": {
        "explanation": "The issue is well-specified and clear. It describes a specific problem with the `partitions()` function in `sympy.utilities.iterables` where the function reuses output dictionaries instead of creating new ones for each yield. This behavior is documented in the docstring as a caveat, but the issue creator argues that:\n\n1. This behavior is confusing and can lead to bugs\n2. Creating a new dictionary copy before yielding shouldn't cause significant performance loss\n3. Simple operations like `list(partitions())` produce incorrect results due to this behavior\n\nThe issue clearly identifies the problematic function, explains the current behavior, why it's problematic, and suggests a straightforward solution (copying the dictionary before yielding it). A developer would know exactly what needs to be changed and why. The solution would involve modifying the `partitions()` function to create new dictionary copies before yielding them to the caller.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20169": {
        "explanation": "The issue description is quite vague and lacks important details needed for implementation. While it identifies a limitation in the current lens maker formula implementation (only works for thin lenses), it doesn't provide:\n\n1. Context about where this formula is implemented in the codebase\n2. The current implementation details or code references\n3. Specific requirements for how the plano-lenses and thick lens calculations should be implemented\n4. Mathematical formulas or references for the extended lens maker formula\n5. Expected inputs and outputs for the enhanced functionality\n6. Any test cases or validation criteria\n\nWithout knowing which files need modification, what the current implementation looks like, or the specific mathematical formulas required for plano-lenses and thick lenses, it would be difficult to create a meaningful solution. The developer would need to first locate the relevant code, understand the current implementation, research the appropriate formulas for the additional lens types, and then implement them without clear guidance on expected behavior.",
        "label": "VAGUE"
    },
    "sympy__sympy-20264": {
        "explanation": "This issue is well-specified with clear reproduction steps and error messages. The user has:\n\n1. Provided a clear description of the problem: RecursionError occurs when converting specific sympy expressions to LaTeX\n2. Specified the exact conditions when the error occurs: when using Pow(Rational(1,n), -1, evaluate=False) where n is a positive integer > 1, or Pow(Rational(-1,n), -1, evaluate=False) where n is a negative integer < -1\n3. Included the exact version of sympy being used (1.62)\n4. Provided complete code examples that reproduce the issue\n5. Included the full error traceback showing the recursion error\n6. Demonstrated several test cases showing which inputs work and which fail\n\nThe issue is clearly a bug in the LaTeX printer's handling of specific rational power expressions, causing infinite recursion. A developer would have all the information needed to reproduce the issue, locate the problem in the code, and implement a fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20428": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue describes a bug where `clear_denoms()` on a complicated polynomial that evaluates to zero produces a polynomial that:\n\n1. Prints as `Poly(0, x, domain='EX')` but has `is_zero = False`\n2. Has an inconsistent internal representation with an unstripped leading zero: `DMP([EX(0)], EX, None)` instead of the correct `DMP([], EX, None)`\n3. Causes errors in various polynomial methods like `terms_gcd()`\n4. Previously caused a `ZeroDivisionError` in `primitive()` (which has been partially fixed)\n\nThe issue provides complete reproduction steps with code examples showing:\n- The input that triggers the problem\n- The inconsistent behavior of the resulting polynomial\n- The errors that occur when using various methods\n- The underlying cause (unstripped DMP representation)\n\nThe solution would clearly involve ensuring that `clear_denoms()` properly strips leading zeros in the DMP representation when returning a zero polynomial, ensuring consistent behavior across all polynomial methods. This is a well-defined bug with a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20438": {
        "explanation": "The issue description shows a problem with the `is_subset` method in SymPy's set implementation. The issue demonstrates that:\n\n1. We have two sets `b` (a ProductSet) and `c` (a FiniteSet) that appear to contain the same elements\n2. `b.intersection(c) == c.intersection(b)` returns True, suggesting they have the same elements\n3. `c.is_subset(b)` returns True, which is expected if they have the same elements\n4. `b.is_subset(c)` returns nothing (implicitly None), which is inconsistent with the previous results\n5. Attempting to use `Eq(b, c).simplify()` results in an error\n\nThe issue is clearly about the inconsistent behavior of the `is_subset` method when comparing a ProductSet with a FiniteSet that contains the same elements. The problem is well-defined, reproducible with the provided code, and the expected behavior can be inferred (the `is_subset` method should return True when `b.is_subset(c)` is called, since the sets contain the same elements).\n\nA developer would need to investigate why `is_subset` behaves differently depending on which set is the caller and which is the argument, and fix the implementation to ensure consistent results.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20442": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user is reporting that the `convert_to` function in SymPy is producing unexpected results when converting between incompatible or orthogonal units.\n\nThe issue provides:\n1. A specific example showing the unexpected behavior: `convert_to(joule*second, joule)` returns `joule**(7/9)` instead of an error or the original expression\n2. Three contrasting examples where the function behaves as expected\n3. The version of SymPy being used (1.4)\n\nThe problem is clearly defined: when trying to convert between dimensionally incompatible units (joule*second to joule), the function attempts some mathematical transformation instead of returning an error or the original expression.\n\nA developer would have enough information to reproduce the issue and understand what the expected behavior should be. The solution would likely involve modifying the `convert_to` function to properly handle cases where the target unit is dimensionally incompatible with the source unit.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20565": {
        "explanation": "This issue is well-specified. It clearly identifies a problem: the `Rationals` set in SymPy does not contain floating point numbers, specifically demonstrated with the example of `0.5`. The issue provides a concrete code example showing the current behavior (`sympy.Rationals.contains(0.5)` returns `False`) and states the expected behavior (it should return `True`). \n\nThe issue is straightforward - the `Rationals` set should recognize floating point numbers as rational numbers, which is mathematically correct since any finite decimal representation (like 0.5) can be expressed as a ratio of integers (1/2). \n\nA developer would need to modify the `contains` method of the `Rationals` class to properly handle floating point inputs by converting them to rational numbers before checking containment. The issue provides enough information to understand the problem and implement a solution without needing further clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20590": {
        "explanation": "This issue is well-specified. The reporter has clearly identified a change in behavior between SymPy versions 1.6.2 and 1.7 regarding Symbol instances. In version 1.6.2, Symbol instances had no `__dict__` attribute (only `__slots__` with 'name'), but in version 1.7, `__dict__` exists and returns an empty dictionary. \n\nThe reporter has provided specific code examples demonstrating the issue, including the error message from version 1.6.2 and the different behavior in 1.7. They've also suggested a potential cause: that a parent class may have accidentally stopped defining `__slots__`, which would explain why `__dict__` is now available.\n\nThis issue provides enough information to:\n1. Verify the behavior change between versions\n2. Investigate the class hierarchy to find what changed\n3. Determine if this is indeed a bug (as the reporter suggests) or an intentional change\n4. Implement a fix if needed (likely by ensuring proper `__slots__` definition in the class hierarchy)\n\nThe solution would involve examining the Symbol class and its parent classes in both versions to identify what changed, and then restoring the previous behavior if this was unintentional.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20639": {
        "explanation": "This issue is well-specified and clear. The problem is that SymPy's pretty printing functionality is incorrectly rendering the mathematical expression pi**(1/E) as \"-1___\\n\u2572\u2571 \u03c0\" (which appears to be showing a negative square root of pi), when it should be displaying pi raised to the power of 1/E. \n\nThe issue description provides:\n1. Clear reproduction steps with input and output\n2. Evidence that other output formats (LaTeX and string representation) are working correctly\n3. The specific component where the bug occurs (printing.pretty)\n4. The version information (1.5.dev)\n\nA developer would be able to:\n1. Reproduce the issue using the provided code\n2. Locate the relevant code in the pretty printing module\n3. Fix the incorrect rendering of the expression pi**(1/E)\n\nThe expected behavior is implied but clear - the pretty printing should match the correct mathematical representation shown in the LaTeX and string outputs.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20741": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The issue shows that matrix expressions involving negation and addition aren't being simplified properly. \n\nThe example demonstrates that:\n1. `-(-A + B) - A + B` should simplify to zero (or at least a simpler form)\n2. When using `.simplify()` on this expression, it remains unchanged\n3. When using `.expand()`, it becomes `-B + A - A + B` which is still not simplified to zero\n4. Adding a zero matrix (`Z`) doesn't change the behavior\n\nThe issue is clearly about the matrix expression simplification system not properly handling negation and addition operations. The expected behavior would be for the expression to simplify to zero or a more simplified form, but it's not happening.\n\nThe issue provides concrete code examples with inputs and outputs, making it clear what the problem is and what needs to be fixed. A developer would need to investigate why the simplification rules for matrix expressions aren't properly handling these cases.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20801": {
        "explanation": "This issue is well-specified with a clear problem description. The issue identifies an inconsistency in the SymPy library where `S(0.0) == S.false` returns `True` while `S.false == S(0.0)` returns `False`. This is clearly a bug since equality should be symmetric. The issue also provides context by showing that `S(0) == S.false` and `S.false == S(0)` both return `False`, suggesting that the expected behavior is that `S(0.0) == S.false` should also return `False`. The issue includes reproducible code examples that demonstrate the problem. The desired outcome is clear: fix the inconsistency in the equality comparison between `S(0.0)` and `S.false`. A developer would need to investigate why floating-point zero is being treated differently from integer zero in this comparison and fix the implementation to ensure consistent behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-20916": {
        "explanation": "The issue is reasonably well-specified. It clearly identifies a problem with Python's pprint module not correctly formatting subscripts on Greek letters. The example shows that subscripts work correctly on Latin letters (t\u2080) but not on Greek letters (\u03c90 instead of \u03c9\u2080). \n\nThe issue provides:\n1. A clear example of the expected output (\"Good\")\n2. A clear example of the current problematic output (\"Bad\")\n3. The specific issue: subscripts don't format correctly when applied to Greek letters\n\nWhat's missing is:\n1. The exact code used to generate these outputs\n2. The version of Python being used\n3. Any specific context or use case\n\nHowever, the core problem is clear enough that a developer could investigate and attempt to fix the issue with the Unicode handling in pprint when dealing with Greek letters and subscripts. The solution would likely involve ensuring that subscript characters are properly applied to Greek letters in the same way they are to Latin letters.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21101": {
        "explanation": "This issue is well-specified. It clearly identifies a bug in the `latex()` function when handling unevaluated multiplication expressions. Specifically, when using `Mul(6, S.Half, evaluate=False)`, the LaTeX output is incorrectly rendered as \"6 1/2\" without a proper separator between the numbers. The implication is that there should be a `numbersep` (number separator) in the output, but it's missing.\n\nThe issue provides:\n1. A specific function call that reproduces the problem (`latex(Mul(6, S.Half, evaluate=False))`)\n2. The actual output (\"6 1/2\")\n3. The expected behavior (should include a `numbersep`)\n\nThis is enough information for a developer to:\n- Reproduce the issue\n- Understand what's wrong\n- Implement a fix to ensure proper LaTeX formatting for unevaluated multiplication expressions\n\nThe fix would likely involve modifying the LaTeX printer to properly handle the case of unevaluated multiplication with numeric terms.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21171": {
        "explanation": "This issue is well-specified with a clear error message and reproducible steps. The problem is that when trying to display the shear force of a beam in a Jupyter notebook, there's an error in the LaTeX printing functionality: `_print_SingularityFunction() got an unexpected keyword argument 'exp'`.\n\nThe error occurs specifically when the `b2.shear_force()` method is called, and the traceback shows that it's related to how the LaTeX printer handles power expressions involving SingularityFunction objects. The issue is that when a SingularityFunction is raised to a power, the LaTeX printer tries to pass an 'exp' keyword argument to the _print_SingularityFunction method, but this method doesn't accept that parameter.\n\nThe complete traceback is provided, showing exactly where the error occurs in the code path. The issue includes all necessary code to reproduce the problem, including the exact imports, variable definitions, and function calls. The environment (Jupyter Notebook with Python 3.8) is also specified.\n\nA developer would be able to understand and fix this issue by:\n1. Examining the _print_SingularityFunction method in the LaTeX printer\n2. Modifying it to handle the 'exp' keyword argument properly\n3. Testing the fix with the provided reproduction steps\n\nThis is a clear bug with a specific error message and a complete reproduction case.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21208": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user has provided:\n\n1. Complete code examples showing how to reproduce the issue\n2. The exact inputs used\n3. The different outputs observed when using different approaches\n4. A clear question about whether this is a bug or expected behavior\n\nThe issue demonstrates that when differentiating a matrix with respect to a function variable using the matrix's `.diff()` method, the result differs from differentiating the individual elements directly or using the `.applyfunc()` method. This appears to be a potential inconsistency in how the chain rule is applied in different contexts.\n\nThe issue provides enough information for a developer to investigate why the matrix differentiation behaves differently from element-wise differentiation, determine if this is a bug in the implementation of Matrix.diff(), and implement a fix if needed. The expected behavior would be consistency between these different approaches to differentiation.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21260": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a complete, runnable code example that demonstrates the problem\n2. Shown the actual output that illustrates the issue\n3. Explained the expected behavior (symbols with the same name and assumptions should be equal across processes)\n4. Included additional context from another developer (@oscarbenjamin) suggesting a potential cause (pickling/unpickling)\n5. Mentioned their attempt to solve it using the `dill` library\n6. Described an interesting related behavior with integrals and derivatives\n\nThe core issue is clearly defined: symbols created with the same name and assumptions in different processes are not being recognized as equal, while symbols created in the same process are. This appears to be related to how SymPy handles symbol identity across process boundaries.\n\nA developer would have enough information to investigate and attempt to fix this issue by examining how SymPy handles symbol creation, caching, and serialization across processes. The reproduction code is complete and demonstrates the problem effectively.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21271": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The description includes:\n\n1. The exact file with the problem: sympy/physics/vector/frame.py\n2. The specific function with the doctest failure: ReferenceFrame.orient_space_fixed\n3. The exact line number (838) where the failure occurs\n4. The complete error output showing the expected vs. actual output\n5. The context (reference to previous issues #20946 and #20954)\n\nThe issue is a doctest failure where the actual output of B.dcm(N).simplify() doesn't match the expected output. Specifically, the middle element in the second row of the matrix is different - the expected output is a simple trigonometric expression, but the actual output is a more complex expression that hasn't been fully simplified.\n\nThe solution would require fixing the simplification logic in the orient_space_fixed method to ensure the DCM (Direction Cosine Matrix) is properly simplified to match the expected form. This is a clear, technical issue with all the necessary information to reproduce and fix the problem.\n\nA developer can understand exactly what's wrong and what needs to be fixed without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21286": {
        "explanation": "The issue is reasonably well-specified. It describes a problem with the `Range` class in SymPy where symbolic ranges (those containing symbolic variables like 'i' or 'k') are not being canonicalized properly, unlike numerical ranges.\n\nThe issue provides clear examples demonstrating:\n1. Numerical ranges are canonicalized correctly (e.g., Range(3, 5, 2) and Range(3, 7, 2))\n2. Symbolic ranges are not canonicalized (e.g., Range(i, i+1, 5), Range(i, i+2, 5), etc.)\n3. The problem affects properties like `inf` and `reversed.sup`\n\nWhat's missing is an explicit statement of what the expected behavior should be, though it's implied that symbolic ranges should be canonicalized similar to numerical ranges. The issue also doesn't specify which file or component needs to be modified, but this can be determined by examining the codebase.\n\nA developer familiar with SymPy would understand that this is about improving the canonicalization of symbolic Range objects to make their behavior consistent with numerical Range objects. The solution would involve modifying the Range class to properly handle symbolic expressions in its arguments.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21313": {
        "explanation": "This issue is well-specified with clear technical details about a problem with SymPy's `imageset` function when used with floating-point numbers. The issue describes:\n\n1. The specific problem: `imageset` is incorrectly canonicalizing floating-point values by applying modulo operations, which leads to unexpected behavior.\n2. A concrete example demonstrating the issue with specific floating-point values.\n3. An explanation of why this is happening: `b` is being replaced with `b%c`.\n4. A clear statement of what should be fixed: \"canonicalization is OK for Rationals, it should not be done to Floats.\"\n5. A workaround implementation (`frange` function) that shows how the issue can be addressed.\n\nThe issue provides enough context, code examples, and expected behavior for a developer to understand the problem and implement a solution. The solution would involve modifying the canonicalization logic in the `imageset` function to handle floating-point numbers differently than rational numbers.\n\nA successful PR would need to:\n1. Identify where in the codebase the canonicalization of `imageset` parameters happens\n2. Modify the code to skip this canonicalization step for Float types\n3. Add tests to verify the fix works as expected\n\nThis is a well-defined technical issue with clear reproduction steps and expected behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21370": {
        "explanation": "This issue is reasonably well-specified. It clearly describes a bug in the `minpoly` function when dealing with complicated algebraic numbers. The issue includes:\n\n1. A specific code example that reproduces the error\n2. The exact error message and traceback\n3. The location in the code where the error occurs (`_choose_factor` function in `polys/numberfields.py`)\n\nThe error is that the `_choose_factor` function is unable to determine the minimal polynomial for a complex algebraic expression, resulting in a \"multiple candidates\" error.\n\nWhile the issue provides enough information to understand the problem and locate where it's happening, it doesn't explicitly state what the expected behavior should be. However, it's reasonable to infer that the expected behavior is for `minpoly` to successfully compute the minimal polynomial for the given expression rather than raising a NotImplementedError.\n\nA developer with knowledge of algebraic number theory and the SymPy codebase should be able to work on this issue without further clarification, though they would need to determine the appropriate approach to resolve the \"multiple candidates\" problem in the `_choose_factor` function.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-21379": {
        "explanation": "This issue is well-specified with a clear description of the problem and a minimal working example (MWE) that reproduces the error. The user has:\n\n1. Provided the exact error message: \"PolynomialError: Piecewise generators do not make sense\"\n2. Included complete code to reproduce the issue\n3. Specified the SymPy version (1.8.dev)\n4. Documented their investigation, including conditions that trigger the error and workarounds\n5. Identified specific patterns that cause or avoid the error:\n   - Only happens with real symbol assumptions\n   - Only occurs with certain functions (sinh, cosh, tanh)\n   - Disappears when removing division by z\n   - Disappears when removing exp\n   - Doesn't happen on subsequent runs without clearing cache\n\nThe issue appears to be a bug in SymPy's substitution mechanism when dealing with Piecewise expressions under specific conditions. The detailed reproduction steps and investigation make this a well-documented bug report that provides enough information for a developer to investigate and fix the underlying issue.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21436": {
        "explanation": "This issue is well-specified with clear details about the problem and expected behavior. The issue describes:\n\n1. The current behavior: When calling `X.orient()`, all adjacent relationships to `X` are cleared, which prevents building a reference frame graph in arbitrary order.\n\n2. The desired behavior: Allow construction of the reference frame graph in any sequence without overwriting existing relationships, except in specific cases like loops or contradictory orientations.\n\n3. A concrete example demonstrating the problem: The IPython session clearly shows how calling `B.orient(C, ...)` wipes out the previously established relationship between A and B.\n\n4. The specific use case that should work but currently doesn't: Building a tree of reference frames in a non-hierarchical order.\n\nThe issue provides enough context about the current implementation, the problem, and the expected solution. A developer would be able to understand what needs to be fixed and how to test if their solution works correctly. The solution would involve modifying the `orient()` method to be more selective about which relationships to clear, while still maintaining graph consistency.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21476": {
        "explanation": "This issue is well-specified with a clear problem description, a minimal working example, and even debugging information that identifies the root cause. The user has:\n\n1. Provided complete, runnable code that reproduces the issue\n2. Included the exact error message and stack trace\n3. Identified the specific problem: when trying to transform between coordinate systems that don't have a direct relation defined (requiring an intermediate coordinate system), a KeyError is raised\n4. Pinpointed the likely cause through debugging: there's a type mismatch in the Dijkstra algorithm where the code is comparing a CoordSystem object with sympy.Str objects\n\nThe issue is in the sympy.diffgeom.CoordSystem.transform method when trying to find an indirect transformation path between coordinate systems. The debugging output clearly shows that the keys in path_dict are sympy.Str objects while the lookup is being done with a CoordSystem object, causing the KeyError.\n\nA developer would have all the information needed to reproduce and fix this issue, likely by addressing the type mismatch in the _dijkstra method of the CoordSystem class.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21527": {
        "explanation": "This issue is well-specified with clear examples demonstrating inconsistent behavior in SymPy's `linsolve` function. The user provides three similar systems of linear equations that differ only slightly in their coefficients (0.0215, 0.0216, and 0.0217), yet they produce dramatically different results:\n\n1. First system returns FiniteSet((0, 0))\n2. Second system returns FiniteSet((-4.07992766242527e+17*y, 1.0*y))\n3. Third system returns FiniteSet((0, 0))\n\nThe issue clearly demonstrates the unexpected behavior with complete code examples that can be reproduced. The problem is likely related to floating-point precision or how SymPy handles nearly parallel lines in its linear equation solver. The question is specific and focused on understanding why these similar systems produce different solutions.\n\nA developer would have enough information to investigate the issue, reproduce the problem, and determine what's causing the inconsistency in the `linsolve` function's behavior with these nearly identical inputs.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21567": {
        "explanation": "This issue is well-specified. It clearly identifies a feature gap in the `expand_trig` function, which currently doesn't support expansion of hyperbolic functions `csch` and `sech`, while it does support other trigonometric and hyperbolic functions.\n\nThe issue provides:\n1. A clear problem statement: `expand_trig` doesn't work for `csch` and `sech`\n2. A reference implementation: The behavior should be similar to how `csc` and `sec` are handled (expanding in the denominator)\n3. Concrete examples: Code snippets showing current behavior and implicitly what the expected behavior should be\n4. Context: Reference to a previously fixed related issue (#21365)\n\nThe solution would involve implementing the expansion formulas for `csch(x+y)` and `sech(x+y)` in the `expand_trig` function, following the pattern used for `csc` and `sec`. The mathematical formulas for these expansions are standard and can be derived from the definitions of `csch` and `sech` in terms of `sinh` and `cosh`.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21586": {
        "explanation": "This issue is well-specified with a clear demonstration of a bug in SymPy's `parse_expr` function. The user has provided:\n\n1. A complete code example showing the problem\n2. Two expressions that should behave identically but don't when using `local_dict`\n3. Clear expected vs. actual behavior\n4. Evidence of the issue through the `atoms(Symbol)` call showing duplicate symbols\n\nThe issue appears to be that when using `local_dict` with the expression that has implicit multiplication (`E**x(...)` without the `*`), the substitution doesn't work correctly. The substitution for `x=1` in `p2` doesn't fully evaluate, leaving `exp(x)` instead of `E` in the result.\n\nThe bug is reproducible with the provided code, and the expected behavior is clear: both expressions should evaluate to the same result regardless of whether `local_dict` is used or not. A developer would have enough information to investigate and fix this issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21596": {
        "explanation": "This issue is well-specified with clear examples of both the current incorrect behavior and the expected correct behavior. \n\nThe bug involves the `is_subset` method when used with `Reals` in the SymPy library. The issue shows that:\n\n1. There's a set `S1` defined as an imageset of complex numbers derived from integers\n2. The value `2` is not in `S1` (which is correct)\n3. But incorrectly, `2` is reported to be in `S1.intersect(Reals)` when it should be `False`\n4. The correct intersection of `S1` with `Reals` should be the set `{-1, 1}`\n\nThe issue provides complete code examples showing both the current incorrect behavior and the expected correct behavior. It's clear what needs to be fixed: the implementation of set intersection with `Reals` is not correctly identifying which elements should be in the intersection. \n\nA developer would have enough information to investigate why the intersection operation is incorrectly including `2` in the result when it shouldn't be there, and why it's missing the correct elements `-1` and `1`.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21612": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The exact problematic LaTeX expression is provided: `\"\\\\frac{\\\\frac{a^3+b}{c}}{\\\\frac{1}{c^2}}\"`\n2. The current incorrect output is shown: `((a**3 + b)/c)/1/(c**2)`\n3. The expected correct output is specified: `((a**3 + b)/c)/(1/(c**2))`\n4. The root cause is identified: missing brackets in the denominator\n5. Versions affected are listed: 1.8 and 1.6.2\n6. Clear reproduction steps are provided with the exact code to run\n\nThe issue is about the LaTeX parser in SymPy incorrectly handling nested fractions, specifically not properly grouping the denominator with brackets when it contains another fraction. This is a well-defined parsing bug with a clear expected behavior. A developer would have all the information needed to locate the issue in the LaTeX parsing code and implement a fix to ensure proper bracket placement for nested fractions.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21614": {
        "explanation": "The issue is well-specified and clear. It identifies a specific inconsistency in the SymPy library where the `kind` attribute behaves differently for `Integral` and `Derivative` operations on matrix symbols. \n\nThe issue provides:\n1. A clear example showing the expected behavior with `Integral` (returning `MatrixKind(NumberKind)`)\n2. A clear example showing the problematic behavior with `Derivative` (returning `UndefinedKind`)\n3. The implication is that the `Derivative` operation should maintain the same kind attribute pattern as `Integral` when operating on matrix symbols\n\nThe solution would involve fixing the implementation of the `kind` attribute for the `Derivative` class when it operates on matrix symbols, making it consistent with the `Integral` class behavior. This is a well-defined task with clear expected behavior.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21627": {
        "explanation": "This issue is well-specified with a clear bug description and a reproducible test case. The problem is clearly identified: when checking if a specific symbolic expression involving `cosh(acos(-i + acosh(-g + i)))` is zero using the `is_zero` property, a Python recursion error occurs. The issue provides the exact code snippet that triggers the error, which makes it straightforward to reproduce and debug. A developer would be able to trace through the code execution to find where the infinite recursion happens in the symbolic manipulation of this expression. The solution would involve fixing the recursion issue in the handling of nested hyperbolic and inverse trigonometric functions in the library's symbolic evaluation system.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21769": {
        "explanation": "This issue is well-specified with clear information about the problem and a proposed solution:\n\n1. The problem is clearly identified: When a Clebsch-Gordan coefficient (CG) is raised to a power (e.g., squared), the LaTeX representation doesn't render correctly in Jupyter notebooks.\n\n2. Evidence is provided:\n   - Screenshots showing the incorrect rendering\n   - Code snippets to reproduce the issue\n   - The version of SymPy being used (v1.7.1)\n\n3. The root cause is identified: The LaTeX representation needs braces around the CG coefficient to render properly when raised to a power.\n\n4. A specific solution is proposed: Modify the LaTeX representation in the CG class to wrap the output in braces.\n\n5. The exact line of code that needs to be changed is identified with a GitHub link.\n\n6. Evidence that the proposed solution works is provided with a screenshot.\n\n7. Related issues are referenced.\n\nWith this information, a developer could confidently implement the fix without needing additional clarification. The issue provides all necessary context, reproduction steps, and a clear path to resolution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21847": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue clearly identifies a bug in the `itermonomials` function when using the `min_degrees` parameter.\n2. It provides a complete, reproducible code example that demonstrates the issue.\n3. It clearly states the expected behavior (should return all monomials with total degree 3, including mixed terms like x1*x2**2) versus the actual behavior (only returns pure power terms like x1**3).\n4. It references the documentation to show why the current behavior is incorrect.\n5. It notes that the issue persists even when max_degrees is increased above min_degrees.\n\nThe issue provides all the necessary information to understand the problem, reproduce it, and determine what a correct solution would look like. A developer could immediately start working on fixing the function to properly generate all monomials with the specified total degree.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21849": {
        "explanation": "This issue is well-specified with a clear description of the problem. The user provides:\n\n1. A specific error scenario: When using LambertW function raised to a power in JupyterLab\n2. Two reproducible examples: `sp.LambertW(2) * sp.LambertW(2)` and `sp.Pow(sp.LambertW(2), 2)`\n3. A complete error traceback showing the exact error: `TypeError: _print_LambertW() got an unexpected keyword argument 'exp'`\n\nThe issue appears to be in the LaTeX printing functionality of SymPy when trying to display LambertW functions raised to powers in JupyterLab. The error occurs specifically in the `_print_LambertW()` method which doesn't accept the 'exp' keyword argument that's being passed to it.\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Locate the problem in the code (in the LaTeX printer's handling of LambertW functions)\n3. Implement a fix (likely by updating the `_print_LambertW()` method to handle the 'exp' parameter)\n\nThis is a clear bug report with all the necessary details to address the problem.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21930": {
        "explanation": "This issue is well-specified with a clear problem description and a minimal reproducible example. The issue identifies:\n\n1. The specific module affected: \"secondquant\" module in SymPy\n2. The exact problem: incorrect LaTeX rendering of double superscripts containing the \"dagger\" command in Jupyter Notebook\n3. A complete code example that reproduces the issue\n4. The current incorrect output: `b^\\dagger_{0}^{2}`\n5. The expected correct output format: `{b^\\dagger_{0}}^{2}`\n\nThe solution path is clear: modify the LaTeX printing functionality in the secondquant module to properly handle double superscripts by adding curly brackets around the first superscript expression. This would ensure proper rendering in Jupyter Notebook.\n\nA developer with access to the codebase would be able to locate the relevant LaTeX printing code for the secondquant module and implement the fix without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21932": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The user reports that the `imageset` function doesn't work when used with a `Range` object, specifically when trying to use `imageset(lambda x: x*2, Range(n))`. The error message is provided: `ValueError: invalid method for symbolic range`, and the issue points to a specific part of the code where the error occurs - when checking `self.size == 1` where `self` is the `Range` object.\n\nFrom this information, I can understand:\n1. The specific function that's failing (`imageset`)\n2. The specific input that causes the failure (`Range(n)`)\n3. The exact error message\n4. A hint about where in the code the problem occurs (during a size check)\n\nThis is enough information to locate the relevant code, understand the issue, and implement a fix. The developer would need to investigate why the `Range` object is incompatible with the `imageset` function, particularly focusing on how the size attribute is handled for symbolic ranges.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-21952": {
        "explanation": "This issue is well-specified with a clear problem statement and a concrete example. The issue describes that when a symbol `n` is defined as an integer and even, `(n**2/2).is_even` should return `True` but currently returns `None`. \n\nThe issue also includes:\n1. A reproducible code example showing the problem\n2. A proposed solution direction (enhancing symbolic integers to track more complex assumptions about divisibility)\n3. An update section describing a specific implementation approach (modifying `_eval_is_odd` to handle integer inputs with denominators)\n4. A reference to the issue number (#8648)\n\nThe issue provides enough context to understand the mathematical problem (if n is even, then n\u00b2/2 should be even), the current behavior, and a direction for the solution. The implementation details about modifying `_eval_is_odd` to handle decomposed integers with denominators gives a clear path forward.\n\nA developer could take this information and implement a solution without needing additional clarification. The mathematical reasoning is sound and the technical approach is specified.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22005": {
        "explanation": "The issue is well-specified with clear examples and a proposed solution. The problem involves the `solve_poly_system` function in SymPy that incorrectly handles certain polynomial systems with infinite solutions.\n\nSpecifically:\n1. When given a system with only one equation in two variables (like `y - 1 = 0`), the function should recognize this as an underdetermined system with infinite solutions and raise a `NotImplementedError`.\n2. However, as shown in the example, it incorrectly returns `[(1,)]` for `solve_poly_system((y - 1,), x, y)` while correctly raising an error for `solve_poly_system((x - 1,), x, y)`.\n\nThe proposed fix is clear - modifying the condition in `_solve_reduced_system` to check both that there is one univariate polynomial AND the number of generators (variables) is also one. This ensures the system is properly determined.\n\nThe issue includes a specific code diff showing exactly what change is needed, and also includes test cases to verify the fix. This makes it very clear what needs to be done to resolve the issue.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22080": {
        "explanation": "This issue is well-specified with a clear description of the bug. The problem occurs when using the `lambdify` function with `modules=[]` on expressions containing the `Mod` function multiplied by another expression. The issue description provides:\n\n1. A precise description of the bug: when lambdifying expressions with structure `expr * Mod(a, b)` with `modules=[]`, sympy incorrectly moves the multiplier into the first argument of Mod.\n\n2. A complete reproducible example showing both the correct behavior (when not using `modules=[]`) and the incorrect behavior (when using `modules=[]`).\n\n3. The expected vs. actual output is clearly demonstrated: `-3` (correct) vs `4` (incorrect).\n\n4. The source code of both generated functions is shown, making it clear how the implementation differs.\n\nThe issue is specific enough that a developer could identify the problem in the lambdify implementation and fix how it handles the Mod function when generating Python code with `modules=[]`. The root cause appears to be that `-Mod(x, y)` is being incorrectly translated to `-x % y` instead of `-(x % y)` when using Python's native modulo operator.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22098": {
        "explanation": "This issue is well-specified with clear examples demonstrating the problem. The user shows that `parse_expr` with `evaluate=False` is not behaving consistently when used with the `sqrt` function. While other functions like `sin` preserve the unevaluated expressions inside them, `sqrt` is partially evaluating expressions like `sqrt(2*2)` and `sqrt(2/2)` even when `evaluate=False` is specified.\n\nThe issue includes:\n1. Clear reproduction steps with specific code examples\n2. The actual output observed\n3. The expected output\n4. Verification that the underlying functions support `evaluate=False` correctly\n\nThe problem is clearly defined: the `evaluate=False` parameter is not being properly propagated to the `sqrt` function during parsing, causing unexpected evaluation of expressions inside the square root. This is inconsistent with how other functions like `sin` behave and with how `sqrt` behaves when called directly.\n\nA solution would need to ensure that `parse_expr` with `evaluate=False` consistently prevents evaluation of expressions inside all functions, including `sqrt`.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22236": {
        "explanation": "This issue is well-specified. It clearly identifies a problem with the printing of unevaluated multiplication expressions in SymPy. The issue shows that when a rational number (1/2) is multiplied by an expression (x + y) without evaluation, the display format is inconsistent depending on the order of arguments:\n\n1. When the rational number comes first, it's displayed as a fraction with the expression in the numerator\n2. When the rational number comes second, it's displayed without proper brackets around (x + y), making it ambiguous\n\nThe expected behavior is that the second case should use brackets around (x + y) to maintain clarity and avoid ambiguity in the mathematical expression. This is a specific formatting issue with a clear example and expected outcome.\n\nThe solution would involve modifying the string representation logic for unevaluated Mul objects to properly handle precedence and add brackets when necessary.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22383": {
        "explanation": "This issue is well-specified with a clear objective: to drop testing for Python 3.6 in the CI pipeline. The issue provides:\n\n1. A clear reference to another issue (#21884) that explains the rationale (adopting NEP 29, which would require dropping Python 3.6 support)\n2. Specific actions to be taken: drop testing in CI for both Python 3.6 and PyPy 3.6\n3. Context about this being part of a maintenance task related to Python version support\n\nThe task is straightforward and doesn't require additional clarification. A developer would need to modify the CI configuration to remove Python 3.6 testing, which is a common and well-understood task in software maintenance. The issue doesn't leave room for ambiguity about what needs to be done.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22456": {
        "explanation": "The issue is reasonably well-specified for someone familiar with the codebase. It clearly identifies:\n\n1. The problem: The `String` class in `codegen.ast` doesn't support argument invariance in the form `expr.func(*expr.args) == expr`\n2. The current behavior: It uses `expr.func(**expr.kwargs()) == expr` instead\n3. The expected behavior: The former invariance should hold for any `Basic` subclass, which `String` is\n\nFor an experienced developer with knowledge of the codebase, this provides enough information to:\n- Locate the `String` class in the `codegen.ast` module\n- Understand the concept of argument invariance in this context\n- Identify that `String` inherits from `Basic` and should follow the same invariance pattern\n- Modify the implementation to support the expected `expr.func(*expr.args) == expr` invariance\n\nThe only potential ambiguity is in the implementation details - how exactly to modify the class to support this invariance. However, with access to the codebase, a developer should be able to examine how other `Basic` subclasses implement this pattern and apply a similar approach to the `String` class.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-22706": {
        "explanation": "This issue is well-specified with a clear error case and reproduction steps. The description provides:\n\n1. A specific error: IndexError in StrPrinter for UnevaluatedMul\n2. An exact code snippet that reproduces the error: `print(Mul(Pow(x,-2, evaluate=False), Pow(3,-1,evaluate=False), evaluate=False))`\n3. The exact error message and traceback information: `if _coeff_isneg(n[0]): IndexError: list index out of range`\n\nFrom this information, a developer can:\n- Reproduce the issue by running the provided code\n- Identify that the problem occurs in the StrPrinter component when handling unevaluated multiplication\n- Locate the specific line of code causing the error (the one checking if _coeff_isneg(n[0]))\n- Understand that the issue involves handling negative powers in unevaluated multiplication expressions\n\nThe error is clearly related to accessing an empty or invalid list (n[0]) when trying to print an unevaluated multiplication with negative powers. This provides enough context to investigate and fix the issue without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22714": {
        "explanation": "This issue is well-specified with clear reproduction steps and error messages. The problem is that using `with sp.evaluate(False)` causes an error when trying to create a Point2D object through string parsing with `sp.S()`, while the same operation works fine without the context manager or with direct `evaluate=False` parameter. The error message \"Imaginary coordinates are not permitted\" is misleading since the coordinates are integers. The issue provides complete code to reproduce the problem, the exact error traceback, and alternative working approaches. A developer would have enough information to investigate why the context manager approach fails while the direct parameter approach works.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22740": {
        "explanation": "This issue is well-specified with a clear problem description and even a suggested solution path. The issue describes a specific bug where lambdifying `Heaviside(Min(x, y))` in SymPy with NumPy as the backend fails with a TypeError. The error occurs because the generated code creates nested `select` statements that don't work properly.\n\nThe issue provides:\n1. A complete, reproducible code example\n2. The exact error message\n3. Exploration of alternative approaches (using `.rewrite(Piecewise)`)\n4. A working solution (using `.rewrite(Piecewise).simplify()`)\n5. A clear suggestion for fixing the issue: detect ITE (If-Then-Else) expressions in Piecewise and rewrite them using And operations or apply simplify_logic\n\nThe author even mentions they figured out the solution while writing the issue, which further confirms the clarity of the problem and its solution. The issue is also linked to related issues (#22726 and #22486) for context.\n\nA developer could implement a solution based on this information without needing additional clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22773": {
        "explanation": "This issue is well-specified with a clear problem statement and even a proposed solution. The issue describes that the LaTeX representation of a determinant is incorrectly displayed as |(A)| instead of the desired |A|. The user has provided their own working solution by modifying the `_print_Determinant` method in the LatexPrinter class, replacing it with custom code instead of using `_print_Determinant = _print_Abs`. \n\nThe code snippet provided is clear and demonstrates exactly what needs to be changed. The solution involves setting matrix delimiters to empty strings temporarily, printing the matrix content, and wrapping it with left and right vertical bars to properly represent a determinant.\n\nWhile the user expresses uncertainty about whether their approach is optimal, they've provided enough information for someone to understand the issue and implement a solution. The context is sufficient to understand that this is related to the LaTeX printing functionality in what appears to be a mathematical library (likely SymPy based on the naming conventions).",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22914": {
        "explanation": "This issue is very well-specified with clear details:\n\n1. The problem is clearly stated: PythonCodePrinter doesn't support Min and Max functions from SymPy when generating Python code.\n2. A reproducible example is provided showing the current behavior.\n3. The expected solution is explicitly provided with the exact code implementation needed.\n4. The issue references a similar previous issue (#16669) for context.\n\nThe implementation is straightforward - adding two methods to the PythonCodePrinter class to handle Min and Max functions by converting them to Python's built-in min() and max() functions. The code snippets provided show exactly what needs to be added and how it should work. There's no ambiguity about what constitutes a successful solution.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-22969": {
        "explanation": "The issue is reasonably well-specified. The user has:\n\n1. Clearly identified the specific class and method with the potential problem (sympy.physics.optics.gaussopt.BeamParameter and its w_0 method)\n2. Provided the current implementation code\n3. Explained the physical problem: when refractive index changes, the Rayleigh length z_r changes, but the beam waist w_0 should remain constant (not change as the current implementation suggests)\n4. Identified that this is physically incorrect behavior\n\nWhat makes this not fully \"well-specified\" is:\n- The exact mathematical correction needed isn't explicitly provided\n- There's no reference to the correct formula that should be used instead\n- No specific test cases showing the incorrect vs. expected behavior\n- No mention of how the refractive index should be incorporated into the calculations\n\nHowever, a developer with knowledge of Gaussian optics would likely understand the issue and be able to implement a correction based on the physical principles mentioned. The user has also offered to help contribute to the solution, suggesting they have domain knowledge that could assist in the implementation.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-23021": {
        "explanation": "This issue is well-specified with clear information about the problem and expected solution:\n\n1. The problem is clearly defined: The `decompogen` function in SymPy doesn't work with `Min`/`Max` functions, resulting in a maximum recursion depth error.\n\n2. The issue includes a specific error trace showing exactly what happens when trying to use `decompogen(Min(5,x),x)`.\n\n3. The expected behavior is clearly stated: After the fix, `decompogen(Min(5, x), x)` should return `[Min, 5, x]`.\n\n4. The issue references related issues (#13205 and #13207) which could provide additional context.\n\n5. The PR description includes \"before\" and \"after\" examples that clearly demonstrate the current problem and the expected outcome after the fix.\n\nThe task is straightforward: implement support for `Min`/`Max` functions in the `decompogen` function. A developer with access to the codebase would be able to understand what needs to be fixed and how the solution should behave.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23262": {
        "explanation": "This issue is well-specified with a clear problem description and a minimal working example (MWE) that demonstrates the bug. The user has:\n\n1. Clearly identified the problem: In SymPy 1.10, the Python code printer doesn't properly generate code for single-element tuples (missing the trailing comma)\n2. Provided exact code to reproduce the issue\n3. Shown the expected output (from SymPy 1.9)\n4. Shown the incorrect output (from SymPy 1.10)\n5. Demonstrated that multi-element tuples work correctly\n6. Explained the impact (breaks their program which expects a tuple return type)\n\nThe issue is a regression in the Python code printer's handling of single-element tuples. The solution would need to ensure that when generating Python code for a single-element tuple, the trailing comma is included to maintain the tuple type. This is a well-defined problem with a clear success criterion: the code printer should generate `(1,)` instead of `(1)` for single-element tuples.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23296": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue describes a bug where dimensionless numbers in exponents are not being recognized correctly in the `_collect_factor_and_dimension` function of the SymPy physics.units module.\n\nThe description provides:\n1. A specific code example that reproduces the error\n2. The exact error message that occurs\n3. A contrasting example showing that `check_dimensions` works correctly for the same input\n4. The PR description even includes the fix that was implemented\n\nFrom this information, I can understand that:\n- The function `_collect_factor_and_dimension` is incorrectly handling expressions with exponentials where the exponent is dimensionless\n- When a quantity is divided by another quantity of the same dimension (u/v where both have energy dimension), the result should be dimensionless\n- The exponential of a dimensionless quantity should also be dimensionless\n- The function is incorrectly returning `exp(Dimension(1))` instead of `Dimension(1)`\n\nThis is sufficient information to locate the issue in the codebase and implement a fix that ensures dimensionless quantities in exponents are properly recognized.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23413": {
        "explanation": "This issue is well-specified with clear information about what the problem is:\n\n1. The user provides the exact code that is causing the issue: `np.flip(hermite_normal_form(Matrix(np.flip(np.array([[5, 8, 12], [0, 0, 1]]))).T).T))`\n2. They clearly state the expected output: `[[5, 8, 0], [0, 0, 1]]`\n3. They clearly state the actual output they're getting: `[[5, 8, 0]]`\n4. They provide a hypothesis about what's going wrong: \"It seems to be falsely identifying my matrix as rank-deficient and removing the row\"\n5. They explain their goal: trying to achieve a row-style Hermite Normal Form using flips and transposes\n\nThe issue involves the Hermite Normal Form (HNF) calculation incorrectly removing a row from the matrix. The problem is reproducible with the exact code provided, and the expected behavior is clearly stated. A developer would be able to investigate why the HNF function is removing the second row when it shouldn't be, and fix the issue in the implementation.\n\nThe only minor detail missing is which library/module the `hermite_normal_form` and `Matrix` functions come from (likely NumPy, SymPy, or a similar mathematical library), but this can be easily determined by examining the codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23534": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected vs. actual results. The problem is clearly defined: when using `symbols()` with an extra layer of parentheses and specifying `cls=smp.Function`, the function returns `Symbol` objects instead of `Function` objects. The issue includes:\n\n1. The exact SymPy version (1.10.1)\n2. A minimal code example that reproduces the issue\n3. The expected output type (`sympy.core.function.UndefinedFunction`)\n4. The actual output type (`sympy.core.symbol.Symbol`)\n5. A clear explanation of why the extra parentheses are being used (to deconstruct the output as separate tuples)\n\nA developer can understand the problem, reproduce it, and work on a fix without needing additional information. The issue is related to how the `symbols()` function handles nested tuples when the `cls` parameter is specified, which is a specific and identifiable bug.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23729": {
        "explanation": "This issue is well-specified with a clear problem description and solution path. The issue explains:\n\n1. The current behavior: `julia_code()` in SymPy generates Julia code with syntax like `2.*y` that produces errors in Julia due to ambiguity between floating point literals and broadcast operators.\n\n2. The exact error message: \"syntax: invalid syntax \"2.*\"; add space(s) to clarify\"\n\n3. The root cause: Julia can't distinguish between `2.` (floating-point number) and `.*` (broadcast multiplication) when they appear together.\n\n4. Proposed solutions:\n   - Either specifically handle the case of integers followed by broadcast operators\n   - Or more generally add spaces around binary operators by default: `(x .^ 2 .* y) * A ^ 3`\n\nThe issue provides concrete examples of both the problematic output and the desired output. It also explains the reasoning behind the proposed solution and mentions potential edge cases (like `2.^x`). A developer would have enough information to implement a fix by modifying the Julia code printer in SymPy to add appropriate spacing around operators.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23808": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The user is experiencing a RecursionError when using the `sum_of_squares` function from SymPy's diophantine module for relatively small inputs (n=1000000, k=2). The error occurs due to maximum recursion depth being exceeded in the recursive implementation of the function.\n\nThe issue includes:\n1. Exact function being used (`sympy.solvers.diophantine.diophantine.sum_of_squares`)\n2. Specific input parameters that cause the problem (n=1000000, k=2)\n3. Complete error traceback showing where the recursion fails\n4. Python and library version context (Python 3.9)\n5. A clear explanation of why they believe this is problematic (relatively small input causing recursion overflow)\n\nA developer would have enough information to:\n1. Reproduce the issue\n2. Identify the problematic recursive implementation in the code\n3. Develop a solution that likely involves rewriting the recursive algorithm to be more efficient or use an iterative approach instead\n\nThe issue is focused on a specific function with a specific failure mode that needs to be fixed.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23824": {
        "explanation": "This issue is very well-specified with clear details about the problem:\n\n1. The issue clearly identifies the specific function with the bug: `kahane_simplify()` in the physics.hep module\n2. It explains exactly what the bug is: leading uncontracted gamma matrices are being reversed in order\n3. It provides a complete test case that reproduces the issue\n4. It includes the expected vs actual output\n5. It even identifies the root cause: \"the leading matrices are removed at the beginning of the function and then inserted at the start of the product at the end of the function, and the insertion loop is just backward\"\n\nThe issue provides all the necessary information to understand the problem and implement a fix. The reporter even mentions they've found the source of the bug and will be submitting a PR. This is an exemplary bug report with all the details needed to solve the problem without requiring any additional information or clarification.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-23950": {
        "explanation": "This issue is well-specified with clear examples of the problem. The issue describes that `Contains.as_set()` incorrectly returns the `Contains` object itself instead of converting it to a proper set representation. This causes problems in other parts of the codebase that expect the result of `as_set()` to be an actual set object with methods like `as_relational()`.\n\nThe issue provides:\n1. A clear example showing the current incorrect behavior: `Contains(x, Reals).as_set()` returns `Contains(x, Reals)` instead of a proper set\n2. An explanation of why this is problematic: Contains is a boolean expression, not a set\n3. A concrete example of where this causes failures in the codebase (in Piecewise)\n4. A traceback showing the exact error that occurs\n\nA developer would need to modify the `as_set()` method of the `Contains` class to return a proper set representation instead of returning self. The solution path is clear and the expected behavior can be inferred from the context and how other parts of the code expect `as_set()` to behave.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24066": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue shows that the `SI._collect_factor_and_dimension()` function in SymPy's physics units module fails to properly detect that an exponent of a dimensionless quantity should itself be dimensionless.\n\nThe example demonstrates that:\n1. `expr = units.second / (units.ohm * units.farad)` is correctly identified as dimensionless (as verified by the assert statement)\n2. When this dimensionless quantity is used as an argument to the exponential function (`exp(expr)`), the system incorrectly reports its dimension as `Dimension(time/(capacitance*impedance))` instead of `Dimension(1)` (dimensionless)\n\nThe error message is clear, and the expected behavior is explicitly stated. The issue is focused on a specific function and provides all the necessary context to understand and address the problem. A developer would have enough information to investigate why the dimensionality check fails for expressions involving transcendental functions like `exp()` when the argument is dimensionless.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24102": {
        "explanation": "This issue is well-specified with clear information about the problem:\n\n1. The issue clearly identifies a specific bug: the new `parse_mathematica` function cannot parse Greek characters (like \u03bb) while the deprecated `mathematica` function could.\n\n2. The description provides complete reproduction steps with code examples for both the working case (using the old function) and the failing case (using the new function).\n\n3. The error message is included, showing exactly what happens when the code fails.\n\n4. The issue is isolated to a specific component (the `parse_mathematica` function in the `sympy.parsing.mathematica` module).\n\n5. The expected behavior is clear: the new function should be able to parse Greek characters just like the old function did.\n\nA developer would have all the necessary information to investigate and fix this issue without needing additional clarification. The problem is specific, reproducible, and the expected outcome is evident.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24213": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible code example. The issue is that the `_collect_factor_and_dimension` method in SymPy's physics units module fails to recognize that velocity (v1) and acceleration*time (a1*t1) are dimensionally equivalent. \n\nThe error occurs because the method is checking for exact dimension matches rather than recognizing that velocity and acceleration*time are physically equivalent dimensions (both are length/time). The traceback clearly shows the error message and the specific function that needs to be fixed.\n\nThe solution would need to modify the `_collect_factor_and_dimension` method to properly handle cases where dimensions are mathematically equivalent through dimensional analysis, even if they're represented differently in the system. This is a common requirement in physics calculations.\n\nThe issue provides all necessary information to understand and address the problem:\n1. Clear reproduction steps with complete code\n2. The exact error message\n3. The specific function that needs to be fixed\n4. The nature of the problem (dimension equivalence detection)",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24325": {
        "explanation": "This issue is well-specified with clear information about what needs to be fixed. The problem is identified as a numerical error in the conversion between coulomb and statcoulomb units in the SymPy physics units module.\n\nThe issue provides:\n1. A reproducible example showing the current output (29979245.8*statcoulomb)\n2. The expected output (2997924580*statcoulomb, approximately 3.00\u00d710^9 statC)\n3. The current implementation of the test function\n4. The proposed corrected implementation\n\nThe fix appears to involve updating the conversion factor between coulomb and statcoulomb in the unit system. The current implementation uses the fraction 149896229/5, while the correct implementation should use 2997924580 (which is approximately 10 times larger).\n\nThe issue provides all the necessary information to understand the problem and implement a solution. A developer would need to locate the relevant code in the SymPy physics units module and update the conversion factor between these units.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24353": {
        "explanation": "This issue is well-specified with clear information about what's broken. The issue describes a specific error when running the `py.bench` command, showing the exact traceback that occurs. The error is an `ImportError: No module named __.test.item` when trying to import from `py.__.test.item`. \n\nThe issue also provides the version of the `py` module being used (1.1.1) and references an original issue (#1741) and its author. This gives context about the history of the problem.\n\nThe problem is clearly a compatibility issue with the current version of the `py` module, as the import statement in `sympy/utilities/benchmarking.py` is trying to use a path that no longer exists in the current version of the module.\n\nA solution would involve updating the import statement in the benchmarking utility to be compatible with the current version of the `py` module, or finding an alternative approach if that module structure has changed significantly. This is a straightforward task with a clear definition of success: making `bin/py.bench` run without the import error.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24370": {
        "explanation": "This issue is well-specified with a clear problem description and reproducible example. The issue describes a bug in SymPy where floor division (`//`) between a `sympy.Integer` and a `sympy.Symbol` fails with a specific error message, while the same operation works when using a Python `int` instead of `sympy.Integer`. \n\nThe error message is clearly provided, along with the full traceback showing where in the code the error occurs. The issue includes a complete code snippet to reproduce the problem, and even notes the inconsistency in behavior between `sympy.Integer` and Python's native `int` type.\n\nFrom this description, I can understand:\n1. The exact operation that's failing (`sympy.Integer(1024)//s0`)\n2. The error message and traceback\n3. The expected behavior (it should work like `1024//s0` does)\n4. The specific inconsistency to fix\n\nThis provides all the necessary information to investigate and fix the issue in the SymPy codebase.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24443": {
        "explanation": "This issue is well-specified with clear information about what's broken and why:\n\n1. The problem is clearly identified: `_check_homomorphism` is broken specifically for PermutationGroups.\n2. A reproducible example is provided showing the exact error.\n3. The location of the bug is pinpointed to the `_image()` function with a specific GitHub link to the problematic code.\n4. The root cause is explained: when `r[i]` is an inverted generator, the `in gens` test fails.\n\nThe issue provides enough context for a developer to understand the problem and start working on a solution. The reporter even suggests that the implementation could be simplified, giving a direction for the fix. A developer with knowledge of the codebase would be able to understand what needs to be fixed and how to approach it.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24455": {
        "explanation": "This issue is well-specified with clear details about the problem:\n\n1. The issue identifies a specific bug in the `is_cyclic` method of the `PermutationGroup` class in SymPy's combinatorics module.\n2. It provides a complete reproducible example showing that when S3 (symmetric group on 3 elements) is constructed using generators rather than the `SymmetricGroup` function, `is_cyclic` incorrectly returns `True` when it should be `False`.\n3. The issue includes a link to the specific code that's causing the problem.\n4. The submitter provides a detailed mathematical analysis of why the current implementation is incorrect, explaining the flaw in the algorithm's logic.\n5. The issue suggests a potential brute-force solution as a fallback.\n6. It also proposes a potential mathematical fix by distinguishing between the subgroup H_p (generated by p-th powers of generators) and G^p (subgroup of all p-th powers).\n\nThe issue contains all the necessary information to understand the problem, reproduce it, and implement a solution. The submitter has done thorough analysis and even suggested potential fixes. A developer with knowledge of group theory would have clear direction on how to approach fixing this bug.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24539": {
        "explanation": "This issue is well-specified with a clear problem description and a reproducible example. The issue states that the `PolyElement.as_expr()` method is supposed to allow users to specify custom symbols when converting a polynomial to an expression, but it's not working correctly. \n\nThe issue provides:\n1. A specific method location in the codebase (with a link)\n2. A clear description of the expected behavior (allowing custom symbols)\n3. A clear description of the actual behavior (either error or ignoring provided symbols)\n4. A reproducible code example showing the problem\n\nFrom the example, we can see that when `f.as_expr(U, V, W)` is called, it should replace the ring symbols `x,y,z` with the provided symbols `U,V,W`, but instead it's still using the original symbols.\n\nThe solution would require modifying the `as_expr()` method to properly use the provided symbols instead of defaulting to `self.ring.symbols`. This is a straightforward bug fix with clear acceptance criteria - the example provided should work as expected after the fix.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24562": {
        "explanation": "The issue is reasonably well-specified. It describes a specific problem with the `Rational` function in sympy 1.11.1 when using string inputs versus float inputs:\n\n1. When using `Rational('0.5', '100')` (string inputs), the result is `1/100100`\n2. When using `Rational(0.5, 100)` (numeric inputs), the result is `1/200`\n\nThe user indicates that the second result (`1/200`) is the correct one, and mentions that in sympy 1.8, both expressions produced the correct result.\n\nThe issue is clear enough to understand what's happening: there appears to be a regression in how sympy's Rational constructor handles string inputs between versions 1.8 and 1.11.1. The expected behavior is that `Rational('0.5', '100')` should produce the same result as `Rational(0.5, 100)`, which is `1/200`.\n\nWhile the issue description is brief, it provides enough information to reproduce the problem and understand what needs to be fixed. A developer could investigate the changes between sympy 1.8 and 1.11.1 that affect the Rational constructor's handling of string inputs.",
        "label": "REASONABLY_SPECIFIED"
    },
    "sympy__sympy-24661": {
        "explanation": "This issue is well-specified with clear information about what's wrong and what the expected behavior should be:\n\n1. The problem is clearly identified: The `evaluate=False` parameter in `parse_expr` is being ignored specifically for relational expressions (like inequalities).\n\n2. A specific example is provided showing the current incorrect behavior:\n   - `parse_expr('1 < 2', evaluate=False)` returns `True` (evaluated)\n   \n3. The expected behavior is explicitly shown:\n   - It should return the unevaluated expression `1 < 2` (as shown with the `Lt(1, 2, evaluate=False)` example)\n\n4. Related issues are referenced (#22305 and #22098) which can provide additional context.\n\nThe issue is focused on a specific function (`parse_expr`), a specific parameter (`evaluate=False`), and a specific type of expression (relationals/inequalities). The expected behavior is clearly defined, making it straightforward to understand what a successful fix would entail.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24723": {
        "explanation": "This issue is well-specified with clear information about what's wrong and how it should be fixed. The problem is precisely identified: the probability density function (PDF) of the Matrix Normal Distribution in SymPy returns an incorrect normalization constant. \n\nThe issue provides:\n1. A clear explanation of the problem with the PDF calculation\n2. The exact mathematical formula that should be implemented (with a Wikipedia reference)\n3. A concrete code example showing the current incorrect output\n4. The expected correct output\n5. The specific difference identified (the constant in the denominator)\n\nThe solution would involve fixing the normalization constant in the Matrix Normal Distribution implementation, changing it from 2\u212f/\u03c0 to \u212f/2\u03c0. This is a well-defined mathematical correction with a clear reference to verify against.\n\nA developer can easily reproduce the issue using the provided code example and verify the fix by comparing the output with the expected result. The scope is narrow and focused on a specific mathematical error.",
        "label": "WELL_SPECIFIED"
    },
    "sympy__sympy-24909": {
        "explanation": "This issue is well-specified with clear reproduction steps and expected behavior. The user has:\n\n1. Provided a clear code example showing the bug\n2. Specified the exact versions (SymPy 1.11.1-1, Python 3.10.9, Arch Linux)\n3. Clearly stated the expected behavior: \"milli*W should evaluate to milli watts / mW\"\n4. Demonstrated the inconsistency: `milli*W == 1` returns True, but `W*milli` returns a different result\n5. Mentioned they've tested with multiple units (Watts and Volts)\n\nThe issue is about the incorrect behavior of the `milli` prefix when multiplied with units in SymPy's physics.units module. The bug is that `milli*W` evaluates to 1 instead of creating a milliwatt unit. This is a clear bug with reproducible steps that can be investigated and fixed without needing additional information.",
        "label": "WELL_SPECIFIED"
    }
}